\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[french]{babel}
\usepackage{csquotes}
\begin{document}

\section*{Résumé de l'article \textquote{An Organizationally-Oriented Approach to Enhancing Explainability and Control in Multi-Agent Reinforcement Learning}}

Cet article s'inscrit dans le domaine de l'intelligence artificielle distribuée et du \textit{Multi-Agent Reinforcement Learning} (MARL). Il aborde une problématique centrale du MARL contemporain : la difficulté à expliquer, analyser et contrôler les comportements collectifs émergents appris par des agents, en particulier dans des environnements coopératifs complexes.

L'article part du constat que, lors de l'apprentissage, les agents développent fréquemment des comportements spécialisés et récurrents pouvant être interprétés a posteriori comme des \textit{rôles implicites} poursuivant des \textit{objectifs intermédiaires}. Toutefois, ces structures organisationnelles émergentes restent généralement non formalisées, difficiles à mesurer et impossibles à contraindre explicitement dans un cadre MARL classique.

Pour répondre à cette limite, l'article propose le cadre \textbf{MOISE+MARL}, qui intègre explicitement un modèle organisationnel issu des systèmes multi-agents (MOISE+) au processus d'apprentissage par renforcement multi-agent, formalisé comme un \textit{Decentralized Partially Observable Markov Decision Process} (Dec-POMDP). Les rôles, missions et objectifs organisationnels sont traduits en contraintes externes au processus d'apprentissage, agissant à la fois sur l'espace d'actions des agents (actions autorisées ou interdites selon le rôle) et sur la fonction de récompense (bonus et pénalités liés aux rôles et aux objectifs).

Cette approche permet d'orienter l'apprentissage vers des comportements conformes à une organisation structurelle et fonctionnelle donnée, sans recourir à l'apprentissage hiérarchique, tout en conservant la compatibilité avec des algorithmes MARL standards.

En complément, l'article introduit une méthode originale d'analyse a posteriori appelée \textbf{TEMM} (\textit{Trajectory-based Evaluation in MOISE+MARL}). TEMM exploite des techniques de clustering non supervisé sur les trajectoires d'agents afin d'inférer automatiquement des rôles implicites et leurs relations d'héritage, ainsi que des objectifs, missions et plans implicites. Cette méthode permet de définir un indicateur quantitatif d'\textit{adéquation organisationnelle} (\textit{organizational fit}), mesurant la cohérence entre les comportements appris et une organisation structurelle et fonctionnelle.

Le cadre est évalué expérimentalement sur cinq environnements MARL (Predator-Prey, Overcooked-AI, gestion d'entrepôt, cyber-défense) et avec différents algorithmes (MADDPG, MAPPO, Q-Mix, COMA). Les résultats montrent que l'introduction de contraintes organisationnelles améliore significativement la stabilité et la robustesse des politiques apprises, accélère la convergence et renforce la conformité des comportements aux spécifications attendues.

Cet article contribue ainsi à rapprocher l'apprentissage par renforcement multi-agent et les modèles organisationnels des systèmes multi-agents, en proposant un cadre unifié pour le contrôle, l'explicabilité et l'évaluation des comportements collectifs appris.

\end{document}
