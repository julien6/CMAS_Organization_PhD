\clearpage
\thispagestyle{empty}
\null
\newpage

\cleardoublepage
\phantomsection
% \pdfbookmark[1]{La méthode MAMAD}{La méthode MAMAD}
\markboth{\spacedlowsmallcaps{La méthode MAMAD}}{\spacedlowsmallcaps{La méthode MAMAD}}
\part{La méthode MAMAD}
\label{part:methode}

\clearpage
\thispagestyle{empty}
\null
\newpage

\chapter*{Introduction}
\addcontentsline{toc}{chapter}{\textbf{Introduction}}

\noindent

% todo : Initialement, dans la partie I, notre problème était un problème de conception logiciel d'un SMA de Cyberdéfense capable d'assurer la Cyberdéfense de façon optimale compte tenu des contraintes dynamique de l'environnement et des concepteur. Nous avons formalisé le fait que pour répondre à cette question de recherche globale, il faut répondre simultanément à 6 critères (Autonomie - C1, Performance - C2, Adaptation - C3, Contrôle - C4, Explicabilité - C5, Robustesse - C6). Nous avons donc poursuivi une première revue de littérature qui nous a permis de comprendre qu'assez peu de travaux s’intéressent à une approche multi-agent pour la Cyberdéfense et que les travaux qui couvrent le plus de critères sont à chercher du côté de l'approche connexioniste qui favorise la performance et l'adaptation là où l'approche purement symbolique favorise l'explicabilité et le controle. Fort de ce constat, nous avons proposé de voir la question de recherche globale au travers le prisme d'un problème d'optimisation sous contraintes où la politique conjointe est à optimiser pour maximiser une récompense qui encode le succès dans l'atteinte d'un objectif de Cyberdéfense (ou un conglomérat d'objectifs) et où les contraintes sont formalisées comme des spécifications organisationnelles. A partir de cette formalisation, la complexité de la question de recherche globale apparait abordable au travers de 4 activités : Modélisation, Entrainement, Analyse et Transfert. On sait pour chaque activité les données en entrée, les données en sortie attendues et donc aussi les objectifs de chaque activité.
% A partir de cette vision, la partie II présente les travaux qui couvrent les objectifs pour chaque activité, permettant ainsi de savoir quels sont les domaines/sous-domaines ou travaux qui semble le plus adaptés (ou qui nécéssitent le moins de contributions supplémentaires) pour atteindre les objectifs de chaque activités. Compte tenu des travaux identifiés comme les plus prometteurs, on en déduit donc les verrous qui restent encore à combler par de nouvelles contributions pour atteindre les objectifs de chaque activités.
% La partie III, présente la méthode qui orchestre les 4 activités et explique dans chacune des activités comment nous avons combler les verrous par de nouvelles contributions. Dans cette partie, une présentation générale de la méthode qui orchestre les 4 activités est d'abord donnée. Ensuite, nous détaillons chaque activité en rappellant rapidement les objectifs et chacun des verrous associés et pour chacun nous détaillons notre contribution en les justifiant. Une fois ceci fait, nous donnons une représentation alogrithmique de l'activité qui sert de support pour détailler de façon formelle l'activité en explicitant chacun des éléments formels (ensemble, élément d'un ensemble, relation...). A la fin de l'activité, on fait une sorte de bilan en expliquant ce qu'on pense qui sera couvert en termes d'objectifs attendus, ce qui l'est moyennement et pourquoi et ce qui n'est pas du tout couvert.
% La partie IV concerne la validation expérimentale de la méthode par la mise en application de cette méthode au travers de trois cas d'études non-orientés Cyberdéfense et de trois cas d'études orientés Cyberdéfense. L'objectif est de vérifier que les 6 critères (Autonomie - C1, Performance - C2, Adaptation - C3, Contrôle - C4, Explicabilité - C5) sont bien couverts. Pour cela, on applique une grille d'évaluation qui associe chaque critères à un ensemble de métriques mesurables et qui peuvent être analysé pour voir si le critère est couvert ou pas. Comme la grille de lecture est commune aux 6 environnements, on peut donc vérifier de manière plus consistante et générique si la méthode permet bien de couvrir les 6 critères. La partie finit donc avec une analyse globale de si la méthode couvre bien les 6 critères posés initialement.

La partie précédente a mis en lumière les lacunes actuelles dans l'intégration des modèles organisationnels au sein des approches d'apprentissage multi-agent, tant du point de vue du contrôle, de l'explicabilité que de l'automatisation de la conception. Elle a également mis en lumière les lacunes dans la modélisation de l'environnement et son intégration dans le processus d’entraînement notamment sur le manque de cadre permettant d'assurer la cohérence entre l'environnement simulé et réel.

\medskip

\noindent
Cette troisième partie présente notre proposition pour répondre à ces lacunes~: la méthode \acn{MAMAD}. Cette méthode repose sur la prémisse que la conception d'un \acn{SMA} peut être abordée par le prisme d'un problème d'optimisation sous contraintes. La méthode est construite autour de cette vision et s'organise donc autour de quatre activités~:

\begin{enumerate}
  \item \textbf{Modélisation}~: modéliser l'environnement réel en un environnement simulé ainsi que les contraintes de conceptions en spécifications organisationnelles~;
  \item \textbf{Apprentissage}~: entraîner les agents dans cet environnement simulé en tenant compte de spécifications organisationnelles comme des rôles durant l'apprentissage~;
  \item \textbf{Analyse}~: extraire des spécifications structurelles et fonctionnelles émergentes à partir des trajectoires des agents entrainés~;
  \item \textbf{Transfert}~: mettre à jour régulièrement les politiques des agents déployés dans l'environnement réel à partir des politiques des agents entrainés en simulation et éventuellement mettre à jour ou améliorer l'environnement simulé.
\end{enumerate}

\noindent
Ces quatres activités peuvent être vues comme exécutées de façon itérative pour produire des \acplu{SMA} adaptés à leur environnement, alignés sur des contraintes organisationnelles, explicables et robustes.

\medskip

\noindent
Comme illustré en \autoref{fig:organisation_manuscrit_partie_3}, \autoref{chap:mamad_global} donne une description globale de la méthode concernant les processus proposés. Les quatre chapitres restants détaillent chacune des étapes de cette méthode~:
Le \autoref{chap:modelling} présente l'activité de modélisation.
Le \autoref{chap:training} présente l'activité d'apprentissage contraint par des spécifications organisationnelles.
Le \autoref{chap:analyzing} présente une méthode permettant d'analyser des trajectoires pour inférer des structures organisationnelles émergentes.
Le \autoref{chap:transferring} décrit l'activité de transfert.

La méthode \acn{MAMAD} ambitionne de réunir les forces des approches symboliques et apprenantes pour une conception de \acn{SMA} à la fois structurée, autonome et explicable.

\begin{figure}[h!]
  \centering
  \resizebox{0.8\linewidth}{!}{%
    \input{figures/organisation_manuscrit_partie_3.tex}
  }
  \caption{Structure de la Partie III~: La méthode MAMAD}
  \label{fig:organisation_manuscrit_partie_3}
\end{figure}



\clearpage
\thispagestyle{empty}
\null
\newpage

\chapter{Présentation globale de la méthode}
\label{chap:mamad_global}

La méthode \acn{MAMAD}~\footnotemark[2] repose sur quatre grandes activités~: (1) la modélisation de l'environnement, de l'objectif global et des contraintes organisationnelles, (2) l'apprentissage des politiques à l'aide de divers algorithmes \acn{MARL}, (3) l'analyse des comportements et l'inférence des spécifications organisationnelles à l'aide d'une méthode proposée, et (4) le maintien de la cohérence entre l'environnement simulé et l'environnement réel en déployant les politiques entraînées et en mettant à jour la simulation. Cette approche guide le processus d'apprentissage des agents tout en imposant des contraintes organisationnelles strictes, garantissant ainsi l'efficacité des politiques apprises.

Le cycle de vie d'un \acn{SMA} conçu avec \acn{MAMAD} est illustré en \autoref{fig:cycle}. Il commence par la modélisation de l'environnement, réalisée à partir d'un ensemble suffisant de trajectoires réelles (issues d'agents déjà transférés ou de toute autre source disponible), ainsi que la définition de l'objectif global et des contraintes de conception sous forme de rôles et d'objectifs. Ensuite, les agents sont entraînés dans cet environnement simulé à l'aide de techniques d'apprentissage par renforcement multi-agent (MARL). Une fois l'entraînement terminé, une analyse post-entraînement permet d'extraire les rôles et objectifs émergents des agents, ce qui conduit à l'amélioration des spécifications organisationnelles appliquées. Enfin, après validation, les politiques apprises sont déployées pour contrôler les actionneurs de l'environnement, générant ainsi de nouvelles traces qui serviront à affiner la modélisation lors des itérations suivantes.

\begin{figure}[h!]
  \centering
  \input{figures/cycle.tex}
  \caption{Cycle de vie d'un SMA conçu avec MAMAD}
  \label{fig:cycle}
\end{figure}

Le coeur de la méthode \acn{MAMAD} est d'envisager la conception d'un \acn{SMA} comme un processus itératif d'optimisation sous contraintes. Nous proposons une description formalisée de la méthode \acn{MAMAD} dans l'\autoref{alg:mamad} qui met en perspective les activités évoquées précédemment. Les données en entrée sont~:
\begin{itemize}
  \item $\mathcal{E}_0$~: l'environnement initial dans lequel les agents peuvent agir~;
  \item $\mathcal{G}_{\text{inf}}$~: une description informelle de l'objectif global recherché~;
  \item $\mathcal{C}_{\text{inf}}$~: une spécification informelle des contraintes de conception~;
  \item $\gamma \in [0,1]$~: le facteur d'actualisation définissant une solution à court ou long terme, généralement fixé empiriquement (par défaut à 1)~;
  \item $A, \Omega$~: respectivement les espaces d'actions et d'observations~;
  \item $\texttt{org\_fit}_{min}$, $\overline{r}_{min}$, $\sigma_{max}$~: les valeurs minimales exigées pour le score d'adéquation organisationnelle, la récompense moyenne, et la valeur maximale pour l'écart-type, servant à valider une politique conjointe. Ces seuils sont en général fixés empiriquement.
\end{itemize}

L'adéquation organisationnelle est un indicateur quantitatif de la conformité d'une politique conjointe aux spécifications organisationnelles, calculé à l'aide de la méthode \acn{TEMM} présentée en \autoref{chap:analyzing}. La récompense moyenne $\overline{r}$ et l'écart-type $\sigma$ sont des métriques classiques en apprentissage par renforcement, reflétant respectivement la performance globale et la stabilité d'une politique.


La méthode \acn{MAMAD} est le cadre méthodologique permet une conception continue du \acn{SMA} via la coordination itérative et asynchrone de deux processus distincts~: le \textit{processus de Transfert}, qui est connecté à l'environnement réel et gère l'exécution en temps réel et la collecte d'historiques conjoints~; et le \textit{processus \acn{MTA}}, qui consomme les données stockées pour améliorer itérativement le modèle simulé, la politique conjointe et les spécifications organisationnelles du \acn{SMA}.

\paragraph{Processus de Transfert~: déploiement des politiques et collecte de données}

Ce processus qui consiste à maintenir la cohérence entre l'environnement simulé et l'environnement est actif en continu tant que le \acn{SMA} est en fonctionnement dans l'environnement réel. Il a deux rôles~: déployer la politique conjointe la plus récente $\pi^j_{\text{latest}}$ auprès des agents déployés dans l'environnement cible, garantissant un comportement à jour sans interruption~; et collecter en continu les trajectoires des agents sous forme d'historiques conjoints $H^j$, stockés par lots. Une fois qu'un nombre suffisant de trajectoires est collecté, le lot est ajouté au dépôt global $\mathcal{D}_{H^j}$. Si le processus de mise à jour n'est pas en cours, il déclenche le lancement du processus \textit{\acn{MTA}}.

% Rajouter des commentaires sur chaque activité

\begin{algorithm}[H]
  \caption{Conception de SMA assistée par MOISE+MARL (MAMAD)}
  \label{alg:mamad}
  \DontPrintSemicolon

  \KwIn{Environnement initial $\mathcal{E}$, objectif $\mathcal{G}_{\text{inf}}$, contraintes de conception $\mathcal{C}_{\text{inf}}$, espace d'observation $\Omega$, espace d'action $A$, nombre maximal de cycles de raffinement $n_{refine}$}
  \KwOut{Un SMA déployé satisfaisant aux exigences de conception, de performance et d'explicabilité~; ainsi que ses spécifications organisationnelles associées}

  Initialiser~: $\mathcal{D}_{H^j} \gets \emptyset$, $\pi^j_{\text{latest}} \gets \pi^j_{\text{init}}$, $running\_MTA \gets False, \mathcal{MM} \gets \emptyset$ \;

  \vspace{0.3em}

  \While{SMA en cours de conception}{
    \tcp*[l]{Transfert~: récupération des trajectoires \& déploiement de la politique}
    $\mathcal{D}_{H^j}, \texttt{need\_update} \gets \text{transfer}(\pi^i_{\text{latest}}, \mathcal{D}_{H^j})$ \tcp*[r]{appel asynchrone}
    \If{\texttt{need\_update} et non \texttt{running\_MTA}}{
      \texttt{launch\_MTA()} \tcp*[r]{appel asynchrone}
    }
  }

  \vspace{1em}
  \SetKwProg{MTA}{Processus \normalfont(MTA)}{}{}
  \MTA{}{}{

  $\texttt{running\_MTA} \gets True$ \tcp*[l]{Variable globale}
  % ttt
  \tcp*[l]{Modélisation~: modéliser l'environnement réel}
  $(\mathcal{T}^j, \Omega^{\mathcal{T}^j}_0, R^j_H, S^j_H, \text{Render}^j_H) \gets \texttt{model}(\mathcal{E}, \mathcal{D}_{H^j}, \mathcal{G}_{\text{inf}}, \gamma, \Omega, A)$ \;

  \tcp*[l]{Politique insatisfaisante ou nombre max. de raffinement non atteint}

  \While{$i < n_{refine} \ \text{ou} \ (\texttt{org\_fit} > \texttt{org\_fit}_{min} \ or \ \overline{r} > \overline{r}_{min} \ or \  \sigma < \sigma_{max})$}{

  \vspace{0.5em}
  \tcp*[l]{Entraînement~: politique sous spec. org.}
  $(\pi^j, \overline{r}, \sigma) \gets \texttt{train}(\mathcal{T}^j, \Omega^{\mathcal{T}^j}_0, R^j_H, S^j_H, \text{Render}^j_H, \mathcal{MM}, \mathcal{C}_{\text{inf}}, \gamma, \Omega, A)$ \;

  \vspace{0.5em}
  \tcp*[l]{Analyse~: inférer les nouvelles spec. org.}
  $(\mathcal{MM}_{\text{imp}}, \texttt{org\_fit}) \gets \texttt{analyze}(\mathcal{T}^j, \mathcal{MM}, \pi^j)$ \;

  $\pi^j_{\text{latest}} \gets \pi^j$ \tcp*[r]{Mise à jour de la politique la plus récente}

  $\mathcal{MM} \gets \mathcal{MM}_{\text{imp}}$ \;

  $i \gets i + 1$

  }

  $\texttt{running\_MTA} \gets False$ \tcp*[l]{Variable globale}

  }
\end{algorithm}



\paragraph{Processus MTA~: optimisation des politiques et raffinement organisationnel}

Ce processus modélise le problème de conception actuel et améliore la politique conjointe des agents ainsi que ses spécifications organisationnelles. Il commence par construire un modèle de prédiction d'observations conjointes (\acparen{JOPM}) $T^j$ à l'aide de \textit{World Models} étendus, à partir des trajectoires collectées. Les exigences de conception sont formalisées sous forme de spécifications organisationnelles MOISE+MARL $\mathcal{MM}$ et l'objectif est formalisé par une fonction de récompense basée sur l'historique $R^j_H$.

Un modèle markovien est ensuite construit à partir des éléments modélisés afin d'entraîner les agents en tenant compte des spécifications organisationnelles, via le framework MOISE+MARL. Une fois l'entraînement terminé, la politique conjointe $\pi^j$ est analysée à l'aide de \acn{TEMM} afin d'inférer les spécifications organisationnelles implicites $\mathcal{MM}_{\text{imp}}$ et de calculer un score d'adéquation organisationnelle.

\paragraph{Boucle de raffinement via les spécifications organisationnelles}

Si la politique apprise montre des performances insuffisantes ou une grande variabilité (par rapport aux seuils définis), les spécifications organisationnelles implicites inférées sont utilisées pour raffiner les spécifications initiales. Ce processus de raffinement peut impliquer une inspection manuelle des structures inférées pour identifier les facteurs clés de succès des comportements émergents. Guidé par ces observations, le concepteur peut réviser les spécifications afin d'orienter les prochaines itérations d'apprentissage.

Cette boucle est répétée jusqu'à un maximum de $n_{refine}$ fois, orientant progressivement l'espace des politiques vers des comportements plus structurés et plus performants. La dernière politique validée est alors enregistrée comme $\pi^j_{\text{latest}}$, prête à être déployée dans l'environnement réel.

La boucle de raffinement est particulièrement utile dans les environnements complexes où la connaissance préalable est limitée ou où la conception manuelle serait trop coûteuse. À chaque itération, elle permet de restreindre l'espace de recherche des politiques en le concentrant sur les régions associées à des régularités organisationnelles émergentes.

A noter que ce processus peut commencer sans aucune spécification organisationnelle initiale, et produire par raffinement successif des contraintes organisationnelles pertinentes, objectives, et indépendantes de toute expertise humaine ou connaissance préalable de l'environnement.

\noindent L'interaction entre ces deux processus asynchrones constitue un cycle de conception de \acn{SMA} complet et fermé. Le système apprend continuellement à partir de l'exécution réelle, met à jour son modèle simulé, réentraîne sous des spécifications évolutives, et déploie des politiques améliorées sans nécessiter d'intervention constante du concepteur. Cette architecture établit un pont entre les principes symboliques de l'ingénierie orientée agents et l'automatisation par apprentissage, assurant conformité, adaptabilité et explicabilité au niveau organisationnel.

\noindent On peut noter que nous proposons d'exploiter un environnement simulé modélisé comme un jumeau numérique (\textit{Digital Twin}) pour l'entraînement ultérieur, tandis que les approches \acn{MBRL} combinent simultanément modélisation et apprentissage. En effet, nous privilégions une séparation entre modélisation et apprentissage pour les raisons suivantes~: i) la réutilisabilité du modèle d'environnement pour d'autres entraînements d'agents, avec des ajustements éventuels~; \quad ii) le besoin d'agents simples n'embarquant pas de modèles coûteux pour planifier~; \quad iii) le besoin d'un environnement simulé de haute fidélité commun à tous les agents.

\section{Application flexible de la méthode MAMAD}
\label{subsec:mamad_flexible}

\input{tables/mamad_taxonomy.tex}

La méthode \acn{MAMAD} a été conçue pour être modulable et adaptable selon les besoins de chaque cas d'application.
En pratique, toutes les activités décrites dans la taxonomie (\autoref{tab:mamad_taxonomy}) ne sont pas nécessairement appliquées dans leur intégralité.
Chaque activité peut être utilisée de façon indépendante ou combinée avec d'autres, et chacune dispose de plusieurs sous-activités offrant différents niveaux d'automatisation et de contraintes.

Cette flexibilité permet :
\begin{itemize}
  \item \textbf{Une application partielle} : un cas d'application peut exploiter uniquement certaines activités (par exemple, \acn{MOD} et \acn{TRN} uniquement) tout en omettant l'analyse et le transfert si ces étapes ne sont pas nécessaires comme dans le cas où l'environnement n'est pas dynamique.
  \item \textbf{Un choix ciblé de sous-activités} : pour chaque activité retenue, une sous-activité peut être choisie en fonction des objectifs, des ressources disponibles et du degré d'automatisation souhaité (par exemple, \acn{MOD-AUT} pour la modélisation automatisée, \acn{TRN-CON} pour l'entraînement avec des contraintes). Cela permet de gérer le coût de conception en fonction du niveau de complexité de l'environnement et du resources (financières, temps, experience) disponibles.
  \item \textbf{Une combinaison adaptative} : certaines activités peuvent être réalisées de manière automatisée tandis que d'autres restent manuelles ou semi-manuelles, afin d'équilibrer précision, contrôle et rapidité.
\end{itemize}

\paragraph{Exemple abstrait}
Considérons un scénario où l'on souhaite concevoir rapidement un \acn{SMA} pour un environnement complexe, avec un budget limité en ressources humaines mais un accès étendu à des données d'exécution.
Dans ce cas, on pourrait adopter la configuration suivante :
\begin{itemize}
  \item \acn{MOD-AUT} : utilisation d'un modèle automatisé basé sur des traces collectées (\textit{World Models}) afin de gagner du temps dans la construction du jumeau numérique.
  \item \acn{TRN-CON} : entraînement multi-agent guidé par des spécifications organisationnelles MOISE+MARL pour garantir la conformité des comportements aux rôles et objectifs définis.
  \item \acn{ANL-AUT} : analyse entièrement automatisée via Auto-TEMM pour extraire rôles et objectifs implicites, et évaluer l'adéquation organisationnelle.
  \item Pas d'activité \acn{TRF} : les politiques apprises sont utilisées uniquement dans l'environnement simulé pour des études exploratoires, sans déploiement réel.
\end{itemize}

Cette configuration peut être notée succinctement grâce aux acronymes de la taxonomie :
\[
  \text{Configuration} = \{\acn{MOD-AUT},\ \acn{TRN-CON},\ \acn{ANL-AUT}\}
\]
Ce formalisme facilite la documentation des choix méthodologiques pour chaque expérimentation et permet de comparer rapidement différents cas d'application.
%
Ainsi, la taxonomie proposée constitue un outil de référence pour spécifier précisément le \textit{chemin méthodologique} suivi dans une étude, tout en mettant en évidence les choix d'automatisation et de guidage organisationnel effectués.
Les chapitres suivantes détaillent chaque activité du cadre \acn{MAMAD}, identifient les défis spécifiques rencontrés, et décrivent les contributions proposées pour y répondre.


\clearpage
\thispagestyle{empty}
\null
\newpage

% TODO : faire un index avec les mots clés (Auto-encodeur, World Models, SMA, LSTM, RNN, MLP, adéquation organisationnelle...)

\chapter{Modéliser l'environnement en simulation}
\label{chap:modelling}

\noindent L'\textit{activité de modélisation} a pour objectif de représenter le problème de conception comme un problème d'optimisation sous contraintes. L'\textbf{activité de modélisation} vise donc à produire les composants formels suivants~:
\begin{displaymath}
  \texttt{model}(\mathcal{D}_{H^j}, \mathcal{G}_{\text{inf}}, \mathcal{S}_{\text{inf}}, \mathcal{E}, \gamma, \Omega, A) \rightarrow \mathcal{T}^j, R^j_H, S^j_H, \text{Render}^j_H
\end{displaymath}

\noindent Pour cela, la tâche la plus difficile est de modéliser l'environnement comme un modèle assimilable à une simulation avec un niveau de fidélité connu. Deux approches sont envisageables pour faciliter cette tâche~:
\begin{itemize}
  \item En raison de l'inexistence de modèle générique servant de chassis pour la modélisation d'un tel environnement, ce travail peut être fastidieux et conduire à différents modèles peu homogènes. C'est pourquoi nous proposons un modèle formel générique pouvant servir de base à la modélisation d'un environnement.
  \item La génération automatisée d'un environnement. Cependant, en raison de l'impossibilité d'accéder directement à l'état réel de l'environnement, nous nous appuyons sur les historiques conjoints stockés. C'est pourquoi nous utilisons les \textit{World Models}, capables de généraliser à partir d'un grand volume d'historiques pour estimer les transitions d'états cachés et d'observations. Toutefois, un obstacle majeur est l'absence de \textit{World Models} explicitement conçus pour les environnements multi-agents. Nous proposons ci-dessous une extension du cadre \textit{World Models} aux contextes multi-agents.
\end{itemize}

En complément de cet environnement simulé, le problème d'optimisation formalisé comprend également la transformation de la description informelle de l'objectif global $\mathcal{G}_{\text{inf}}$ en une fonction de récompense basée sur l'historique $R^j_H: H^j \rightarrow \mathbb{R}$ ainsi qu'une fonction d'arrêt basée sur l'historique $S^j_H: H^j \rightarrow \{0,1\}$ (qui correspond le plus souvent à un arrêt conditioné par un nombre maximal d'étapes). Enfin, de façon optionnelle, il est également possible de définir une fonction de rendu basée sur l'historique $Render^j_H: H^j \rightarrow \text{Render}$ pour visualiser les trajectoires des agents dans l'environnement simulé.

Nous considérons ici que la formalisation et la mise en œuvre de l'objectif $R^j_H$, de la fonction d'arrêt $S^j_H$ et, le cas échéant, de la fonction de rendu $Render^j_H$, à partir de leur description informelle, relèvent d'un processus manuel classique réalisé par les concepteurs.

\begin{algorithm}[H]
  \caption{Algorithme de l'activité de modélisation}
  \label{alg:modeling}
  \DontPrintSemicolon

  \KwIn{Historiques conjoints $\mathcal{D}_{H^j}$, objectif informel $\mathcal{G}_{\text{inf}}$, contraintes informelles $\mathcal{C}_{\text{inf}}$, facteur d'actualisation $\gamma$, espace des actions $A$, espace des observations $\Omega$}
  \KwOut{JOPM $\mathcal{T}^j$, fonction de récompense $R^j_H$, spécifications MOISE+MARL $\mathcal{MM}$}

  \vspace{0.5em}
  \tcp{1. Formalisation manuelle des fonctions composantes}
  $(R^j_H, S^j_H, \text{Render}^j_H) \gets \texttt{manual\_formalize}(\mathcal{S}_{\text{inf}}, \mathcal{G}_{\text{inf}}, \mathcal{E}, A, \Omega)$ \;

  \vspace{0.5em}
  \tcp{2. Entraîner les auto-encodeurs pour les observations}
  Extraire les observations $\Omega^j = \{\omega^j_t\}$ à partir des historiques $\mathcal{D}_{H^j}$ \;
  Entraîner un auto-encodeur $(Enc_{\omega^j}, Dec_{\omega^j})$ sur $\Omega^j$ en minimisant l'erreur de reconstruction \;

  \vspace{0.5em}
  \tcp{3. Encoder les observations dans les historiques}
  Pour chaque historique $h^j = (\omega_t^j, a_t^j) \in \mathcal{D}_{H^j}$, encoder chaque observation conjointe ${z}_t = Enc_{\omega^j}(\omega^j_t)$ pour constituer l'ensemble d'entraînement $\mathcal{B} = \{ \{(z_t, a^j_t, z_{t+1})\} = h_z^j, h_z^j \in \mathcal{D}_{H^j}\}$

  \vspace{0.5em}
  \tcp{4. Entraîner le RLDM}
  Initialiser le RLDM (fonctions $f$ et $g$) $\mathcal{T}^z = f(g)$

  \For{$h_z^j \in \mathcal{B}$}{
    \For{$(z_t, a^j_t, z_{t+1}) \in h^j$}{
      Entraîner le RLDM $\mathcal{T}^{z}$ en minimisant l'erreur quadratique moyenne (MSE) entre la prédiction $\hat{z}_{t+1}$ et la valeur réelle $z_{t+1}$.
    }
  }

  \vspace{0.5em}
  \tcp{5. Sauvegarder les observations initiales et former le JOPM}

  $\Omega^{\mathcal{T}^j}_0 \gets \{\omega^j_0\}$ extraites des historiques $\mathcal{D}_{H^j}$

  $\mathcal{T}^j(h_{t-1}, \omega_t, a_t) = \langle f(h_{t-1}, Enc(\omega^j_t), a^j_t), Dec(\mathcal{T}^{z}(h_{t-1}, Enc(\omega^j_t), a^j_t)) \rangle$ \;

  \vspace{0.5em}
  \tcp{6. Retourner les éléments modélisés}
  \Return{$\mathcal{T}^j, \Omega^{\mathcal{T}^j}_0, R^j_H, S^j_H, \text{Render}^j_H$}
\end{algorithm}


\section{Les \textit{World Models} Multi-Agents pour la génération automatique du modèle simulé}

Dans cette approche automatisée, on commence par générer un environnement simulé de haute fidélité en construisant un \acn{JOPM} $\mathcal{T}^j~: H^j \times \Omega^j \times A^j \rightarrow \mathcal{H} \times \hat{\Omega}^j$ à partir des traces d'interactions réelles $\mathcal{D}_{H^j}$. À un pas de temps $t \in \mathbb{N}$, pour tout état caché récurrent $\tilde{h}_{t-1} \in \mathcal{H}$ représentant l'historique conjoint jusqu'à $t-1$, l'observation conjointe reçue $\omega_t^j \in H^j$ et l'action conjointe $a_t^j \in A^j$, le modèle $\mathcal{T}^j$ renvoie le nouvel état caché $\tilde{h}_t \in \mathcal{H}$ ainsi que la prédiction de la prochaine observation conjointe $\hat{\omega}^j \in \hat{\Omega}^j$. Ce mécanisme permet à \acn{MAMAD} de construire l'environnement de simulation depuis zéro.

Dans les environnements multi-agents, les observations conjointes deviennent rapidement de grande dimension à mesure que le nombre d'agents augmente. Pour pallier cela, des fonctions d'encodage conjoint sont introduites pour les observations et les actions.

\begin{figure}[h]
  \centering
  \resizebox{\textwidth}{!}{%
    \input{figures/jopm_synthesis.tex}
  }
  \caption{Schéma de l'architecture d'un JOPM incluant le RDLM et l'Auto-encodeur}
  \label{fig:jopm_architecture}
\end{figure}


Plus précisément, les observations conjointes $\omega_t^{j} = (\omega_t^1, \dots, \omega_t^n) \in \Omega^{j}$ sont transformées en représentations latentes compactes à l'aide d'un encodeur d'observations conjointes $Enc_{\omega^j}~: \Omega^j \rightarrow z$, produisant $z_t = Enc_{\omega^j}(\omega_t^j)$. Un décodeur $Dec_z~: z \rightarrow \hat{\Omega}^j$ permet la reconstruction des observations conjointes.

On utilise généralement des \acn{MLP}s ou des architectures à base d'attention pour ces encodeurs, afin d'agréger les informations multi-agents en vecteurs de caractéristiques de taille fixe, tout en capturant les dépendances critiques entre agents.

Une fois l'encodage effectué, le \textit{World Model} multi-agent fonctionne comme en contexte mono-agent, en utilisant les observations encodées $z_t$ dans les historiques transmis au \acn{RLDM} $\mathcal{T}^{z}$. Dans le cadre de \acn{MAMAD}, ces \textit{World Models} constituent le cœur de la simulation mise en œuvre par l'\hyperref[sec:modelling]{activité de modélisation}, agissant comme des jumeaux numériques de haute fidélité de l'environnement cible.

\section{Un modèle de simulation pour la génération manuelle du modèle simulé}

Dans cette approche manuelle, on part d'une description informelle de l'environnement réel, de l'objectif global et des contraintes de conception. On utilise un modèle formel générique pour représenter l'environnement, les agents et les interactions entre eux. Ce modèle est conçu pour être extensible et adaptable à différents contextes.

\subsection{Modélisation générale Dec-POMDP de l'environnement et des agents}

% \rem{Manque d'organisation~: modélisation de l'environnement, des agents attaquants, des agents défenseurs, des stratégies de défense/d'attaque, ... On s'attend à une suite, quitte à redécouper...}

Afin de définir une modélisation, nous avons pris un cas d'utilisation tiré de l'\acn{AICA}~\cite{theron_autonomous_2021}. Nous nous intéressons à la modélisation d'un environnement réseau composé de \textit{nœuds} sur lesquels des cyberattaquants et des cyberdéfenseurs \textit{agents} peuvent être déployés pour observer et agir. Ces nœuds peuvent être décrits par un ensemble de \textit{propriétés} liées aux processus, aux systèmes de fichiers, aux systèmes d'exploitation, à l'architecture matérielle, etc.
Les \textit{observations} et les \textit{actions} des agents sont conditionnées par leurs propres propriétés (y compris celles qu'ils connaissent) et par des incertitudes. Par exemple, la lecture d'un fichier donné ou le remappage de ports peut nécessiter un niveau de privilège élevé~; ou la réception de données provenant d'un capteur physique n'est pas garantie à tout moment.
Chaque agent qui applique des actions modifie les propriétés d'un ou plusieurs nœuds. Cela modifie l'état de l'environnement, rapprochant ou éloignant les agents de leur objectif.
Les principales caractéristiques de cette description englobent des notions telles que l'incertitude dans les observations, les conditions dans les actions pour la transition d'état et les métriques que nous considérons comme exprimées dans une modélisation Dec-POMDP.
%\after{Ceci vise à répondre au besoin d'un modèle formel d'agents attaquants et défenseurs englobant divers contextes d'application.}

\

\begin{figure*}[]
  \centering
  \includegraphics[trim=0.7cm 0.6cm 0.7cm 1cm, clip,width=1\textwidth]{figures/model_example_illustration.pdf}
  \caption{Vue illustrative du modèle de simulation}
  \label{fig:model_example_illustration}
\end{figure*}

\noindent
D'un point de vue global, le modèle Dec-POMDP proposé exprime l'état d'un environnement comme l'ensemble des propriétés des nœuds, y compris les propriétés observables des agents. Nous définissons une propriété comme un couple composé d'un identifiant et d'une valeur. L'état de l'environnement est modifié lorsqu'une action est appliquée par un agent. Une action ne peut être appliquée que si la condition préalable booléenne basée sur les propriétés est satisfaite dans l'état actuel de l'environnement. L'état résultant est ensuite modifié en fonction de la post-condition, ce qui conduit finalement à l'ajout de certaines nouvelles propriétés et à la suppression d'autres propriétés. Une fois qu'une action a été appliquée avec succès par un agent, les propriétés observables du même agent lui sont renvoyées sous forme d'observations issues de ce nouvel état. Une récompense est également calculée en fonction de cet état actuel et renvoyée à l'agent. Un agent est choisi pour être modélisé comme une fonction de comportement qui doit sélectionner la prochaine action à effectuer en fonction des observations et des récompenses reçues.

Il existe différentes façons d'exécuter plusieurs agents dans un même environnement~\cite{terry2020pettingzoo} (en particulier \acn{AEC} et \textit{parallel}) en ajustant le nombre d'agents à exécuter dans un intervalle de temps et le nombre d'actions à jouer par un agent dans cet intervalle de temps. Compte tenu de la simplicité du mode \acn{AEC}, nous avons choisi ce mode comme première approximation en faisant jouer plusieurs agents une action à chaque tour de manière cyclique et séquentielle. Le cycle d'itération est présenté à travers une vue informelle illustrative du modèle de simulation dans la figure~\ref{fig:model_example_illustration}. Elle montre $i$ nœuds avec leurs propriétés, y compris celles observées par les $m$ agents, et comment chacune des $j$ actions disponibles associe un ensemble de propriétés préalables à un sous-ensemble de nouvelles propriétés à ajouter à l'environnement, en supprimant éventuellement les propriétés obsolètes ayant les mêmes identifiants que les nouvelles propriétés~: \begin{enumerate*}[label=\arabic*),itemjoin={;\quad}]
  \item Un agent choisit une action parmi ses observations précédentes et les récompenses en fonction d'une fonction de comportement. Dans la figure~\ref{fig:model_example_illustration}, lorsque $Agent_1$ commence son premier tour, il ne reçoit que les observations initiales ($p_{1}$) et zéro récompense, et choisit $Action_1$



  \item L'environnement est mis à jour par une fonction de transition qui dépend de l'état actuel et de l'action entreprise par l'agent (changement des propriétés une fois que la condition préalable est remplie). Une action est utilisée pour modifier les propriétés de l'environnement en mettant à jour la relation entre les identifiants de propriétés et les valeurs de propriétés.
  Par exemple, dans la figure~\ref{fig:model_example_illustration}, dans l'état actuel, les propriétés de $Node_1$ sont $p_1,p_3,p_4,p_2,p_9$. Lorsque l'action $Action_1$ est appliquée, la relation associe les sous-ensembles $\{p_3\}$ ou $\{p_4, \allowbreak p_{10}\}$ à $\{p_{15}, \allowbreak p_{14}, \allowbreak p_{11}\}$. La condition préalable basée sur les propriétés peut être comprise comme $p_3 \lor (p_4 \land p_{10})$. Comme $p_{15}$ et $p_{14}$ sont identifiés par $ID_4$ et $ID_2$ qui définissent déjà respectivement $p_{2}$ et $p_{4}$, $p_{4}$ et $p_{2}$ sont supprimés et $p_{11}$ et $p_{14}$ sont ajoutés ($p_{15}$ n'est pas ajouté car $id_2$ n'est associé à aucune valeur)



  \item Les propriétés observées sont renvoyées à l'agent exécutant actuel pour son prochain tour. Dans la figure~\ref{fig:model_example_illustration}, $p_{11}$ et $p_1$ sont renvoyés après l'application de $Action_1$.

\end{enumerate*}

\noindent
Les agents sont sélectionnés selon un ordre séquentiel. Chacun reçoit les dernières observations et récompenses de son dernier tour (ou simplement l'observation initiale et zéro récompense s'il s'agit de son premier tour)~; il choisit ensuite l'action suivante à jouer lors de son tour actuel. Une fois que le dernier agent a terminé de jouer (par exemple $Agent_m$), les récompenses sont calculées et envoyées aux cyberattaquants et aux cyberdéfenseurs en fonction de l'évaluation des métriques collectées lors du dernier état. Ensuite, les agents jouent à nouveau en suivant le même ordre séquentiel pour une autre itération.


\subsection{Modélisation formelle Dec-POMDP}

Nous définissons les éléments liés aux propriétés des nœuds, des agents et des actions de l'environnement suivant~:

\begin{itemize}

  \item $Ag = \{ag_1,..,ag_{|Ag|}\}$~: ensemble des agents (cyberattaquants et cyberdéfenseurs).
        % \begin{itemize}
        %     \item Avec $Attackers \subseteq Ag$~: l'ensemble des agents attaquants
        %     \item Avec $Defenders \subseteq Ag$~: l'ensemble des agents défenseurs
        % \end{itemize}

  \item Nous appelons le couple $p = (id_{j}, v_{j})$ avec $id_j \in {ID}$ et $v_j \in V$, une propriété.
        \begin{itemize}
          \item $ID$~: l'ensemble des identifiants de propriétés indiquant éventuellement comment les propriétés sont organisées dans une structure de données non plate (telle que $PC1.proc\allowbreak-esses.agents.agent1$). Ces identifiants de propriétés peu-vent être utilisés pour un chemin d'accès à un fichier, le type de système d'exploitation utilisé dans un nœud, une ligne de commande utilisée par un agent\dots
          \item $V$~: Ensemble des valeurs de propriétés. Celles-ci peuvent inclure le contenu d'un fichier, une description complète du système d'exploitation, le résultat d'une ligne de commande\dots
                % \item $Valeurs~: ID \rightarrow \mathcal{P}(V) = \{(id_{j}, V_{j}) \: | \: id_j \in {ID},$ $V_j \in \mathcal{P}(V)\}$~: une bijection associant un identifiant de propriété à l'ensemble des différentes valeurs auxquelles il peut être associé. Par exemple, l'identifiant $ls\_command\_output$ peut être associé aux valeurs suivantes $\{file.txt,\{file.txt,passwd.txt\}\}$
        \end{itemize}

  \item $P_{j} = \{ p_1, .., p_{|P_{j}|} \}$~: l'ensemble des propriétés $p_{l}$ (avec $l \in \{1,..,|P_{j}|\}$) du nœud $j$ ($j \in \mathbb{N} $). Par exemple, ces propriétés peuvent inclure certains identifiants de processus en cours d'exécution, la liste des fichiers d'un dossier, le type de système d'exploitation avec une description, des connaissances spécifiques d'un agent, etc.
        \begin{itemize}
          \item $P = P_1 \cup P_2 .. \cup P_{|P|} $~: Ensemble de toutes les propriétés des nœuds.
        \end{itemize}

  \item $Obs~: \mathcal{P}(P) \times Ag \rightarrow \mathcal{P}(P_{Ag}), P_{Ag} \subset P$~: Relation qui associe les propriétés des nœuds et un agent au sous-ensemble de propriétés observées par l'agent.



  \item $Action~: P_{pre} \rightarrow P_{post}$~: Relation qui associe un sous-ensemble de propriétés implicite dans une pré-condition booléenne conjonctive équivalente ($P_{pre} \subset \mathcal{P}(P)$) à un sous-ensemble de toutes les propriétés de la post-condition ($P_{post} \in \mathcal{P}(P)$). Par exemple, les propriétés $p_1 = (agent\_X \allowbreak \_privilege\_level, \allowbreak root)$, $p_2 = (agent\_X \allowbreak \_accessed\_text\_editor, \allowbreak Vim)$ et $p_3 = (agent\_X\_bashrc\_known \allowbreak \_filepath, \allowbreak /home/user/.bashrc)$ peuvent former une pré-condition ($p_1 \land p_2 \land p_3$) pour associer un nouvel ensemble de propriétés contenant $p4 = (bashrc\_file\_modified\_by\_X\_agent, \top)$. Deux sous-ensembles de pré-conditions peuvent être associés au même sous-ensemble de post-conditions pour modéliser une disjonction booléenne.

  \item $Metrics: \mathcal{P}(P) \times A \rightarrow \mathbb{R}^{n}$~: donne des métriques associées à un ensemble de propriétés et à une action conjointe. Par exemple, le nombre de nœuds encore actifs, les mouvements latéraux, etc.

\end{itemize}


En utilisant la description formelle d'un Dec-POMDP~\cite{Oliehoek2016}, nous proposons le modèle suivant~:

\begin{itemize}
  \item $S = \{s_1, ..s_{|S|}\}, s_{i} \subseteq P \: et \: 1 \le i \le |S|$~: L'espace des états en tant qu'ensembles de propriétés possibles.

  \item $A_{i} = \{a_{i}^{1},..,a_{i}^{|A_{i}|}\}, a_{i}^j \in Action \: et \: 1 \le j \le |A_i|$~: l'ensemble des actions possibles pour l'agent $i$.

  \item $T$~: Ensemble des probabilités de transition conditionnelles entre les états
        \begin{itemize}
          \item Avec $T(s,a,s') = \probP(s'|s,a)$, la relation qui associe la probabilité d'aller de l'état $s \in S$ à l'état $s' \in S$ sachant que nous avons joué $a = (P^a_{pre} \times P^a_{post}) \in A$ avec $P^a_{pre} \subset \mathcal{P}(P)$ et $P^a_{post} \in \mathcal{P}(P)$
          \item Avec $\probP(s'|s,a) = 0$ si $s$ ne satisfait pas la condition préalable de $a$ (c'est-à-dire $\exists \: P_{pre_s}^{a} \in P_{pre}^{a} \: | \: P_{pre_s}^{a} \not\in \mathcal{P}(s)$).
          \item Avec $s' = (s - \{p_l=(id_l, v_l) \: | \: p_l \in s \: et$ $id_l \in \{id_k \: | \: (id_k, v_k) \in P^a_{post} \: et \: v_k \neq \varnothing\}\}) \cup P^a_{post}$
        \end{itemize}



  \item $R~: S \times A \rightarrow \mathbb{R}^2 = Eval \circ Metrics$~: La fonction de récompense qui prend un état et une action et associe un indicateur de performance (à l'aide des métriques de l'état) pour les attaquants et les défenseurs.
        \begin{itemize}
          \item Avec $Eval~: \mathbb{R}^{n} \rightarrow \mathbb{R}^2$, associe un vecteur métrique à une récompense pour les cyberattaquants et les cyberdéfenseurs.
        \end{itemize}



  \item $\Omega_{i} \subset Range(Obs \: | \: \{ (s, ag_i) | s \in S \: et \: ag_i \in Ag \}) \subset P$~: ensemble des propriétés observables pour l'agent $ag_i$. Par exemple, le contenu d'un fichier, la sortie du journal d'une commande, le résultat d'un scan de port, etc.
        \begin{itemize}
          \item $\Omega = \Omega_1 \cup \Omega_2 .. \cup \Omega_{|Ag|} = Range(Obs)$~: Ensemble de toutes les propriétés observables pour tous les agents.
        \end{itemize}

  \item $O$~: Ensemble des probabilités d'observation conditionnelles.
        \begin{itemize}
          \item Avec $O(s',a,o) = \probP(o|s',a)$, la relation qui associe la probabilité d'observer une observation $o \subset \Omega$ à partir de l'état $s' \in S$ induit par $a \in A$
          \item Avec $\probP(o|s',a) = 0$ si l'état $s' \in S$ ne contient pas les propriétés de $o \subset \Omega$ (c'est-à-dire $o \not\in \mathcal{P}(s')$). Par exemple, un agent joue l'action $x\_reads\_a\_log\_file$, ce qui donne lieu à un nouvel état dont une propriété appartenant à la connaissance de l'agent x est $(log\_file\_content\_known\_by\_x, \allowbreak abc)$. Cette propriété sera donc incluse dans les observations renvoyées à l'agent x.
        \end{itemize}

\end{itemize}


\subsection{Intégration des scénarios d'attaque/défense\label{sec:ad_integration}}

\noindent
D'un point de vue brut, la modélisation formelle Dec-POMDP proposée s'appuie sur des actions pour simuler la manière dont un système en réseau réel réagirait, y compris les vulnérabilités et les contre-mesures appliquées par les agents cyberattaquants et cyberdéfenseurs.

Un premier défi consiste à construire un scénario d'attaque/défense représentatif d'un système en réseau comportant des vulnérabilités afin de permettre de rendre une attaque en reliant les seules informations disponibles (telles que les tactiques, techniques et procédures connues de MITRE ATT\&CK) et en choisissant des contre-mesures de défense pertinentes (issues des mesures d'atténuation de MITRE ATT\&CK) et un environnement de déploiement. Un deuxième défi consiste à établir les actions correspondant au scénario d'attaque/défense. Comme les actions modifient les propriétés de l'environnement, elles ont également un impact sur l'espace des états possibles et les transitions entre ceux-ci.
De plus, lorsque l'on considère un faible niveau d'abstraction, de nombreuses actions simples peuvent permettre de décrire avec précision les changements opérés dans le réseau. Cependant, cela augmente le nombre d'actions, et encore plus le nombre d'états, car ceux-ci sont des combinaisons des effets des actions.

Ces défis sont directement liés aux questions étudiées concernant la génération automatisée de graphiques d'attaque à l'aide de bases de données disponibles intégrant éventuellement des techniques d'\acn{IA}, comme dans ~\cite{GFalco2018}. Nous n'avons pas l'intention de nous attarder davantage sur ces questions, car elles dépassent le cadre de ce travail.

\
% TODO : motiver
\noindent
\textbf{Approche d'intégration MITRE ATT\&CK}~: Nous suggérons une approche manuelle de haut niveau que nous avons utilisée pour intégrer les informations MITRE ATT\&CK sous forme d'arborescence AD, car elle formalise les actions à jouer dans un scénario et leurs interactions avec l'environnement. Elle vise à aider à établir les actions d'attaque/défense qui seront finalement intégrées dans le simulateur~:
\begin{enumerate*}[label=\arabic*),itemjoin={;\quad}]

  \item Pour une menace persistante avancée (APT) donnée, nous avons identifié les tactiques, techniques et procédures pertinentes de MITRE ATT\&CK qui semblaient pertinentes pour un système en réseau



  \item Nous avons produit une description reliant les tactiques identifiées entre elles et les techniques, sous-techniques et procédures associées afin de créer un scénario décrivant comment le groupe APT pourrait attaquer le système en réseau. Cette étape définit la topologie du réseau avec ses principales propriétés
  %(telles qu'un réseau d'entreprise composé de plusieurs serveurs de bases de données dédiés communiquant via FTP et HTTP, etc.)

  \item Nous avons créé une arborescence AD comme proposé dans ~\cite{BKordy2010} avec les tactiques comme objectifs d'action principaux et les techniques, sous-techniques et procédures dans la partie inférieure de l'arborescence. Nous avons veillé à disposer de plusieurs chemins pour atteindre un même objectif d'action principal. Nous avons pris soin de définir chaque action d'attaque avec des conditions préalables et des conditions postérieures basées sur les propriétés de l'environnement

  \item Nous avons extrait les techniques/sous-techniques MITRE ATT\&CK liées à la détection et aux mesures d'atténuation que nous avons ajoutées dans l'arbre AD afin d'enrichir les nœuds d'attaque. Nous avons veillé à définir chaque action de défense avec des conditions préalables et des conditions postérieures basées sur des propriétés dans l'environnement.



  % \item Nous avons également répertorié et défini les principales actions environnementales spécifiques au déploiement à partir de la description précédente de l'environnement de déploiement ou des actions d'attaque/défense étendues qui sont communes aux cyberdéfenseurs et aux cyberattaquants. Cette étape permet d'obtenir un environnement plus réaliste, fournissant un nombre représentatif d'actions plausibles qu'un agent peut choisir dans de nombreux systèmes.
  % Ces actions communes pourraient inclure au moins~:
  % \begin{itemize}
  %     \item Lecture et écriture de fichiers
  %     \item Créer, supprimer, copier, déplacer, renommer, modifier les propriétés des fichiers/dossiers.
  %     \item Accéder à un dossier, accéder au dossier parent
  %     \item Sélectionner un fichier/dossier pour y appliquer des actions ultérieures
  %     \item Exécuter un fichier binaire
  %     \item Utilisation d'un protocole réseau (tel que HTTP, FTP, SSH, etc.).
  %     \item Autres interactions avec les lignes de commande de base concernant la surveillance ou le contrôle du système.
  % \end{itemize}
  % Ensuite, les propriétés environnementales associées doivent décrire un système de fichiers, une interface de terminal, un port avec des règles, les propriétés des paramètres du système d'exploitation, etc.

\end{enumerate*}


\clearpage
\thispagestyle{empty}
\null
\newpage

\chapter{Entraînement des politiques sous contraintes}
\label{chap:training}

% TODO: référence l'algo

L'\textit{activité d'entraînement} vise à résoudre le problème de conception modélisé comme un problème d'optimisation sous contraintes en s'appuyant sur le cadre MOISE+MARL. L'\textbf{activité d'entraînement} traite donc les éléments formels suivants~:
%
\begin{displaymath}
  \pi^j_i \gets \texttt{train}(\mathcal{T}^j, \Omega^{\mathcal{T}^j}_0, R^j_H, S^j_H, \text{Render}^j_H, \mathcal{MM}, \mathcal{C}_{\text{inf}}, \gamma, \Omega, A)
\end{displaymath}

La formalisation des contraintes issues des exigences de conception $\mathcal{C}_{\text{inf}}$ sous la forme de spécifications organisationnelles MOISE+MARL $\mathcal{MM}$ contraintes de conception en une fonction  et en spécifications $\mathcal{MM}$ est prise en charge.

\noindent Une limite importante réside dans le fait que MOISE+MARL repose sur le formalisme \acn{Dec-POMDP}, qui suppose un accès complet à l'état réel de l'environnement. En pratique, notre approche se fonde possiblement uniquement sur les données observables (à savoir les historiques conjoints des agents) sans accès aux états internes. Pour combler cet écart, un nouveau formalisme markovien est introduit, opéré sur des séquences observables via le \acn{JOPM}, tout en restant compatible avec les cadres algorithmiques \acn{MARL} existants.
Dans ce chapitre, nous commençons par présenter le framework MOISE+MARL avant d'explorer son extension aux \textit{World Models} Multi-Agents.

\section{MOISE+MARL pour lier $\mathcal{M}OISE^+$ avec le MARL}

\begin{figure}[h!]
  \centering
  \input{figures/mm_synthesis_single_column.tex}
  \caption[Vue minimale du framework MOISE+MARL]{Vue minimale du framework MOISE+MARL~: Les utilisateurs définissent d'abord les spécifications $\mathcal{M}OISE^+$, qui incluent les rôles ($\mathcal{R}$) et les missions ($\mathcal{M}$), tous deux associés via $rds$. Ils créent ensuite les spécifications MOISE+MARL en définissant d'abord des guides de contraintes tels que $rag$ et $rrg$ pour spécifier la logique des rôles, et $grg$ pour la logique des objectifs. Des linkers sont ensuite utilisés pour connecter les agents aux rôles via $ar$ et pour lier la logique des guides de contraintes aux spécifications $\mathcal{M}OISE^+$ définies. Une fois ces éléments configurés, les rôles peuvent être attribués aux agents, et le framework MARL est mis à jour en conséquence pendant l'apprentissage.
  }
  \label{fig:mm_synthesis}
\end{figure}

\noindent MOISE+MARL introduit des moyens de contrôler ou de guider l'apprentissage des agents en MARL. Sa principale contribution réside dans les \textbf{Guides de contraintes}, qui sont trois nouvelles relations introduites pour décrire la logique des rôles et des objectifs dans le formalisme Dec-POMDP :
%
\begin{itemize}
  % \begin{enumerate*}[label={\roman*) },itemjoin={; \quad}]

  \item \textbf{Guide d'action des rôles} \quad $rag: H \times \Omega \rightarrow \mathcal{P}(A \times \mathbb{R})$, relation modélisant un rôle comme un ensemble de règles qui, pour chaque couple constitué d'un historique $h \in H$ et d'une observation reçue par l'agent $\omega \in \Omega$, associe des actions attendues $A \in \mathcal{P}(A)$ chacune associée à une contrainte de dureté $ch \in [0,1]$ ($ch = 1$ par défaut). En restreignant le choix de l'action suivante parmi celles autorisées, l'agent est contraint d'adhérer au comportement attendu du rôle
  \item \textbf{Guide de récompense des rôles} \quad $rrg: H \times \Omega \times A \to \mathbb{R} = \{r_m \text{ if } a \notin A_\omega \text{, } rag(h, \omega) \allowbreak = \allowbreak A_\omega \times \mathbb{R} \text{, } h \in H; \text{ else } 0\}$, la relation qui modélise un rôle en ajoutant une pénalité $r_m$ à la récompense globale si la dernière action choisie par l'agent $a \in A$ n'est pas autorisée. Ceci vise à encourager l'agent à adhérer au comportement attendu.
  \item \textbf{Guide de récompense d'objectif} \quad $grg: H \rightarrow \mathbb{R}$, la relation qui modélise un objectif comme une contrainte souple ajoutant un bonus de récompense $r_b \in \mathbb{R}$ si l'historique $h \in H$ de l'agent contient une sous-séquence caractéristique d'un objectif $h_g \in H_g$, encourageant l'agent à l'atteindre.
        % \end{enumerate*}
\end{itemize}

\noindent Enfin, nous introduisons les \textbf{Linkers} pour lier les spécifications organisationnelles $\mathcal{M}OISE^+$ aux guides de contraintes et aux agents :
%
\begin{itemize}
  % \begin{enumerate*}[label={\roman*) },itemjoin={; \quad}]

  \item \textbf{Agent vers Rôle} \quad $ar: \mathcal{A} \to \mathcal{R}$, la relation bijective reliant un agent à un rôle ;
  \item \textbf{Guide Rôle vers Contrainte} \quad $rcg: \mathcal{R} \rightarrow rag \cup rrg$, la relation associant chaque rôle $\mathcal{M}OISE^+$ à une relation $rag$ ou $rrg$, forçant/encourageant l'agent à suivre les actions attendues pour le rôle $\rho \in \mathcal{R}$ ;
  \item \textbf{Guide Objectif vers Contrainte} \quad $gcg: \mathcal{G} \rightarrow grg$, la relation reliant les objectifs aux relations $grg$, représentant les objectifs comme des récompenses dans MARL.
        % \end{enumerate*}
\end{itemize}

\subsection*{Résolution du Dec-POMDP avec MOISE+MARL}

Un modèle MOISE+MARL est défini par $\mathcal{MM} = \langle \mathcal{OS}, ar, rcg, gcg, rag, rrg, grg \rangle$.
La résolution d'un Dec-POMDP avec $mm \in \mathcal{MM}$ consiste à trouver une politique conjointe $\pi^j = {\pi^j_0, \pi^j_1, \dots, \pi^j_n}$ qui maximise la récompense cumulative espérée (ou satisfait un seuil minimal), représentée par la fonction état-valeur $V^{\pi^j}$. Cette valeur reflète le rendement d'un état initial $s \in S$ lors de l'application d'actions conjointes successives $a^j \in A^n$ sous les contraintes organisationnelles supplémentaires.
%
La définition de $V^{\pi^j}$ suit le schéma d'exécution d'agent séquentiel et cyclique (mode AEC) et est formalisée dans \hyperref[eq:single_value_function]{Définition 1}, intégrant des adaptations basées sur les rôles (en rouge) et les missions (en bleu) qui influencent à la fois l'espace d'action et la récompense.
\autoref{fig:mm_synthesis} illustre comment les spécifications MOISE+ sont intégrées à la résolution Dec-POMDP via le cadre MOISE+MARL.


\begin{figure*}[h!]
  \label{eq:single_value_function}
  \raggedright
  \textbf{\textit{Definition 1} \quad Fonction État-Valeur adaptée aux guides de contraintes en AEC :}

  \begin{scriptsize}
    \vspace{-0.6cm}
    \begin{gather*}
      V^{\pi^j}(s_t) = \hspace{-0.75cm}
      %
      \sum_{\textcolor{red}{ \substack{a_{t} \in A \text{ if } rn() < ch_{t}, \\
            a_{t} \in A_{t} \text{ else}}
        }}{\hspace{-0.7cm} \pi_i(a_{t} | \omega_t)}
      %
      \sum_{s_{t+1} \in S}
      %
      {\hspace{-0.1cm} T(s_{t+1} | s_t, a_{t})
      \Bigl[R(s_t,a_{t},s_{t+1}) + \hspace{-0.1cm}
      \textcolor{blue}{ \sum_{m \in \mathcal{M}_i}{ \hspace{-0.1cm} v_m(t) \frac{grg_m(h_{t+1})}{1 - p + \epsilon} } }
      + } \\
      {\textcolor{red}{(1-ch_t) \times rrg(\omega_t,a_{t+1})} + V^{\pi^j_{i+1 \ mod \ n}}(s_{t+1})\Bigr]}
    \end{gather*}
    %
    \vspace{-0.5cm}
    \textcolor{red}{\[\text{ \hspace{-0.1cm} Avec } rag(h_t, \omega_t) = A_{t} \times \mathbb{R} \text{, } \langle a_t, ch_{t} \rangle \in A_{t} \times \mathbb{R} \text{ ; } rn: \emptyset \to [0,1[ \text{, une fonction aléatoire uniforme}\]}
    %
    \vspace{-0.6cm}
    \textcolor{blue}{
      \begin{gather*}
        \hspace{-0.001cm}
        \text{Avec } \omega_t = O(\omega_t | s_t, a_t) \text{ ; } h_t = \{h_0 = \langle \rangle, h_{t+1} = \langle h_t, \langle \omega_{t+1}, a_{t+1} \rangle \rangle \} \text{ ; } \epsilon \in \mathbb{R}_{>0} \text{ ; } grg_m(h) =
      \end{gather*}
    }
    \vspace{-0.95cm}
    \textcolor{blue}{
      \begin{gather*}
        \hspace{-0.5cm} \sum_{\hspace{0.3cm}(grg_i,w_i) \in mo(m)}{\hspace{-0.9cm} w_i \times grg_i(h)}
        \text{ ; } v_m(t) = \{ 1 \text{ if } t \in t_c \text{ ; else } 0 \} \text{ ; } \mathcal{M}_i = \{m_j | \langle ar(i),m_j,t_c,p \rangle \in \mathcal{M}\}
      \end{gather*}
    }
    \vspace{-0.6cm}
  \end{scriptsize}

\end{figure*}

À chaque pas de temps $t \in \mathbb{N}$ (à partir de $t=0$), l'agent $i = t \bmod n$ se voit attribuer le rôle $\rho_i = ar(i)$. Pour chaque spécification déontique temporellement valide $d_i = rds(\rho_i) = \langle tc_i, y_i, m_i \rangle$, l'agent est autorisé ($y_i = 0$) ou obligé ($y_i = 1$) à s'engager dans la mission $m_i \in \mathcal{M}$, avec un objectif $\mathcal{G}_{m_i} = mo(m_i)$ et $n \in \mathbb{N}$ agents.
%
En observant $\omega_t$, l'agent sélectionne une action parmi $A_t$ (actions attendues par le rôle) avec une probabilité $ch_t$, ou parmi $A$ sinon. Si $ch_t = 1$, l'agent est strictement contraint par son rôle.
%
L'action sélectionnée fait passer le système de $s_t$ à $s_{t+1}$, génère l'observation $\omega_{t+1}$ et renvoie une récompense composée de :
i) des bonus pour les objectifs atteints dans les missions valides (via les guides de récompenses d'objectifs), pondérés par $\frac{1}{1 - p + \epsilon}$ ;
ii) des pénalités du guide de récompenses de rôle, échelonnées par $ch_t$.
%
Le processus se poursuit dans l'état $s_{t+1}$ avec l'agent $(i + 1) \bmod n$.



\section{Extension de MOISE+MARL aux \textit{World Models} Multi-Agents}

\noindent Dans des environnements réalistes, on ne dispose que des transitions issues des historiques d'actions et d'observations reçues. Pour mieux représenter ce contexte, nous introduisons un nouveau formalisme appelé \textbf{Dec-POMDP basé sur les observations} (\acparen{ODec-POMDP}). Un \acn{ODec-POMDP} $d_\Omega \in OD_\Omega$ (avec $OD_\Omega$, l'ensemble des ODec-POMDPs) est défini comme un quintuplet~:
%
$d_\Omega = \left(\Omega, A, \mathcal{T}^j, R^j_H, \gamma \right)$
%
où~:
\begin{itemize}
  \item $A$~: l'espace d'actions.
  \item $\Omega$~: l'espace d'observations.
  \item $\Omega^{\mathcal{T}^j}_0$~: l'ensemble des observations initiales conjointes.
  \item $\mathcal{T}^j(h, \omega, a) = \langle \tilde{h}', \mathbb{P}(\omega' \mid h, \omega, a) \rangle$~: le \acn{JOPM} estimant la prochaine observation conjointe $\omega'$ à partir de l'historique $\tilde{h} \in \mathcal{H}$, de l'observation conjointe actuelle $\omega$ et de l'action conjointe $a$. Le modèle renvoie également l'état caché récurrent mis à jour $\tilde{h}'$.
  \item $R^j_H~: H \times \Omega \times A \times \Omega \rightarrow \mathbb{R}$~: la fonction de récompense basée sur l'historique, calculant la récompense depuis l'historique précédent, l'observation et action courante et l'observation suivante.
  \item $\gamma \in [0, 1]$~: le facteur d'actualisation.
\end{itemize}

\noindent Cette formulation permet aux agents MARL d'opérer uniquement à partir de données observables, rendant la méthode compatible avec les environnements simulés appris.

\subsection*{Résolution d'un ODec-POMDP avec MOISE+MARL}

Résoudre un \acn{ODec-POMDP} avec des contraintes $mm \in \mathcal{MM}$ consiste à trouver une politique conjointe $\pi^j = \{\pi^j_0, \pi^j_1, \dots, \pi^j_n\}$ qui maximise la récompense cumulée espérée (ou qui satisfait un seuil minimal), via la fonction de valeur basée sur les observations $V_{\mathcal{T}^j}^{\pi^j}$. Cette fonction représente le retour attendu depuis une observation conjointe initiale $\omega^j \in \Omega^{\mathcal{T}^j}_0$, un historique $h^j$ et un état caché $\tilde{h}$, en appliquant des actions conjointes $a^j \in A^n$ sous contraintes organisationnelles $\mathcal{MM}$, et en utilisant $\mathcal{T}^j$ pour approximer les transitions.

La définition complète de $V_{\mathcal{T}^j}^{\pi^j}$ est donnée dans \hyperref[eq:single_value_function]{Définition 2}, et intègre les adaptations basées sur les rôles (en rouge) et sur les missions (en bleu), qui influencent à la fois l'espace d'actions conjointes et la récompense. La \autoref{fig:mm_synthesis} illustre comment les spécifications $\mathcal{M}OISE^+$ sont injectées dans la résolution d'un \acn{ODec-POMDP} à l'aide du cadre MOISE+MARL.

\medskip

\begin{figure*}[h!]
  \label{eq:single_value_function}
  \raggedright
  \textbf{\textit{Definition 2} \quad Fonction Observation-Valeur adaptée aux guides de contraintes en mode parallèle:}

  \begin{scriptsize}
    \vspace{-0.6cm}
    \begin{gather*}
      \hspace{-1cm}V^{\pi^j}(\tilde{h}_{t-1},h^j_{t-1},\hat{\omega}^j_t) = \hspace{-0.95cm}
      %
      \sum_{\textcolor{red}{ \substack{a^j_{t} \in A^j \text{ if } rn() < ch_{t}, \\
      a^j_{t} \in A^j_{t} \text{ else}}
      }}{\hspace{-0.9cm} \pi_i(a^j_{t} | \hat{\omega}^j_t)}
      %
      \hspace{-1.2cm}
      \sum_{\phantom{XXXX}(\tilde{h}_t,\hat{\omega}^j_{t+1}) \in \mathcal{H} \times \hat{\Omega}^j}
      %
      {\hspace{-1.2cm} \mathcal{T}^j(\langle \tilde{h}_t,\hat{\omega}^j_{t+1} \rangle | \tilde{h}_{t-1}, \hat{\omega}_t, a^j_{t})
      \Bigl[R^j_H(h^j_{t-1},\hat{\omega}^j_t,a^j_t,\hat{\omega}^j_{t+1}) \hspace{-0.1cm} }
    \end{gather*}
    %
    \vspace{-1cm}
    \begin{gather*}
      \hspace{3cm}
      {+ \  \textcolor{blue}{grg^j_m(h^j_t)}
      +
      \textcolor{red}{(1-ch_t) \times rrg^j(\hat{\omega}^j_t,a^j_{t+1})} + V^{\pi^j}(\tilde{h}_{t}, h^j_t, \hat{\omega}^j_{t+1})\Bigr]}
    \end{gather*}
    %
    \vspace{-0.15cm}
    %
    \[\hspace{-0.9cm}\text{Avec \ } \tilde{h}_{-1} = \mathbf{0} \text{ and } \tilde{\omega}^j_0 \in \Omega_0^{\mathcal{T}^j} \text{ ; } a^j_t = \langle a_{t,0}, a_{t,1} \dots a_{t,|\mathcal{A}|} \rangle \text{ ; } \omega^j_t = \langle \omega_{t,0}, \omega_{t,1} \dots \omega_{t,|\mathcal{A}|} \rangle \text{ ; }\]
    %
    \vspace{-0.25cm}
    \[\hspace{-0.5cm} h^j_t = \langle h_{t,0}, h_{t,1} \dots h_{t,|\mathcal{A}|} \rangle = \langle \langle h_{t-1,i}, \omega_{t,i}, a_{t,i} \rangle \rangle_{i \in \mathcal{A}}\]
    %
    \vspace{-0.2cm}
    \textcolor{red}{\[\hspace{-1cm}\text{ \hspace{-0.1cm} Avec } \langle rag_i, rrg_i \rangle = rcg(ar(i)) \text{ ; } rn: \emptyset \to [0,1[ \text{, une fonction aléatoire uniformef}\]}
    %
    \vspace{-0.3cm}
    \textcolor{red}{\[A^j_t \times \mathbf{R}^{|\mathcal{A}|} = rag^j(h^j_t, \tilde{\omega}^j_t) = \langle rag_i(h_{t,i}, \omega_{t,i}) \rangle_{i \in \mathcal{A}} \text{ ; } rrg^j(h^j_t, \tilde{\omega}^j_t, a^j_t) = \sum_{i \in \mathcal{A}}{rrg_i(h_{t,i}, \omega_{t,i}, a_{t,i})}\]}
    %
    \vspace{-0.75cm}
    \textcolor{blue}{
      \begin{gather*}
        \hspace{-1cm} grg_m(h) = \hspace{-1cm} \sum_{\hspace{0.3cm}(grg_i,w_i) \in mo(m)}{\hspace{-1.1cm} w_i \times grg_i(h)}
        \text{ ; }
        grg^j_m(h^j_t) = \hspace{-0.1cm} \sum_{i \in \mathcal{A}}{\sum_{m \in \mathcal{M}_i}{ \hspace{-0.1cm} v_m(t) \frac{grg_m(h_{t,i})}{1 - p + \epsilon} }} \text{ ; } \epsilon \in \mathbb{R}_{>0} \text{ ; }
      \end{gather*}
    }
    \vspace{-0.9cm}
    \textcolor{blue}{
      \begin{gather*}
        \hspace{-1cm}
        v_m(t) = \{ 1 \text{ if } t \in t_c \text{ ; else } 0 \} \text{ ; } \mathcal{M}_i = \{m_j | \langle ar(i),m_j,t_c,p \rangle \in \mathcal{M}\}
      \end{gather*}
    }
  \end{scriptsize}

\end{figure*}

\noindent En mode parallèle, à chaque pas de temps $t \in \mathbb{N}$ (en commençant à $t=0$), chaque agent $i \in \mathcal{A}$ est assigné à un rôle $\rho_i = ar(i)$. Pour chaque spécification déontique temporellement valide $d_i = rds(\rho_i) = \langle tc_i, y_i, m_i \rangle$, l'agent est soit autorisé ($y_i = 0$), soit obligé ($y_i = 1$) de s'engager dans la mission $m_i \in \mathcal{M}$, avec ensemble d'objectifs $\mathcal{G}_{m_i} = mo(m_i)$.

Lorsque les agents observent $\tilde{\omega}_t^j$, ils sélectionnent leurs actions dans $A_{i,t}$ (dérivées via les guides de récompense de rôle) avec une probabilité $ch_t$, ou dans $A_t$ sinon. Si $ch_t = 1$, les agents sont strictement contraints par leur rôle.

Les transitions d'observation et d'état sont approximées via la fonction $\mathcal{T}^j$ à partir de l'état caché précédent $\tilde{h}_{t-1}$, de l'observation conjointe $\omega^j_t$ et de l'action conjointe $a^j_t$. La fonction de récompense $R^j_H$ utilise ces mêmes informations, ainsi que l'observation suivante, pour produire la récompense. Des bonus ou malus sont ensuite ajoutés selon~:
i) l'atteinte d'objectifs valides (via les guides de récompense des objectifs, pondérés par $\frac{1}{1 - p + \epsilon}$),
ii) la conformité au rôle (via les guides de récompense de rôle, pondérés par $1 - ch_t$).


\begin{algorithm}[H]
  \caption{Algorithme de l'activité d'entraînement}
  \label{alg:training_mamad}
  \DontPrintSemicolon

  \KwIn{
  Modèle de Prédiction d'Observations Conjointes (JOPM) $\mathcal{T}^j$,
  Observations initiales conjointes $\Omega_0^{\mathcal{T}^j}$,
  Fonction de récompense $R_H^j$,
  Fonction d'arrêt $S_H^j$,
  Fonction de rendue $\text{Render}^j_H$,
  Spécifications organisationnelles $\mathcal{MM}$,
  Contraintes de conception informelles $\mathcal{C}_{\text{inf}}$,
  Facteur d'actualisation $\gamma$
  Espace d'observations $\Omega$,
  Espace d'actions $A$
  }
  \KwOut{$\pi^j$~: Politique conjointe entraînée}

  \vspace{0.3em}

  \If{$\mathcal{MM} = \emptyset$}{
    $\mathcal{MM} \gets \texttt{manual\_formalize}(\mathcal{C}_{\text{inf}})$ \tcp*{Formalisation manuelle specs. orgs.} \;
  }

  Initialiser les paramètres de la politique $\pi^j$ et du buffer de replay $\mathcal{B}$ \;

  \ForEach{épisode $e = 1 \dots N$}{
  Échantillonner $\omega_0^j \sim \Omega_0^{\mathcal{T}^j}$, initialiser $\tilde{h}_{-1} \gets \mathbf{0}$ \;
  Initialiser l'historique $h_{-1}^j \gets \emptyset$ \;

  \ForEach{étape $t = 0 \dots T$}{
  Calculer $A_t^j = rag^j(h^j_t, \omega^j_t)$ via les guides de récompense de rôle dans $\mathcal{MM}$ \;
  \If{$rn() < ch_t$}{
    Sélectionner $a_t^j \sim \pi^j(\cdot | \omega_t^j)$ dans l'ensemble $A_t^j$ (contraint) \;
  }
  \Else{
    Sélectionner $a_t^j \sim \pi^j(\cdot | \omega_t^j)$ dans l'ensemble $A_t$ \;
  }

  $(\tilde{h}_t, \omega_{t+1}^j) \gets \mathcal{T}^j(\tilde{h}_{t-1}, \omega_t^j, a_t^j)$ \tcp*{Prédiction JOPM}

  $r_t \gets \gamma^t \times R_H^j(h^j_{t-1}, \omega_t^j, a_t^j, \omega_{t+1}^j)$ \tcp*{Récompense de base}

  $r_t \gets r_t + grg^j(h^j_t)$ \tcp*{Bonus via guides d'objectifs}

  $r_t \gets r_t + (1 - ch_t) \times rrg^j(h^j_t, \omega_t^j, a_t^j)$ \tcp*{Bonus/malus via guides de rôle}

  Ajouter $(\omega_t^j, a_t^j, r_t, \omega_{t+1}^j)$ à $\mathcal{B}$ \;
  Mettre à jour $h^j_t \gets \langle \langle h_{t-1,i}, \omega_{t,i}, a_{t,i} \rangle \rangle_{i \in \mathcal{A}}$ \;

  Entraîner $\pi^j$ avec des mini-lots tirés de $\mathcal{B}$ en utilisant toute méthode MARL \;
  }
  }

  \Return{$\pi^j$}
\end{algorithm}



\clearpage
\thispagestyle{empty}
\null
\newpage

\chapter{Analyser les comportements émergents}
\label{chap:analyzing}

% TODO: recontextualiser cette activité au sein de la méthode

Cette activité poursuit deux objectifs~: (i) fournir une explication de la politique conjointe apprise en termes de spécifications organisationnelles MOISE+MARL (rôles, objectifs, missions), et (ii) calculer l'adéquation organisationnelle, c'est-à-dire l'alignement entre les comportements appris et une organisation régulière, explicite ou implicite.
L'\textbf{activité d'analyse} traite donc les éléments formels suivants~:
\[
  (\mathcal{MM}_{i,\text{implicit}}, \text{OF}) \gets \texttt{analyze}(d_i, \mathcal{MM}_i, \pi^j_i, d_\Omega)
\]

\noindent
Pour cela nous nous appuyons sur la méthode \textbf{TEMM}~\cite{soule2025moisemarl} dont nous montrons l’intérêt pour détérminer automatiquement des éléments à la base de spécifications organisationnelles MOISE+MARL mais nous montrons aussi les limites en termes de configuration manuelle et de <<fine-tuning>> dans l'utilisation des techniques \acn{ML} utlisées. Ce chapitre commence par introduire la méthode TEMM et une extension de cette méthode pour automatiser la configuration des hyperparamètres dans un soucis de réduire la dépendance à l'expertise humaine.

% TODO : expliquer pourquoi on a interêt à utiliser TEMM auto ou TEMM manuel
% TODO: introduire adéquation organisationnelle avec tous les composants

\section{La méthode TEMM}
\label{sec:TEMM_algorithm}

La méthode \acn{TEMM} fait partie du composant d'explicabilité du cadre \acn{MOISE+MARL}. Elle repose sur l’hypothèse que les comportements des agents, malgré une variabilité apparente, présentent des régularités lorsqu'ils atteignent des récompenses cumulées comparables. Ainsi, des comportements différents peuvent être interprétés comme des variantes bruitées d'un nombre limité de stratégies latentes. D’après la loi des grands nombres, une moyenne sur un ensemble suffisant d’historiques conjoints réussis permet de filtrer le bruit et de révéler des stratégies typiques.

La méthode exploite des techniques d’apprentissage non supervisé pour inférer des spécifications organisationnelles à partir des trajectoires observées des agents, et pour calculer l’\textbf{adéquation organisationnelle} (\acn{OF}) entre les comportements émergents et les rôles, objectifs et missions attendus. Elle se décline en trois volets.

\paragraph{1) Rôles et héritage de rôles}
Les trajectoires $(\omega, a) \in \Omega \times A$ sont regroupées en clusters à l’aide de métriques de distance (par exemple \acparen{LCS}, Smith-Waterman), éventuellement après encodage one-hot des actions.
Dans ce cadre, un \textbf{rôle} $\rho$ est défini comme une politique dont les agents partagent une \textit{séquence commune la plus longue} (CLS) dans leurs historiques.
Un rôle $\rho_2$ hérite de $\rho_1$ si $\text{CLS}(\rho_2) \subseteq \text{CLS}(\rho_1)$.
Le clustering hiérarchique permet d’extraire ces CLS et de construire une hiérarchie des rôles.
Pour chaque cluster, un centroïde de transitions moyennes par pas de temps est calculé. Une procédure de sélection retient les transitions les plus représentatives, interprétées comme des \textbf{règles comportementales} associées à un rôle.
Une faible représentativité conduit à inclure toutes les transitions, au risque de sur-apprentissage.
Le \textbf{\acn{SOF}} (structural organizational fit) est calculé comme l’inverse normalisé de la variance globale dans les clusters de transitions~: une faible variance indique une forte cohérence structurelle.

\paragraph{2) Objectifs, plans et missions}
Les trajectoires d’observations sont regroupées en clusters à l’aide de métriques de distance ou via des méthodes vectorielles (par ex. K-means sur des embeddings de trajectoires). Pour chaque cluster, une trajectoire centroïde est calculée, associant chaque pas de temps à une observation moyenne.
La \textbf{représentativité} est définie comme l’inverse normalisé de la variance locale par pas de temps.
Un seuil minimal de représentativité est appliqué pour sélectionner les observations saillantes, interprétées comme des \textbf{objectifs intermédiaires} – jalons importants vers l’objectif global.
Si la représentativité minimale est élevée, seules les observations très fréquentes sont retenues, assurant robustesse et pertinence.
Les \textbf{plans} sont inférés comme des sous-séquences de transitions menant systématiquement à ces objectifs.
Une \textbf{mission} regroupe un ou plusieurs objectifs poursuivis collectivement par un ou plusieurs agents.
Le \textbf{\acn{FOF}} (functional organizational fit) évalue la cohérence fonctionnelle des agents dans l’atteinte de ces objectifs intermédiaires, calculé comme l’inverse normalisé de la variance dans les clusters d’observations.

\paragraph{3) Permissions et obligations}
Les permissions et obligations sont dérivées en analysant si les agents remplissant un rôle accomplissent systématiquement (ou exclusivement) certaines missions dans des contraintes temporelles données.
Une obligation implique exclusivité, tandis qu’une permission implique optionnalité.

\paragraph{Agrégation et interprétation}
L’\textbf{adéquation organisationnelle globale} est obtenue en agrégeant les scores structurel et fonctionnel :
\[
  \text{OF} = \frac{1}{2} \left( \text{SOF} + \text{FOF} \right)
\]
Un score élevé indique que les spécifications inférées (rôles, objectifs, missions) sont représentatives des comportements réellement appris.
Un score faible suggère une faible structuration ou des comportements peu cohérents.
Bien que certains hyperparamètres de clustering puissent nécessiter un ajustement manuel pour garantir la robustesse de l’extraction des rôles et objectifs, \acn{TEMM} fournit une approche méthodique pour analyser les comportements organisationnels émergents et affiner les spécifications en conséquence.


\section{Auto-TEMM : la méthode TEMM étendue avec optimisation des hyperparamètres}

Un problème majeur rencontré avec \acn{TEMM} est la nécessité de choisir manuellement plusieurs hyperparamètres (métriques de distance, seuils de clustering, seuils de représentativité), ce qui ralentit le processus d'analyse et limite son automatisation. Une représentativité trop faible conduit à du sur-apprentissage, tandis qu'une représentativité trop élevée limite les contraintes et ralentit la convergence.


Pour surmonter cette difficulté, nous proposons un processus d'\textbf{optimisation d'hyperparamètres} (\acparen{HPO}) consistant en une recherche par grille (grid search) sur les combinaisons possibles, visant à maximiser le adéquation  organisationnel et minimiser le nombre de clusters~:

\begin{itemize}
  \item (i) Pour les observations et les transitions, appliquer une recherche conjointe sur les métriques de distance et les seuils de clustering afin de minimiser la variance intra-cluster et le nombre de clusters~;
  \item (ii) Déterminer les représentativités minimales (structurelle et fonctionnelle) pour obtenir des objectifs et des rôles concis et pertinents. Comme illustré en \autoref{fig:conv_time_repr}, diminuer cette représentativité augmente la couverture mais réduit la robustesse des contraintes organisationnelles. Une valeur élevée limite la généralisation, tandis qu'une valeur trop faible entraîne un sur-apprentissage.
\end{itemize}

Nous adoptons un compromis basé sur le \textbf{point de coude} du graphique convergence/temps (voir \autoref{fig:conv_time_repr}), en choisissant la plus grande représentativité assurant une convergence normalisée de 3.5\%. Cette stratégie permet d'obtenir des spécifications utiles, interprétables et généralisables, sans complexité excessive.

\begin{figure}[h!]
  \centering
  \includegraphics[trim=0cm 0cm 0cm 0cm, clip, width=1.\linewidth]{figures/convergence_time_relative_to_representativeness.pdf}
  \caption{Temps de convergence normalisé en fonction de la représentativité minimale}
  \label{fig:conv_time_repr}
\end{figure}

\begin{algorithm}[H]
  \caption{Algorithme de l'activité d'analyse}
  \label{alg:auto_temm}
  \DontPrintSemicolon

  \KwIn{
    Politique conjointe entraînée $\pi^j$~;
    ODec-POMDP $d_\Omega$~;
    Spécification initiale $\mathcal{MM}$~;
    Seuil de convergence normalisé (défaut~: 3.5\%) $\eta$
  }

  \KwOut{
  Spécification organisationnelle inférée $\mathcal{MM}_{\text{implicit}}$~;
  Score de adéquation  organisationnel $\text{OF}$
  }

  \tcp*[l]{1. Collecte des trajectoires}
  Générer les historiques individuels $\mathcal{D}_{\text{trans}}$ depuis $d_\Omega$ sous $\pi^j$ \;
  $\mathcal{D}_{\text{obs}} \gets$ trajectoires d'observations individuelles issues de $\mathcal{D}_{\text{full}}$ \;

  \tcp*[l]{2. HPO sur distance et seuil de clustering}
  \For{$t \in \{obs, trans\}$}{
    \ForEach{métrique de distance $\delta_t$}{
      \ForEach{seuil minimal de cluster $\tau_t$}{
        Appliquer clustering avec $(\delta_t, \tau_t)$ \;
        Calculer $\sigma_{\text{obs}}, \sigma_{\text{trans}}, N_{\text{clusters}}$ \;
        Score $\gets \alpha (\sigma_{\text{obs}} + \sigma_{\text{trans}}) + \beta N_{\text{clusters}}$ \tcp*[l]{par défaut~: $\alpha=0.4$, $\beta=0.6$}
        Retenir $(\delta_t^*, \tau_t^*)$ avec Score minimal \;
      }
    }
  }

  \tcp*[l]{3. Application du clustering avec HPO optimal}
  Clustering des observations~: $\mathcal{D}_{\text{obs}} \rightarrow C_{obs}$ via $(\delta_{obs}^*, \tau_{obs}^*)$ \;
  Clustering des transitions~: $\mathcal{D}_{\text{trans}} \rightarrow C_{trans}$ via $(\delta_{trans}^*, \tau_{trans}^*)$ \;

  \tcp*[l]{4. HPO sur la représentativité (convergence)}
  \For{$t \in \{obs, trans\}$}{
  \ForEach{représentativité $\rho_t$}{
  Inférer $\mathcal{MM}_{\rho_t}$ à partir des clusters \;
  Initialiser une politique $\pi^j_{\rho_t}$ \;
  Entraîner $\pi^j_{\rho_t}$ sur $(d_\Omega, \mathcal{MM}_{\rho_t})$ jusqu'à atteindre $R_{\min}$ \;
  Enregistrer le temps de convergence $c_{\rho_t}$ tel que $ct_t(\rho_t) = c_{\rho_t}$ \;
  }

  \tcp*[l]{5. Sélectionner le point de coude}
  $\rho_t^* \gets max(\{\rho_t \ | \ ct_t(\rho_t) < \eta \})$ \tcp*[r]{par défaut $\eta = 3.5\%$}
  }

  \tcp*[l]{6. Inférence finale des rôles et objectifs}
  Inférer les rôles à partir de $\mathcal{D}_{\text{trans}}, \delta^*, \tau^*, \rho^*$ \;
  Inférer les objectifs à partir de $\mathcal{D}_{\text{obs}}, \delta^*, \tau^*, \rho^*$ \;

  \tcp*[l]{7. Calcul du adéquation  organisationnel}
  Calculer SOF (structurel) et FOF (fonctionnel) à partir des variances intra-cluster \;
  $\text{OF} \gets \frac{1}{2}(\text{SOF} + \text{FOF})$ \;

  \Return{$\mathcal{MM}_{\text{implicit}}, \text{OF}$}
\end{algorithm}



\clearpage
\thispagestyle{empty}
\null
\newpage

\chapter{Transférer et superviser en environnement réel}
\label{chap:transferring}

L'\textit{activité de transfert} a deux objectifs principaux~: (1) déployer en continu la politique conjointe la plus récente $\pi^j_{\text{latest}}$ dans l'environnement réel $\mathcal{E}$ afin d'assurer l'action et l'interaction efficaces des agents~; et (2) collecter de nouvelles trajectoires réelles $(\omega^j_t, a^j_t, \omega^j_{t+1})$ pour enrichir l'ensemble de trajectoires $\mathcal{D}_{H^j}$ utilisé pour mettre à jour l'environnement simulé et les spécifications organisationnelles.
\noindent L'\textbf{activité de transfert} traite donc les éléments formels suivants~:
\[
  \mathcal{D}_{H^j}, \texttt{need\_update} \gets \texttt{transfer}(\pi^j_{\text{latest}}, \mathcal{E}, \mathcal{D}_{H^j})
\]

\noindent À notre connaissance, aucun framework n'offre à la fois un déploiement asynchrone des politiques et une collecte automatique des historiques conjoints, déclenchée par seuil, dans une boucle fermée capable de se synchroniser avec une chaîne d'apprentissage. Cette absence limite fortement l'automatisation du cycle de conception des \acplu{SMA} en environnement réel.

\paragraph{Cadre de transfert}

Nous proposons un cadre théorique général mettant en œuvre un système de contrôle asynchrone et événementiel, responsable de l'exécution des politiques et de la collecte des traces. Il maintient un tampon de trajectoires, déclenche la activité de réentraînement lorsqu'un seuil de taille est atteint, et veille à ce que la politique la plus récente soit toujours utilisée. Cette logique de contrôle repose sur deux mécanismes~:
(i) une \texttt{Boucle de transfert} (\texttt{Transfer Loop}) pour le déploiement temps réel et la collecte de données, et
(ii) un \texttt{Déclencheur de mise à jour} (\texttt{Update Trigger}) qui lance le processus de conception dès qu'assez de données sont disponibles. Le système empêche les mises à jour parallèles multiples et garantit la synchronisation entre les activités de transfert et de modélisation.

\vspace{-0.3em}
\begin{algorithm}[H]
  \caption{Activité de transfert}
  \label{alg:transferring}
  \DontPrintSemicolon
  \KwIn{Politique actuelle $\pi^j_{\text{latest}}$, environnement réel $\mathcal{E}$, base de trajectoires $\mathcal{D}_{H^j}$}
  \KwOut{Base de trajectoires mise à jour $\mathcal{D}_{H^j}$, signal de mise à jour $\texttt{need\_update}$}

  \vspace{0.3em}
  \SetKwProg{Transfer}{Procédure \normalfont BoucleDeTransfert}{}{}
  \Transfer{}{

  \While{le SMA est actif dans l'environnement $\mathcal{E}$}{
  \tcp*[l]{Exécution de la politique la plus récente}
  $\omega^j_t \gets \texttt{observe}(\mathcal{E})$ \;
  $a^j_t \gets \pi^j_{\text{latest}}(\omega^j_t)$ \;
  $\omega^j_{t+1} \gets \texttt{apply}(\mathcal{E}, a^j_t)$ \;
  Ajouter $(\omega^j_t, a^j_t, \omega^j_{t+1})$ au tampon temporaire $\mathcal{B}$ \;

  \tcp*[l]{Vérification du déclenchement de la mise à jour}
  \If{$|\mathcal{B}| \geq \texttt{batch\_size}$}{
    Ajouter $\mathcal{B}$ à $\mathcal{D}_{H^j}$ et vider $\mathcal{B}$ \;
    $\texttt{need\_update} \gets \texttt{True}$ \;
    \If{$\texttt{not running\_update} = \texttt{False}$}{
      \texttt{launch\_update()} \tcp*[r]{Appel asynchrone}
    }
  }
  }
  }
\end{algorithm}

Ce mécanisme garantit (i) la continuité d'exécution, (ii) la réactivité à l'arrivée de nouvelles données, et (iii) l'automatisation des cycles de mise à jour. Il constitue le lien entre le monde simulé et le contexte réel de déploiement en ajustant en continu la boucle de conception à l'évolution de l'environnement, condition essentielle pour l'autonomie à long terme d'un \acn{SMA}.

\clearpage
\thispagestyle{empty}
\null
\newpage

\chapter*{Conclusion}
\addcontentsline{toc}{chapter}{\textbf{Conclusion}}

\noindent
Cette troisième partie a introduit la méthode \textbf{\acn{MAMAD}} comme une réponse concrète aux limites identifiées dans les approches actuelles de conception de \acn{SMA}. Reposant sur un cycle itératif structuré en quatre activités (\textit{Modélisation}, \textit{Apprentissage}, \textit{Analyse}, \textit{Transfert}), \acn{MAMAD} articule de manière cohérente des outils symboliques (spécifications organisationnelles) et \acn{MARL} pour guider la conception, l'entraînement et l'adaptation d'agents intelligents dans des environnements complexes.

\medskip

\noindent
La méthode s'appuie notamment~:
\begin{itemize}
  \item sur une modélisation fidèle des environnements à partir de données empiriques,
  \item sur un entraînement contraint par des spécifications organisationnelles intégrées au sein du cadre \textit{MOISE+MARL},
  \item sur une analyse des trajectoires pour inférer les structures émergentes de l'organisation apprise,
  \item et enfin sur un transfert contrôlé permettant l'amélioration itérative du \acn{SMA}.
\end{itemize}

\noindent
Dans la partie suivante, nous proposons de valider expérimentalement cette méthode à travers son implémentation concrète dans un outil dédié, \acn{CybMASDE}, et son application à plusieurs environnements représentatifs. L'objectif est de démontrer la capacité de \acn{MAMAD} à produire automatiquement des \acplu{SMA} performants, explicables et conformes à des exigences organisationnelles dans des contextes variés.

\medskip

\noindent
Nous évaluons notamment la méthode selon des critères d'efficacité, d'automatisation, de conformité aux contraintes et d'explicabilité, tout en comparant ses résultats à ceux d'approches classiques non guidées par des modèles organisationnels.

\bigskip

La \autoref{part:experimentation} met donc à l'épreuve la méthode \acn{MAMAD}, en analysant ses performances et sa pertinence au regard des verrous identifiés sur différents scénarios.
