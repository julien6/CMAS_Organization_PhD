\cleardoublepage
\phantomsection
% \pdfbookmark[1]{La méthode MAMAD}{La méthode MAMAD}
\markboth{\spacedlowsmallcaps{La méthode MAMAD}}{\spacedlowsmallcaps{La méthode MAMAD}}
\part{La méthode MAMAD}
\label{part:methode}

\chapter*{Introduction}
\addcontentsline{toc}{chapter}{\textbf{Introduction}}

\noindent
La partie précédente a mis en lumière les lacunes actuelles dans l'intégration des modèles organisationnels au sein des approches d'apprentissage multi-agent, tant du point de vue du contrôle, de l'explicabilité que de l'automatisation de la conception.

\medskip

\noindent
Cette troisième partie présente notre proposition pour répondre à ces lacunes : la méthode \ac{MAMAD}. Elle repose sur l'hypothèse que la conception d'un \ac{SMA} peut être abordée comme un problème d'optimisation sous contraintes. Elle s'organise autour d'une boucle en quatre étapes :

\begin{enumerate}
    \item \textbf{Modélisation} : créer un environnement simulé et contraint à partir d'un environnement réel et d'exigences organisationnelles ;
    \item \textbf{Apprentissage} : entraîner les agents dans cet environnement en tenant compte des rôles, missions et contraintes spécifiés ;
    \item \textbf{Analyse} : extraire des structures organisationnelles émergentes à partir des trajectoires apprises ;
    \item \textbf{Transfert} : redéployer les politiques dans l'environnement réel et collecter de nouvelles données pour réitérer le cycle si nécessaire.
\end{enumerate}

\noindent
Ce cycle itératif vise à produire des \ac{SMA} adaptés à leur environnement, alignés sur des contraintes organisationnelles, explicables et robustes. Il s'appuie sur un formalisme markovien contraint, des outils d'analyse comportementale, et un cadre logiciel développé pour supporter l'ensemble de la démarche.

\medskip

\noindent
Comme illustré en \autoref{fig:organisation_manuscrit_partie_3}, un premier chapitre donne une description globale de le partie structurelle de la méthode concernant les processus proposés et leurs fonctionnements et dynamiques mutuels d'autre part. Les quatre chapitres restants détaillent chacune des étapes de cette méthode :
la modélisation (\textbf{Chapitre 7}), l'apprentissage organisationnel contraint (\textbf{Chapitre 8}), l'analyse des trajectoires pour inférer des structures organisationnelles émergentes (\textbf{Chapitre 9}) et enfin le transfert et l'évaluation dans l'environnement réel (\textbf{Chapitre 10}).

La méthode \ac{MAMAD} ambitionne de réunir les forces des approches symboliques et apprenantes pour une conception de \ac{SMA} à la fois structurée, autonome et explicable.

\begin{figure}[h!]
    \centering
    \resizebox{\linewidth}{!}{%
        \input{figures/organisation_manuscrit_partie_3.tex}
    }
    \caption{Structure de la Partie III : La méthode MAMAD}
    \label{fig:organisation_manuscrit_partie_3}
\end{figure}



\chapter{Présentation globale de la méthode}
\label{sec:mamad}


La méthode \ac{MAMAD}~\footnotemark[1] repose sur quatre grandes activités : (1) la modélisation de l'environnement, de l'objectif global et des contraintes organisationnelles, (2) l'apprentissage des politiques à l'aide de divers algorithmes \ac{MARL}, (3) l'analyse des comportements et l'inférence des spécifications organisationnelles à l'aide d'une méthode proposée, et (4) le maintien de la cohérence entre l'environnement simulé et l'environnement réel en déployant les politiques entraînées et en mettant à jour la simulation. Cette approche guide le processus d'apprentissage des agents tout en imposant des contraintes organisationnelles strictes, garantissant ainsi l'efficacité des politiques apprises. Le cycle de vie d'un \ac{SMA} conçu avec \ac{MAMAD} est illustré en \autoref{fig:cycle}.

\begin{figure}[h!]
    \centering
    \input{figures/cycle.tex}
    \caption[Cycle de vie d'un SMA conçu avec MAMAD]{Cycle de vie d'un SMA conçu avec MAMAD : i) L'utilisateur commence par modéliser l'environnement à partir d'un ensemble suffisant de trajectoires réelles (obtenues à l'aide d'agents initialement transférés) ou de toute autre source disponible, ainsi que l'objectif global et les contraintes de conception sous forme de rôles et d'objectifs ; \quad ii) Ensuite, il lance l'entraînement des agents via des techniques MARL ; \quad iii) Une analyse post-entraînement est réalisée pour extraire les rôles et objectifs émergents des agents, ce qui permet d'améliorer les spécifications organisationnelles appliquées ; \quad v) Une fois validées, les politiques apprises sont déployées pour contrôler les actionneurs de l'environnement, générant de nouvelles traces utiles pour une meilleure modélisation.}
    \label{fig:cycle}
\end{figure}

La méthode \ac{MAMAD} conçoit la conception d'un \ac{SMA} comme un processus itératif d'optimisation sous contraintes. On suppose donnés :
\begin{itemize}
    \item $\mathcal{E}_0$ : l'environnement initial dans lequel les agents peuvent agir ;
    \item $\mathcal{G}_{\text{inf}}$ : une description informelle de l'objectif global recherché ;
    \item $\mathcal{C}_{\text{inf}}$ : une spécification informelle des contraintes de conception ;
    \item $\gamma \in [0,1]$ : le facteur d'actualisation définissant une solution à court ou long terme, généralement fixé empiriquement (par défaut à 1) ;
    \item $A, \Omega$ : respectivement les espaces d'actions et d'observations ;
    \item $\texttt{org\_fit}_{min}$, $\overline{r}_{min}$, $\sigma_{min}$ : les valeurs minimales exigées pour le score d'adéquation organisationnelle, la récompense moyenne, et l'écart-type, servant à valider une politique conjointe. Ces seuils sont en général fixés empiriquement.
\end{itemize}

\noindent Comme décrit dans \autoref{alg:mamad}, le cadre méthodologique permet une conception continue du \ac{SMA} via la coordination itérative et asynchrone de deux processus distincts : le \textit{processus de Transfert}, qui est connecté à l'environnement réel et gère l'exécution en temps réel et la collecte d'historiques conjoints ; et le \textit{processus \ac{MTA} (Modéliser-Entraîner-Analyser)}, qui consomme les données stockées pour améliorer itérativement le modèle simulé, la politique conjointe et les spécifications organisationnelles du \ac{SMA}.

\paragraph{Processus de Transfert : déploiement des politiques et collecte de données}

Ce processus est actif en continu tant que le \ac{SMA} est en fonctionnement dans l'environnement réel. Il a deux rôles : déployer la politique conjointe la plus récente $\pi^j_{\text{latest}}$ auprès des agents réels, garantissant un comportement à jour sans interruption ; et collecter en continu les trajectoires des agents sous forme d'historiques conjoints $H^j$, stockés par lots. Une fois qu'un nombre suffisant de trajectoires est collecté, le lot est ajouté au dépôt global $\mathcal{D}_{H^j}$. Si le processus de mise à jour n'est pas en cours, il déclenche le lancement du processus \textit{\ac{MTA}}.

\paragraph{Processus MTA : optimisation des politiques et raffinement organisationnel}

Ce processus modélise le problème de conception actuel et améliore la politique du \ac{SMA} ainsi que ses spécifications organisationnelles. Il commence par construire un modèle de prédiction d'observations conjointes (\acparen{JOPM}) $T^j$ à l'aide de World Models étendus, à partir des trajectoires collectées. Les exigences de conception sont formalisées sous forme de spécifications organisationnelles MOISE+MARL $\mathcal{MM}$ et l'objectif est formalisé par une fonction de récompense basée sur l'historique $R^j_H$.

Un modèle markovien est ensuite construit à partir des éléments modélisés afin d'entraîner les agents en tenant compte des spécifications organisationnelles, via le framework MOISE+MARL. Une fois l'entraînement terminé, la politique conjointe $\pi^j$ est analysée à l'aide de \ac{TEMM} afin d'inférer les spécifications organisationnelles implicites $\mathcal{MM}_{\text{imp}}$ et de calculer un score d'adéquation organisationnelle.

\paragraph{Boucle de raffinement via les spécifications organisationnelles}

Si la politique apprise montre une faible adéquation organisationnelle, des performances insuffisantes ou une grande variabilité (par rapport aux seuils définis), les spécifications organisationnelles implicites inférées sont utilisées pour raffiner les spécifications initiales. Ce processus de raffinement peut impliquer une inspection manuelle des structures inférées pour identifier les facteurs clés de succès des comportements émergents. Guidé par ces observations, le concepteur peut réviser les spécifications afin d'orienter les prochaines itérations d'apprentissage.

Cette boucle est répétée jusqu'à un maximum de $n_{refine}$ fois, orientant progressivement l'espace des politiques vers des comportements plus structurés et plus performants. La dernière politique validée est alors enregistrée comme $\pi^j_{\text{latest}}$, prête à être déployée dans l'environnement réel.

La boucle de raffinement est particulièrement utile dans les environnements complexes où la connaissance préalable est limitée ou où la conception manuelle serait trop coûteuse. À chaque itération, elle permet de restreindre l'espace de recherche des politiques en le concentrant sur les régions associées à des régularités organisationnelles émergentes. Remarquablement, ce processus peut commencer sans aucune spécification organisationnelle initiale, et produire par raffinement successif des contraintes organisationnelles pertinentes, objectives, et indépendantes de toute expertise humaine ou connaissance préalable de l'environnement.

\noindent L'interaction entre ces deux processus asynchrones constitue un cycle de conception de \ac{SMA} complet et fermé. Le système apprend continuellement à partir de l'exécution réelle, met à jour son modèle simulé, réentraîne sous des spécifications évolutives, et déploie des politiques améliorées sans nécessiter d'intervention constante du concepteur. Cette architecture établit un pont entre les principes symboliques de l'ingénierie orientée agents et l'automatisation par apprentissage, assurant conformité, adaptabilité et explicabilité au niveau organisationnel.

\begin{algorithm}[H]
    \caption{Conception de SMA assistée par MOISE+MARL (MAMAD)}
    \label{alg:mamad}
    \DontPrintSemicolon

    \KwIn{Environnement initial $\mathcal{E}$, objectif $\mathcal{G}_{\text{inf}}$, contraintes de conception $\mathcal{C}_{\text{inf}}$, $n_{refine}$ nombre maximal de cycles de raffinement}
    \KwOut{Un SMA déployé satisfaisant aux exigences de conception, de performance et d'explicabilité ; ainsi que ses spécifications organisationnelles associées}

    Initialiser : $\mathcal{D}_{H^j} \gets \emptyset$, $\pi^j_{\text{latest}} \gets \pi^j_{\text{init}}$, $running\_MTA \gets False$ \;

    \vspace{0.3em}

    \While{le SMA est actif dans l'environnement $\mathcal{E}$}{
        \tcp*[l]{Transfert : récupération des trajectoires \& déploiement de la politique}
        $\mathcal{D}_{H^j}, \texttt{need\_update} \gets \text{transfer}(\pi^i_{latest}, \mathcal{D}_{H^j})$ \tcp*[r]{appel asynchrone}
        \If{\texttt{need\_update} et non \texttt{running\_MTA}}{
            \texttt{launch\_MTA()} \tcp*[r]{appel asynchrone}
        }
    }

    \vspace{1em}
    \SetKwProg{MTA}{Processus \normalfont(MTA)}{}{}
    \MTA{}{}{

    $\texttt{running\_MTA} \gets True$ \tcp*[l]{Variable globale}

    \tcp*[l]{Modélisation : modéliser l'environnement réel}
    $\mathcal{T}^j, R^j_H, \mathcal{MM} \gets \texttt{model}(\mathcal{E}, \mathcal{D}_{H^j}, \mathcal{G}_{\text{inf}}, \mathcal{C}_{\text{inf}}, \gamma, A, \Omega)$ \;

    \While{$i < n_{refine}$}{

    \vspace{0.5em}
    \tcp*[l]{Entraînement : politique sous contraintes organisationnelles}
    $\pi^j, \overline{r}, \sigma \gets \texttt{train}(\mathcal{T}^j, \mathcal{MM})$ \;

    \vspace{0.5em}
    \tcp*[l]{Analyse : inférer les nouvelles spécifications organisationnelles}
    $(\mathcal{MM}_{\text{imp}}, \texttt{org\_fit}) \gets \texttt{analyze}(\mathcal{T}^j, \mathcal{MM}, \pi^j)$ \;

    \vspace{0.5em}
    \tcp*[l]{Si politique insatisfaisante, raffinement}
    \If{$\texttt{org\_fit} < \texttt{org\_fit}_{min} \ or \ \overline{r} < \overline{r}_{min} \ or \  \sigma > \sigma_{min}$}{
    $\mathcal{MM} \gets \mathcal{MM}_{\text{imp}}$ \;
    retour à 'Analyzing' \;
    }

    $\pi^j_{\text{latest}} \gets \pi^j$ \tcp*[r]{Mise à jour de la politique la plus récente}

    $i \gets i + 1$

    }

    $\texttt{running\_MTA} \gets False$ \tcp*[l]{Variable globale}

    }
\end{algorithm}

\noindent On peut noter que nous proposons d'exploiter un environnement simulé modélisé comme un jumeau numérique (\textit{Digital Twin}) pour l'entraînement ultérieur, tandis que les approches \ac{MBRL} combinent simultanément modélisation et apprentissage. En effet, nous privilégions une séparation entre modélisation et apprentissage pour les raisons suivantes : i) la réutilisabilité du modèle d'environnement pour d'autres entraînements d'agents, avec des ajustements éventuels ; \quad ii) le besoin d'agents simples n'embarquant pas de modèles coûteux pour planifier ; \quad iii) le besoin d'un environnement simulé de haute fidélité commun à tous les agents.

\noindent Les chapitres suivantes détaillent chaque activité du cadre \ac{MAMAD}, identifient les défis spécifiques rencontrés, et décrivent les contributions proposées pour y répondre.


\chapter{Modéliser l'environnement en simulation}
\label{chap:modelling}

L'\textbf{activité de modélisation} vise à produire les composants formels suivants :
\begin{displaymath}
    \texttt{model}(\mathcal{D}_{H^j}, \mathcal{G}_{\text{inf}}, \mathcal{C}_{\text{inf}}, \gamma, A, \Omega) = \mathcal{T}^j, R^j_H, \mathcal{MM}
\end{displaymath}

\noindent L'\textit{activité de modélisation} a pour objectif de représenter le problème de conception comme un problème d'optimisation sous contraintes. Pour cela, elle commence par générer un environnement simulé de haute fidélité en construisant un \ac{JOPM} $\mathcal{T}^j : H^j \times \Omega^j \times A^j \rightarrow \mathcal{H} \times \hat{\Omega}^j$ à partir des traces d'interactions réelles $\mathcal{D}_{H^j}$. À un pas de temps $t \in \mathbb{N}$, pour tout état caché récurrent $\tilde{h}_{t-1} \in \mathcal{H}$ représentant l'historique conjoint jusqu'à $t-1$, l'observation conjointe reçue $\omega_t^j \in H^j$ et l'action conjointe $a_t^j \in A^j$, le modèle $\mathcal{T}^j$ renvoie le nouvel état caché $\tilde{h}_t \in \mathcal{H}$ ainsi que la prédiction de la prochaine observation conjointe $\hat{\omega}^j \in \hat{\Omega}^j$. Ce mécanisme permet à \ac{MAMAD} de construire l'environnement de simulation depuis zéro.

En complément de cet environnement simulé, le problème d'optimisation formalisé comprend également la transformation de la description informelle de l'objectif global $\mathcal{G}_{\text{inf}}$ en une fonction de récompense basée sur l'historique $R^j_H: H^j \times \Omega^j \rightarrow \mathbb{R}$, ainsi que la formalisation des contraintes issues des exigences de conception $\mathcal{C}_{\text{inf}}$ sous la forme de spécifications organisationnelles MOISE+MARL $\mathcal{MM}$.

Nous supposons ici que la formalisation manuelle de la description informelle de l'objectif et des contraintes de conception en une fonction $R^j_H$ et en spécifications $\mathcal{MM}$ est prise en charge.

Étant donné l'impossibilité d'accéder directement à l'état réel de l'environnement, nous nous appuyons sur les historiques conjoints stockés. C'est pourquoi nous utilisons les World Models, capables de généraliser à partir d'un grand volume d'historiques pour estimer les transitions d'états cachés et d'observations. Toutefois, un obstacle majeur est l'absence de World Models explicitement conçus pour les environnements multi-agents. Nous proposons ci-dessous une extension du cadre World Models aux contextes multi-agents.

\subsubsection*{Extension aux World Models Multi-Agents}

Dans les environnements multi-agents, les observations conjointes deviennent rapidement de grande dimension à mesure que le nombre d'agents augmente. Pour pallier cela, des fonctions d'encodage conjoint sont introduites pour les observations et les actions.

Plus précisément, les observations conjointes $\omega_t^{j} = (\omega_t^1, \dots, \omega_t^n) \in \Omega^{j}$ sont transformées en représentations latentes compactes à l'aide d'un encodeur d'observations conjointes $Enc_{\omega^j} : \Omega^j \rightarrow z$, produisant $z_t = Enc_{\omega^j}(\omega_t^j)$. Un décodeur $Dec_z : z \rightarrow \hat{\Omega}^j$ permet la reconstruction des observations conjointes si nécessaire.

On utilise généralement des \ac{MLP}s ou des architectures à base d'attention pour ces encodeurs, afin d'agréger les informations multi-agents en vecteurs de caractéristiques de taille fixe, tout en capturant les dépendances critiques entre agents.

Une fois l'encodage effectué, le World Model multi-agent fonctionne comme en contexte mono-agent, en utilisant les observations encodées $z_t$ dans les historiques transmis au \ac{RLDM} $\mathcal{T}^{z}$. Ce modèle permet une modélisation évolutive, tout en conservant les motifs d'interaction clés entre agents. Dans le cadre de \ac{MAMAD}, ces World Models constituent le cœur de la simulation mise en œuvre par l'\hyperref[sec:modelling]{activité de modélisation}, agissant comme des jumeaux numériques de haute fidélité de l'environnement cible.

\begin{algorithm}[H]
    \caption{Algorithme de l'activité de modélisation}
    \label{alg:modeling}
    \DontPrintSemicolon

    \KwIn{Historiques conjoints $\mathcal{D}_{H^j}$, objectif informel $\mathcal{G}_{\text{inf}}$, contraintes informelles $\mathcal{C}_{\text{inf}}$, facteur d'actualisation $\gamma$, espace des actions $A$, espace des observations $\Omega$}
    \KwOut{JOPM $\mathcal{T}^j$, fonction de récompense $R^j_H$, spécifications MOISE+MARL $\mathcal{MM}$}

    \vspace{0.5em}
    \tcp{1. Formalisation manuelle des exigences symboliques}
    $R^j_H \gets \texttt{manual\_formalize}(\mathcal{G}_{\text{inf}})$ \;
    $\mathcal{MM} \gets \texttt{manual\_formalize}(\mathcal{C}_{\text{inf}})$ \;

    \vspace{0.5em}
    \tcp{2. Entraîner les encodeurs pour les observations et actions conjointes}
    Extraire les observations $\Omega^j = \{\omega^j_t\}$ à partir des historiques $\mathcal{D}_{H^j}$ \;
    Entraîner un auto-encodeur $(Enc_{\omega^j}, Dec_{\omega^j})$ sur $\Omega^j$ en minimisant l'erreur de reconstruction \;

    \vspace{0.5em}
    \tcp{3. Encoder les observations dans les historiques}
    Pour chaque historique $h^j = \{\omega_t^j, a_t^j\} \in \mathcal{D}_{H^j}$, encoder chaque observation conjointe ${z}_t = Enc_{\omega^j}(\omega^j_t)$ pour constituer l'ensemble d'entraînement $\mathcal{B} = \{ \{(z_t, a^j_t, z_{t+1})\} = h_z^j, h_z^j \in \mathcal{D}_{H^j}\}$

    \vspace{0.5em}
    \tcp{4. Entraîner le RLDM}
    Initialiser le RLDM (fonctions $f$ et $g$) $\mathcal{T}^z = f(g)$

    \For{$h_z^j \in \mathcal{B}$}{
        \For{$(z_t, a^j_t, z_{t+1}) \in h^j$}{
            Entraîner le RLDM $\mathcal{T}^{z}$ en minimisant l'erreur quadratique moyenne (MSE) entre la prédiction $\hat{z}_{t+1}$ et la valeur réelle $z_{t+1}$.
        }
    }

    \vspace{0.5em}
    \tcp{5. Sauvegarder les observations initiales et former le JOPM}

    $\Omega^{\mathcal{T}^j}_0 \gets \{\omega^j_0\}$ extraites des historiques $\mathcal{D}_{H^j}$

    $\mathcal{T}^j(h_{t-1}, \omega_t, a_t) = \langle f(h_{t-1}, Enc(\omega^j_t), a^j_t), Dec(\mathcal{T}^{z}(h_{t-1}, Enc(\omega^j_t), a^j_t)) \rangle$ \;

    \vspace{0.5em}
    \tcp{6. Retourner les éléments modélisés}
    \Return{$\mathcal{T}^j, \Omega^{\mathcal{T}^j}_0, R^j_H, \mathcal{MM}$}
\end{algorithm}


\chapter{Entraînement des politiques sous contraintes}
\label{chap:training}

L'\textbf{activité d'entraînement} traite les éléments formels suivants :
%
\begin{displaymath}
    \pi^j_i \gets \texttt{train}(\mathcal{T}^j, \Omega^{\mathcal{T}^j}_0, \gamma, R^j_H, \Omega, A, \mathcal{MM})
\end{displaymath}

L'\textit{activité d'entraînement} vise à résoudre le problème de conception modélisé comme un problème d'optimisation sous contraintes en s'appuyant sur le cadre MOISE+MARL. Cependant, une limite importante réside dans le fait que MOISE+MARL repose sur le formalisme \ac{Dec-POMDP}, qui suppose un accès complet à l'état réel de l'environnement. En pratique, notre approche se fonde uniquement sur les données observables (à savoir les historiques conjoints des agents) sans accès aux états internes. Pour combler cet écart, un nouveau formalisme markovien est introduit, opéré sur des séquences observables via le \ac{JOPM}, tout en restant compatible avec les cadres algorithmiques \ac{MARL} existants.

\subsubsection{Extension de MOISE+MARL aux World Models Multi-Agents}

\noindent Dans des environnements réalistes, on ne dispose que des transitions issues des historiques d'actions et d'observations reçues. Pour mieux représenter ce contexte, nous introduisons un nouveau formalisme appelé \textbf{Dec-POMDP basé sur les observations} (\acparen{ODec-POMDP}).

Un \ac{ODec-POMDP} $d_\Omega \in OD_\Omega$ (où $OD_\Omega$ désigne l'ensemble des ODec-POMDPs) est défini comme un quintuplet :
%
$d_\Omega = \left(\Omega, A, \mathcal{T}^j, R^j_H, \gamma \right)$
%
où :
\begin{itemize}
    \item $A$ : l'espace d'actions.
    \item $\Omega$ : l'espace d'observations.
    \item $\Omega^{\mathcal{T}^j}_0$ : l'ensemble des observations initiales conjointes enregistrées.
    \item $\mathcal{T}^j(h, \omega, a) = \langle \tilde{h}', \mathbb{P}(\omega' \mid h, \omega, a) \rangle$ : le JOPM estimant la prochaine observation conjointe $\omega'$ à partir de l'historique $\tilde{h} \in \mathcal{H}$, de l'observation conjointe actuelle $\omega$ et de l'action conjointe $a$. Le modèle renvoie également l'état caché récurrent mis à jour $\tilde{h}'$.
    \item $R^j_H : H \times \Omega \times A \times \Omega \rightarrow \mathbb{R}$ : la fonction de récompense basée sur l'historique, calculant la récompense à partir de l'historique précédent, de l'observation utilisée, de l'action et de l'observation suivante.
    \item $\gamma \in [0, 1]$ : le facteur d'actualisation.
\end{itemize}

\noindent Cette formulation permet aux agents MARL d'opérer uniquement à partir de données observables, rendant la méthode compatible avec les environnements simulés appris. Étant donné la proximité conceptuelle entre Dec-POMDP et \ac{ODec-POMDP}, nous les regroupons sous une notation commune \textbf{O$\backslash$Dec-POMDP}.

\paragraph{\textbf{Résolution d'un ODec-POMDP avec MOISE+MARL}}

Résoudre un \ac{ODec-POMDP} avec des contraintes $mm \in \mathcal{MM}$ consiste à trouver une politique conjointe $\pi^j = \{\pi^j_0, \pi^j_1, \dots, \pi^j_n\}$ qui maximise la récompense cumulée espérée (ou qui satisfait un seuil minimal), via la fonction de valeur basée sur les observations $V_{\mathcal{T}^j}^{\pi^j}$. Cette fonction représente le retour attendu depuis une observation conjointe initiale $\omega^j \in \Omega^{\mathcal{T}^j}_0$, un historique $h^j$ et un état caché $\tilde{h}$, en appliquant des actions conjointes $a^j \in A^n$ sous contraintes organisationnelles $\mathcal{MM}$, et en utilisant $\mathcal{T}^j$ pour approximer les transitions.

La définition complète de $V_{\mathcal{T}^j}^{\pi^j}$ est donnée dans \hyperref[eq:single_value_function]{Définition 2}, et intègre les adaptations basées sur les rôles (en rouge) et sur les missions (en bleu), qui influencent à la fois l'espace d'actions conjointes et la récompense. La \autoref{fig:mm_synthesis} illustre comment les spécifications $\mathcal{M}OISE^+$ sont injectées dans la résolution d'un \ac{ODec-POMDP} à l'aide du cadre MOISE+MARL.

\begin{figure*}[h!]
    \label{eq:single_value_function}
    \raggedright
    \textbf{\textit{Définition 2} \quad Fonction de valeur observationnelle avec guidages en mode parallèle :}

    \begin{scriptsize}
        % traduction déjà fournie dans ton document LaTeX, à conserver tel quel car mathématique
    \end{scriptsize}
\end{figure*}

\noindent En mode parallèle, à chaque pas de temps $t \in \mathbb{N}$ (en commençant à $t=0$), chaque agent $i \in \mathcal{A}$ est assigné à un rôle $\rho_i = ar(i)$. Pour chaque spécification déontique temporellement valide $d_i = rds(\rho_i) = \langle tc_i, y_i, m_i \rangle$, l'agent est soit autorisé ($y_i = 0$), soit obligé ($y_i = 1$) de s'engager dans la mission $m_i \in \mathcal{M}$, avec ensemble d'objectifs $\mathcal{G}_{m_i} = mo(m_i)$.

Lorsque les agents observent $\tilde{\omega}_t^j$, ils sélectionnent leurs actions dans $A_{i,t}$ (dérivées via les guides de récompense de rôle) avec une probabilité $ch_t$, ou dans $A_t$ sinon. Si $ch_t = 1$, les agents sont strictement contraints par leur rôle.

Les transitions d'observation et d'état sont approximées via la fonction $\mathcal{T}^j$ à partir de l'état caché précédent $\tilde{h}_{t-1}$, de l'observation conjointe $\omega^j_t$ et de l'action conjointe $a^j_t$. La fonction de récompense $R^j_H$ utilise ces mêmes informations, ainsi que l'observation suivante, pour produire la récompense. Des bonus ou malus sont ensuite ajoutés selon :
i) l'atteinte d'objectifs valides (via les guides de récompense des objectifs, pondérés par $\frac{1}{1 - p + \epsilon}$),
ii) la conformité au rôle (via les guides de récompense de rôle, pondérés par $1 - ch_t$).

\begin{algorithm}[H]
    \caption{Algorithme de l'activité d'entraînement}
    \label{alg:training_mamad}
    \DontPrintSemicolon

    \KwIn{
        Modèle de Prédiction d'Observations Conjointes (JOPM) $\mathcal{T}^j$,
        Observations initiales conjointes $\Omega_0^{\mathcal{T}^j}$,
        Fonction de récompense $R_H^j$,
        Spécifications organisationnelles $\mathcal{MM}$,
        Facteur d'actualisation $\gamma$
    }
    \KwOut{$\pi^j$ : Politique conjointe entraînée}

    \vspace{0.3em}

    Initialiser les paramètres de la politique $\pi^j$ et du buffer de replay $\mathcal{B}$ \;

    \ForEach{épisode $e = 1 \dots N$}{
    Échantillonner $\omega_0^j \sim \Omega_0^{\mathcal{T}^j}$, initialiser $\tilde{h}_{-1} \gets \mathbf{0}$ \;
    Initialiser l'historique $h_{-1}^j \gets \emptyset$ \;

    \ForEach{étape $t = 0 \dots T$}{
    Calculer $A_t^j = rag^j(h^j_t, \omega^j_t)$ via les guides de récompense de rôle dans $\mathcal{MM}$ \;
    \If{$rn() < ch_t$}{
        Sélectionner $a_t^j \sim \pi^j(\cdot | \omega_t^j)$ dans l'ensemble $A_t^j$ (contraint) \;
    }
    \Else{
        Sélectionner $a_t^j \sim \pi^j(\cdot | \omega_t^j)$ dans l'ensemble $A_t$ \;
    }

    $(\tilde{h}_t, \omega_{t+1}^j) \gets \mathcal{T}^j(\tilde{h}_{t-1}, \omega_t^j, a_t^j)$ \tcp*{Prédiction JOPM}

    $r_t \gets \gamma^t \times R_H^j(h^j_{t-1}, \omega_t^j, a_t^j, \omega_{t+1}^j)$ \tcp*{Récompense de base}

    $r_t \gets r_t + grg^j(h^j_t)$ \tcp*{Bonus via guides d'objectifs}

    $r_t \gets r_t + (1 - ch_t) \times rrg^j(h^j_t, \omega_t^j, a_t^j)$ \tcp*{Bonus/malus via guides de rôle}

    Ajouter $(\omega_t^j, a_t^j, r_t, \omega_{t+1}^j)$ à $\mathcal{B}$ \;
    Mettre à jour $h^j_t \gets \langle \langle h_{t-1,i}, \omega_{t,i}, a_{t,i} \rangle \rangle_{i \in \mathcal{A}}$ \;

    Entraîner $\pi^j$ avec des mini-lots tirés de $\mathcal{B}$ en utilisant toute méthode MARL \;
    }
    }

    \Return{$\pi^j$}
\end{algorithm}



\chapter{Analyser et interpréter les comportements émergents}
\label{chap:analyzing}

\noindent L'\textbf{activité d'analyse} traite les éléments formels suivants :
\[
    (\mathcal{MM}_{i,\text{implicit}}, \text{OF}) \gets \texttt{analyze}(d_i, \mathcal{MM}_i, \pi^j_i, d_\Omega)
\]

\noindent Cette phase poursuit deux objectifs : (i) fournir une explication de la politique conjointe apprise en termes de spécifications organisationnelles MOISE+MARL (rôles, objectifs, missions), et (ii) calculer l'"organizational fit", c'est-à-dire l'alignement entre les comportements appris et une organisation régulière, explicite ou implicite.

Pour cela, nous nous appuyons sur la méthode \textbf{TEMM}~\cite{soule2025moisemarl}. \ac{TEMM} repose sur l'hypothèse que les comportements des agents, malgré une variabilité apparente, présentent des régularités lorsqu'ils atteignent des récompenses cumulées comparables. Ainsi, des comportements différents peuvent être interprétés comme des variantes bruitées d'un nombre limité de stratégies latentes. D'après la loi des grands nombres, une moyenne sur un ensemble suffisant d'historiques conjoints réussis permet de filtrer le bruit et de révéler des stratégies typiques.

Les trajectoires d'observations sont regroupées en clusters à l'aide de métriques de distance (par exemple \acparen{LCS}, Smith-Waterman), formant des groupes d'agents aux comportements similaires. Pour chaque cluster, une trajectoire centroïde est calculée, associant chaque pas de temps à une observation moyenne. On évalue la \textbf{représentativité} comme l'inverse normalisé de la variance locale pour chaque pas. Une forte représentativité indique une observation moyenne proche de celles réellement rencontrées, tandis qu'une faible représentativité indique des observations éparses ou peu fréquentes. La \textbf{variance globale} est également calculée sur l'ensemble du centroïde.

En utilisant un seuil minimal de représentativité, un mécanisme de sélection identifie les observations les plus saillantes de chaque trajectoire (celles les plus fréquemment visitées par les agents lors des exécutions réussies). Ces observations sont regroupées en \textbf{objectifs intermédiaires}, représentant des jalons importants à atteindre en vue de l'objectif global. Ces ensembles d'observations représentatives constituent la base de l'inférence des objectifs. Si la représentativité minimale est élevée, seules les observations très fréquentes seront retenues, assurant une meilleure robustesse. À l'inverse, une représentativité faible peut mener à une surspécification, regroupant des observations rares et peu significatives.

De manière similaire, les trajectoires $(\omega, a) \in \Omega \times A$ sont regroupées pour l'inférence des rôles, éventuellement après encodage one-hot des actions. Chaque cluster donne lieu à un centroïde de transitions moyennes par pas de temps. Une procédure de sélection retient les transitions les plus représentatives, interprétées comme des \textbf{règles comportementales} associées à un rôle. Une faible représentativité conduit à inclure toutes les transitions, au risque d'un sur-apprentissage.

Pour quantifier l'adéquation entre la politique apprise et une organisation implicite, on calcule le \textbf{fit organisationnel} selon deux composantes :
\begin{itemize}
    \item \textbf{\ac{SOF}} : mesure la régularité comportementale des agents par rôle, calculée comme l'inverse normalisé de la variance globale dans les clusters de transitions. Une faible variance indique une forte cohérence structurelle.
    \item \textbf{\ac{FOF}} : évalue la cohérence fonctionnelle des agents dans l'atteinte d'objectifs intermédiaires, calculée comme l'inverse normalisé de la variance dans les clusters d'observations.
\end{itemize}

\noindent Le \textbf{fit organisationnel global} est défini comme la moyenne de ces deux scores : $\text{OF} = \frac{1}{2} \left( \text{SOF} + \text{FOF} \right)$.

Un fit organisationnel élevé indique que les spécifications inférées (rôles et objectifs) sont représentatives des comportements réellement appris. Un fit faible suggère au contraire une faible structuration ou des comportements peu cohérents.

Un problème majeur rencontré avec \ac{TEMM} est la nécessité de choisir manuellement plusieurs hyperparamètres (métriques de distance, seuils de clustering, seuils de représentativité), ce qui ralentit le processus d'analyse et limite son automatisation. Une représentativité trop faible conduit à du sur-apprentissage, tandis qu'une représentativité trop élevée limite les contraintes et ralentit la convergence.

\subsubsection{Méthode TEMM étendue avec optimisation des hyperparamètres}

Pour surmonter cette difficulté, nous proposons un processus d'\textbf{optimisation d'hyperparamètres} (\acparen{HPO}) consistant en une recherche par grille (grid search) sur les combinaisons possibles, visant à maximiser le fit organisationnel et minimiser le nombre de clusters :

\begin{itemize}
    \item (i) Pour les observations et les transitions, appliquer une recherche conjointe sur les métriques de distance et les seuils de clustering afin de minimiser la variance intra-cluster et le nombre de clusters ;
    \item (ii) Déterminer les représentativités minimales (structurelle et fonctionnelle) pour obtenir des objectifs et des rôles concis et pertinents. Comme illustré en \autoref{fig:conv_time_repr}, diminuer cette représentativité augmente la couverture mais réduit la robustesse des contraintes organisationnelles. Une valeur élevée limite la généralisation, tandis qu'une valeur trop faible entraîne un sur-apprentissage.
\end{itemize}

Nous adoptons un compromis basé sur le \textbf{point de coude} du graphique convergence/temps (voir \autoref{fig:conv_time_repr}), en choisissant la plus grande représentativité assurant une convergence normalisée de 3.5\%. Cette stratégie permet d'obtenir des spécifications utiles, interprétables et généralisables, sans complexité excessive.

\begin{figure}[h!]
    \centering
    \includegraphics[trim=0cm 0cm 0cm 0cm, clip, width=1.\linewidth]{figures/convergence_time_relative_to_representativeness.pdf}
    \caption{Temps de convergence normalisé en fonction de la représentativité minimale}
    \label{fig:conv_time_repr}
\end{figure}

\begin{algorithm}[H]
    \caption{Algorithme de l'activité d'analyse}
    \label{alg:auto_temm}
    \DontPrintSemicolon

    \KwIn{
        Politique conjointe entraînée $\pi^j$ ;
        ODec-POMDP $d_\Omega$ ;
        Spécification initiale $\mathcal{MM}$ ;
        Seuil de convergence normalisé (défaut : 3.5\%) $\eta$
    }

    \KwOut{
    Spécification organisationnelle inférée $\mathcal{MM}_{\text{implicit}}$ ;
    Score de fit organisationnel $\text{OF}$
    }

    \tcp*[l]{1. Collecte des trajectoires}
    Générer les historiques individuels $\mathcal{D}_{\text{trans}}$ depuis $d_\Omega$ sous $\pi^j$ \;
    $\mathcal{D}_{\text{obs}} \gets$ trajectoires d'observations individuelles issues de $\mathcal{D}_{\text{full}}$ \;

    \tcp*[l]{2. HPO sur distance et seuil de clustering}
    \For{$t \in \{obs, trans\}$}{
        \ForEach{métrique de distance $\delta_t$}{
            \ForEach{seuil minimal de cluster $\tau_t$}{
                Appliquer clustering avec $(\delta_t, \tau_t)$ \;
                Calculer $\sigma_{\text{obs}}, \sigma_{\text{trans}}, N_{\text{clusters}}$ \;
                Score $\gets \alpha (\sigma_{\text{obs}} + \sigma_{\text{trans}}) + \beta N_{\text{clusters}}$ \tcp*[l]{par défaut : $\alpha=0.4$, $\beta=0.6$}
                Retenir $(\delta_t^*, \tau_t^*)$ avec Score minimal \;
            }
        }
    }

    \tcp*[l]{3. Application du clustering avec HPO optimal}
    Clustering des observations : $\mathcal{D}_{\text{obs}} \rightarrow C_{obs}$ via $(\delta_{obs}^*, \tau_{obs}^*)$ \;
    Clustering des transitions : $\mathcal{D}_{\text{trans}} \rightarrow C_{trans}$ via $(\delta_{trans}^*, \tau_{trans}^*)$ \;

    \tcp*[l]{4. HPO sur la représentativité (convergence)}
    \For{$t \in \{obs, trans\}$}{
    \ForEach{représentativité $\rho_t$}{
    Inférer $\mathcal{MM}_{\rho_t}$ à partir des clusters \;
    Initialiser une politique $\pi^j_{\rho_t}$ \;
    Entraîner $\pi^j_{\rho_t}$ sur $(d_\Omega, \mathcal{MM}_{\rho_t})$ jusqu'à atteindre $R_{\min}$ \;
    Enregistrer le temps de convergence $c_{\rho_t}$ tel que $ct_t(\rho_t) = c_{\rho_t}$ \;
    }

    \tcp*[l]{5. Sélectionner le point de coude}
    $\rho_t^* \gets max(\{\rho_t \ | \ ct_t(\rho_t) < \eta \})$ \tcp*[r]{par défaut $\eta = 3.5\%$}
    }

    \tcp*[l]{6. Inférence finale des rôles et objectifs}
    Inférer les rôles à partir de $\mathcal{D}_{\text{trans}}, \delta^*, \tau^*, \rho^*$ \;
    Inférer les objectifs à partir de $\mathcal{D}_{\text{obs}}, \delta^*, \tau^*, \rho^*$ \;

    \tcp*[l]{7. Calcul du fit organisationnel}
    Calculer SOF (structurel) et FOF (fonctionnel) à partir des variances intra-cluster \;
    $\text{OF} \gets \frac{1}{2}(\text{SOF} + \text{FOF})$ \;

    \Return{$\mathcal{MM}_{\text{implicit}}, \text{OF}$}
\end{algorithm}



\chapter{Transférer et superviser en environnement réel}
\label{chap:transferring}

\noindent L'\textbf{activité de transfert} traite les éléments formels suivants :
\[
    \mathcal{D}_{H^j}, \texttt{need\_update} \gets \texttt{transfer}(\pi^j_{\text{latest}}, \mathcal{E}, \mathcal{D}_{H^j})
\]

L'\textit{activité de transfert} a deux objectifs principaux : (1) déployer en continu la politique conjointe la plus récente $\pi^j_{\text{latest}}$ dans l'environnement réel $\mathcal{E}$ afin d'assurer l'action et l'interaction efficaces des agents ; et (2) collecter de nouvelles trajectoires réelles $(\omega^j_t, a^j_t, \omega^j_{t+1})$ pour enrichir l'ensemble de trajectoires $\mathcal{D}_{H^j}$ utilisé pour mettre à jour l'environnement simulé et les spécifications organisationnelles.

À notre connaissance, aucun framework n'offre à la fois un déploiement asynchrone des politiques et une collecte automatique des historiques conjoints, déclenchée par seuil, dans une boucle fermée capable de se synchroniser avec une chaîne d'apprentissage. Cette absence limite fortement l'automatisation du cycle de conception des \ac{SMA} en environnement réel.

\paragraph{Cadre de transfert}

Nous proposons un cadre théorique général mettant en œuvre un système de contrôle asynchrone et événementiel, responsable de l'exécution des politiques et de la collecte des traces. Il maintient un tampon de trajectoires, déclenche la phase de réentraînement lorsqu'un seuil de taille est atteint, et veille à ce que la politique la plus récente soit toujours utilisée. Cette logique de contrôle repose sur deux mécanismes :
(i) une \texttt{Boucle de transfert} (\texttt{Transfer Loop}) pour le déploiement temps réel et la collecte de données, et
(ii) un \texttt{Déclencheur de mise à jour} (\texttt{Update Trigger}) qui lance le processus de conception dès qu'assez de données sont disponibles. Le système empêche les mises à jour parallèles multiples et garantit la synchronisation entre les phases de transfert et de modélisation.

\vspace{-0.3em}
\begin{algorithm}[H]
    \caption{Activité de transfert}
    \label{alg:transferring}
    \DontPrintSemicolon
    \KwIn{Politique actuelle $\pi^j_{\text{latest}}$, environnement réel $\mathcal{E}$, base de trajectoires $\mathcal{D}_{H^j}$}
    \KwOut{Base de trajectoires mise à jour $\mathcal{D}_{H^j}$, signal de mise à jour $\texttt{need\_update}$}

    \vspace{0.3em}
    \SetKwProg{Transfer}{Procédure \normalfont BoucleDeTransfert}{}{}
    \Transfer{}{

    \While{le SMA est actif dans l'environnement $\mathcal{E}$}{
    \tcp*[l]{Exécution de la politique la plus récente}
    $\omega^j_t \gets \texttt{observe}(\mathcal{E})$ \;
    $a^j_t \gets \pi^j_{\text{latest}}(\omega^j_t)$ \;
    $\omega^j_{t+1} \gets \texttt{apply}(\mathcal{E}, a^j_t)$ \;
    Ajouter $(\omega^j_t, a^j_t, \omega^j_{t+1})$ au tampon temporaire $\mathcal{B}$ \;

    \tcp*[l]{Vérification du déclenchement de la mise à jour}
    \If{$|\mathcal{B}| \geq \texttt{batch\_size}$}{
        Ajouter $\mathcal{B}$ à $\mathcal{D}_{H^j}$ et vider $\mathcal{B}$ \;
        $\texttt{need\_update} \gets \texttt{True}$ \;
        \If{$\texttt{running\_update} = \texttt{False}$}{
            \texttt{launch\_update()} \tcp*[r]{Appel asynchrone}
        }
    }
    }
    }
\end{algorithm}

Ce mécanisme garantit (i) la continuité d'exécution, (ii) la réactivité à l'arrivée de nouvelles données, et (iii) l'automatisation des cycles de mise à jour. Il constitue le lien entre le monde simulé et le contexte réel de déploiement en ajustant en continu la boucle de conception à l'évolution de l'environnement, condition essentielle pour l'autonomie à long terme d'un \ac{SMA}.



\chapter*{Conclusion}
\addcontentsline{toc}{chapter}{\textbf{Conclusion}}

\noindent
Cette troisième partie a introduit la méthode \textbf{\ac{MAMAD}} comme une réponse concrète aux limites identifiées dans les approches actuelles de conception de \ac{SMA}. Reposant sur un cycle itératif structuré en quatre phases — \textit{Modélisation}, \textit{Apprentissage}, \textit{Analyse}, \textit{Transfert} —, \ac{MAMAD} articule de manière cohérente des outils symboliques (spécifications organisationnelles) et \ac{MARL} pour guider la conception, l'entraînement et l'adaptation d'agents intelligents dans des environnements complexes.

\medskip

\noindent
La méthode s'appuie notamment :
\begin{itemize}
    \item sur une modélisation fidèle des environnements à partir de données empiriques,
    \item sur un entraînement contraint par des spécifications organisationnelles intégrées au sein du cadre \textit{MOISE+MARL},
    \item sur une analyse des trajectoires pour inférer les structures émergentes de l'organisation apprise,
    \item et enfin sur un transfert contrôlé permettant l'amélioration itérative du \ac{SMA}.
\end{itemize}

\noindent
Dans la partie suivante, nous proposons de valider expérimentalement cette méthode à travers son implémentation concrète dans un outil dédié, \ac{CybMASDE}, et son application à plusieurs environnements représentatifs. L'objectif est de démontrer la capacité de \ac{MAMAD} à produire automatiquement des \ac{SMA} performants, explicables et conformes à des exigences organisationnelles dans des contextes variés.

\medskip

\noindent
Nous évaluons notamment la méthode selon des critères d'efficacité, d'automatisation, de conformité aux contraintes et d'explicabilité, tout en comparant ses résultats à ceux d'approches classiques non guidées par des modèles organisationnels.

\bigskip

La \autoref{part:experimentation} met donc à l'épreuve la méthode \ac{MAMAD}, en analysant ses performances et sa pertinence au regard des verrous identifiés sur différents scénarios.
