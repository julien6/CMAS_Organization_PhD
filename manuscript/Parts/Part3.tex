\clearpage
\thispagestyle{empty}
\null
\newpage

\cleardoublepage
\phantomsection
% \pdfbookmark[1]{La méthode MAMAD}{La méthode MAMAD}
\markboth{\spacedlowsmallcaps{La méthode MAMAD}}{\spacedlowsmallcaps{La méthode MAMAD}}
\part{La méthode MAMAD}
\label{part:methode}

\clearpage
\thispagestyle{empty}
\null
\newpage

% todo : Initialement, dans la partie I, notre problème était un problème de conception logiciel d'un \acn{SMA} de Cyberdéfense capable d'assurer la Cyberdéfense de façon optimale compte tenu des contraintes dynamique de l'environnement et des concepteur. Nous avons formalisé le fait que pour répondre à cette question de recherche globale, il faut répondre simultanément à 6 critères (Autonomie - C1, Performance - C2, Adaptation - C3, Contrôle - C4, Explicabilité - C5, Robustesse - C6). Nous avons donc poursuivi une première revue de littérature qui nous a permis de comprendre qu'assez peu de travaux s'intéressent à une approche multi-agent pour la Cyberdéfense et que les travaux qui couvrent le plus de critères sont à chercher du côté de l'approche connexioniste qui favorise la performance et l'adaptation là où l'approche purement symbolique favorise l'explicabilité et le controle. Fort de ce constat, nous avons proposé de voir la question de recherche globale au travers le prisme d'un problème d'optimisation sous contraintes où la politique conjointe est à optimiser pour maximiser une récompense qui encode le succès dans l'atteinte d'un objectif de Cyberdéfense (ou un conglomérat d'objectifs) et où les contraintes sont formalisées comme des spécifications organisationnelles. A partir de cette formalisation, la complexité de la question de recherche globale apparait abordable au travers de 4 activités : Modélisation, Entrainement, Analyse et Transfert. On sait pour chaque activité les données en entrée, les données en sortie attendues et donc aussi les objectifs de chaque activité.
% A partir de cette vision, la partie \acn{II} présente les travaux qui couvrent les objectifs pour chaque activité, permettant ainsi de savoir quels sont les domaines/sous-domaines ou travaux qui semble le plus adaptés (ou qui nécéssitent le moins de contributions supplémentaires) pour atteindre les objectifs de chaque activités. Compte tenu des travaux identifiés comme les plus prometteurs, on en déduit donc les verrous qui restent encore à combler par de nouvelles contributions pour atteindre les objectifs de chaque activités.
% La partie \acn{III}, présente la méthode qui orchestre les 4 activités et explique dans chacune des activités comment nous avons combler les verrous par de nouvelles contributions. Dans cette partie, une présentation générale de la méthode qui orchestre les 4 activités est d'abord donnée. Ensuite, nous détaillons chaque activité en rappellant rapidement les objectifs et chacun des verrous associés et pour chacun nous détaillons notre contribution en les justifiant. Une fois ceci fait, nous donnons une représentation alogrithmique de l'activité qui sert de support pour détailler de façon formelle l'activité en explicitant chacun des éléments formels (ensemble, élément d'un ensemble, relation...). A la fin de l'activité, on fait une sorte de bilan en expliquant ce qu'on pense qui sera couvert en termes d'objectifs attendus, ce qui l'est moyennement et pourquoi et ce qui n'est pas du tout couvert.
% La partie \acn{IV} concerne la validation expérimentale de la méthode par la mise en application de cette méthode au travers de trois cas d'études non-orientés Cyberdéfense et de trois cas d'études orientés Cyberdéfense. L'objectif est de vérifier que les 6 critères (Autonomie - C1, Performance - C2, Adaptation - C3, Contrôle - C4, Explicabilité - C5) sont bien couverts. Pour cela, on applique une grille d'évaluation qui associe chaque critères à un ensemble de métriques mesurables et qui peuvent être analysé pour voir si le critère est couvert ou pas. Comme la grille de lecture est commune aux 6 environnements, on peut donc vérifier de manière plus consistante et générique si la méthode permet bien de couvrir les 6 critères. La partie finit donc avec une analyse globale de si la méthode couvre bien les 6 critères posés initialement.

% \acn{TODO}:
%  - Globalement, harmoniser le vocabulaire
%  - Globalement, introduire correctement les termes techniques/théoriques comme "Adéquation organisationel", "\acn{RNN}", "\acn{VAE}", "\acn{LSTM}", "World Models" notamment pour permettre à un lecteur non familier avec le \acn{ML} ou l'\acn{IA} en général de comprendre.
%  - Dans les chapitres concernant les activités de "Modélisation", "Entrainement", "Analyse" et "Transfert", il faudra veiller à bien expliciter les objectifs de chacune des activités, les hypothèses qui délimitent notre espace de recherche dans la littérature, les travaux les plus susceptibles d'atteindre les objectifs de l'activité (par exemple : le Constrained-\acn{RL}, Shielding...) et les verrous associés restants à relever (par exemple : le manque de moyen permettant de guider l'apprentissage avec un modèle organisationel) par de nouvelles contributions. Ensuite, on présente ces différentes contributions (par exemple : le framework MOISE+MARL). Ensuite, on peut présenter l'activité sous la forme d'un algorithme qui articule les travaux identifiés et/ou contributions pour répondre aux objectifs de l'activité. Il faudra veiller à bien expliciter en détail cet algorithme (en référençant chacun des éléments formel de l'algorithme) pour donner une description la plus complète possible de l'activité. Pour finir, on doit dire en quoi nous pensons que cette implémentation de l'activité permet bien de répondre aux objectifs de l'activité ou pas. Plus loin, on peut même dire si nous pensons que les critères associé à une activité sont bien couvertes avec cette implémentation actuelle de l'activité.

\chapter*{Introduction}
\addcontentsline{toc}{chapter}{\textbf{Introduction}}

\noindent

La partie précédente a mis en lumière les lacunes actuelles dans l'intégration des modèles organisationnels au sein des approches d'apprentissage multi-agent, tant du point de vue du contrôle, de l'explicabilité que de l'automatisation de la conception. Elle a également mis en lumière les lacunes dans la modélisation de l'environnement et son intégration dans le processus d'entraînement notamment sur le manque de cadre permettant d'assurer la cohérence entre l'environnement simulé et réel.

\medskip

\noindent
Cette troisième partie présente notre proposition pour répondre à ces lacunes~: la méthode \acn{MAMAD}\index{MAMAD}. Cette méthode repose sur la prémisse que la conception d'un \acn{SMA} peut être abordée par le prisme d'un problème d'optimisation sous contraintes. La méthode est construite autour de cette vision et s'organise donc autour de quatre activités~:

\begin{enumerate}
  \item \textbf{Modélisation}~: modéliser l'environnement réel en un environnement simulé ainsi que les contraintes de conceptions en spécifications organisationnelles~;
  \item \textbf{Apprentissage}~: entraîner les agents dans cet environnement simulé en tenant compte de spécifications organisationnelles comme des rôles durant l'apprentissage~;
  \item \textbf{Analyse}~: extraire des spécifications structurelles et fonctionnelles émergentes à partir des trajectoires des agents entrainés~;
  \item \textbf{Transfert}~: mettre à jour régulièrement les politiques des agents déployés dans l'environnement réel à partir des politiques des agents entrainés en simulation et éventuellement mettre à jour ou améliorer l'environnement simulé.
\end{enumerate}

\noindent
Ces quatres activités peuvent être vues comme exécutées de façon itérative pour produire des \acplu{SMA} adaptés à leur environnement, alignés sur des contraintes organisationnelles, explicables et robustes.

\medskip

\noindent
Le \autoref{chap:mamad_global} donne une description globale de la méthode concernant les processus proposés. Les quatre chapitres restants détaillent chacune des étapes de cette méthode~:
Le \autoref{chap:modelling} présente l'activité de modélisation.
Le \autoref{chap:training} présente l'activité d'apprentissage contraint par des spécifications organisationnelles.
Le \autoref{chap:analyzing} présente une méthode permettant d'analyser des trajectoires pour inférer des structures organisationnelles émergentes.
Le \autoref{chap:transferring} décrit l'activité de transfert.
La \autoref{fig:organisation_manuscrit_partie_3} illustre cette organisation de cette partie.

La méthode \acn{MAMAD} ambitionne de réunir les forces des approches symboliques et connexionnistes pour une conception de \acn{SMA} à la fois structurée, autonome et explicable.

\begin{figure}[h!]
  \centering
  \resizebox{0.7\linewidth}{!}{%
    \input{figures/organisation_manuscrit_partie_3}
  }
  \caption{Structure de la Partie III~: La méthode MAMAD}
  \label{fig:organisation_manuscrit_partie_3}
\end{figure}



\clearpage
\thispagestyle{empty}
\null
\newpage

\chapter{Présentation globale de la méthode}
\label{chap:mamad_global}

La méthode \acn{MAMAD}~\footnotemark[2] repose sur quatre grandes activités~: (1) la modélisation de l'environnement, de l'objectif global et des contraintes organisationnelles, (2) l'apprentissage des politiques à l'aide de divers algorithmes \acn{MARL}, (3) l'analyse des comportements et l'inférence des spécifications organisationnelles à l'aide d'une méthode proposée, et (4) le maintien de la cohérence entre l'environnement simulé et l'environnement réel en déployant les politiques entraînées et en mettant à jour la simulation. Cette approche guide le processus d'apprentissage des agents tout en imposant des contraintes organisationnelles strictes, garantissant ainsi l'efficacité des politiques apprises.

Le cycle de vie d'un \acn{SMA} conçu avec \acn{MAMAD} est illustré en \autoref{fig:cycle}. Il commence par la modélisation de l'environnement, réalisée à partir d'un ensemble suffisant de trajectoires réelles (issues d'agents déjà transférés ou de toute autre source disponible), ainsi que la définition de l'objectif global et des contraintes de conception sous forme de rôles et d'objectifs. Ensuite, les agents sont entraînés dans cet environnement simulé à l'aide de techniques d'apprentissage par renforcement multi-agent (\acn{MARL}). Une fois l'entraînement terminé, une analyse post-entraînement permet d'extraire les rôles et objectifs émergents des agents, ce qui conduit à l'amélioration des spécifications organisationnelles appliquées. Enfin, après validation, les politiques apprises sont déployées pour contrôler les actionneurs de l'environnement, générant ainsi de nouvelles traces qui serviront à affiner la modélisation lors des itérations suivantes.

\begin{figure}[h!]
  \centering
  \input{figures/cycle}
  \caption{Cycle de vie d'un SMA conçu avec MAMAD}
  \label{fig:cycle}
\end{figure}

Le coeur de la méthode \acn{MAMAD} est d'envisager la conception d'un \acn{SMA} comme un processus itératif d'optimisation sous contraintes. Nous proposons une description formalisée de la méthode \acn{MAMAD} dans l'\autoref{alg:mamad} qui met en perspective les activités évoquées précédemment. Les données en entrée sont~:
\begin{itemize}
  \item $\mathcal{E}_0$~: l'environnement initial dans lequel les agents peuvent agir~;
  \item $\mathcal{G}_{\text{inf}}$~: une description informelle de l'objectif global recherché~;
  \item $\mathcal{C}_{\text{inf}}$~: une spécification informelle des contraintes de conception~;
  \item $\gamma \in [0,1]$~: le facteur d'actualisation définissant une solution à court ou long terme, généralement fixé empiriquement (par défaut à 1)~;
  \item $A, \Omega$~: respectivement les espaces d'actions et d'observations~;
  \item $\texttt{org\_fit}_{min}$, $\overline{r}_{min}$, $\sigma_{max}$~: les valeurs minimales exigées pour le score d'adéquation organisationnelle, la récompense moyenne, et la valeur maximale pour l'écart-type, servant à valider une politique conjointe. Ces seuils sont généralement déterminés empiriquement, à partir d'une analyse préliminaire des résultats obtenus lors d'une première série d'épisodes.
\end{itemize}

L'adéquation organisationnelle est introduit de façon théorique comme un indicateur quantitatif que nous théorisons et qui est compris entre 0 et 1 et qui mesure dans quelle mesure les comportements des agents sont structurés et conformes à des spécifications organisationnelles (qu'elles soient implicites ou explicites). Une valeur proche de 1 indique que les agents adoptent des comportements réguliers, stables et fortement alignés avec une organisation définie. À l'inverse, une valeur proche de 0 signifie que les comportements sont très irréguliers et qu'aucun schéma organisationnel cohérent n'émerge. En résumé, l'adéquation organisationnelle évalue la conformité d'une politique conjointe à une organisation structurée et fonctionnelle.
La récompense moyenne $\overline{r}$ et l'écart-type $\sigma$ sont des métriques classiques en apprentissage par renforcement, reflétant respectivement la performance globale et la stabilité d'une politique.


La méthode \acn{MAMAD} propose un cadre méthodologique permettant une conception continue du \acn{SMA} via la coordination itérative et asynchrone de deux processus distincts~: le \textit{processus de Transfert}, qui est connecté à l'environnement réel et gère l'exécution en temps réel et la collecte d'historiques conjoints~; et le \textit{processus \acn{MTA}}, qui consomme les données stockées pour améliorer itérativement le modèle simulé, la politique conjointe et les spécifications organisationnelles du \acn{SMA}.

\paragraph{Processus de Transfert~: déploiement des politiques et collecte de données}

Ce processus qui consiste à maintenir la cohérence entre l'environnement simulé et l'environnement est actif en continu tant que le \acn{SMA} est en fonctionnement dans l'environnement réel. Il a deux rôles~: déployer la politique conjointe la plus récente $\pi^j_{\text{latest}}$ auprès des agents déployés dans l'environnement cible, garantissant un comportement à jour sans interruption~; et collecter en continu les trajectoires des agents sous forme d'historiques conjoints $H^j$, stockés par lots. Une fois qu'un nombre suffisant de trajectoires est collecté, le lot est ajouté au dépôt global $\mathcal{D}_{H^j}$. Si le processus de mise à jour n'est pas en cours, il déclenche le lancement du processus \acn{MTA}.

% Rajouter des commentaires sur chaque activité

\begin{algorithm}[H]
  \caption{Conception de SMA assistée par MOISE+MARL}
  \label{alg:mamad}
  \DontPrintSemicolon

  \KwIn{Environnement initial $\mathcal{E}$, objectif $\mathcal{G}_{\text{inf}}$, contraintes de conception $\mathcal{C}_{\text{inf}}$, espace d'observation $\Omega$, espace d'action $A$, nombre maximal de cycles de raffinement $n_{refine}$}
  \KwOut{Un SMA déployé satisfaisant aux exigences de conception, de performance et d'explicabilité~; ainsi que ses spécifications organisationnelles associées}

  Initialiser~: $\mathcal{D}_{H^j} \gets \emptyset$, $\pi^j_{\text{latest}} \gets \pi^j_{\text{init}}$, $running\_MTA \gets False, \mathcal{MM} \gets \emptyset$ \;

  \vspace{0.3em}

  \While{\acn{SMA} en cours de conception}{
    \tcp*[l]{Transfert~: récupération des trajectoires \& déploiement de la politique}
    $\mathcal{D}_{H^j}, \texttt{need\_update} \gets \text{transfer}(\pi^i_{\text{latest}}, \mathcal{D}_{H^j})$ \tcp*[r]{appel asynchrone}
    \If{\texttt{need\_update} et non \texttt{running\_MTA}}{
      \texttt{launch\_MTA()} \tcp*[r]{appel asynchrone}
    }
  }

  \vspace{1em}
  \SetKwProg{MTA}{Processus \normalfont(\acn{MTA})}{}{}
  \MTA{}{}{

  $\texttt{running\_MTA} \gets True$ \tcp*[l]{Variable globale}
  % ttt
  \tcp*[l]{Modélisation~: modéliser l'environnement réel}
  $(\mathcal{T}^j, \Omega^{\mathcal{T}^j}_0, R^j_H, S^j_H, \text{Render}^j_H) \gets \texttt{model}(\mathcal{E}, \mathcal{D}_{H^j}, \mathcal{G}_{\text{inf}}, \gamma, \Omega, A)$ \;

  \tcp*[l]{Politique insatisfaisante ou nombre max. de raffinement non atteint}

  \While{$i < n_{refine} \ \text{ou} \ (\texttt{org\_fit} > \texttt{org\_fit}_{min} \ or \ \overline{r} > \overline{r}_{min} \ or \  \sigma < \sigma_{max})$}{

  \vspace{0.5em}
  \tcp*[l]{Entraînement~: politique sous spec. org.}
  $(\pi^j, \overline{r}, \sigma) \gets \texttt{train}(\mathcal{T}^j, \Omega^{\mathcal{T}^j}_0, R^j_H, S^j_H, \text{Render}^j_H, \mathcal{MM}, \mathcal{C}_{\text{inf}}, \gamma, \Omega, A)$ \;

  \vspace{0.5em}
  \tcp*[l]{Analyse~: inférer les nouvelles spec. org.}
  $(\mathcal{MM}_{\text{imp}}, \texttt{org\_fit}) \gets \texttt{analyze}(\mathcal{T}^j, \mathcal{MM}, \pi^j)$ \;

  $\pi^j_{\text{latest}} \gets \pi^j$ \tcp*[r]{Mise à jour de la politique la plus récente}

  $\mathcal{MM} \gets \mathcal{MM}_{\text{imp}}$ \;

  $i \gets i + 1$

  }

  $\texttt{running\_MTA} \gets False$ \tcp*[l]{Variable globale}

  }
\end{algorithm}

\paragraph{Processus MTA~: optimisation des politiques et raffinement organisationnel}

Ce processus modélise le problème de conception actuel et améliore la politique conjointe des agents ainsi que ses spécifications organisationnelles. Il commence par construire un modèle de prédiction d'observations conjointes (\acparen{JOPM}) $T^j$ à l'aide de \textit{World Models} étendus, à partir des trajectoires collectées. Les exigences de conception sont formalisées sous forme de spécifications organisationnelles MOISE+MARL $\mathcal{MM}$ et l'objectif est formalisé par une fonction de récompense basée sur l'historique $R^j_H$.

Un modèle markovien est ensuite construit à partir des éléments modélisés afin d'entraîner les agents en tenant compte des spécifications organisationnelles, via le framework MOISE+MARL. Une fois l'entraînement terminé, la politique conjointe $\pi^j$ est analysée à l'aide de \acn{TEMM} afin d'inférer les spécifications organisationnelles implicites $\mathcal{MM}_{\text{imp}}$ et de calculer un score d'adéquation organisationnelle.

\paragraph{Boucle de raffinement via les spécifications organisationnelles}

Si la politique apprise montre des performances insuffisantes ou une grande variabilité (par rapport aux seuils définis), les spécifications organisationnelles implicites inférées sont utilisées pour raffiner les spécifications initiales. Ce processus de raffinement peut impliquer une inspection manuelle des structures inférées pour identifier les facteurs clés de succès des comportements émergents. Guidé par ces observations, le concepteur peut réviser les spécifications afin d'orienter les prochaines itérations d'apprentissage.

Cette boucle est répétée jusqu'à un maximum de $n_{refine}$ fois, orientant progressivement l'espace des politiques vers des comportements plus structurés et plus performants. La dernière politique validée est alors enregistrée comme $\pi^j_{\text{latest}}$, prête à être déployée dans l'environnement réel.

La boucle de raffinement est particulièrement utile dans les environnements complexes où la connaissance préalable est limitée ou où la conception manuelle serait trop coûteuse. À chaque itération, elle permet de restreindre l'espace de recherche des politiques en le concentrant sur les régions associées à des régularités organisationnelles émergentes.

A noter que ce processus peut commencer sans aucune spécification organisationnelle initiale, et produire par raffinement successif des contraintes organisationnelles pertinentes, objectives, et indépendantes de toute expertise humaine ou connaissance préalable de l'environnement.

\noindent L'interaction entre ces deux processus asynchrones constitue un cycle de conception de \acn{SMA} complet et fermé. Le système apprend continuellement à partir de l'exécution réelle, met à jour son modèle simulé, réentraîne sous des spécifications évolutives, et déploie des politiques améliorées sans nécessiter d'intervention constante du concepteur. Cette architecture établit un pont entre les principes symboliques de l'ingénierie orientée agents et l'automatisation par apprentissage, assurant conformité, adaptabilité et explicabilité au niveau organisationnel.

\noindent Il est à noter que nous proposons d'exploiter un environnement simulé modélisé comme un jumeau numérique (\textit{Digital Twin}) pour l'entraînement ultérieur, tandis que les approches \acn{MBRL} combinent simultanément modélisation et apprentissage. En effet, nous privilégions une séparation entre modélisation et apprentissage pour les raisons suivantes~: i) la réutilisabilité du modèle d'environnement pour d'autres entraînements d'agents, avec des ajustements éventuels~; \quad ii) le besoin d'agents simples n'embarquant pas de modèles coûteux pour planifier~; \quad iii) le besoin d'un environnement simulé de haute fidélité commun à tous les agents.

\section{Application flexible de la méthode MAMAD}
\label{subsec:mamad_flexible}

\input{tables/mamad_taxonomy}

La méthode \acn{MAMAD} a été conçue pour être modulable et adaptable selon les besoins de chaque cas d'application.
En pratique, toutes les activités décrites dans la taxonomie (\autoref{tab:mamad_taxonomy}) ne sont pas nécessairement appliquées dans leur intégralité.
Chaque activité peut être utilisée de façon indépendante ou combinée avec d'autres, et chacune dispose de plusieurs sous-activités offrant différents niveaux d'automatisation et de contraintes.

Cette flexibilité permet :
\begin{itemize}
  \item \textbf{Une application partielle} : un cas d'application peut exploiter uniquement certaines activités (par exemple, \textbf{MOD} et \textbf{TRN} uniquement) tout en omettant l'analyse et le transfert si ces étapes ne sont pas nécessaires comme dans le cas où l'environnement n'est pas dynamique.
  \item \textbf{Un choix ciblé de sous-activités} : pour chaque activité retenue, une sous-activité peut être choisie en fonction des objectifs, des ressources disponibles et du degré d'automatisation souhaité (par exemple, \acn{MOD-AUT} pour la modélisation automatisée, \acn{TRN-CON} pour l'entraînement avec des contraintes). Cela permet de gérer le coût de conception en fonction du niveau de complexité de l'environnement et des resources (financières, temps, experience) disponibles.
  \item \textbf{Une combinaison adaptative} : certaines activités peuvent être réalisées de manière automatisée tandis que d'autres restent manuelles ou semi-manuelles, afin d'équilibrer précision, contrôle et rapidité.
\end{itemize}

\paragraph{Exemple abstrait}
Considérons un scénario où l'on souhaite concevoir rapidement un \acn{SMA} pour un environnement complexe, avec un budget limité en ressources humaines mais un accès étendu à des données d'exécution.
Dans ce cas, on pourrait adopter la configuration suivante :
\begin{itemize}
  \item \acn{MOD-AUT} : utilisation d'un modèle automatisé basé sur des traces collectées (\textit{World Models}) afin de gagner du temps dans la construction du jumeau numérique.
  \item \acn{TRN-CON} : entraînement multi-agent guidé par des spécifications organisationnelles MOISE+MARL pour garantir la conformité des comportements aux rôles et objectifs définis.
  \item \acn{ANL-AUT} : analyse entièrement automatisée via Auto-\acn{TEMM} pour extraire rôles et objectifs implicites, et évaluer l'adéquation organisationnelle.
  \item Pas d'activité \textbf{TRF} : les politiques apprises sont utilisées uniquement dans l'environnement simulé pour des études exploratoires, sans déploiement réel.
\end{itemize}

Cette configuration peut être notée succinctement grâce aux acronymes de la taxonomie :
\[
  \text{Configuration} = \{\acn{MOD-AUT},\ \acn{TRN-CON},\ \acn{ANL-AUT}\}
\]
Ce formalisme facilite la documentation des choix méthodologiques pour chaque expérimentation et permet de comparer rapidement différents cas d'application.
%
Ainsi, la taxonomie proposée constitue un outil de référence pour spécifier précisément le \textit{chemin méthodologique} suivi dans une étude, tout en mettant en évidence les choix d'automatisation et de guidage organisationnel effectués.
Les chapitres suivantes détaillent chaque activité du cadre \acn{MAMAD}, identifient les défis spécifiques rencontrés, et décrivent les contributions proposées pour y répondre.


\clearpage
\thispagestyle{empty}
\null
\newpage

% \acn{TODO} : faire un index avec les mots clés (Auto-encodeur, World Models, \acn{SMA}, \acn{LSTM}, \acn{RNN}, \acn{MLP}, adéquation organisationnelle...)

\chapter{Modéliser l'environnement en simulation}
\label{chap:modelling}

L'\textit{activité de modélisation}\index{Modélisation (MOD)} occupe une place centrale dans la méthode \acn{MAMAD}.
Elle consiste à représenter le problème de conception comme un problème d'optimisation sous contraintes, en produisant une abstraction fidèle de l'environnement dans lequel évolueront les agents.
Cette activité joue le rôle de socle pour l'ensemble de la méthode : sans un modèle cohérent et suffisamment riche, les étapes suivantes (entraînement, analyse, transfert) ne peuvent pas être réalisées de manière robuste.

En pratique, la modélisation doit fournir un \textit{jumeau numérique} de l'environnement réel, c'est-à-dire une simulation dans laquelle les agents peuvent interagir, recevoir des observations, exécuter des actions, et accumuler des récompenses en fonction d'objectifs donnés.
Ce modèle sert ainsi à la fois de banc d'essai pour optimiser les politiques d'agents et de support formel pour raisonner sur la validité des comportements obtenus.

\subsection*{Objectifs formels}
L'objectif de cette activité est de transformer les informations disponibles (traces d'interactions passées, objectifs et contraintes formulés de manière informelle, description partielle de l'environnement) en un formalisme standardisé utilisable par la suite de la méthode.

Concrètement, les \textbf{entrées} de cette activité sont :
\begin{itemize}
  \item les historiques conjoints d'interactions $\mathcal{D}_{H^j}$ ;
  \item l'objectif global informel $\mathcal{G}_{\text{inf}}$ ;
  \item les contraintes organisationnelles informelles $\mathcal{S}_{\text{inf}}$ ;
  \item la description de l'environnement $\mathcal{E}$ ;
  \item le facteur d'actualisation $\gamma$ ;
  \item l'espace des observations $\Omega$ ;
  \item l'espace des actions $A$.
\end{itemize}

Les \textbf{sorties attendues} sont :
\begin{itemize}
  \item un modèle de transition conjoint $\mathcal{T}^j$ assimilable à une simulation ;
  \item une fonction de récompense basée sur l'historique $R^j_H$ ;
  \item une fonction d'arrêt $S^j_H$ déterminant les conditions de terminaison de l'épisode ;
  \item une fonction de rendu optionnelle $Render^j_H$ permettant de visualiser les trajectoires.
\end{itemize}

La relation globale peut être exprimée par :
\begin{displaymath}
  \texttt{model}(\mathcal{D}_{H^j}, \mathcal{G}_{\text{inf}}, \mathcal{S}_{\text{inf}}, \mathcal{E}, \gamma, \Omega, A) \rightarrow \mathcal{T}^j, R^j_H, S^j_H, \text{Render}^j_H
\end{displaymath}

\section{Travaux mobilisés et verrous identifiés}

La modélisation d'un environnement multi-agents pour la Cyberdéfense repose sur plusieurs piliers théoriques.
D'une part, le formalisme des \textbf{\acn{Dec-POMDP}} fournit un cadre mathématique rigoureux pour décrire les environnements multi-agents stochastiques et partiellement observables.
Dans ce cadre, l'état global du système est caché, les agents ne disposent que d'observations partielles, et doivent prendre des décisions coordonnées pour maximiser une récompense commune.

D'autre part, les \textbf{World Models} constituent une approche connexionniste visant à apprendre un simulateur d'environnement à partir de données historiques.
Un World Model combine généralement des autoencodeurs pour compresser les observations, un modèle de dynamique récurrent pour prédire l'évolution des représentations latentes, et un décodeur pour reconstruire les observations futures.
Ces modèles permettent de générer un environnement simulé de haute fidélité sans nécessiter une description complète a priori.

Enfin, plusieurs \textbf{travaux de simulation multi-agents} (ex. environnements PettingZoo, frameworks pour la robotique collective, simulateurs de réseaux) montrent l'importance de disposer d'outils capables de représenter les interactions complexes entre agents.
Ces environnements, bien que puissants, sont souvent spécifiques à un domaine et difficilement généralisables à la Cyberdéfense.

En résumé, les approches symboliques (basées sur des modèles explicites comme \acn{Dec-POMDP}) apportent de l'explicabilité et du contrôle, tandis que les approches connexionnistes (basées sur les World Models) favorisent l'adaptation et la performance.
La modélisation doit donc chercher à articuler ces deux dimensions.

Malgré ces apports, plusieurs verrous scientifiques et techniques demeurent :
\begin{itemize}
  \item \textbf{Absence de modèle générique} : il n'existe pas de châssis unifié permettant d'homogénéiser la modélisation des environnements multi-agents de Cyberdéfense. Chaque modèle est souvent ad hoc et difficilement réutilisable.
  \item \textbf{Limites des World Models existants} : les architectures actuelles sont principalement conçues pour des contextes mono-agent. Leur extension directe aux environnements multi-agents se heurte à la croissance combinatoire des observations conjointes et des actions.
\end{itemize}

Ces verrous motivent le développement de nouvelles contributions, combinant l'élaboration d'un modèle générique pour la simulation manuelle et l'extension des World Models au contexte multi-agents.

\section{Positionnement et contributions proposées}

Les approches existantes de modélisation se répartissent en deux grandes catégories.
D'un côté, les approches \textbf{manuelles}, qui consistent à construire un modèle formel de l'environnement à partir d'une description experte (ex. modèles \acn{Dec-POMDP} adaptés à un cas particulier de Cyberdéfense). Ces approches présentent l'avantage d'être explicables et contrôlables, mais elles sont chronophages, nécessitent une expertise approfondie du domaine, et conduisent souvent à des modèles hétérogènes difficilement réutilisables ou comparables.

De l'autre côté, les approches \textbf{automatisées}, telles que les World Models, qui permettent d'apprendre directement un simulateur à partir de traces d'interactions passées. Ces méthodes offrent une grande capacité d'adaptation et permettent de capturer des dynamiques complexes. Néanmoins, elles souffrent d'un manque d'explicabilité, et leurs extensions au cadre multi-agents restent limitées par la dimensionnalité des observations et la coordination des agents.

Dans le contexte de la Cyberdéfense, ni l'une ni l'autre de ces approches n'est suffisante. La conception d'une méthode générique impose de combiner les avantages des deux :
\begin{itemize}
  \item proposer un \textbf{modèle générique formel} qui serve de châssis commun pour homogénéiser la modélisation manuelle des environnements multi-agents ;
  \item développer une \textbf{extension multi-agent des World Models}, afin d'automatiser la génération de simulations tout en capturant les interactions entre agents.
\end{itemize}

Ce double positionnement permet de tirer parti à la fois de l'explicabilité et de la réutilisabilité offertes par les modèles formels, et de la capacité d'adaptation offerte par les approches connexionnistes.


\subsection{Les \textit{World Models} Multi-Agents pour la génération automatique du modèle simulé}

Dans cette approche automatisée, on commence par générer un environnement simulé de haute fidélité en construisant un \acn{JOPM} $\mathcal{T}^j~: H^j \times \Omega^j \times A^j \rightarrow \mathcal{H} \times \hat{\Omega}^j$ à partir des traces d'interactions réelles $\mathcal{D}_{H^j}$. À un pas de temps $t \in \mathbb{N}$, pour tout état caché récurrent $\tilde{h}_{t-1} \in \mathcal{H}$ représentant l'historique conjoint jusqu'à $t-1$, l'observation conjointe reçue $\omega_t^j \in H^j$ et l'action conjointe $a_t^j \in A^j$, le modèle $\mathcal{T}^j$ renvoie le nouvel état caché $\tilde{h}_t \in \mathcal{H}$ ainsi que la prédiction de la prochaine observation conjointe $\hat{\omega}^j \in \hat{\Omega}^j$. Cette architecture, illustrée en \autoref{fig:jopm_architecture}, permet à \acn{MAMAD} de construire l'environnement de simulation depuis zéro\index{Joint-Observation Prediction Model (JOPM)}.

Dans les environnements multi-agents, les observations conjointes deviennent rapidement de grande dimension à mesure que le nombre d'agents augmente. Pour pallier cela, des fonctions d'encodage conjoint sont introduites pour les observations et les actions.

\begin{figure}[h]
  \centering
  \resizebox{\textwidth}{!}{%
    \input{figures/jopm_synthesis}
  }
  \caption{Schéma de l'architecture d'un JOPM incluant le RDLM et l'Auto-encodeur}
  \label{fig:jopm_architecture}
\end{figure}


Plus précisément, les observations conjointes $\omega_t^{j} = (\omega_t^1, \dots, \omega_t^n) \in \Omega^{j}$ sont transformées en représentations latentes compactes à l'aide d'un encodeur d'observations conjointes $Enc_{\omega^j}~: \Omega^j \rightarrow z$, produisant $z_t = Enc_{\omega^j}(\omega_t^j)$. Un décodeur $Dec_z~: z \rightarrow \hat{\Omega}^j$ permet la reconstruction des observations conjointes.

On utilise généralement des \acn{MLP}s ou des architectures à base d'attention pour ces encodeurs, afin d'agréger les informations multi-agents en vecteurs de caractéristiques de taille fixe, tout en capturant les dépendances critiques entre agents.

Une fois l'encodage effectué, le \textit{World Model} multi-agent fonctionne comme en contexte mono-agent, en utilisant les observations encodées $z_t$ dans les historiques transmis au \acn{RLDM} $\mathcal{T}^{z}$. Dans le cadre de \acn{MAMAD}, ces \textit{World Models} constituent le cœur de la simulation mise en œuvre par l'activité de modélisation, agissant comme des jumeaux numériques de haute fidélité de l'environnement cible.

\subsection{Un modèle de simulation pour la génération manuelle du modèle simulé}

Dans l'approche manuelle, une description informelle de l'environnement, de l'objectif global et des contraintes est traduite en un modèle formel générique, extensible à divers contextes.

\subsubsection{Modélisation générale Dec-POMDP pré-spécialisé}

Nous considérons un environnement réseau composé de nœuds, chacun décrit par un ensemble de propriétés $(id, v)$. Des agents (cyberattaquants/défenseurs) interagissent avec ces nœuds via des actions conditionnées par leurs observations et privilèges. Une action modifie l'état de l'environnement si ses préconditions (sur les propriétés) sont satisfaites, produisant un nouvel état et de nouvelles observations pour l'agent. Les agents agissent séquentiellement (mode \acn{AEC}), chaque tour consistant à choisir une action, appliquer la transition, et recevoir l'observation/récompense.

\begin{figure*}[]
  \centering
  \includegraphics[trim=0.7cm 0.6cm 0.7cm 1cm, clip,width=1\textwidth]{figures/model_example_illustration.pdf}
  \caption{Vue illustrative du modèle de simulation}
  \label{fig:model_example_illustration}
\end{figure*}

\noindent
Le modèle \acn{Dec-POMDP} exprime l'état comme l'ensemble des propriétés des nœuds. Les actions sont définies par des pré/post-conditions sur ces propriétés. Les transitions et observations sont conditionnées par ces propriétés, et les récompenses sont calculées à partir de métriques sur l'état.

\subsubsection{Modélisation formelle Dec-POMDP}

Nous définissons les éléments liés aux propriétés des nœuds, des agents et des actions de l'environnement suivant :

\begin{itemize}

  \item $Ag = \{ag_1,..,ag_{|Ag|}\}$ : ensemble des agents (cyberattaquants et Cyberdéfenseurs).
        % \begin{itemize}
        %     \item Avec $Attackers \subseteq Ag$ : l'ensemble des agents attaquants
        %     \item Avec $Defenders \subseteq Ag$ : l'ensemble des agents défenseurs
        % \end{itemize}

  \item Nous appelons le couple $p = (id_{j}, v_{j})$ avec $id_j \in {ID}$ et $v_j \in V$, une propriété.
        \begin{itemize}
          \item $\acn{ID}$ : l'ensemble des identifiants de propriétés indiquant éventuellement comment les propriétés sont organisées dans une structure de données non plate (telle que $PC1.processes.agents.agent1$). Ces identifiants de propriétés peuvent être utilisés pour un chemin d'accès à un fichier, le type de système d'exploitation utilisé dans un nœud, une ligne de commande utilisée par un agent\dots
          \item $V$ : Ensemble des valeurs de propriétés. Celles-ci peuvent inclure le contenu d'un fichier, une description complète du système d'exploitation, le résultat d'une ligne de commande\dots
                % \item $Valeurs : \acn{ID} \rightarrow \mathcal{P}(V) = \{(id_{j}, V_{j}) \: | \: id_j \in {ID},$ $V_j \in \mathcal{P}(V)\}$ : une bijection associant un identifiant de propriété à l'ensemble des différentes valeurs auxquelles il peut être associé. Par exemple, l'identifiant $ls\_command\_output$ peut être associé aux valeurs suivantes $\{file.txt,\{file.txt,passwd.txt\}\}$
        \end{itemize}

  \item $P_{j} = \{ p_1, .., p_{|P_{j}|} \}$ : l'ensemble des propriétés $p_{l}$ (avec $l \in \{1,..,|P_{j}|\}$) du nœud $j$ ($j \in \mathbb{N} $). Par exemple, ces propriétés peuvent inclure certains identifiants de processus en cours d'exécution, la liste des fichiers d'un dossier, le type de système d'exploitation avec une description, des connaissances spécifiques d'un agent, etc.
        \begin{itemize}
          \item $P = P_1 \cup P_2 .. \cup P_{|P|} $ : Ensemble de toutes les propriétés des nœuds.
        \end{itemize}

  \item $Obs : \mathcal{P}(P) \times Ag \rightarrow \mathcal{P}(P_{Ag}), P_{Ag} \subset P$ : Relation qui associe les propriétés des nœuds et un agent au sous-ensemble de propriétés observées par l'agent.



  \item $Action : P_{pre} \rightarrow P_{post}$ : Relation qui associe un sous-ensemble de propriétés implicite dans une pré-condition booléenne conjonctive équivalente ($P_{pre} \subset \mathcal{P}(P)$) à un sous-ensemble de toutes les propriétés de la post-condition ($P_{post} \in \mathcal{P}(P)$). Par exemple, les propriétés $p_1 = (agent\_X\_privilege\_level, \allowbreak root)$, $p_2 = (agent\_X\_accessed\_text\_editor, \allowbreak Vim)$ et $p_3 = (agent\_X\_bashrc\_known\_filepath, \allowbreak /home/user/.bashrc)$ peuvent former une pré-condition ($p_1 \land p_2 \land p_3$) pour associer un nouvel ensemble de propriétés contenant $p4 = (bashrc\_file\_modified\_by\_X\_agent, \top)$. Deux sous-ensembles de pré-conditions peuvent être associés au même sous-ensemble de post-conditions pour modéliser une disjonction booléenne.

  \item $Metrics: \mathcal{P}(P) \times A \rightarrow \mathbb{R}^{n}$ : donne des métriques associées à un ensemble de propriétés et à une action conjointe. Par exemple, le nombre de nœuds encore actifs, les mouvements latéraux, etc.

\end{itemize}


En utilisant la description formelle d'un \acn{Dec-POMDP}~\cite{Oliehoek2016}, nous proposons le modèle \acn{Dec-POMDP} pré-spécialisé suivant :

\begin{itemize}
  \item $S = \{s_1, ..s_{|S|}\}, s_{i} \subseteq P \: et \: 1 \le i \le |S|$ : L'espace des états en tant qu'ensembles de propriétés possibles.

  \item $A_{i} = \{a_{i}^{1},..,a_{i}^{|A_{i}|}\}, a_{i}^j \in Action \: et \: 1 \le j \le |A_i|$ : l'ensemble des actions possibles pour l'agent $i$.

  \item $T$ : Ensemble des probabilités de transition conditionnelles entre les états
        \begin{itemize}
          \item Avec $T(s,a,s') = \probP(s'|s,a)$, la relation qui associe la probabilité d'aller de l'état $s \in S$ à l'état $s' \in S$ sachant que nous avons joué $a = (P^a_{pre} \times P^a_{post}) \in A$ avec $P^a_{pre} \subset \mathcal{P}(P)$ et $P^a_{post} \in \mathcal{P}(P)$
          \item Avec $\probP(s'|s,a) = 0$ si $s$ ne satisfait pas la condition préalable de $a$ (c'est-à-dire $\exists \: P_{pre_s}^{a} \in P_{pre}^{a} \: | \: P_{pre_s}^{a} \not\in \mathcal{P}(s)$).
          \item Avec $s' = (s - \{p_l=(id_l, v_l) \: | \: p_l \in s \: et$ $id_l \in \{id_k \: | \: (id_k, v_k) \in P^a_{post} \: et \: v_k \neq \varnothing\}\}) \cup P^a_{post}$
        \end{itemize}



  \item $R : S \times A \rightarrow \mathbb{R}^2 = Eval \circ Metrics$ : La fonction de récompense qui prend un état et une action et associe un indicateur de performance (à l'aide des métriques de l'état) pour les attaquants et les défenseurs.
        \begin{itemize}
          \item Avec $Eval : \mathbb{R}^{n} \rightarrow \mathbb{R}^2$, associe un vecteur métrique à une récompense pour les cyberattaquants et les Cyberdéfenseurs.
        \end{itemize}



  \item $\Omega_{i} \subset Range(Obs \: | \: \{ (s, ag_i) | s \in S \: et \: ag_i \in Ag \}) \subset P$ : ensemble des propriétés observables pour l'agent $ag_i$. Par exemple, le contenu d'un fichier, la sortie du journal d'une commande, le résultat d'un scan de port, etc.
        \begin{itemize}
          \item $\Omega = \Omega_1 \cup \Omega_2 .. \cup \Omega_{|Ag|} = Range(Obs)$ : Ensemble de toutes les propriétés observables pour tous les agents.
        \end{itemize}

  \item $O$ : Ensemble des probabilités d'observation conditionnelles.
        \begin{itemize}
          \item Avec $O(s',a,o) = \probP(o|s',a)$, la relation qui associe la probabilité d'observer une observation $o \subset \Omega$ à partir de l'état $s' \in S$ induit par $a \in A$
          \item Avec $\probP(o|s',a) = 0$ si l'état $s' \in S$ ne contient pas les propriétés de $o \subset \Omega$ (c'est-à-dire $o \not\in \mathcal{P}(s')$). Par exemple, un agent joue l'action $x\_reads\_a\_log\_file$, ce qui donne lieu à un nouvel état dont une propriété appartenant à la connaissance de l'agent x est $(log\_file\_content\_known\_by\_x, \allowbreak abc)$. Cette propriété sera donc incluse dans les observations renvoyées à l'agent x.
        \end{itemize}

\end{itemize}


\subsubsection{Intégration des scénarios d'attaque/défense}

\noindent
D'un point de vue brut, la modélisation du Dec-POMDP pré-spécialisé proposée s'appuie sur des actions pour simuler la manière dont un système en réseau réel réagirait, y compris les vulnérabilités et les contre-mesures appliquées par les agents cyberattaquants et Cyberdéfenseurs.

Un premier défi consiste à construire un scénario d'attaque/défense représentatif d'un système en réseau comportant des vulnérabilités afin de permettre de rendre une attaque en reliant les seules informations disponibles (telles que les tactiques, techniques et procédures connues de MITRE ATT\&CK) et en choisissant des contre-mesures de défense pertinentes (issues des mesures d'atténuation de MITRE ATT\&CK) et un environnement de déploiement. Un deuxième défi consiste à établir les actions correspondant au scénario d'attaque/défense. Comme les actions modifient les propriétés de l'environnement, elles ont également un impact sur l'espace des états possibles et les transitions entre ceux-ci.
De plus, lorsque l'on considère un faible niveau d'abstraction, de nombreuses actions simples peuvent permettre de décrire avec précision les changements opérés dans le réseau. Cependant, cela augmente le nombre d'actions, et encore plus le nombre d'états, car ceux-ci sont des combinaisons des effets des actions.

Ces défis sont directement liés aux questions étudiées concernant la génération automatisée de graphiques d'attaque à l'aide de bases de données disponibles intégrant éventuellement des techniques d'intelligence artificielle, comme dans ~\cite{GFalco2018}. Nous n'avons pas l'intention de nous attarder davantage sur ces questions, car elles dépassent le cadre de ce travail.

Ce modèle, illustrée en \autoref{fig:model_example_illustration}, permet de générer un simulateur multi-agents fidèle, où chaque action et observation est explicitement définie, facilitant l'extension à divers contextes de Cyberdéfense.

\

\noindent
\textbf{Approche d'intégration MITRE ATT\&CK} : Nous suggérons une approche manuelle de haut niveau que nous avons utilisée pour intégrer les informations MITRE ATT\&CK sous forme d'arborescence AD, car elle formalise les actions à jouer dans un scénario et leurs interactions avec l'environnement. Elle vise à aider à établir les actions d'attaque/défense qui seront finalement intégrées dans le simulateur :
%
% \begin{enumerate*}[label=\arabic*),itemjoin={;\quad}]
\begin{itemize}
  \item Pour une menace persistante avancée (APT) donnée, nous avons identifié les tactiques, techniques et procédures pertinentes de MITRE ATT\&CK qui semblaient pertinentes pour un système en réseau

  \item Nous avons produit une description reliant les tactiques identifiées entre elles et les techniques, sous-techniques et procédures associées afin de créer un scénario décrivant comment le groupe APT pourrait attaquer le système en réseau. Cette étape définit la topologie du réseau avec ses principales propriétés
        (telles qu'un réseau d'entreprise composé de plusieurs serveurs de bases de données dédiés communiquant via FTP et HTTP, etc.)

  \item Nous avons créé une arborescence AD comme proposé dans ~\cite{BKordy2010} avec les tactiques comme objectifs d'action principaux et les techniques, sous-techniques et procédures dans la partie inférieure de l'arborescence. Nous avons veillé à disposer de plusieurs chemins pour atteindre un même objectif d'action principal. Nous avons pris soin de définir chaque action d'attaque avec des conditions préalables et des conditions postérieures basées sur les propriétés de l'environnement

  \item Nous avons extrait les techniques/sous-techniques MITRE ATT\&CK liées à la détection et aux mesures d'atténuation que nous avons ajoutées dans l'arbre AD afin d'enrichir les nœuds d'attaque. Nous avons veillé à définir chaque action de défense avec des conditions préalables et des conditions postérieures basées sur des propriétés dans l'environnement.

  \item Nous avons également répertorié et défini les principales actions environnementales spécifiques au déploiement à partir de la description précédente de l'environnement de déploiement ou des actions d'attaque/défense étendues qui sont communes aux Cyberdéfenseurs et aux cyberattaquants. Cette étape permet d'obtenir un environnement plus réaliste, fournissant un nombre représentatif d'actions plausibles qu'un agent peut choisir dans de nombreux systèmes.
        Ces actions communes pourraient inclure au moins :
        \begin{enumerate*}[label=\arabic*),itemjoin={;\quad}]
          \item Lecture et écriture de fichiers
          \item Créer, supprimer, copier, déplacer, renommer, modifier les propriétés des fichiers/dossiers.
          \item Accéder à un dossier, accéder au dossier parent
          \item Sélectionner un fichier/dossier pour y appliquer des actions ultérieures
          \item Exécuter un fichier binaire
          \item Utilisation d'un protocole réseau (tel que HTTP, FTP, SSH, etc.).
          \item Autres interactions avec les lignes de commande de base concernant la surveillance ou le contrôle du système.
        \end{enumerate*}
        Ensuite, les propriétés environnementales associées doivent décrire un système de fichiers, une interface de terminal, un port avec des règles, les propriétés des paramètres du système d'exploitation, etc.
\end{itemize}

\section{Description et mise en oeuvre dans l'activité}
% Présenter l'algorithme générique de l'activité de modélisation (inputs → outputs).
% Inclure le pseudo-code / algorithme en environnement LaTeX comme dans l'ancien chapitre.

L'\autoref{alg:modeling} décrit le déroulement général de l'activité de modélisation.
Chaque étape est explicitée ci-dessous afin d'en préciser les objectifs et le rôle dans la construction du modèle final.

\paragraph{Étape 1 : Formalisation manuelle des fonctions composantes.}
La première étape consiste à dériver manuellement, à partir des descriptions informelles de l'objectif global et des contraintes organisationnelles, trois fonctions fondamentales : la fonction de récompense $R^j_H$, la fonction d'arrêt $S^j_H$, et la fonction de rendu optionnelle $Render^j_H$.
Cette étape requiert l'expertise des concepteurs, qui doivent transformer des objectifs de haut niveau (souvent exprimés en langage naturel ou sous forme de règles métiers) en spécifications formelles permettant l'évaluation de trajectoires dans l'environnement simulé.

\paragraph{Étape 2 : Entraînement des auto-encodeurs pour les observations.}
Une fois les historiques d'interactions collectés, les observations conjointes $\Omega^j$ en sont extraites.
Comme leur dimension peut être très élevée dans un contexte multi-agent, elles sont compressées à l'aide d'auto-encodeurs.
L'encodeur $Enc_{\omega^j}$ apprend à transformer les observations en représentations latentes compactes $z_t$, tandis que le décodeur $Dec_{\omega^j}$ reconstruit les observations originales à partir de ces latents.
L'objectif est de minimiser l'erreur de reconstruction, garantissant ainsi que les latents conservent l'information essentielle.

\paragraph{Étape 3 : Encodage des observations dans les historiques.}
Les auto-encodeurs entraînés sont ensuite utilisés pour transformer l'ensemble des historiques $\mathcal{D}_{H^j}$ en séquences d'états latents.
Chaque observation $\omega_t^j$ est convertie en une représentation $z_t$, ce qui permet de constituer un nouvel ensemble d'entraînement $\mathcal{B}$ composé de triplets $(z_t, a_t^j, z_{t+1})$.
Cet encodage réduit la complexité des données d'entrée et prépare l'entraînement du modèle de dynamique.

\paragraph{Étape 4 : Entraînement du modèle de dynamique récurrent (RLDM).}
À partir de l'ensemble encodé $\mathcal{B}$, un modèle récurrent de dynamique latente (\acn{RLDM}) est entraîné.
Ce modèle, noté $\mathcal{T}^z$, apprend à prédire l'évolution des états latents en fonction de l'historique caché $\tilde{h}_{t-1}$, de l'état encodé courant $z_t$, et de l'action conjointe $a_t^j$.
L'entraînement se fait en minimisant l'erreur quadratique moyenne entre la prédiction $\hat{z}_{t+1}$ et le latent réel $z_{t+1}$.
Ce mécanisme d'apprentissage permet de capturer la dynamique de l'environnement sans accès direct à l'état global.

\paragraph{Étape 5 : Construction du JOPM.}
Une fois le \acn{RLDM} entraîné, les observations initiales $\Omega^{\mathcal{T}^j}_0$ sont extraites des historiques et utilisées pour initialiser le simulateur.
Le modèle de transition conjoint $\mathcal{T}^j$ est alors défini en combinant le modèle récurrent $f$, l'encodeur $Enc$, et le décodeur $Dec$.
Ainsi, pour toute observation et action donnée, $\mathcal{T}^j$ met à jour l'état caché et génère une observation prédite, constituant ainsi un simulateur complet des interactions multi-agents.

\paragraph{Étape 6 : Sorties de l'activité.}
Enfin, l'activité retourne l'ensemble des éléments modélisés : le modèle de transition conjoint $\mathcal{T}^j$, l'ensemble des observations initiales $\Omega^{\mathcal{T}^j}_0$, la fonction de récompense $R^j_H$, la fonction d'arrêt $S^j_H$, et la fonction de rendu éventuelle $Render^j_H$.
Ces composants constituent le cœur du jumeau numérique qui sera exploité par les activités d'entraînement, d'analyse et de transfert de la méthode \acn{MAMAD}.


\begin{algorithm}[H]
  \caption{Algorithme de l'activité de modélisation}
  \label{alg:modeling}
  \DontPrintSemicolon

  \KwIn{Historiques conjoints $\mathcal{D}_{H^j}$, objectif informel $\mathcal{G}_{\text{inf}}$, contraintes informelles $\mathcal{C}_{\text{inf}}$, facteur d'actualisation $\gamma$, espace des actions $A$, espace des observations $\Omega$}
  \KwOut{\acn{JOPM} $\mathcal{T}^j$, fonction de récompense $R^j_H$, spécifications MOISE+MARL $\mathcal{MM}$}

  \vspace{0.5em}
  \tcp{1. Formalisation manuelle des fonctions composantes}
  $(R^j_H, S^j_H, \text{Render}^j_H) \gets \texttt{manual\_formalize}(\mathcal{S}_{\text{inf}}, \mathcal{G}_{\text{inf}}, \mathcal{E}, A, \Omega)$ \;

  \vspace{0.5em}
  \tcp{2. Entraîner les auto-encodeurs pour les observations}
  Extraire les observations $\Omega^j = \{\omega^j_t\}$ à partir des historiques $\mathcal{D}_{H^j}$ \;
  Entraîner un auto-encodeur $(Enc_{\omega^j}, Dec_{\omega^j})$ sur $\Omega^j$ en minimisant l'erreur de reconstruction \;

  \vspace{0.5em}
  \tcp{3. Encoder les observations dans les historiques}
  Pour chaque historique $h^j = (\omega_t^j, a_t^j) \in \mathcal{D}_{H^j}$, encoder chaque observation conjointe ${z}_t = Enc_{\omega^j}(\omega^j_t)$ pour constituer l'ensemble d'entraînement $\mathcal{B} = \{ \{(z_t, a^j_t, z_{t+1})\} = h_z^j, h_z^j \in \mathcal{D}_{H^j}\}$

  \vspace{0.5em}
  \tcp{4. Entraîner le \acn{RLDM}}
  Initialiser le \acn{RLDM} $\mathcal{T}^z = f(g)$

  \For{$h_z^j \in \mathcal{B}$}{
    \For{$(z_t, a^j_t, z_{t+1}) \in h^j$}{
      Entraîner le \acn{RLDM} $\mathcal{T}^{z}$ en minimisant l'erreur quadratique moyenne (\acparen{MSE}) entre la prédiction $\hat{z}_{t+1}$ et la valeur réelle $z_{t+1}$.
    }
  }

  \vspace{0.5em}
  \tcp{5. Sauvegarder les observations initiales et former le JOPM}

  $\Omega^{\mathcal{T}^j}_0 \gets \{\omega^j_0\}$ extraites des historiques $\mathcal{D}_{H^j}$

  $\mathcal{T}^j(h_{t-1}, \omega_t, a_t) = \langle f(h_{t-1}, Enc(\omega^j_t), a^j_t), Dec(\mathcal{T}^{z}(h_{t-1}, Enc(\omega^j_t), a^j_t)) \rangle$ \;

  \vspace{0.5em}
  \tcp{6. Retourner les éléments modélisés}
  \Return{$\mathcal{T}^j, \Omega^{\mathcal{T}^j}_0, R^j_H, S^j_H, \text{Render}^j_H$}
\end{algorithm}



\section{Synthèse}

\noindent
En synthèse, l'activité de modélisation vise à fournir un environnement simulé fidèle et exploitable pour l'entraînement multi-agent, en combinant formalisme explicite (\acn{Dec-POMDP} générique) et génération automatique par World Models multi-agents. Les sorties produites — le modèle de transition conjoint (\acn{JOPM}), la fonction de récompense, la fonction d'arrêt et la fonction de rendu — constituent le socle du jumeau numérique utilisé dans les étapes suivantes. Cette approche permet d'assurer l'adaptation (via l'apprentissage sur données réelles), l'explicabilité (par la structure formelle du modèle) et la réutilisabilité (grâce au châssis générique). Toutefois, la fidélité du modèle dépend de la qualité et de la diversité des historiques collectés, et le coût computationnel de l'entraînement des auto-encodeurs et du \acn{RLDM} peut être élevé pour des environnements complexes. Enfin, la granularité des actions et la couverture des dynamiques réelles restent des limites inhérentes à toute simulation. L'activité d'entraînement exploitera ce modèle simulé pour optimiser les politiques sous contraintes organisationnelles, amorçant ainsi le cycle itératif de la méthode \acn{MAMAD}.

\clearpage
\thispagestyle{empty}
\null
\newpage

\chapter{Entraîner des politiques sous contraintes}
\label{chap:training}

L'\textit{activité d'entraînement}\index{Entraînement (TRN)} consiste à optimiser les politiques conjointes des agents dans l'environnement simulé, en tenant compte des contraintes organisationnelles.
Elle correspond à la phase de résolution du problème de conception, en exploitant les modèles produits par l'activité de modélisation.

Cette activité est cruciale car elle relie les critères définis dans \autoref{sec:criteres-evaluation} concernant la performance (C2), l'adaptation (C3),l'explicabilité (C5) et le contrôle (C4), via l'intégration de contraintes explicites dans l'apprentissage multi-agent.

\section*{Objectifs formels}

Les \textbf{entrées} de l'activité d'entraînement sont :
\begin{itemize}
  \item le modèle de transition conjoint $\mathcal{T}^j$ produit par la modélisation ;
  \item les observations initiales $\Omega^{\mathcal{T}^j}_0$ ;
  \item la fonction de récompense $R^j_H$ et la fonction d'arrêt $S^j_H$ ;
  \item les spécifications organisationnelles $\mathcal{MM}$ issues des contraintes informelles $\mathcal{C}_{\text{inf}}$ ;
  \item les espaces d'observations $\Omega$ et d'actions $A$ ;
  \item le facteur d'actualisation $\gamma$.
\end{itemize}

La \textbf{sortie attendue} est :
\begin{itemize}
  \item une politique conjointe entraînée $\pi^j = \{\pi^j_0, \pi^j_1, \dots, \pi^j_n\}$.
\end{itemize}

\noindent La relation globale peut s'exprimer par :
\begin{displaymath}
  \pi^j \gets \texttt{train}(\mathcal{T}^j, \Omega^{\mathcal{T}^j}_0, R^j_H, S^j_H, \text{Render}^j_H, \mathcal{MM}, \mathcal{C}_{\text{inf}}, \gamma, \Omega, A)
\end{displaymath}
\section{Travaux mobilisés et verrous identifiés}

L'activité d'entraînement des politiques sous contraintes s'appuie sur plusieurs familles de travaux issus du domaine du \acn{MARL} et de l'intégration de contraintes organisationnelles.

Du côté du \acn{MARL}, les méthodes classiques telles que l'apprentissage indépendant, l'apprentissage centralisé avec exécution décentralisée (\acn{CTDE}), ou encore les algorithmes de type Q-learning, Policy Gradient et leurs variantes multi-agents, constituent la base pour optimiser des politiques conjointes dans des environnements simulés. Ces approches sont efficaces pour maximiser la performance collective, mais elles n'intègrent pas nativement de contraintes organisationnelles explicites.

Pour pallier ce manque, plusieurs travaux issus du Safe \acn{RL} et des Constrained MDPs (\acn{CMDP}) ont été mobilisés. Les méthodes comme Constrained Policy Optimization (\acn{CPO}) ou Deep Constrained Q-Learning (\acn{DCQL}) permettent d'intégrer des contraintes numériques (sûreté, consommation, risque) dans le processus d'apprentissage, mais leur expressivité reste limitée à des contraintes locales et numériques, sans prise en compte de structures organisationnelles complexes.

Les approches de reward shaping, shielding, ou feedback humain offrent des mécanismes de guidage souple, permettant d'influencer indirectement les politiques apprises. Cependant, elles ne garantissent pas le respect formel de contraintes organisationnelles et restent difficiles à interpréter.

Enfin, les travaux sur l'intégration de modèles organisationnels symboliques, tels que $\mathcal{M}OISE^+$, proposent une formalisation riche des rôles, missions et relations collectives. Toutefois, leur intégration directe dans le processus d'apprentissage \acn{MARL} reste un verrou majeur, en raison de la difficulté à traduire ces spécifications en contraintes opérationnelles exploitables par les algorithmes d'apprentissage.

En synthèse, les principaux verrous identifiés sont :
\begin{itemize}
  \item l'absence de cadre unifié permettant d'intégrer des contraintes organisationnelles symboliques dans l'apprentissage \acn{MARL} ;
  \item la difficulté à garantir le respect de ces contraintes tout en maintenant la performance et l'adaptabilité des politiques ;
  \item le manque d'explicabilité et de contrôle sur les politiques apprises dans des environnements complexes et dynamiques.
\end{itemize}

\section{Positionnement et contributions proposées}

Pour lever ces verrous, notre approche propose d'hybrider les forces des cadres symboliques et connexionnistes en introduisant le framework MOISE+MARL. Ce cadre permet d'intégrer explicitement des spécifications organisationnelles (rôles, missions, permissions, obligations) dans le processus d'apprentissage multi-agent, en les traduisant sous forme de guides de contraintes (action masking, shaping de récompense, guides d'objectifs) injectés dans les algorithmes \acn{MARL}.

Notre contribution principale consiste à :
\begin{itemize}
  \item formaliser l'intégration des spécifications organisationnelles $\mathcal{M}OISE^+$ dans le \acn{MARL} via des guides de contraintes, permettant de restreindre ou d'orienter l'espace des politiques apprises ;
  \item proposer un nouveau formalisme, l'\acn{ODec-POMDP}, compatible avec les environnements simulés appris (World Models), pour permettre l'entraînement à partir de données observables uniquement ;
  \item développer un algorithme d'entraînement générique (voir \autoref{alg:training_mamad}) qui articule ces guides de contraintes avec les méthodes \acn{MARL} existantes, assurant ainsi la compatibilité entre apprentissage connexionniste et respect des contraintes organisationnelles ;
  \item offrir un certain degré d'explicabilité et de contrôle sur les politiques apprises, grâce à la traçabilité des guides de contraintes et à l'analyse post-hoc des comportements émergents.
\end{itemize}

Ce positionnement permet de concilier performance, adaptation, contrôle et explicabilité dans l'entraînement des politiques multi-agents sous contraintes, ouvrant la voie à une conception plus robuste et transparente des \acn{SMA} pour des environnements critiques comme la Cyberdéfense.

\subsection{MOISE+MARL pour lier $\mathcal{M}OISE^+$ avec le MARL}

\begin{figure}[h!]
  \centering
  \input{figures/mm_synthesis_single_column}
  \caption[Vue minimale du framework MOISE+MARL]{Vue minimale du framework MOISE+MARL~: Les utilisateurs définissent d'abord les spécifications $\mathcal{M}OISE^+$, qui incluent les rôles ($\mathcal{R}$) et les missions ($\mathcal{M}$), tous deux associés via $rds$. Ils créent ensuite les spécifications MOISE+MARL en définissant d'abord des guides de contraintes tels que $rag$ et $rrg$ pour spécifier la logique des rôles, et $grg$ pour la logique des objectifs. Des linkers sont ensuite utilisés pour connecter les agents aux rôles via $ar$ et pour lier la logique des guides de contraintes aux spécifications $\mathcal{M}OISE^+$ définies. Une fois ces éléments configurés, les rôles peuvent être attribués aux agents, et le framework \acn{MARL} est mis à jour en conséquence pendant l'apprentissage.
  }
  \label{fig:mm_synthesis}
\end{figure}

\noindent MOISE+MARL\index{MOISE+MARL} introduit des moyens de contrôler ou de guider l'apprentissage des agents en \acn{MARL}. Sa principale contribution réside dans les \textbf{Guides de contraintes}, qui sont trois nouvelles relations introduites pour décrire la logique des rôles et des objectifs dans le formalisme \acn{Dec-POMDP} :
%
\begin{itemize}
  % \begin{enumerate*}[label={\roman*) },itemjoin={; \quad}]

  \item \textbf{Guide d'action des rôles} \quad $rag: H \times \Omega \rightarrow \mathcal{P}(A \times \mathbb{R})$, relation modélisant un rôle comme un ensemble de règles qui, pour chaque couple constitué d'un historique $h \in H$ et d'une observation reçue par l'agent $\omega \in \Omega$, associe des actions attendues $A \in \mathcal{P}(A)$ chacune associée à une contrainte de dureté $ch \in [0,1]$ ($ch = 1$ par défaut). En restreignant le choix de l'action suivante parmi celles autorisées, l'agent est contraint d'adhérer au comportement attendu du rôle
  \item \textbf{Guide de récompense des rôles} \quad $rrg: H \times \Omega \times A \to \mathbb{R} = \{r_m \text{ if } a \notin A_\omega \text{, } rag(h, \omega) \allowbreak = \allowbreak A_\omega \times \mathbb{R} \text{, } h \in H; \text{ else } 0\}$, la relation qui modélise un rôle en ajoutant une pénalité $r_m$ à la récompense globale si la dernière action choisie par l'agent $a \in A$ n'est pas autorisée. Ceci vise à encourager l'agent à adhérer au comportement attendu.
  \item \textbf{Guide de récompense d'objectif} \quad $grg: H \rightarrow \mathbb{R}$, la relation qui modélise un objectif comme une contrainte souple ajoutant un bonus de récompense $r_b \in \mathbb{R}$ si l'historique $h \in H$ de l'agent contient une sous-séquence caractéristique d'un objectif $h_g \in H_g$, encourageant l'agent à l'atteindre.
        % \end{enumerate*}
\end{itemize}

\noindent Enfin, nous introduisons les \textbf{Linkers} pour lier les spécifications organisationnelles $\mathcal{M}OISE^+$ aux guides de contraintes et aux agents :
%
\begin{itemize}
  % \begin{enumerate*}[label={\roman*) },itemjoin={; \quad}]

  \item \textbf{Agent vers Rôle} \quad $ar: \mathcal{A} \to \mathcal{R}$, la relation bijective reliant un agent à un rôle ;
  \item \textbf{Guide Rôle vers Contrainte} \quad $rcg: \mathcal{R} \rightarrow rag \cup rrg$, la relation associant chaque rôle $\mathcal{M}OISE^+$ à une relation $rag$ ou $rrg$, forçant/encourageant l'agent à suivre les actions attendues pour le rôle $\rho \in \mathcal{R}$ ;
  \item \textbf{Guide Objectif vers Contrainte} \quad $gcg: \mathcal{G} \rightarrow grg$, la relation reliant les objectifs aux relations $grg$, représentant les objectifs comme des récompenses dans \acn{MARL}.
        % \end{enumerate*}
\end{itemize}

\subsubsection*{Résolution du \acn{Dec-POMDP} avec MOISE+MARL}

Un modèle MOISE+MARL est défini par $\mathcal{MM} = \langle \mathcal{OS}, ar, rcg, gcg, rag, rrg, grg \rangle$.
La résolution d'un \acn{Dec-POMDP} avec $mm \in \mathcal{MM}$ consiste à trouver une politique conjointe $\pi^j = {\pi^j_0, \pi^j_1, \dots, \pi^j_n}$ qui maximise la récompense cumulative espérée (ou satisfait un seuil minimal), représentée par la fonction état-valeur $V^{\pi^j}$. Cette valeur reflète le rendement d'un état initial $s \in S$ lors de l'application d'actions conjointes successives $a^j \in A^n$ sous les contraintes organisationnelles supplémentaires.
%
La définition de $V^{\pi^j}$ suit le schéma d'exécution d'agent séquentiel et cyclique (mode \acn{AEC}) et est formalisée dans \hyperref[eq:single_value_function]{Définition 1}, intégrant des adaptations basées sur les rôles (en rouge) et les missions (en bleu) qui influencent à la fois l'espace d'action et la récompense.
\autoref{fig:mm_synthesis} illustre comment les spécifications $\mathcal{M}OISE^+$ sont intégrées à la résolution \acn{Dec-POMDP} via le cadre MOISE+MARL.


\begin{figure*}[h!]
  \label{eq:single_value_function}
  \raggedright
  \textbf{\textit{Definition 1} \quad Fonction État-Valeur adaptée aux guides de contraintes en \acn{AEC} :}

  \begin{scriptsize}
    \vspace{-0.6cm}
    \begin{gather*}
      V^{\pi^j}(s_t) = \hspace{-0.75cm}
      %
      \sum_{\textcolor{red}{ \substack{a_{t} \in A \text{ if } rn() < ch_{t}, \\
            a_{t} \in A_{t} \text{ else}}
        }}{\hspace{-0.7cm} \pi_i(a_{t} | \omega_t)}
      %
      \sum_{s_{t+1} \in S}
      %
      {\hspace{-0.1cm} T(s_{t+1} | s_t, a_{t})
      \Bigl[R(s_t,a_{t},s_{t+1}) + \hspace{-0.1cm}
      \textcolor{blue}{ \sum_{m \in \mathcal{M}_i}{ \hspace{-0.1cm} v_m(t) \frac{grg_m(h_{t+1})}{1 - p + \epsilon} } }
      + } \\
      {\textcolor{red}{(1-ch_t) \times rrg(\omega_t,a_{t+1})} + V^{\pi^j_{i+1 \ mod \ n}}(s_{t+1})\Bigr]}
    \end{gather*}
    %
    \vspace{-0.5cm}
    \textcolor{red}{\[\text{ \hspace{-0.1cm} Avec } rag(h_t, \omega_t) = A_{t} \times \mathbb{R} \text{, } \langle a_t, ch_{t} \rangle \in A_{t} \times \mathbb{R} \text{ ; } rn: \emptyset \to [0,1[ \text{, une fonction aléatoire uniforme}\]}
    %
    \vspace{-0.6cm}
    \textcolor{blue}{
      \begin{gather*}
        \hspace{-0.001cm}
        \text{Avec } \omega_t = O(\omega_t | s_t, a_t) \text{ ; } h_t = \{h_0 = \langle \rangle, h_{t+1} = \langle h_t, \langle \omega_{t+1}, a_{t+1} \rangle \rangle \} \text{ ; } \epsilon \in \mathbb{R}_{>0} \text{ ; } grg_m(h) =
      \end{gather*}
    }
    \vspace{-0.95cm}
    \textcolor{blue}{
      \begin{gather*}
        \hspace{-0.5cm} \sum_{\hspace{0.3cm}(grg_i,w_i) \in mo(m)}{\hspace{-0.9cm} w_i \times grg_i(h)}
        \text{ ; } v_m(t) = \{ 1 \text{ if } t \in t_c \text{ ; else } 0 \} \text{ ; } \mathcal{M}_i = \{m_j | \langle ar(i),m_j,t_c,p \rangle \in \mathcal{M}\}
      \end{gather*}
    }
    \vspace{-0.6cm}
  \end{scriptsize}

\end{figure*}

À chaque pas de temps $t \in \mathbb{N}$ (à partir de $t=0$), l'agent $i = t \bmod n$ se voit attribuer le rôle $\rho_i = ar(i)$. Pour chaque spécification déontique temporellement valide $d_i = rds(\rho_i) = \langle tc_i, y_i, m_i \rangle$, l'agent est autorisé ($y_i = 0$) ou obligé ($y_i = 1$) à s'engager dans la mission $m_i \in \mathcal{M}$, avec un objectif $\mathcal{G}_{m_i} = mo(m_i)$ et $n \in \mathbb{N}$ agents.
%
En observant $\omega_t$, l'agent sélectionne une action parmi $A_t$ (actions attendues par le rôle) avec une probabilité $ch_t$, ou parmi $A$ sinon. Si $ch_t = 1$, l'agent est strictement contraint par son rôle.
%
L'action sélectionnée fait passer le système de $s_t$ à $s_{t+1}$, génère l'observation $\omega_{t+1}$ et renvoie une récompense composée de :
i) des bonus pour les objectifs atteints dans les missions valides (via les guides de récompenses d'objectifs), pondérés par $\frac{1}{1 - p + \epsilon}$ ;
ii) des pénalités du guide de récompenses de rôle, échelonnées par $ch_t$.
%
Le processus se poursuit dans l'état $s_{t+1}$ avec l'agent $(i + 1) \bmod n$.

\subsubsection*{Faciliter l'implémentation des \textbf{Guides de contraintes}}

Puisque les rôles, objectifs et missions sont de simples étiquettes, leur définition est implicite. Cependant, implémenter une relation \(rag\), \(rrg\) ou \(grg\) nécessite de définir de nombreux historiques, souvent redondants, rendant une définition extensionnelle fastidieuse. De plus, la logique de chaque \textbf{Guide de contraintes} analyse la trajectoire de l'agent pour vérifier son appartenance à un ensemble prédéfini. Par exemple, \(rag\) détermine les actions attendues selon l'appartenance de la trajectoire à un ensemble donné et la nouvelle observation.

Une approche consiste à laisser l'utilisateur définir ses \textbf{Guides de contraintes} via une logique personnalisée (par script, par exemple). Dans ce cas, la relation \(b_g: H \to \{0,1\}\) formalise la décision d'appartenance d'un historique à un ensemble \(H_g\).
Pour simplifier l'implémentation, nous proposons un \acn{TP}, inspiré du Traitement Automatique du Langage, noté \(p \in P\), permettant de définir intentionnellement un ensemble d'historiques.

Un \acn{TP} implique que toute observation ou action réelle considérée est connue et associée à une étiquette \(l \in L\) (via \(l: \Omega \cup A \to L\)) afin d'être gérée de manière pratique. Un \acn{TP} \(p \in P\) est défini comme suit : \(p\) est soit une « séquence feuille » notée comme un couple historique-cardinalité \(s_l = \langle h, \{c_{min}, c_{max}\} \rangle\) (où \(h \in H\), \(c_{min} \in \mathbb{N}\), \(c_{max} \in \mathbb{N} \cup \{``*"\}\)) ; soit une « séquence nœud » notée comme un couple composé d'un tuple de séquences et d'une cardinalité \(s_n = \langle \langle s_{l_1}, s_{l_2}, \dots \rangle, \{c_{min}, c_{max}\} \rangle\). Par exemple, le pattern
$
  p = ``[o_1,a_1,[o_2,a_2]\langle0,2\rangle]\langle1,*\rangle"
$
peut être formalisé comme la séquence nœud
$
  \langle \langle \langle o_1,a_1\rangle,\langle 1,1 \rangle \rangle, \langle \langle o_2,a_2\rangle,\langle 0,2 \rangle \rangle \rangle \langle 1,``*"\rangle,
$
indiquant l'ensemble des historiques \(H_p\) contenant au moins une fois la sous-séquence constituée d'une première paire \(\langle o_1,a_1\rangle\) suivie d'au maximum deux répétitions de la paire \(\langle o_2,a_2\rangle\).

\subsection{Extension de MOISE+MARL aux \textit{World Models} Multi-Agents}

\noindent Dans des environnements réalistes, on ne dispose que des transitions issues des historiques d'actions et d'observations reçues. Pour mieux représenter ce contexte, nous introduisons un nouveau formalisme appelé \textbf{\acn{Dec-POMDP} basé sur les observations} (\acparen{ODec-POMDP}). Un \acn{ODec-POMDP} $d_\Omega \in OD_\Omega$ (avec $OD_\Omega$, l'ensemble des ODec-POMDPs) est défini comme un quintuplet~:
%
$d_\Omega = \left(\Omega, A, \mathcal{T}^j, R^j_H, \gamma \right)$
%
où~:
\begin{itemize}
  \item $A$~: l'espace d'actions.
  \item $\Omega$~: l'espace d'observations.
  \item $\Omega^{\mathcal{T}^j}_0$~: l'ensemble des observations initiales conjointes.
  \item $\mathcal{T}^j(h, \omega, a) = \langle \tilde{h}', \mathbb{P}(\omega' \mid h, \omega, a) \rangle$~: le \acn{JOPM} estimant la prochaine observation conjointe $\omega'$ à partir de l'historique $\tilde{h} \in \mathcal{H}$, de l'observation conjointe actuelle $\omega$ et de l'action conjointe $a$. Le modèle renvoie également l'état caché récurrent mis à jour $\tilde{h}'$.
  \item $R^j_H~: H \times \Omega \times A \times \Omega \rightarrow \mathbb{R}$~: la fonction de récompense basée sur l'historique, calculant la récompense depuis l'historique précédent, l'observation et action courante et l'observation suivante.
  \item $\gamma \in [0, 1]$~: le facteur d'actualisation.
\end{itemize}

\noindent Cette formulation permet aux agents \acn{MARL} d'opérer uniquement à partir de données observables, rendant la méthode compatible avec les environnements simulés appris.

\subsubsection*{Résolution d'un \acn{ODec-POMDP} avec MOISE+MARL}

Résoudre un \acn{ODec-POMDP} avec des contraintes $mm \in \mathcal{MM}$ consiste à trouver une politique conjointe $\pi^j = \{\pi^j_0, \pi^j_1, \dots, \pi^j_n\}$ qui maximise la récompense cumulée espérée (ou qui satisfait un seuil minimal), via la fonction de valeur basée sur les observations $V_{\mathcal{T}^j}^{\pi^j}$. Cette fonction représente le retour attendu depuis une observation conjointe initiale $\omega^j \in \Omega^{\mathcal{T}^j}_0$, un historique $h^j$ et un état caché $\tilde{h}$, en appliquant des actions conjointes $a^j \in A^n$ sous contraintes organisationnelles $\mathcal{MM}$, et en utilisant $\mathcal{T}^j$ pour approximer les transitions.

La définition complète de $V_{\mathcal{T}^j}^{\pi^j}$ est donnée dans \hyperref[eq:single_value_function_parallel]{Définition 2}, et intègre les adaptations basées sur les rôles (en rouge) et sur les missions (en bleu), qui influencent à la fois l'espace d'actions conjointes et la récompense. La \autoref{fig:mm_synthesis} illustre comment les spécifications $\mathcal{M}OISE^+$ sont injectées dans la résolution d'un \acn{ODec-POMDP} à l'aide du cadre MOISE+MARL.

\medskip

\begin{figure*}[h!]
  \label{eq:single_value_function_parallel}
  \raggedright
  \textbf{\textit{Definition 2} \quad Fonction Observation-Valeur adaptée aux guides de contraintes en mode parallèle:}

  \begin{scriptsize}
    \vspace{-0.6cm}
    \begin{gather*}
      \hspace{-1cm}V^{\pi^j}(\tilde{h}_{t-1},h^j_{t-1},\hat{\omega}^j_t) = \hspace{-0.95cm}
      %
      \sum_{\textcolor{red}{ \substack{a^j_{t} \in A^j \text{ if } rn() < ch_{t}, \\
      a^j_{t} \in A^j_{t} \text{ else}}
      }}{\hspace{-0.9cm} \pi_i(a^j_{t} | \hat{\omega}^j_t)}
      %
      \hspace{-1.2cm}
      \sum_{\phantom{XXXX}(\tilde{h}_t,\hat{\omega}^j_{t+1}) \in \mathcal{H} \times \hat{\Omega}^j}
      %
      {\hspace{-1.2cm} \mathcal{T}^j(\langle \tilde{h}_t,\hat{\omega}^j_{t+1} \rangle | \tilde{h}_{t-1}, \hat{\omega}_t, a^j_{t})
      \Bigl[R^j_H(h^j_{t-1},\hat{\omega}^j_t,a^j_t,\hat{\omega}^j_{t+1}) \hspace{-0.1cm} }
    \end{gather*}
    %
    \vspace{-1cm}
    \begin{gather*}
      \hspace{3cm}
      {+ \  \textcolor{blue}{grg^j_m(h^j_t)}
      +
      \textcolor{red}{(1-ch_t) \times rrg^j(\hat{\omega}^j_t,a^j_{t+1})} + V^{\pi^j}(\tilde{h}_{t}, h^j_t, \hat{\omega}^j_{t+1})\Bigr]}
    \end{gather*}
    %
    \vspace{-0.15cm}
    %
    \[\hspace{-0.9cm}\text{Avec \ } \tilde{h}_{-1} = \mathbf{0} \text{ and } \tilde{\omega}^j_0 \in \Omega_0^{\mathcal{T}^j} \text{ ; } a^j_t = \langle a_{t,0}, a_{t,1} \dots a_{t,|\mathcal{A}|} \rangle \text{ ; } \omega^j_t = \langle \omega_{t,0}, \omega_{t,1} \dots \omega_{t,|\mathcal{A}|} \rangle \text{ ; }\]
    %
    \vspace{-0.25cm}
    \[\hspace{-0.5cm} h^j_t = \langle h_{t,0}, h_{t,1} \dots h_{t,|\mathcal{A}|} \rangle = \langle \langle h_{t-1,i}, \omega_{t,i}, a_{t,i} \rangle \rangle_{i \in \mathcal{A}}\]
    %
    \vspace{-0.2cm}
    \textcolor{red}{\[\hspace{-1cm}\text{ \hspace{-0.1cm} Avec } \langle rag_i, rrg_i \rangle = rcg(ar(i)) \text{ ; } rn: \emptyset \to [0,1[ \text{, une fonction aléatoire uniformef}\]}
    %
    \vspace{-0.3cm}
    \textcolor{red}{\[A^j_t \times \mathbf{R}^{|\mathcal{A}|} = rag^j(h^j_t, \tilde{\omega}^j_t) = \langle rag_i(h_{t,i}, \omega_{t,i}) \rangle_{i \in \mathcal{A}} \text{ ; } rrg^j(h^j_t, \tilde{\omega}^j_t, a^j_t) = \sum_{i \in \mathcal{A}}{rrg_i(h_{t,i}, \omega_{t,i}, a_{t,i})}\]}
    %
    \vspace{-0.75cm}
    \textcolor{blue}{
      \begin{gather*}
        \hspace{-1cm} grg_m(h) = \hspace{-1cm} \sum_{\hspace{0.3cm}(grg_i,w_i) \in mo(m)}{\hspace{-1.1cm} w_i \times grg_i(h)}
        \text{ ; }
        grg^j_m(h^j_t) = \hspace{-0.1cm} \sum_{i \in \mathcal{A}}{\sum_{m \in \mathcal{M}_i}{ \hspace{-0.1cm} v_m(t) \frac{grg_m(h_{t,i})}{1 - p + \epsilon} }} \text{ ; } \epsilon \in \mathbb{R}_{>0} \text{ ; }
      \end{gather*}
    }
    \vspace{-0.9cm}
    \textcolor{blue}{
      \begin{gather*}
        \hspace{-1cm}
        v_m(t) = \{ 1 \text{ if } t \in t_c \text{ ; else } 0 \} \text{ ; } \mathcal{M}_i = \{m_j | \langle ar(i),m_j,t_c,p \rangle \in \mathcal{M}\}
      \end{gather*}
    }
  \end{scriptsize}

\end{figure*}

\noindent En mode parallèle, à chaque pas de temps $t \in \mathbb{N}$ (en commençant à $t=0$), chaque agent $i \in \mathcal{A}$ est assigné à un rôle $\rho_i = ar(i)$. Pour chaque spécification déontique temporellement valide $d_i = rds(\rho_i) = \langle tc_i, y_i, m_i \rangle$, l'agent est soit autorisé ($y_i = 0$), soit obligé ($y_i = 1$) de s'engager dans la mission $m_i \in \mathcal{M}$, avec ensemble d'objectifs $\mathcal{G}_{m_i} = mo(m_i)$.

Lorsque les agents observent $\tilde{\omega}_t^j$, ils sélectionnent leurs actions dans $A_{i,t}$ (dérivées via les guides de récompense de rôle) avec une probabilité $ch_t$, ou dans $A_t$ sinon. Si $ch_t = 1$, les agents sont strictement contraints par leur rôle.

Les transitions d'observation et d'état sont approximées via la fonction $\mathcal{T}^j$ à partir de l'état caché précédent $\tilde{h}_{t-1}$, de l'observation conjointe $\omega^j_t$ et de l'action conjointe $a^j_t$. La fonction de récompense $R^j_H$ utilise ces mêmes informations, ainsi que l'observation suivante, pour produire la récompense. Des bonus ou malus sont ensuite ajoutés selon~:
i) l'atteinte d'objectifs valides (via les guides de récompense des objectifs, pondérés par $\frac{1}{1 - p + \epsilon}$),
ii) la conformité au rôle (via les guides de récompense de rôle, pondérés par $1 - ch_t$).


\section{Description et mise en oeuvre dans l'activité}

L'\autoref{alg:training_mamad} décrit le déroulement général de l'activité d'entraînement.
Chaque étape est détaillée ci-après.

\paragraph{Étape 1 : Formalisation des contraintes.}
Si les spécifications $\mathcal{MM}$ ne sont pas fournies, elles sont dérivées manuellement à partir des contraintes informelles $\mathcal{C}_{\text{inf}}$.

\paragraph{Étape 2 : Initialisation.}
Initialiser les paramètres de la politique conjointe $\pi^j$ et un buffer d'expérience $\mathcal{B}$.

\paragraph{Étape 3 : Exécution d'épisodes simulés.}
Pour chaque épisode, échantillonner une observation initiale $\omega_0^j \in \Omega^{\mathcal{T}^j}_0$, initialiser l'historique $h_{-1}^j$ et l'état caché $\tilde{h}_{-1}$.

\paragraph{Étape 4 : Sélection des actions sous contraintes.}
À chaque étape $t$, calculer l'ensemble des actions autorisées $A_t^j = rag^j(h^j_t,\omega_t^j)$.
L'agent sélectionne son action $a_t^j$ parmi $A_t^j$ avec probabilité $ch_t$, sinon dans $A$.

\paragraph{Étape 5 : Transition et mise à jour du JOPM.}
La transition $(\tilde{h}_t,\omega_{t+1}^j)$ est générée par $\mathcal{T}^j(\tilde{h}_{t-1},\omega_t^j,a_t^j)$.

\paragraph{Étape 6 : Calcul des récompenses.}
La récompense $r_t$ combine :
\begin{itemize}
  \item la récompense de base $R^j_H$,
  \item un bonus $grg^j(h^j_t)$ si des objectifs sont atteints,
  \item un bonus/malus $(1-ch_t)\times rrg^j(h^j_t,\omega_t^j,a_t^j)$ lié au respect du rôle.
\end{itemize}

\paragraph{Étape 7 : Mise à jour de l'expérience et apprentissage.}
Les transitions sont stockées dans $\mathcal{B}$, et la politique $\pi^j$ est optimisée par tout algorithme \acn{MARL} compatible (Q-learning, Policy Gradient, \acn{CTDE}, etc.).

\paragraph{Étape 8 : Sortie.}
À la fin des épisodes, l'activité retourne la politique conjointe entraînée $\pi^j$.


\begin{algorithm}[H]
  \caption{Algorithme de l'activité d'entraînement}
  \label{alg:training_mamad}
  \DontPrintSemicolon

  \KwIn{
  Modèle de Prédiction d'Observations Conjointes (\acn{JOPM}) $\mathcal{T}^j$,
  Observations initiales conjointes $\Omega_0^{\mathcal{T}^j}$,
  Fonction de récompense $R_H^j$,
  Fonction d'arrêt $S_H^j$,
  Fonction de rendue $\text{Render}^j_H$,
  Spécifications organisationnelles $\mathcal{MM}$,
  Contraintes de conception informelles $\mathcal{C}_{\text{inf}}$,
  Facteur d'actualisation $\gamma$
  Espace d'observations $\Omega$,
  Espace d'actions $A$
  }
  \KwOut{$\pi^j$~: Politique conjointe entraînée}

  \vspace{0.3em}

  \If{$\mathcal{MM} = \emptyset$}{
    $\mathcal{MM} \gets \texttt{manual\_formalize}(\mathcal{C}_{\text{inf}})$ \tcp*{Formalisation manuelle specs. orgs.} \;
  }

  Initialiser les paramètres de la politique $\pi^j$ et du buffer de replay $\mathcal{B}$ \;

  \ForEach{épisode $e = 1 \dots N$}{
  Échantillonner $\omega_0^j \sim \Omega_0^{\mathcal{T}^j}$, initialiser $\tilde{h}_{-1} \gets \mathbf{0}$ \;
  Initialiser l'historique $h_{-1}^j \gets \emptyset$ \;

  \ForEach{étape $t = 0 \dots T$}{
  Calculer $A_t^j = rag^j(h^j_t, \omega^j_t)$ via les guides de récompense de rôle dans $\mathcal{MM}$ \;
  \If{$rn() < ch_t$}{
    Sélectionner $a_t^j \sim \pi^j(\cdot | \omega_t^j)$ dans l'ensemble $A_t^j$ (contraint) \;
  }
  \Else{
    Sélectionner $a_t^j \sim \pi^j(\cdot | \omega_t^j)$ dans l'ensemble $A_t$ \;
  }

  $(\tilde{h}_t, \omega_{t+1}^j) \gets \mathcal{T}^j(\tilde{h}_{t-1}, \omega_t^j, a_t^j)$ \tcp*{Prédiction JOPM}

  $r_t \gets \gamma^t \times R_H^j(h^j_{t-1}, \omega_t^j, a_t^j, \omega_{t+1}^j)$ \tcp*{Récompense de base}

  $r_t \gets r_t + grg^j(h^j_t)$ \tcp*{Bonus via guides d'objectifs}

  $r_t \gets r_t + (1 - ch_t) \times rrg^j(h^j_t, \omega_t^j, a_t^j)$ \tcp*{Bonus/malus via guides de rôle}

  Ajouter $(\omega_t^j, a_t^j, r_t, \omega_{t+1}^j)$ à $\mathcal{B}$ \;
  Mettre à jour $h^j_t \gets \langle \langle h_{t-1,i}, \omega_{t,i}, a_{t,i} \rangle \rangle_{i \in \mathcal{A}}$ \;

  Entraîner $\pi^j$ avec des mini-lots tirés de $\mathcal{B}$ en utilisant toute méthode \acn{MARL} \;
  }
  }

  \Return{$\pi^j$}
\end{algorithm}



\section{Synthèse}

En synthèse, l'activité d'entraînement permet de produire des politiques conjointes optimisées, intégrant à la fois les contraintes organisationnelles explicites et les dynamiques apprises via les World Models multi-agents.
Cette approche concilie performance et adaptation tout en offrant un certain degré de contrôle et d'explicabilité.

Les principales limites identifiées concernent :
\begin{itemize}
  \item la scalabilité du \acn{MARL} contraint à un grand nombre d'agents ;
  \item la dépendance aux données disponibles pour apprendre un \acn{JOPM} fidèle ;
  \item le compromis entre respect strict des contraintes et performance optimale.
\end{itemize}

Ces éléments préparent l'\textbf{activité d'analyse}, qui vise à évaluer et expliciter les politiques entraînées.


\clearpage
\thispagestyle{empty}
\null
\newpage

\chapter{Analyser les comportements émergents}
\label{chap:analyzing}

L'\textit{activité d'analyse}\index{Analyse (ANL)} vise à évaluer et expliciter les politiques conjointes apprises. Elle fournit une explication des comportements observés en termes de rôles, objectifs et missions. Cette activité joue un rôle central pour l'explicabilité en reliant les dynamiques apprises aux structures organisationnelles interprétables.


\section*{Objectifs formels}


Les \textbf{entrées} de l'activité d'analyse sont :
\begin{itemize}
  \item une politique conjointe entraînée $\pi^j$ ;
  \item un \acn{ODec-POMDP} $d_\Omega$ représentant l'environnement simulé appris ;
  \item une spécification organisationnelle initiale $\mathcal{MM}$ (optionnelle).
\end{itemize}

Les \textbf{sorties attendues} sont :
\begin{itemize}
  \item une spécification implicite inférée $\mathcal{MM}_{\text{implicit}}$ ;
  \item un score d'adéquation organisationnelle $\text{OF}$.
\end{itemize}

\noindent La relation globale peut être exprimée par :
\[
  (\mathcal{MM}_{\text{implicit}}, \text{OF}) \gets \texttt{analyze}(d_\Omega, \pi^j, \mathcal{MM})
\]


\section{Travaux mobilisés et verrous identifiés}

L'activité d'analyse des comportements émergents s'appuie sur trois axes principaux : l'explicabilité post-hoc en apprentissage automatique, l'analyse des politiques multi-agents, et l'inférence organisationnelle à partir de trajectoires.

Les méthodes d'explicabilité locales (\acn{SHAP}, \acn{LIME}, \acn{CAV}) expliquent les décisions individuelles mais restent limitées pour l'analyse globale des dynamiques collectives. Les modèles interprétables (arbres de décision, extraction de concepts) offrent une certaine lisibilité mais peinent à capturer la complexité organisationnelle à grande échelle.

Des approches de clustering de trajectoires ou d'inférence de rôles permettent d'identifier des spécialisations ou missions collectives, mais nécessitent souvent un paramétrage manuel et restent déconnectées des modèles organisationnels symboliques.

À ce jour, aucune méthode n'extrait automatiquement des spécifications organisationnelles complètes (rôles, missions, permissions, obligations) à partir des trajectoires, ni ne relie systématiquement les comportements émergents à un cadre symbolique tel que $\mathcal{M}OISE^+$.

Les principaux verrous sont :
\begin{itemize}
  \item l'absence de cadre pour évaluer quantitativement l'explicabilité organisationnelle ;
  \item le manque d'automatisation dans l'inférence des structures organisationnelles ;
  \item l'absence de méthode pour relier les structures extraites à des modèles symboliques existants.
\end{itemize}

Ces limites motivent le développement de la méthode \acn{TEMM} et d'Auto-\acn{TEMM}, pour automatiser l'inférence organisationnelle, quantifier l'adéquation organisationnelle, et relier les comportements émergents à des spécifications formelles exploitables dans la boucle de conception.



\section{Positionnement et contributions proposées}
L'activité d'analyse est centrale dans la méthode \acn{MAMAD}, car elle relie les dynamiques apprises à des structures organisationnelles interprétables et évalue quantitativement leur alignement.

\textbf{Définition~:} L'\textit{adéquation organisationnelle} est théorisé comme la mesure de la conformité entre les comportements collectifs observés et les spécifications organisationnelles attendues (rôles, objectifs, missions, permissions, obligations). Elle est quantifiée par un indicateur global (\textbf{\acn{OF}}), combinant cohérence structurelle (\acn{SOF}) et fonctionnelle (\acn{FOF}) extraites des trajectoires.

Notre approche propose~:
\begin{itemize}
  \item une mesure quantitative globale de l'adéquation organisationnelle \acn{OF} ;
  \item l'extraction automatique de spécifications organisationnelles implicites à partir des trajectoires ;
  \item une méthode flexible, utilisable en mode manuel (\acn{TEMM}) pour l'analyse experte, ou automatisé (Auto-\acn{TEMM}) pour l'industrialisation.
\end{itemize}

\textbf{TEMM} permet un contrôle expert et une analyse qualitative, tandis que \textbf{Auto-TEMM} automatise l'analyse et optimise les hyperparamètres pour une utilisation à grande échelle.

Les contributions principales sont~:
\begin{itemize}
  \item l'introduction d'un indicateur robuste d'adéquation organisationnelle ;
  \item la formalisation d'une méthode d'inférence organisationnelle adaptée au multi-agent ;
  \item l'automatisation de l'analyse via Auto-\acn{TEMM} ;
  \item l'intégration de cette analyse dans la boucle de conception \acn{MAMAD}.
\end{itemize}

En synthèse, l'adéquation organisationnelle permet d'évaluer, comparer et raffiner objectivement les comportements émergents, garantissant que les \acn{SMA} produits sont performants, explicables et alignés sur les exigences organisationnelles.

\subsection{La méthode TEMM}
\label{sec:TEMM_algorithm}

La méthode \acn{TEMM} fait partie du composant d'explicabilité du cadre MOISE+MARL. Elle repose sur l'hypothèse que les comportements des agents, malgré une variabilité apparente, présentent des régularités lorsqu'ils atteignent des récompenses cumulées comparables. Ainsi, des comportements différents peuvent être interprétés comme des variantes bruitées d'un nombre limité de stratégies latentes. D'après la loi des grands nombres, une moyenne sur un ensemble suffisant d'historiques conjoints réussis permet de filtrer le bruit et de révéler des stratégies typiques.

La méthode exploite des techniques d'apprentissage non supervisé pour inférer des spécifications organisationnelles à partir des trajectoires observées des agents, et pour calculer l'\textbf{adéquation organisationnelle} (\acn{OF}) entre les comportements émergents et les rôles, objectifs et missions attendus. Elle se décline en trois volets.

\paragraph{1) Rôles et héritage de rôles}
Les trajectoires $(\omega, a) \in \Omega \times A$ sont regroupées en clusters à l'aide de métriques de distance (par exemple \acparen{LCS}, Smith-Waterman), éventuellement après encodage one-hot des actions.
Dans ce cadre, un \textbf{rôle} $\rho$ est défini comme une politique dont les agents partagent une \textbf{Séquence Commune} (SC) dans leurs historiques.
Un rôle $\rho_2$ hérite de $\rho_1$ si $\text{SC}(\rho_2) \subseteq \text{SC}(\rho_1)$.
Le clustering hiérarchique permet d'extraire ces \acn{SC} et de construire une hiérarchie des rôles.
Pour chaque cluster, un centroïde de transitions moyennes par pas de temps est calculé. Une procédure de sélection retient les transitions les plus représentatives, interprétées comme des \textbf{règles comportementales} associées à un rôle.
Une faible représentativité conduit à inclure toutes les transitions, au risque de sur-apprentissage.
Le \textbf{\acn{SOF}} (structural organizational fit) est calculé comme l'inverse normalisé de la variance globale dans les clusters de transitions~: une faible variance indique une forte cohérence structurelle.

\paragraph{2) Objectifs, plans et missions}
Les trajectoires d'observations sont regroupées en clusters à l'aide de métriques de distance ou via des méthodes vectorielles (par ex. K-means sur des embeddings de trajectoires). Pour chaque cluster, une trajectoire centroïde est calculée, associant chaque pas de temps à une observation moyenne.
La \textbf{représentativité} est définie comme l'inverse normalisé de la variance locale par pas de temps.
Un seuil minimal de représentativité est appliqué pour sélectionner les observations saillantes, interprétées comme des \textbf{objectifs intermédiaires} – jalons importants vers l'objectif global.
Si la représentativité minimale est élevée, seules les observations très fréquentes sont retenues, assurant robustesse et pertinence.
Les \textbf{plans} sont inférés comme des sous-séquences de transitions menant systématiquement à ces objectifs.
Une \textbf{mission} regroupe un ou plusieurs objectifs poursuivis collectivement par un ou plusieurs agents.
Le \textbf{\acn{FOF}} (functional organizational fit) évalue la cohérence fonctionnelle des agents dans l'atteinte de ces objectifs intermédiaires, calculé comme l'inverse normalisé de la variance dans les clusters d'observations.

\paragraph{3) Permissions et obligations}
Les permissions et obligations sont dérivées en analysant si les agents remplissant un rôle accomplissent systématiquement (ou exclusivement) certaines missions dans des contraintes temporelles données.
Une obligation implique exclusivité, tandis qu'une permission implique optionnalité.

\paragraph{Agrégation et interprétation}
L'\textbf{adéquation organisationnelle globale} est obtenue en agrégeant les scores structurel et fonctionnel :
\[
  \text{OF} = \frac{1}{2} \left( \text{SOF} + \text{FOF} \right)
\]
Un score élevé indique que les spécifications inférées (rôles, objectifs, missions) sont représentatives des comportements réellement appris.
Un score faible suggère une faible structuration ou des comportements peu cohérents.
Bien que certains hyperparamètres de clustering puissent nécessiter un ajustement manuel pour garantir la robustesse de l'extraction des rôles et objectifs, \acn{TEMM} fournit une approche méthodique pour analyser les comportements organisationnels émergents et affiner les spécifications en conséquence.


\subsection{Auto-TEMM : la méthode TEMM étendue avec optimisation des hyperparamètres}

Un problème majeur rencontré avec \acn{TEMM} est la nécessité de choisir manuellement plusieurs hyperparamètres (métriques de distance, seuils de clustering, seuils de représentativité), ce qui ralentit le processus d'analyse et limite son automatisation. Une représentativité trop faible conduit à du sur-apprentissage, tandis qu'une représentativité trop élevée limite les contraintes et ralentit la convergence.


Pour surmonter cette difficulté, nous proposons un processus d'\textbf{optimisation d'hyperparamètres} (\acparen{HPO}) consistant en une recherche par grille (grid search) sur les combinaisons possibles, visant à maximiser le adéquation  organisationnel et minimiser le nombre de clusters~:

\begin{itemize}
  \item (i) Pour les observations et les transitions, appliquer une recherche conjointe sur les métriques de distance et les seuils de clustering afin de minimiser la variance intra-cluster et le nombre de clusters~;
  \item (ii) Déterminer les représentativités minimales (structurelle et fonctionnelle) pour obtenir des objectifs et des rôles concis et pertinents. Diminuer cette représentativité augmente la couverture mais réduit la robustesse des contraintes organisationnelles. Une valeur élevée limite la généralisation, tandis qu'une valeur trop faible entraîne un sur-apprentissage. Cela est illustré dans la \autoref{fig:conv_time_repr}, où le temps de convergence normalisé est tracé en fonction de la représentativité minimale. Une convergence rapide indique des contraintes organisationnelles fortes et cohérentes, tandis qu'une convergence lente suggère des contraintes faibles ou incohérentes.
\end{itemize}

Nous adoptons un compromis basé sur le \textbf{point de coude} du graphique convergence/temps (voir \autoref{fig:conv_time_repr}), en choisissant la plus grande représentativité assurant une convergence normalisée de 3.5\%. Cette stratégie permet d'obtenir des spécifications utiles, interprétables et généralisables, sans complexité excessive.

\begin{figure}[h!]
  \centering
  \includegraphics[trim=0cm 0cm 0cm 0cm, clip, width=1.\linewidth]{figures/convergence_time_relative_to_representativeness.pdf}
  \caption{Temps de convergence normalisé en fonction de la représentativité minimale}
  \label{fig:conv_time_repr}
\end{figure}




\section{Description et mise en oeuvre dans l'activité}

L'\autoref{alg:auto_temm} formalise le déroulement général de l'activité d'analyse.
Chaque étape est détaillée ci-dessous afin d'expliciter les mécanismes qui permettent d'inférer une spécification organisationnelle implicite et de calculer l'adéquation organisationnelle.

\paragraph{Étape 1 : Collecte des trajectoires.}
La première étape consiste à exécuter la politique conjointe $\pi^j$ dans l'environnement $d_\Omega$ afin de générer des historiques complets de transitions $(\omega, a, \omega')$.
Deux ensembles de données sont extraits :
\begin{itemize}
  \item $\mathcal{D}_{\text{trans}}$ contenant les séquences de transitions $(\omega_t, a_t, \omega_{t+1})$, utilisées pour l'inférence des rôles ;
  \item $\mathcal{D}_{\text{obs}}$ contenant uniquement les séquences d'observations $(\omega_t)$, utilisées pour l'inférence des objectifs et missions.
\end{itemize}

\paragraph{Étape 2 : Optimisation des distances et seuils de clustering.}
Pour réduire la variabilité entre trajectoires et identifier des structures récurrentes, les ensembles $\mathcal{D}_{\text{obs}}$ et $\mathcal{D}_{\text{trans}}$ sont soumis à un processus de clustering.
On explore plusieurs métriques de distance $\delta_t$ (ex. \acn{LCS}, Smith-Waterman, distances vectorielles) et seuils de regroupement $\tau_t$.
Chaque combinaison $(\delta_t, \tau_t)$ est évaluée selon un score pondérant :
\[
  \text{Score} = \alpha (\sigma_{\text{obs}} + \sigma_{\text{trans}}) + \beta N_{\text{clusters}}
\]
où $\sigma$ désigne la variance intra-cluster et $N_{\text{clusters}}$ le nombre total de clusters.
La combinaison minimisant ce score est retenue, garantissant un compromis entre cohérence interne et compacité des clusters.

\paragraph{Étape 3 : Application du clustering optimal.}
Une fois les hyperparamètres optimaux $(\delta^*, \tau^*)$ déterminés, les trajectoires sont regroupées :
\begin{itemize}
  \item les clusters de transitions $C_{trans}$ permettent d'inférer les \textbf{rôles} par extraction des séquences communes (\acn{CLS}) et des règles comportementales associées ;
  \item les clusters d'observations $C_{obs}$ servent à identifier les \textbf{objectifs intermédiaires} et les plans associés.
\end{itemize}

\paragraph{Étape 4 : Recherche de représentativité optimale.}
Le degré de représentativité $\rho_t$ fixe le seuil minimal pour qu'une transition ou observation soit retenue comme caractéristique d'un rôle ou objectif.
Une recherche par grille est effectuée sur différentes valeurs de $\rho_t$.
Pour chaque $\rho_t$, une spécification $\mathcal{MM}_{\rho_t}$ est inférée, puis une nouvelle politique $\pi^j_{\rho_t}$ est réentraînée dans $d_\Omega$.
On enregistre alors le temps de convergence $c_{\rho_t}$ pour atteindre une performance minimale $R_{\min}$.
Le paramètre optimal $\rho_t^*$ est choisi comme la plus grande représentativité garantissant une convergence inférieure au seuil $\eta$ (par défaut 3.5\%).

\paragraph{Étape 5 : Inférence finale des rôles et objectifs.}
Avec $(\delta^*, \tau^*, \rho^*)$, on extrait les rôles $\mathcal{R}$, les objectifs intermédiaires $\mathcal{G}$ et leurs relations hiérarchiques (missions, héritage de rôles).
Les permissions et obligations sont déduites en observant la systématicité (obligations) ou la variabilité (permissions) des associations rôle–mission dans les trajectoires.

\paragraph{Étape 6 : Calcul de l'adéquation organisationnelle.}
Deux indicateurs partiels sont calculés :
\begin{itemize}
  \item le \textbf{\acn{SOF}} (structural organizational fit), mesurant la cohérence des rôles via la variance des transitions intra-cluster ;
  \item le \textbf{\acn{FOF}} (functional organizational fit), mesurant la cohérence fonctionnelle dans l'atteinte des objectifs intermédiaires.
\end{itemize}
L'indicateur global est obtenu par agrégation :
\[
  \text{OF} = \frac{1}{2}(\text{SOF} + \text{FOF})
\]

\paragraph{Étape 7 : Sorties de l'activité.}
L'activité retourne :
\begin{itemize}
  \item une spécification implicite $\mathcal{MM}_{\text{implicit}}$ décrivant rôles, missions, permissions et obligations inférés automatiquement ;
  \item un score d'adéquation organisationnelle $\text{OF}$ permettant de quantifier la qualité organisationnelle des comportements émergents.
\end{itemize}


\begin{algorithm}[H]
  \caption{Algorithme de l'activité d'analyse}
  \label{alg:auto_temm}
  \DontPrintSemicolon

  \KwIn{
    Politique conjointe entraînée $\pi^j$~;
    \acn{ODec-POMDP} $d_\Omega$~;
    Spécification initiale $\mathcal{MM}$~;
    Seuil de convergence normalisé (défaut~: 3.5\%) $\eta$
  }

  \KwOut{
  Spécification organisationnelle inférée $\mathcal{MM}_{\text{implicit}}$~;
  Score de adéquation  organisationnel $\acn{OF}$
  }

  \tcp*[l]{1. Collecte des trajectoires}
  Générer les historiques individuels $\mathcal{D}_{\text{trans}}$ depuis $d_\Omega$ sous $\pi^j$ \;
  $\mathcal{D}_{\text{obs}} \gets$ trajectoires d'observations individuelles issues de $\mathcal{D}_{\text{full}}$ \;

  \tcp*[l]{2. \acn{HPO} sur distance et seuil de clustering}
  \For{$t \in \{obs, trans\}$}{
    \ForEach{métrique de distance $\delta_t$}{
      \ForEach{seuil minimal de cluster $\tau_t$}{
        Appliquer clustering avec $(\delta_t, \tau_t)$ \;
        Calculer $\sigma_{\text{obs}}, \sigma_{\text{trans}}, N_{\text{clusters}}$ \;
        Score $\gets \alpha (\sigma_{\text{obs}} + \sigma_{\text{trans}}) + \beta N_{\text{clusters}}$ \tcp*[l]{par défaut~: $\alpha=0.4$, $\beta=0.6$}
        Retenir $(\delta_t^*, \tau_t^*)$ avec Score minimal \;
      }
    }
  }

  \tcp*[l]{3. Application du clustering avec \acn{HPO} optimal}
  Clustering des observations~: $\mathcal{D}_{\text{obs}} \rightarrow C_{obs}$ via $(\delta_{obs}^*, \tau_{obs}^*)$ \;
  Clustering des transitions~: $\mathcal{D}_{\text{trans}} \rightarrow C_{trans}$ via $(\delta_{trans}^*, \tau_{trans}^*)$ \;

  \tcp*[l]{4. \acn{HPO} sur la représentativité (convergence)}
  \For{$t \in \{obs, trans\}$}{
  \ForEach{représentativité $\rho_t$}{
  Inférer $\mathcal{MM}_{\rho_t}$ à partir des clusters \;
  Initialiser une politique $\pi^j_{\rho_t}$ \;
  Entraîner $\pi^j_{\rho_t}$ sur $(d_\Omega, \mathcal{MM}_{\rho_t})$ jusqu'à atteindre $R_{\min}$ \;
  Enregistrer le temps de convergence $c_{\rho_t}$ tel que $ct_t(\rho_t) = c_{\rho_t}$ \;
  }

  \tcp*[l]{5. Sélectionner le point de coude}
  $\rho_t^* \gets max(\{\rho_t \ | \ ct_t(\rho_t) < \eta \})$ \tcp*[r]{par défaut $\eta = 3.5\%$}
  }

  \tcp*[l]{6. Inférence finale des rôles et objectifs}
  Inférer les rôles à partir de $\mathcal{D}_{\text{trans}}, \delta^*, \tau^*, \rho^*$ \;
  Inférer les objectifs à partir de $\mathcal{D}_{\text{obs}}, \delta^*, \tau^*, \rho^*$ \;

  \tcp*[l]{7. Calcul du adéquation  organisationnel}
  Calculer \acn{SOF} et \acn{FOF} à partir des variances intra-cluster \;
  $\text{OF} \gets \frac{1}{2}(\text{SOF} + \text{FOF})$ \;

  \Return{$\mathcal{MM}_{\text{implicit}}, \text{OF}$}
\end{algorithm}


\section{Synthèse}

En synthèse, l'activité d'analyse permet d'établir un lien objectif entre les comportements émergents des agents et les structures organisationnelles attendues. Grâce à la méthode \acn{TEMM} et à son extension Auto-\acn{TEMM}, il devient possible d'inférer automatiquement des rôles, objectifs et missions à partir des trajectoires, et de quantifier leur adéquation organisationnelle par un indicateur robuste. Cette démarche favorise l'explicabilité, la traçabilité et le raffinement itératif des spécifications organisationnelles, tout en fournissant des outils d'évaluation pour comparer différentes politiques ou configurations. Toutefois, la qualité de l'analyse dépend de la diversité des trajectoires collectées et du choix des hyperparamètres de clustering, même si l'automatisation par optimisation conjointe permet de limiter l'intervention humaine. Cette activité constitue ainsi un pivot essentiel pour la boucle de conception \acn{MAMAD}, en préparant le transfert et l'amélioration continue des politiques dans l'environnement réel.


\clearpage
\thispagestyle{empty}
\null
\newpage

\chapter{Transférer et superviser en environnement réel}
\label{chap:transferring}

L'\textit{activité de transfert}\index{Transfert (TRF)} correspond à la mise en production et au suivi des politiques conjointes dans l'environnement réel.
Elle joue un double rôle : (i) assurer l'exécution continue de la politique la plus récente $\pi^j_{\text{latest}}$ dans $\mathcal{E}$, garantissant l'action efficace des agents, et (ii) collecter de nouvelles trajectoires réelles $(\omega^j_t, a^j_t, \omega^j_{t+1})$ pour enrichir la base de données $\mathcal{D}_{H^j}$, permettant la mise à jour du modèle simulé et des spécifications organisationnelles.

\section*{Objectifs formels}

Les \textbf{entrées} de l'activité de transfert sont :
\begin{itemize}
  \item la politique conjointe la plus récente $\pi^j_{\text{latest}}$ ;
  \item l'environnement réel $\mathcal{E}$ ;
  \item la base de trajectoires accumulées $\mathcal{D}_{H^j}$.
\end{itemize}

Les \textbf{sorties attendues} sont :
\begin{itemize}
  \item une base enrichie de trajectoires $\mathcal{D}_{H^j}$ ;
  \item un signal $\texttt{need\_update}$ déclenchant la reprise du cycle de conception.
\end{itemize}

La relation globale peut être exprimée par :
\[
  \mathcal{D}_{H^j}, \texttt{need\_update} \gets \texttt{transfer}(\pi^j_{\text{latest}}, \mathcal{E}, \mathcal{D}_{H^j})
\]

\section{Travaux mobilisés et verrous identifiés}

L'activité de transfert et de supervision en environnement réel s'appuie sur plusieurs axes : le transfert de politiques (policy transfer), l'adaptation de domaine (sim2real), la calibration dynamique des modèles (online model calibration), et la supervision continue des systèmes multi-agents.

Les approches de \textit{Robust Reinforcement Learning}~\cite{pinto2017robust} visent à rendre les politiques résistantes aux écarts simulation/réalité, mais n'intègrent pas la mise à jour du modèle simulé après déploiement. Les méthodes d'adaptation de domaine et \textit{Sim2Real}~\cite{tobin2017domain,ganin2016domain} réduisent l'écart simulation/réel via la randomisation ou l'apprentissage de représentations invariantes, mais leur adaptation en ligne reste limitée. Les techniques de calibration dynamique~\cite{deisenroth2011pilco} mettent à jour le modèle simulé à partir des retours du réel, sans prise en compte explicite de l'adaptation des politiques multi-agents. Enfin, la synchronisation manuelle reste courante mais peu adaptée aux environnements dynamiques.

Les principaux verrous sont :
\begin{itemize}
  \item l'absence de cadre unifié pour la mise à jour conjointe du modèle simulé et des politiques déployées ;
  \item la difficulté à détecter et corriger automatiquement les écarts simulation/réalité ;
  \item le manque de mécanismes intégrés pour garantir robustesse et sécurité lors du transfert ;
  \item la nécessité d'une supervision continue et automatisée.
\end{itemize}

Ces limites motivent le développement d'un cadre méthodologique assurant l'adaptation conjointe du jumeau numérique et des politiques multi-agents, avec supervision automatisée du transfert.

\section{Positionnement et contributions proposées}

L'approche proposée introduit un \textbf{cadre de transfert asynchrone et événementiel} comprenant :
\begin{itemize}
  \item une \textbf{Boucle de transfert} (\texttt{Transfer Loop}) qui assure l'exécution en continu de la politique et la collecte des trajectoires dans un tampon temporaire $\mathcal{B}$ ;
  \item un \textbf{Déclencheur de mise à jour} (\texttt{Update Trigger}) qui ajoute les trajectoires à la base $\mathcal{D}_{H^j}$ et active, de façon asynchrone, les activités de modélisation et d'entraînement dès qu'un seuil $\texttt{batch\_size}$ est atteint.
\end{itemize}

Ce double mécanisme assure la continuité du fonctionnement des agents, tout en maintenant la boucle de conception synchronisée avec les données réelles.

\section{Description et mise en œuvre de l'activité}

L'\autoref{alg:transferring} formalise le fonctionnement de l'activité de transfert.
Chaque élément est décrit ci-dessous afin de préciser les mécanismes et leur rôle dans la boucle de conception.

\paragraph{Entrées et sorties.}
L'activité reçoit en entrée :
\begin{itemize}
  \item la politique conjointe la plus récente $\pi^j_{\text{latest}}$, produite lors de l'entraînement ;
  \item l'environnement réel $\mathcal{E}$, représentant le domaine opérationnel où le \acn{SMA} agit ;
  \item la base courante de trajectoires $\mathcal{D}_{H^j}$, enrichie au fil des déploiements.
\end{itemize}
En sortie, elle retourne :
\begin{itemize}
  \item une base de trajectoires mise à jour $\mathcal{D}_{H^j}$ ;
  \item un signal booléen $\texttt{need\_update}$ indiquant si les activités de modélisation et d'entraînement doivent être relancées.
\end{itemize}

\paragraph{Boucle de transfert.}
La boucle de transfert s'exécute tant que le \acn{SMA} est actif dans $\mathcal{E}$.
À chaque pas de temps $t$ :
\begin{enumerate}
  \item une observation $\omega^j_t$ est collectée via la fonction $\texttt{observe}(\mathcal{E})$ ;
  \item l'action $a^j_t$ est choisie en appliquant la politique $\pi^j_{\text{latest}}$ à l'observation courante ;
  \item cette action est exécutée dans l'environnement via $\texttt{apply}(\mathcal{E}, a^j_t)$, produisant la nouvelle observation $\omega^j_{t+1}$ ;
  \item la transition $(\omega^j_t, a^j_t, \omega^j_{t+1})$ est stockée dans un tampon temporaire $\mathcal{B}$.
\end{enumerate}

\paragraph{Déclencheur de mise à jour.}
Lorsque la taille du tampon $\mathcal{B}$ dépasse un seuil $\texttt{batch\_size}$, le contenu est ajouté à la base de trajectoires $\mathcal{D}_{H^j}$ puis le tampon est vidé.
Le signal $\texttt{need\_update}$ est alors activé.
Si aucun processus de mise à jour n'est en cours (\texttt{not running\_update}), la procédure \texttt{launch\_update()} est déclenchée de manière asynchrone pour relancer les activités de modélisation et d'entraînement.

\paragraph{Fonctionnement global.}
Ce schéma assure trois propriétés essentielles :
\begin{itemize}
  \item la \textbf{continuité d'exécution} : les agents opèrent toujours avec la dernière politique disponible ;
  \item la \textbf{réactivité} : les données réelles sont intégrées dès qu'un volume suffisant est collecté ;
  \item la \textbf{automatisation} : les mises à jour se déclenchent sans intervention humaine, tout en évitant les conflits entre processus parallèles.
\end{itemize}

\paragraph{Éléments formels.}
Ainsi, l'algorithme encode la relation :
\[
  \mathcal{D}_{H^j}, \texttt{need\_update} \gets \texttt{transfer}(\pi^j_{\text{latest}}, \mathcal{E}, \mathcal{D}_{H^j})
\]
où :
\begin{itemize}
  \item $\mathcal{B}$ désigne le tampon temporaire,
  \item $\texttt{batch\_size}$ fixe la granularité de déclenchement des mises à jour,
  \item $\texttt{launch\_update}()$ assure la synchronisation avec les autres activités de la méthode \acn{MAMAD}.
\end{itemize}

\vspace{-0.3em}
\begin{algorithm}[H]
  \caption{Activité de transfert}
  \label{alg:transferring}
  \DontPrintSemicolon
  \KwIn{Politique actuelle $\pi^j_{\text{latest}}$, environnement réel $\mathcal{E}$, base de trajectoires $\mathcal{D}_{H^j}$}
  \KwOut{Base de trajectoires mise à jour $\mathcal{D}_{H^j}$, signal de mise à jour $\texttt{need\_update}$}

  \vspace{0.3em}
  \SetKwProg{Transfer}{Procédure \normalfont BoucleDeTransfert}{}{}
  \Transfer{}{

  \While{le \acn{SMA} est actif dans l'environnement $\mathcal{E}$}{
  \tcp*[l]{Exécution de la politique la plus récente}
  $\omega^j_t \gets \texttt{observe}(\mathcal{E})$ \;
  $a^j_t \gets \pi^j_{\text{latest}}(\omega^j_t)$ \;
  $\omega^j_{t+1} \gets \texttt{apply}(\mathcal{E}, a^j_t)$ \;
  Ajouter $(\omega^j_t, a^j_t, \omega^j_{t+1})$ au tampon temporaire $\mathcal{B}$ \;

  \tcp*[l]{Vérification du déclenchement de la mise à jour}
  \If{$|\mathcal{B}| \geq \texttt{batch\_size}$}{
    Ajouter $\mathcal{B}$ à $\mathcal{D}_{H^j}$ et vider $\mathcal{B}$ \;
    $\texttt{need\_update} \gets \texttt{True}$ \;
    \If{$\texttt{not running\_update} = \texttt{False}$}{
      \texttt{launch\_update()} \tcp*[r]{Appel asynchrone}
    }
  }
  }
  }
\end{algorithm}


\section{Synthèse}

En synthèse, l'activité de transfert assure l'exécution continue de la politique la plus récente et la collecte automatisée de trajectoires réelles pour raffiner le modèle et réentraîner les politiques.

Ses atouts sont :
\begin{itemize}
  \item automatisation du déploiement et de la collecte en environnement réel ;
  \item synchronisation robuste avec les autres activités de \acn{MAMAD} ;
  \item adaptation continue des agents à l'environnement.
\end{itemize}

Ses limites portent sur le coût de supervision, la gestion des environnements critiques, et la fréquence optimale des mises à jour.

Cette activité permet une méthode réellement \textbf{autoadaptative}, alternant apprentissage simulé et déploiement réel pour garantir la robustesse des \acn{SMA} en contexte dynamique.



\clearpage
\thispagestyle{empty}
\null
\newpage

\chapter*{Conclusion}
\addcontentsline{toc}{chapter}{\textbf{Conclusion}}

\noindent
Cette troisième partie a introduit la méthode \textbf{\acn{MAMAD}} comme une réponse concrète aux limites identifiées dans les approches actuelles de conception de \acn{SMA}. Reposant sur un cycle itératif structuré en quatre activités (\textit{Modélisation}, \textit{Apprentissage}, \textit{Analyse}, \textit{Transfert}), \acn{MAMAD} articule de manière cohérente des outils symboliques (spécifications organisationnelles) et \acn{MARL} pour guider la conception, l'entraînement et l'adaptation d'agents intelligents dans des environnements complexes.

\medskip

\noindent
La méthode s'appuie notamment~:
\begin{itemize}
  \item sur une modélisation fidèle des environnements à partir de données empiriques,
  \item sur un entraînement contraint par des spécifications organisationnelles intégrées au sein du cadre \textit{MOISE+MARL},
  \item sur une analyse des trajectoires pour inférer les structures émergentes de l'organisation apprise,
  \item et enfin sur un transfert contrôlé permettant l'amélioration itérative du \acn{SMA}.
\end{itemize}

\noindent
Dans la partie suivante, nous proposons de valider expérimentalement cette méthode à travers son implémentation concrète dans un outil dédié, \acn{CybMASDE}\index{CybMASDE}, et son application à plusieurs environnements représentatifs. L'objectif est de démontrer la capacité de \acn{MAMAD} à produire automatiquement des \acplu{SMA} performants, explicables et conformes à des exigences organisationnelles dans des contextes variés.

\medskip

\noindent
Nous évaluons notamment la méthode selon des critères d'efficacité, d'automatisation, de conformité aux contraintes et d'explicabilité, tout en comparant ses résultats à ceux d'approches classiques non guidées par des modèles organisationnels.

\bigskip

La \autoref{part:experimentation} met donc à l'épreuve la méthode \acn{MAMAD}, en analysant ses performances et sa pertinence au regard des verrous identifiés sur différents scénarios.
