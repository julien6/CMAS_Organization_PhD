\cleardoublepage
\phantomsection
% \pdfbookmark[1]{Validation expérimentale de la méthode}{Validation expérimentale de la méthode}
\markboth{\spacedlowsmallcaps{Validation expérimentale de la méthode}}{\spacedlowsmallcaps{Validation expérimentale de la méthode}}
\part{Validation expérimentale de la méthode}

\chapter*{Introduction}
\addcontentsline{toc}{chapter}{\textbf{Introduction}}
\label{part:experimentation}


\noindent
Cette partie vise à démontrer l'applicabilité et la pertinence de la méthode \acn{MAMAD} dans différents contextes de conception de systèmes multi-agents. Pour cela, nous avons développé une plateforme dédiée, \acn{CybMASDE}, qui implémente l'ensemble du pipeline proposé (modélisation, apprentissage, analyse, transfert) de manière modulaire et reproductible.

\medskip

\noindent
Comme illustré en \autoref{fig:organisation_manuscrit_partie_4}, dans un premier temps, nous décrivons en détail l'environnement expérimental, les outils logiciels et matériels mobilisés, ainsi que les environnements de test retenus. Nous présentons également les spécifications organisationnelles associées à chaque environnement, ainsi que les métriques d'évaluation permettant de valider les performances de la méthode.

\medskip

\noindent
Dans un second temps, nous analysons les résultats obtenus afin de répondre aux objectifs de recherche identifiés dans la partie précédente. Cela inclut une évaluation de l'efficacité de la méthode, de sa capacité d'automatisation, de l'adéquation des politiques apprises avec les contraintes organisationnelles, ainsi que de leur explicabilité.

\medskip

\noindent
Cette étude expérimentale nous permettra de mieux cerner les atouts et les limites de la méthode \acn{MAMAD}, et de dégager des perspectives d'amélioration pour une automatisation encore plus poussée de la conception organisationnelle en \acn{MARL}.



\begin{figure}[h!]
    \centering
    \resizebox{\linewidth}{!}{%
        \input{figures/organisation_manuscrit_partie_4.tex}
    }
    \caption{Structure de la Partie IV : Cadre expérimental et analyse des résultats}
    \label{fig:organisation_manuscrit_partie_4}
\end{figure}

\chapter{CybMASDE: Un framework supportant MAMAD}
\label{sec:cybmasde}

Pour soutenir la mise en œuvre modulaire et reproductible de la méthode \acn{MAMAD}, nous avons conçu la plateforme \acn{CybMASDE}~\footnotemark[1]. Elle orchestre les phases de modélisation, d'entraînement, d'analyse et de déploiement de \acn{SMA} fondés sur le cadre MOISE+MARL.

\footnotetext[1]{Code source et documentation disponibles à \url{https://github.com/julien6/CybMASDE}}

Le module de modélisation construit un modèle de prédiction d'observations conjointes (\acparen{JOPM}) via PyTorch, avec des dynamiques \acn{LSTM} entraînées à partir des historiques réels $\mathcal{D}_{H^j}$. Les observations sont compressées à l'aide d'encodeurs \acn{VAE} (dimensions latentes entre 16 et 64), tandis que les actions sont encodées via des \acn{MLP}. Le \acn{LSTM} utilise des tailles cachées de 64 ou 128, et est optimisé par Adam (taux d'apprentissage entre $1 \times 10^{-4}$ et $5 \times 10^{-4}$). La fonction de récompense $R^j_H$ est dérivée manuellement de $\mathcal{G}_{\text{inf}}$ et les contraintes organisationnelles $\mathcal{C}_{\text{inf}}$ sont formalisées en spécifications MOISE+MARL $\mathcal{MM}$ via l'API \acn{MMA}.

L'entraînement est effectué avec MARLlib~\cite{hu2022marllib}, qui prend en charge \acn{MAPPO}, \acn{MADDPG}, \acn{QMix}, \acn{IQL}, \acn{VDN} et \acn{ROMA}. Les contraintes MOISE+MARL sont appliquées via du masquage d'actions par rôle et du shaping de récompense. L'apprentissage utilise Ray RLlib avec un taux d'apprentissage dans $[1e^{-4}, 5e^{-4}]$, un facteur d'actualisation dans $[0.9, 0.99]$, une valeur de clip \acn{PPO} dans $[0.1, 0.3]$ et des tailles de batchs dans $\{64, 128\}$, avec des politiques implémentées par \acn{MLP}s de 64 à 256 neurones.

L'analyse implémente la méthode Auto-TEMM, une extension de \acn{TEMM} avec optimisation complète des hyperparamètres via Optuna. Les trajectoires sont regroupées par clustering hiérarchique selon des métriques (\acparen{DTW}, \acparen{LCS}...), avec optimisation de la distance minimale intra-cluster. Un balayage de la représentativité (de 0.0 à 1.0) est effectué pour identifier celle qui minimise le temps de convergence vers une récompense cible (par défaut : 3.5\%). Le "fit organisationnel" est ensuite calculé comme la moyenne des composantes \acn{SOF} (structurelle) et \acn{FOF} (fonctionnelle) issues des variances intra-cluster.

Le module de transfert assure le déploiement continu des politiques $\pi^j_{\text{latest}}$ dans des environnements réels ou simulés via PettingZoo ou des API spécifiques. Une fois un seuil atteint (ex. 512 étapes), une nouvelle boucle de modélisation-entraînement-analyse est déclenchée.

Ainsi, \acn{CybMASDE} constitue une chaîne d'outils complète et extensible pour exécuter la méthode \acn{MAMAD}, en intégrant simulation, apprentissage \acn{MARL}, inférence organisationnelle et déploiement dans un même environnement.


\section{Mise en place d'une architecture de services}
\section{Intégration des World Models multi-agents}
\section{Intégration du framework MOISE+MARL}


\chapter{Protocole expérimental}

\section{Objectifs d'évaluation}

Pour valider l'efficacité de \acn{MAMAD}, nous structurons notre protocole expérimental selon les volets suivants :

\subsubsection{Comparaison avec des approches classiques}

\begin{itemize}
    \item \textbf{Baseline Référence (RB)} : agents entraînés sans contraintes organisationnelles, via \acn{MARL} standard ;
    \item \textbf{Baseline Organisationnelle (OB)} : agents entraînés avec contraintes $\mathcal{M}OISE^+$ définies manuellement ;
    \item \textbf{\acn{SMA} basé sur \acn{MAMAD} (MB)} : agents entraînés via \acn{MAMAD} avec inférence automatisée des contraintes.
\end{itemize}

Les expériences sont répétées sur les quatre environnements avec les mêmes paramètres.

\subsubsection{Validation de l'explicabilité et de la conformité organisationnelle}

\begin{itemize}
    \item \textbf{Analyse comparative des rôles et missions} : comparaison des structures inférées et définies ;
    \item \textbf{Analyse de similarité} : score de similarité des rôles ;
    \item \textbf{Visualisation} : \acn{PCA} des observations et transitions pour interpréter les rôles et objectifs inférés.
\end{itemize}

La stabilité des rôles et missions inférés à travers les essais constitue un critère clé de validation de la méthode \acn{MAMAD}.

\section{Configurations expérimentale}
\label{sec:experimental_setup}

Nous avons développé un outil facilitant la mise en œuvre de la méthode \acn{MAMAD} sur quatre environnements distincts, selon un protocole d'évaluation structuré.

\subsection{Ressources de calcul}

Toutes les expériences ont été réalisées sur un cluster académique de calcul haute performance, avec différentes configurations de nœuds GPU :
\begin{itemize}
    \item \textbf{GPUs :} NVIDIA A100, AMD MI210 ;
    \item \textbf{Frameworks :} TensorFlow, PyTorch ;
    \item \textbf{Optimisation d'hyperparamètres :} \textbf{Optuna}~\cite{akiba2019optuna} pour le taux d'apprentissage, l'équilibre exploration/exploitation, et l'architecture des réseaux.
\end{itemize}

Chaque combinaison algorithme-environnement a été exécutée sur 5 instances parallèles pour garantir des résultats robustes.

\subsection{Métriques d'évaluation}

Pour juger si la méthode \acn{MAMAD} comble efficacement les lacunes identifiées dans la littérature, nous définissons des métriques quantitatives selon quatre critères : \textbf{automatisation}, \textbf{efficacité}, \textbf{conformité aux exigences de conception}, et \textbf{explicabilité}.

\subsubsection{Métriques d'automatisation}

\begin{itemize}
    \item \textbf{Performance relative au temps de conception} ($T_{design}$) : compare la performance du \acn{SMA} au temps de conception manuel (estimé en jours) ;
    \item \textbf{Quantité de connaissances injectées} ($K_{design}$) : nombre de lignes nécessaires pour spécifier rôles et objectifs (effort humain) ;
    \item \textbf{Nombre d'itérations jusqu'à convergence} ($N_{iter}$) : nombre de cycles nécessaires pour stabiliser une politique optimale (0 pour \acparen{SMA} fait main).
\end{itemize}

\subsubsection{Métriques d'efficacité}

\begin{itemize}
    \item \textbf{Récompense cumulée} ($R_{cum}$) : somme des récompenses, reflet de la performance globale ;
    \item \textbf{Stabilité de la politique} ($\sigma_R$) : écart-type de la récompense, mesure de robustesse ;
    \item \textbf{Taux de convergence} ($CR$) : rapidité de stabilisation de l'apprentissage ;
    \item \textbf{Score de robustesse} ($R_{robust}$) : maintien des performances sous perturbations externes.
\end{itemize}

\subsubsection{Métriques de conformité et d'explicabilité}

\begin{itemize}
    \item \textbf{Taux de violation des contraintes} ($V_c$) : pourcentage d'exécutions ne respectant pas les contraintes organisationnelles ;
    \item \textbf{Niveau de fit organisationnel} ($F_{org}$) : similarité entre structure organisationnelle inférée et attendue ;
    \item \textbf{Score de cohérence} ($S_{cons}$) : correspondance entre rôles/missions assignés et attendus (via \acn{TEMM}).
\end{itemize}



\chapter{Études de cas}
\section{Les environnements non-orientés Cyberdéfense}

\subsection{Environnements de test et spécifications organisationnelles}

Pour évaluer la méthode \acn{MAMAD}, nous utilisons quatre environnements multi-agents distincts, chacun servant de banc d'essai contrôlé dans un domaine applicatif différent. Ils nécessitent coordination, décisions stratégiques et interactions basées sur les rôles. Chaque environnement est formellement décrit (états, observations, actions, récompenses, objectif global) avec les spécifications organisationnelles correspondantes.

\begin{table}[h!]
    \centering
    \begin{footnotesize}
        \renewcommand{\arraystretch}{1.3}
        \begin{tabular}{p{2cm}p{2.2cm}p{2.2cm}p{2.2cm}p{2.2cm}}
            \hline
            \textbf{Aspect Clé} & \textbf{CybORG}                & \textbf{Overcooked-AI} & \textbf{Predator-prey} & \textbf{Warehouse Mgmt} \\ \hline
            Réalisme            & Cyberdéfense, menaces dynamiques & Travail en cuisine réaliste & Communication abstraite & Logistique en entrepôt \\ \hline
            Rôles émergents     & Firewall, nettoyeur, sauveteur & Cuisinier, serveur      & Parleur, écouteur       & Collecteur, assembleur, emballeur \\ \hline
            Objectif            & Missions multi-activités        & Tâches séquentielles    & Objectif commun         & Pipeline ordonné \\ \hline
            Observabilité       & Bruit, vision partielle         & Occlusion, congestion   & Communication requise   & Zones locales et partagées \\ \hline
            Évaluation org. fit & Cohérence sous attaque          & Délégation de tâches    & Rôles via communication & Efficacité de coordination \\ \hline
        \end{tabular}
        \caption{Caractéristiques des environnements utilisés pour évaluer MAMAD}
        \label{tab:mamad_env_characteristics}
    \end{footnotesize}
\end{table}

Les paragraphes suivants décrivent chaque environnement (WM, PP, OA, CS) avec leur figure respective et spécifications organisationnelles. (cf. figures déjà traduites dans la version anglaise ; on les laisse identiques pour conserver le code source).

\bigskip

\noindent Ces quatre environnements couvrent des situations coopératives, compétitives, hiérarchiques et adversariales, permettant une évaluation représentative de la méthode.


\section{Infrastructure d'entreprise}
\section{Essaim de drones}
\section{Architecture de microservices}

\chapter{Résultats expérimentaux et analyse comparative}
\section{Validation expérimentale des hypothèses}
\section{Comparaison entre scénarios}
\section{Discussion des résultats et généralité}

\chapter*{Conclusion}
\addcontentsline{toc}{chapter}{\textbf{Conclusion}}
% TODO
