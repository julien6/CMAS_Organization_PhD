% \usepackage[T1]{fontenc}
% \usepackage{graphicx}
% %\usepackage{color}
% %\renewcommand\UrlFont{\color{blue}\rmfamily}

% \usepackage{amsmath,amssymb,amsfonts}
% \usepackage[inline, shortlabels]{enumitem}
% \usepackage{tabularx}
% \usepackage{caption}
% \usepackage{listings}
% % \usepackage{titlesec}
% \usepackage[english]{babel}
% \captionsetup{font=it}
% \usepackage{ragged2e}
% \usepackage[hyphens]{url}
% \usepackage{hyperref}
% \usepackage{xurl}
% \usepackage{pifont}
% \usepackage{footmisc}
% \usepackage{multirow}
% \usepackage{enumitem}
% \usepackage{algorithm2e}
% \usepackage{float}
% \usepackage{listings}
% \usepackage{xcolor}

% \definecolor{codegreen}{rgb}{0,0.6,0}
% \definecolor{codegray}{rgb}{0.5,0.5,0.5}
% \definecolor{codepurple}{rgb}{0.58,0,0.82}
% \definecolor{backcolour}{rgb}{0.95,0.95,0.92}

% \lstdefinestyle{mystyle}{
%     backgroundcolor=\color{backcolour},   
%     commentstyle=\color{codegreen},
%     keywordstyle=\color{magenta},
%     numberstyle=\tiny\color{codegray},
%     stringstyle=\color{codepurple},
%     basicstyle=\footnotesize,
%     breakatwhitespace=false,         
%     breaklines=true,                 
%     captionpos=b,                    
%     keepspaces=true,                 
%     numbers=left,                    
%     numbersep=5pt,                  
%     showspaces=false,                
%     showstringspaces=false,
%     showtabs=false,                  
%     tabsize=2
% }

% \lstset{style=mystyle}

% % --- Tickz
% \usepackage{physics}
% \usepackage{amsmath}
% \usepackage{tikz}
% \usepackage{mathdots}
% \usepackage{yhmath}
% \usepackage{cancel}
% \usepackage{color}
% \usepackage{siunitx}
% \usepackage{array}
% \usepackage{multirow}
% \usepackage{amssymb}
% \usepackage{gensymb}
% \usepackage{tabularx}
% \usepackage{extarrows}
% \usepackage{booktabs}
% \usetikzlibrary{fadings}
% \usetikzlibrary{patterns}
% \usetikzlibrary{shadows.blur}
% \usetikzlibrary{shapes}

% % ---------
% % \usepackage{titlesec}
% \usepackage{pdfpages}
% \usepackage{booktabs}
% \usepackage{csquotes}
% \usepackage{lipsum}  
% \usepackage{arydshln}
% \usepackage{smartdiagram}
% \usepackage[inkscapeformat=png]{svg}
% \usepackage{textcomp}
% \usepackage{xcolor}
% \def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
%     T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
% \usepackage{cite}
% \usepackage{amsmath}
% \newcommand{\probP}{\text{I\kern-0.15em P}}
% \usepackage{etoolbox}
% \patchcmd{\thebibliography}{\section*{\refname}}{}{}{}

% \setlength\tabcolsep{0.5pt}

% \newcommand{\before}[1]{\textcolor{red}{#1}}
% \newcommand{\after}[1]{\textcolor{green}{#1}}

% \newcommand{\old}[1]{\textcolor{orange}{#1}}
% \newcommand{\rem}[1]{\textcolor{red}{#1}}
% \newcommand{\todo}[1]{\textcolor{orange}{\newline \textit{\textbf{TODO:} #1}} \newline \newline }



% \newcounter{relation}
% \setcounter{relation}{0}
% \renewcommand{\therelation}{\arabic{relation}}
% \newcommand{\relationautorefname}{Relation}

% \newenvironment{relation}[1][]{%
%     \refstepcounter{relation}%
%     \noindent \raggedright \textit{\textbf{Relation. \therelation}} \hfill$}
% {%
% $ \hfill \phantom{x}

% }

% \newcounter{proof}
% \setcounter{proof}{0}
% \renewcommand{\theproof}{\arabic{proof}}
% \newcommand{\proofautorefname}{Proof}

% \renewenvironment{proof}[1][]{
%     \refstepcounter{proof}
%     \noindent \raggedright \textit{\textbf{Proof. \theproof}}

%     \setlength{\leftskip}{1em}

% }
% {

% \
% \setlength{\leftskip}{0pt}
% }



\section{Introduction}

% Contexte :

Les MAS ont suscité un intérêt considérable dans le domaine industriel en raison de leur capacité à traiter des problèmes complexes et distribués~\cite{Raileanu2023}.
Ce paradigme permet de décomposer une tâche complexe en missions qui sont déléguées à des agents autonomes qui les accomplissent grâce à des mécanismes de coopération. Ils fournissent notamment des modèles et des approches pour gérer les objectifs contradictoires, le calcul parallèle, la robustesse des systèmes et l'évolutivité.
%Les applications des MAS sont diverses, notamment la robotique collective, les réseaux ad hoc de véhicules (VANET)\cite{Oliveira1999, Gembarski2020}.
Dans les MAS, l'organisation est un concept fondamental qui a un impact sur la manière dont les agents coordonnent leurs activités pour atteindre un objectif commun en collaborant~\cite{Hubner2007}.
Les aspects organisationnels répondent au défi de la conception des MAS dans des environnements dynamiques et incertains, où le comportement d'exécution doit être flexible~\cite{Kathleen2020}. L'organisation dans la conception des MAS est un concept central dans les méthodologies et les cadres permettant l'ingénierie de MAS spécifiques à une application~\cite{Bakliwal2018}.

Les méthodes de conception/développement des MAS ont souvent été proposées conjointement avec des modèles organisationnels afin d'aider les concepteurs à trouver des spécifications organisationnelles appropriées permettant à un MAS d'atteindre efficacement un objectif. Des méthodes telles que GAIA~\cite{Wooldridge2000,Cernuzzi2014}, ADELFE~\cite{Mefteh2015} ou DIAMOND~\cite{Jamont2015}, KB-ORG~\cite{Sims2008} fournissent des protocoles qui s'appuient sur l'expérience du concepteur pour élaborer manuellement les règles de l'agent (également appelées « politiques ») en tirant parti de mécanismes d'auto-organisation/réorganisation pour adapter le MAS à l'environnement de déploiement.
Les méthodes susmentionnées sont couramment appliquées par le biais de simulations, car elles permettent un cadre de surveillance sûr pour le processus de conception et l'évaluation. Un MAS développé dans des environnements simulés avec une grande fidélité au système cible devrait pouvoir être transféré vers le système cible pour fonctionner de manière adéquate~\cite{Schon2021}.

Le concepteur définit les politiques des agents de différentes manières, allant du point de vue individuel de l'agent au point de vue global de l'organisation. Un MAS correctement conçu devrait permettre l'émergence ou le choix d'organisations permettant d'atteindre un objectif~\cite{Picard2009reorganisation}. Cette approche de conception se déroule souvent sous la forme d'un processus itératif procédant par essais et erreurs. Elle présente toutefois les limites suivantes :
\begin{enumerate*}[label=\roman*),itemjoin={;\quad}]
  \item Elle nécessite des concepteurs suffisamment expérimentés
  \item La convergence vers un MAS suffisamment estimé comme efficace peut s'avérer coûteuse
  \item Elle est difficile à appliquer à des environnements de déploiement cibles complexes et à haute dimension.
\end{enumerate*}
Par exemple, les recherches sur les agents de cyberdéfense autonomes intelligents~\cite{Kott2023} (AICA) visent à développer des agents de cyberdéfense coopératifs déployés dans des réseaux informatiques très complexes. Le développement d'un AICA se heurte à un manque de compréhension visuelle et intuitive des environnements en réseau tels que les réseaux d'entreprise.

% Problème :

Même si certaines méthodes peuvent automatiser certaines parties de la conception organisationnelle des MAS, telles que KB-ORG~\cite{Sims2008}, elles nécessitent tout de même certaines connaissances et des interactions manuelles pour guider le processus de conception. En effet, il est nécessaire de disposer de
\begin{enumerate*}[label=\roman*),itemjoin={; and \ }]
  \item Trouver automatiquement les politiques des agents adaptées aux contraintes de conception
  \item Rendre explicites les mécanismes organisationnels qui émergent des agents formés pour le processus de conception.
\end{enumerate*}

% Contribution

Pour répondre à ces questions, nous introduisons AMOEA, une approche de conception MAS dont l'idée sous-jacente est de relier un processus MARL donné à un modèle organisationnel qui relie les politiques des agents en formation à des spécifications organisationnelles explicites. Elle peut être considérée comme un outil d'ingénierie permettant de générer automatiquement des spécifications organisationnelles exploitables uniquement en fonction des performances dans la réalisation de l'objectif donné et des contraintes de conception. Pour le concepteur, les spécifications organisationnelles obtenues constituent des informations sur les mécanismes organisationnels à mettre en place pour développer un MAS qui réponde aux exigences de performance.

% Résultats

% Nous avons appliqué l'AOMEA à trois jeux Atari spatiaux nécessitant différents degrés de coopération entre les agents afin qu'ils atteignent un objectif de la manière la plus efficace possible, tout en respectant les spécifications organisationnelles en tant que contraintes de conception. Les spécifications organisationnelles obtenues sont effectivement exploitables, cohérentes avec les attentes et respectent les contraintes de conception.
% Nous avons également appliqué notre approche dans un environnement de cyberdéfense avec un essaim de drones, dont les spécifications organisationnelles ont permis de développer un MAS avec des scores comparables à ceux des meilleurs.

La section II commence par présenter le contexte théorique de l'AOMEA et se concentre sur les concepts fondamentaux que nous avons utilisés pour les modèles organisationnels et le MARL.
% et la motivation pour intégrer un modèle organisationnel MAS dans un processus MARL afin d'améliorer le processus de conception MAS.
Dans la section III, nous présentons l'AOMEA, de l'approche à l'outil mis en œuvre. Nous avons évalué l'AOMEA dans quatre environnements simulés et discuté des résultats obtenus dans la section IV. Enfin, la section V conclut sur la viabilité de l'AOMEA et met en évidence les limites à surmonter et les travaux futurs.

% ====================================================================================================

\section{Contexte théorique}

% // Mettre en avant les briques du raisonnement en expliquant le titre pour préparer l'introduction de la contribution avec AOMEA sans les justifier (sans faire de comparaison avec l'existant, pas d'édt, dire juste les points forts)

% Organisation
%   -&gt; moise (justifier parmi les existants)
% Organisation


% Méthodes MAS (ALAADIN, GAIAI mais pas de moyens pour trouver une organisation automatiquement)

% MARL (basiques) // DECPOMDP (basiques)

Dans cette section, nous présentons les bases du modèle organisationnel $\mathcal{M}OISE^+$ et les bases du MARL sur lesquels repose notre contribution.

\subsection{Contexte des systèmes multi-agents}

% définition, modèle AEIO, focus Moise, Dec-POMDP

Un agent est une entité immergée dans un environnement qui perçoit des observations et prend des décisions pour agir de manière autonome dans cet environnement afin d'atteindre les objectifs qui lui ont été assignés.
Il existe différents types d'agents : les agents réactifs, qui réagissent aux événements pour faire face aux incertitudes d'un environnement, et les agents proactifs, qui exploitent les interactions avec d'autres agents. Un MAS est un ensemble d'agents dans un environnement partagé où chaque agent n'a qu'une perception locale. Ces agents doivent être dotés de capacités d'auto-organisation et de réorganisation qui leur permettent de modifier leur structure organisationnelle de manière adaptative en fonction de leur environnement.

Un MAS est étroitement lié à l'entité organisationnelle (que nous appelons simplement « organisation ») que nous considérons comme toujours existante à travers les interactions des agents en cours d'exécution, même si elle peut être implicite.
%
% Ces méthodes sont essentielles pour garantir que les MAS peuvent coordonner, communiquer et exécuter efficacement des tâches dans un environnement distribué et souvent dynamique.
% Parmi les méthodes les plus notables, on peut citer
% \emph{Tropos}, une méthodologie de développement logiciel orientée agent qui met l'accent sur l'analyse précoce des exigences et leur affinement continu tout au long des phases de conception et de mise en œuvre~\cite{Bresciani2004} ;
% \emph{Gaia}, une méthodologie d'analyse et de conception de MAS, axée sur la structure organisationnelle du système~\cite{Zambonelli2003}; \emph{DIAMOND}, qui s'appuie sur une approche itérative en quatre phases pour améliorer le développement de systèmes physiques multi-agents; et {ADELFE}, qui utilise les compétences et les attitudes pendant la conception pour créer des systèmes auto-organisés et répondre aux exigences finales.
% Dans le cadre des travaux méthodologiques, il convient également de mentionner le modèle AEIO (Voyelles), qui met l'accent sur la structuration des entités au sein des systèmes multi-agents en intégrant les agents, l'environnement, les interactions et l'organisation comme éléments clés.
%
Un \textbf{modèle organisationnel} spécifie (au moins partiellement) l'organisation, qu'il soit utilisé comme moyen de décrire une organisation explicite connue de manière descendante ou de décrire une organisation implicite de manière ascendante. Un exemple de modèle organisationnel est le modèle \emph{Agent/Groupe/Rôle} (AGR)~\cite{Ferber2004}. Nous faisons référence aux \textbf{spécifications organisationnelles}, les composants utilisés dans un modèle organisationnel pour caractériser l'organisation. $\mathcal{M}OISE^+$ est un modèle organisationnel qui permet de relier les politiques des agents aux spécifications organisationnelles. Il prend explicitement en compte les aspects sociaux entre les agents, tandis que \emph{AGR} se concentre sur l'intégration de normes orientées vers la conception. $\mathcal{M}OISE^+$~\cite{Hubner2007} considère trois types de spécifications :

Les \textbf{spécifications structurelles} décrivent les moyens dont disposent les agents pour atteindre un objectif. Elles comprennent l'ensemble des \emph{rôles}, des sous-groupes, des \emph{liens} intra-groupe et inter-groupe, des \emph{compatibilités} intra-groupe et inter-groupe, ainsi que les \emph{cardinalités} des rôles et des sous-groupes.
Un \emph{lien} indique si deux rôles sont liés par une relation de connaissance, de communication ou d'autorité. Une \emph{compatibilité} indique si deux rôles peuvent être adoptés par le même agent. Les \emph{cardinalités} de rôle et de sous-groupe désignent respectivement le nombre minimal et maximal de rôles et de sous-groupes.

Les \textbf{spécifications fonctionnelles} décrivent la manière d'atteindre un objectif. Elles comprennent les \emph{schémas sociaux} et l'\emph{ordre de préférence}. Un \emph{schéma social} est décrit par des objectifs globaux, des étiquettes de mission accompagnées de plans et la cardinalité des agents engagés dans une mission. Un \emph{ordre de préférence} signifie qu'un agent a une préférence sociale pour s'engager dans une mission spécifique parmi plusieurs missions possibles.

Les \textbf{spécifications déontiques} permettent de relier les spécifications fonctionnelles et structurelles à travers un ensemble d'\emph{autorisations} et d'obligations. Une \emph{autorisation} signifie qu'un agent jouant le rôle $\rho_a$ est autorisé à s'engager dans la mission $m$ pour une contrainte temporelle donnée $tc$. De même, une \emph{obligation} signifie qu'un agent jouant le rôle $\rho_a$ doit s'engager dans la mission $m$ pour une contrainte temporelle donnée $tc$. Une contrainte temporelle $tc $ spécifie un ensemble de périodes déterminant si une autorisation ou une obligation est valide.

% Cependant, ces travaux méthodologiques s'appuient largement sur l'expérience des concepteurs humains, et aucun d'entre eux ne permet d'automatiser l'assistance au processus de conception des MAS en garantissant une efficacité suffisante tout en tenant compte des aspects organisationnels dans un contexte multi-agents.

\subsection{Notions de base sur les MARL}

L'apprentissage par renforcement est un paradigme d'apprentissage automatique dans lequel des agents apprennent à prendre des décisions en interagissant avec un environnement. L'objectif est que l'agent maximise un signal de récompense cumulatif au fil du temps grâce à un processus d'essais et d'erreurs.
MARL étend ce concept à plusieurs agents qui apprennent tout en tenant compte des actions des autres agents, ce qui les pousse à s'appuyer sur des mécanismes de coopération.

Le MARL permet de converger automatiquement vers les politiques des agents qui permettent d'atteindre l'objectif donné. Cependant, contrairement à la conception humaine, la logique des agents formés est explicitement spécifiée d'un point de vue collectif. Peu de travaux tentent d'aborder cette question et peu sont orientés vers des objectifs méthodologiques.
Kazhdan et al.~\cite{Kazhdan2020} ont proposé des moyens d'extraire des modèles symboliques des systèmes MARL afin d'améliorer leur interprétabilité.
Wang et al.~\cite{Wang2020} ont introduit une approche MARL orientée vers les rôles, dans laquelle les rôles sont émergents et les agents ayant des rôles similaires ont tendance à partager leur apprentissage et à se spécialiser dans certaines sous-tâches.
Tosic et al.~\cite{Tosic2010} ont proposé un cadre pour traiter la coordination dans les MAS collaboratifs en s'appuyant sur les capacités de communication des systèmes multi-agents.
Zheng et al.~\cite{Zheng2018} ont présenté une plateforme pour MARL qui vise à faciliter la recherche sur l'intelligence collective artificielle en fournissant un ensemble complet de références et de mesures d'évaluation pour comparer les performances des algorithmes MARL.

Des modèles markoviens sont nécessaires pour modéliser l'environnement et appliquer les techniques MARL. Couramment utilisé, le Dec-POMDP décentralisé~\cite{Oliehoek2016} considère plusieurs agents de manière similaire à un MAS. Il s'appuie sur des processus stochastiques pour modéliser l'incertitude de l'environnement liée aux changements induits par les actions, les observations reçues et les communications. Sa fonction de récompense est commune à tous les agents, ce qui favorise l'apprentissage d'actions collaboratives~\cite{Beynier2013}. Formellement, un Dec-POMDP est un tuple à 7 éléments $(S,\{A_i\},T,R,\{\Omega_i\},O,\gamma)$ , où : $S = \{s_1, ..s_{|S|}\}$ : L'ensemble des états possibles ; $A_{i} = \{a_{1}^{i},..,a_{|A_{i}|}^{i}\}$ : L'ensemble des actions possibles pour l'agent $i$ ; $T$ tel que $T(s,a,s') = \probP{(s'|s,a)}$ : L'ensemble des probabilités de transition conditionnelles entre les états ; $R: S \times A \times S \rightarrow \mathbb{R}$ : La fonction de récompense ; $\Omega_{i} = \{o_{1}^{i},..,o_{|\Omega_{i}|}^{i}\}$ : L'ensemble des observations pour l'agent $ag_i$ ; $O$ tel que $O(s',a,o) = \probP{(o|s',a)}$ : L'ensemble des probabilités d'observation conditionnelles ; $\gamma \in [0,1]$, le facteur de réduction.

Nous appelons « résoudre » le Dec-POMDP pour l'équipe $t$ le fait de trouver une politique conjointe $\pi_{joint,i} \in \Pi_{joint}$ qui maximise la récompense cumulative attendue sur un horizon fini.
Nous appelons « résolution sous-optimale » du Dec-POMDP à l'espérance $s$ le fait de trouver les politiques conjointes $\pi_{joint,i} \in \Pi_{joint}$ qui obtiennent la récompense cumulative attendue sur un horizon fini au moins à $s \in \mathbb{R}$.

% ====
% \paragraph{\textbf{Fragments de méthode et transformations de modèles}}
% Un ensemble de fragments de méthode pour le développement de MAS, qui s'appuie sur le processus de développement de deux systèmes multi-agents, souligne l'importance des transformations de modèles dans le processus d'ingénierie~\cite{Garcia2011}. Ces fragments peuvent être considérés comme des méthodologies modulaires pouvant être adaptées et réutilisées dans différents projets MAS, offrant ainsi une approche flexible du développement de systèmes.

% \paragraph{\textbf{Méta-modèles pour l'analyse et la conception}}
% L'utilisation de méta-modèles permettrait d'améliorer les activités d'analyse et de conception dans l'ingénierie des MAS~\cite{Gomez2004}. Les méta-modèles fournissent une abstraction de haut niveau qui peut aider à comprendre et à concevoir les interactions et les comportements complexes des agents au sein du système, facilitant ainsi une approche plus structurée du développement des MAS.

% \paragraph{\textbf{Paradigmes de l'ingénierie logicielle orientée agent}}

% La technologie des agents représente un nouveau paradigme de génie logiciel qui offre de nouvelles perspectives pour l'analyse, la conception et la construction de systèmes logiciels~\cite{Li2016}. Ce changement de paradigme vers le génie logiciel orienté agent (AOSE) encourage les développeurs à penser en termes d'agents autonomes et de leurs interactions, ce qui conduit à des MAS plus robustes et plus adaptables.

% \paragraph{\textbf{Méthodologies complètes}}

% \paragraph{\textbf{Caractérisation des stratégies collectives émergentes}}

% % IA collective explicable : expliquer les stratégies coopératives et la contribution des agents dans l'apprentissage par renforcement multi-agents à l'aide des valeurs de Shapley
% Heuillet et. al.~\cite{Heuillet2022} proposent une nouvelle approche pour expliquer les stratégies coopératives dans l'apprentissage par renforcement multi-agents (RL) à l'aide des valeurs de Shapley, un concept de la théorie des jeux utilisé dans l'IA explicable (XAI). L'étude vise à rendre le RL profond plus compréhensible et à répondre au besoin de méthodes offrant une meilleure compréhension et une meilleure interprétabilité. Les résultats expérimentaux sur les dilemmes sociaux séquentiels et les particules multi-agents démontrent l'efficacité des valeurs de Shapley pour expliquer la logique derrière les décisions prises par les agents. Cependant, l'article souligne également que les valeurs de Shapley ne peuvent fournir que des explications générales sur un modèle et ne peuvent pas expliquer les actions spécifiques prises par les agents. Les auteurs suggèrent que les travaux futurs devraient se concentrer sur ces limites. Les implications de cette étude s'étendent à des domaines tels que la prise de décision non discriminatoire, les décisions éthiques et responsables issues de l'IA et l'élaboration de politiques dans le respect des contraintes d'équité.

% % L'influence sociale comme motivation intrinsèque pour l'apprentissage profond par renforcement multi-agents
% Jaques et. al.~\cite{Jaques2019} proposent un mécanisme permettant d'assurer la coordination et la communication dans le MARL en récompensant les agents qui ont une influence causale sur les actions des autres agents. Cette influence causale est évaluée à l'aide d'un raisonnement contrefactuel, dans lequel les agents simulent des actions alternatives afin de calculer leur effet sur le comportement des autres agents. L'article démontre que cette approche permet d'améliorer la coordination et la communication, ainsi que d'obtenir des protocoles de communication appris plus significatifs. La méthode proposée permet d'augmenter considérablement les courbes d'apprentissage des agents d'apprentissage par renforcement profond, ce qui se traduit par un comportement plus diversifié des équipes et de meilleures performances de la population dans son ensemble. L'article souligne également que les récompenses d'influence pour tous les agents peuvent être calculées de manière décentralisée, ce qui ouvre de nouvelles perspectives de recherche dans ce domaine.

% \paragraph{\textbf{Adaptation du MARL pour répondre aux exigences}}

% % MARL efficace grâce à la supervision automatisée
% Chongjie et. al.~\cite{Chongjie2008} proposent un mécanisme unifié pour assurer la coordination et la communication dans MARL. L'approche consiste à former plusieurs agents à maximiser indépendamment leur propre récompense individuelle sans partager les poids. L'article présente une méthode de supervision automatisée qui permet aux agents d'apprendre à se coordonner et à communiquer efficacement. Ce mécanisme de supervision automatisée améliore la coordination, la communication et la pertinence des protocoles de communication appris, ce qui se traduit par une amélioration des courbes d'apprentissage des agents d'apprentissage par renforcement profond et des performances globales de la population d'agents.
%
% % Groupe auto-organisé pour MARL coopératif
% Shao et. al.~\cite{Shao2022} introduit une méthode appelée Self-Organized Group (SOG) pour le MARL coopératif. Dans cette approche, un certain nombre d'agents sont choisis au hasard pour être des conducteurs, et les groupes correspondants sont construits avec un consensus conducteur-suiveur, ce qui permet de réorganiser les groupes à intervalles réguliers. Il a été constaté que le groupe organisé sous le commandement unifié d'un conducteur intègre le système multi-agents avec une capacité de généralisation zéro-shot plus forte que les méthodes traditionnelles. La méthode SOG offre une forte adaptabilité aux scénarios avec un nombre variable d'agents et une visibilité variable des agents. L'article présente cette approche comme un mécanisme permettant d'améliorer les tâches multi-agents coopératives avec des caractéristiques dynamiques, dans le but d'améliorer l'adaptabilité et la généralisation des systèmes MARL.
%
% % Un modèle MARL d'appropriation des ressources communes
% Perolat et. al.~\cite{Perolat2017} introduisent un modèle axé sur l'appropriation des ressources communes, un dilemme social multi-agents qui inclut des questions telles que l'utilisation durable de l'eau douce, la pêche commune, les pâturages et les systèmes d'irrigation. Le modèle souligne l'importance de l'apprentissage par essais et erreurs pour relever les défis de la durabilité et de l'inégalité des ressources communes. Il explore le comportement émergent de groupes d'agents apprenant de manière indépendante dans un jeu de Markov partiellement observé, mettant en lumière la relation entre exclusion, coopération et durabilité dans le contexte de l'appropriation des ressources. La recherche souligne le potentiel de l'apprentissage par renforcement profond pour comprendre et relever les défis sociétaux et environnementaux complexes liés à la gestion des ressources communes. L'article fournit des informations précieuses sur l'application du MARL dans le contexte des dilemmes sociaux et de la gestion des ressources dans le monde réel.
%
% Promouvoir la coordination par la régularisation des politiques dans l'apprentissage par renforcement profond multi-agents
% Roy et. al.~\cite{Roy2020} abordent le défi de la coordination entre agents dans le MARL. La recherche examine l'utilisation de la régularisation des politiques pour promouvoir la coordination entre agents et discute deux approches basées sur la modélisation inter-agents et la sélection synchronisée de sous-politiques. Les méthodes proposées sont conçues pour améliorer les comportements coopératifs sans recourir à des canaux de communication explicites, ce qui permet aux agents d'adopter des comportements coordonnés lors des tests lorsqu'ils agissent de manière décentralisée. L'article présente deux méthodes de régularisation des politiques, TeamReg et CoachReg, et évalue leurs performances sur des problèmes coopératifs multi-agents complexes, montrant des résultats améliorés. Cette recherche contribue à l'avancement des approches multi-agents axées sur la coordination dans l'apprentissage par renforcement et fournit des informations précieuses sur la promotion de la coordination inter-agents par la régularisation des politiques.

% En fin de compte, parmi les travaux considérés, aucun n'utilise spécifiquement un modèle organisationnel comme moyen général pour exprimer le MARL résultant de l'OCPV et pour contraindre le MARL lui-même en fonction des spécifications organisationnelles. Il semble que l'augmentation du MARL avec un modèle organisationnel afin qu'il puisse être au cœur d'une approche de conception MAS assistée par l'ingénierie ne soit pas explicitement couverte à notre connaissance.

% ====================================================================================================

\section{Approche AOMEA}

% Mettre d'abord en avant le schéma général de l'approche
% Donner la philosophie de l'approche

%     Aperçu global (Fig. 1)
%     Noyau théorique
%     Ingénierie
%   Mise en œuvre (vers une mise en œuvre de type PoC)

\subsection{Aperçu}

\begin{figure}[h!]
  \centering
  \input{figures/approach}
  \caption{Aperçu de notre approche de la conception MAS}
  \label{fig:design_approach}
\end{figure}

Nous présentons AOMEA comme une approche de conception de MAS qui automatise la conception préliminaire d'un MAS en fonction de certaines contraintes de conception. Les spécifications organisationnelles obtenues après la formation permettent le développement d'un MAS organisé.
L'idée sous-jacente de notre approche est de considérer qu'une politique commune ou une histoire commune peut être décrite, au moins en partie, en termes de spécifications organisationnelles.
Nous appelons cette approche générale « MARL orientée organisation » (OMARL).
%
% De manière générale, AOMEA considère l'environnement avec des agents qui doivent atteindre certains objectifs. Elle permet de trouver automatiquement des informations pertinentes sous la forme de spécifications organisationnelles. Celles-ci expriment de manière transparente comment les agents peuvent agir individuellement et collaborer pour atteindre les objectifs. Le processus de conception du MAS peut alors être facilité à la lumière de ces indications. Une représentation illustrative de notre approche est présentée dans \autoref{fig:design_approach}.
%
L'AOMEA se compose de 4 phases séquentielles : modélisation, résolution, analyse et développement (respectivement $1.x$, $2.x$, $3.x$, $4.x$ dans les étiquettes des flèches dans \autoref{fig:design_approach}).

\textbf{Phase 1 : Modélisation} \quad Au cours de cette phase, le concepteur doit développer manuellement une simulation de l'environnement cible ($1.1$ dans \autoref{fig:design_approach}) dans lequel les agents doivent coopérer pour atteindre efficacement l'objectif du concepteur ($1.2$ dans \autoref{fig:design_approach}) à l'aide d'un retour d'information quantitatif. Lors du développement de l'environnement simulé, le concepteur peut relier des parties de la politique d'un agent (sous forme de couples observations-actions) à des spécifications organisationnelles connues de tout modèle organisationnel choisi.
Par exemple, dans les organisations \textquote{leader-follower}, les actions qui envoient des ordres à d'autres agents suiveurs sont des caractéristiques des agents leaders.
En option, le concepteur peut également souhaiter restreindre l'ensemble des politiques possibles que les agents peuvent explorer en fonction des spécifications organisationnelles données, afin de respecter les exigences de conception ou d'aider les agents à converger (1.3 dans \autoref{fig:design_approach}).

\textbf{Phase 2 : résolution} \quad Dans cette phase, en s'appuyant sur les relations établies entre les couples observation-action et les spécifications organisationnelles, un algorithme MARL est utilisé conjointement avec le modèle organisationnel MAS choisi dans le cadre d'un processus OMARL. Cela permet de trouver automatiquement les politiques optimales qui satisfont les spécifications organisationnelles de conception données ($2.1$ dans \autoref{fig:design_approach}) et qui conduisent à la meilleure récompense cumulative attendue, ainsi que d'obtenir les spécifications organisationnelles associées ($2.2$ dans \autoref{fig:design_approach}). Par exemple, lors de la formation d'agents à l'organisation \textquote{leader-follower}, certains agents peuvent être interdits d'envoyer des ordres tandis que d'autres peuvent être contraints de le faire. Après la formation, le processus OMARL caractérise les rôles émergents, les liens entre les rôles ou les sous-objectifs organisés en plans pour atteindre l'objectif.

\textbf{Phase 3 : Analyse} \quad Au cours de cette phase, le concepteur observe les politiques des agents formés ($3.2$ dans \autoref{fig:design_approach}) et prend en compte les spécifications organisationnelles associées déduites ($3.1$ dans \autoref{fig:design_approach}) afin de comprendre comment ces agents peuvent atteindre l'objectif. À la lumière de ces résultats bruts, le concepteur peut extraire des modèles de conception précieux à partir des décisions bruitées ou inutiles des agents. L'intérêt est de fournir au moins quelques indications sur les spécifications organisationnelles capables d'atteindre l'objectif et de satisfaire les contraintes de conception. Nous appelons ces indications précieuses des spécifications organisationnelles sélectionnées ($3.3$ dans \autoref{fig:design_approach}). Par exemple, après avoir entraîné plusieurs agents dans un environnement \textquote{prédateur-proie}, il est possible d'analyser qu'un prédateur \textquote{leader} accompagné de prédateurs \textquote{suiveurs} semble plus efficace pour attraper des proies.

\textbf{Phase 4 : Développement} \quad Au cours de cette phase, le concepteur prend en compte les spécifications organisationnelles sélectionnées comme modèle pour la mise en œuvre d'un MAS. À partir de là, un développement MAS classique peut être appliqué à l'aide de l'une des méthodes disponibles, utilisée conjointement avec le modèle organisationnel choisi. Contrairement aux agents formés qui peuvent provoquer des comportements inattendus, les agents mis en œuvre manuellement permettent de fournir les garanties de sécurité requises pour les environnements sensibles. Enfin, les agents mis en œuvre sont lancés dans des simulations afin d'évaluer si le MAS mis en œuvre permet d'atteindre efficacement l'objectif.

\subsection{Noyau théorique}

Pour mettre en œuvre un processus OMARL, nous proposons l'algorithme \emph{Partial Relations with Agent History and Organization Model} (PRAHOM) afin de relier les politiques des agents et leur formation à un modèle organisationnel.
Il s'agit d'une synthèse de deux processus qui s'inscrivent dans les objectifs de l'OMARL. Le premier processus obtient les spécifications à partir des politiques des agents, et le second processus obtient les politiques conjointes qui satisfont aux spécifications de conception données. Une vue illustrative de \emph{PRAHOM} est donnée dans \autoref{fig:prahom_process}.
Nous nous contentons ici de présenter l'idée sous-jacente à ces deux processus à un niveau élevé afin d'éviter tout formalisme inutile. Vous trouverez plus d'informations sur l'utilisation et la mise en œuvre de \emph{PRAHOM} dans \autoref{PettingZoo-wrapper}.
% Étant donné que \emph{PRAHOM} repose sur des historiques conjoints plutôt que sur des politiques conjointes, il est indépendant de la fonction d'approximation utilisée pour mettre en œuvre les politiques des agents, telles que celles basées sur les réseaux neuronaux.

\begin{figure}[h!]
  \centering
  \input{figures/prahom_process}
  \caption{Vue d'ensemble du processus PRAHOM}
  \label{fig:prahom_process}
\end{figure}

% \RestyleAlgo{ruled}
% \SetKwComment{Comment}{// }

% \begin{algorithm}[hbt!]
%     \caption{Relations partielles avec l'historique des agents et le modèle d'organisation (PRAHOM)}\label{alg:prahom}

%     \KwData{$d$, le Dec-POMDP à résoudre}
%     \KwData{$ep_{max} \in \mathbb{N}$, le nombre maximal d'épisodes}
%     \KwData{$step_{max} \in \mathbb{N}$, le nombre maximal d'étapes par épisode}
%     \KwData{$s \in \mathbb{R}$, l'espérance de récompense cumulative}
%     \KwData{$\pi_{joint}$, les politiques conjointes à former}
%     \KwData{$os_{init}$, les spécifications de conception à respecter}
%     \KwData{$Solv$, un algorithme MARL mettant à jour les politiques pour résoudre un Dec-POMDP}

%     \KwResult{$(s\pi_{joint,s,i} \in S\Pi_{joint,s,i}, os_{s,i} \in OS_{s,i})$, les politiques sous-optimales associées aux spécifications organisationnelles}

%     $s\pi_{joint,s,i} = PRAHOM-osh-training(s,i)$

%     $os_{s,i} = PRAHOM-hos(s\pi_{joint,s,i})$

% \end{algorithm}

\paragraph{\textbf{Déduction des spécifications organisationnelles}}

Plutôt que d'utiliser directement des politiques conjointes, nous utilisons les historiques conjoints, car ils peuvent être construits à partir des actions observées lorsque des observations sont reçues au cours d'une série d'épisodes de test. En effet, pour une politique donnée $\pi \in \Pi$, l'historique associé est par définition $h \in h_{joint} = \langle(\omega_k,a_k) | k \in \mathbb{N}\rangle$ et $(\omega_k,a_k) \in \pi$.
Ensuite, en raison de la difficulté à déduire des informations relatives aux spécifications organisationnelles, il est possible d'associer chaque observation ou action à des spécifications organisationnelles sous la forme d'une relation « plusieurs à plusieurs ». Cela établit un premier cadre pour identifier les spécifications organisationnelles dans les historiques. Nous abordons ce problème dans la suite de cette section.

On peut définir certaines relations entre les spécifications $\mathcal{M}OISE^+$ et les histoires conjointes. Leurs prémisses proviennent du fait que certaines spécifications du modèle organisationnel $\mathcal{M}OISE^+$ peuvent être mappées à des sous-ensembles d'actions provenant d'une seule politique conjointe sous-optimale.
À partir de ces relations, il est possible d'utiliser des approches empiriques ou statistiques pour déduire des spécifications organisationnelles à partir d'historiques conjoints. Nous décrivons ci-dessous de manière informelle les points clés pour comprendre ce processus.

Comme nous n'avons qu'un seul groupe, nous ne tenons pas compte des interconnexions et des intercompatibilités. De plus, pour simplifier, nous ne considérons qu'un seul schéma social.
Tout d'abord, nous examinons le niveau individuel en essayant de déterminer les rôles, les liens, les sous-groupes, les objectifs individuels, les missions et les plans joués par les agents en échantillonnant des sous-séquences historiques $h \in H$ et en les comparant à des sous-séquences historiques connues dont nous connaissons le rôle associé grâce aux relations établies.

Après avoir analysé plusieurs politiques conjointes, nous essayons de renforcer une vision globale des objectifs, des missions, des plans et de la connaissance de la mission par rapport à l'objectif, à l'aide des informations partiellement déduites au niveau individuel.
Au final, notre processus tente de synthétiser les connaissances déduites jusqu'à obtenir une meilleure vision de la cardinalité des agents par sous-groupe, de la cardinalité des agents pour chaque mission, de la cardinalité des rôles, des compatibilités entre les rôles, des autorisations et des obligations.

\paragraph{\textbf{Contraintes sur l'espace des politiques}}

Nous considérons un algorithme MARL donné qui converge de manière itérative vers une politique conjointe de sorte que la politique de chaque agent soit mise à jour à chaque étape jusqu'à un horizon fini.
Nous avons privilégié l'optimisation de la politique proximale pour son efficacité prouvée dans des environnements multi-agents coopératifs sans nécessiter de modifications algorithmiques ou d'architectures spécifiques au domaine~\cite{Yu2022}.
Afin de limiter les politiques conjointes possibles à celles qui satisfont les spécifications organisationnelles de conception $os_{init}$, nous proposons de contraindre les ensembles d'actions et d'observations de chaque agent en fonction de $os_{init}$ à chaque étape. En fin de compte, cela contraint un agent à un rôle en interdisant les actions liées à d'autres rôles.

Tout d'abord, nous utilisons les relations établies entre les spécifications organisationnelles et les couples action-observation pour déterminer les actions autorisées ou interdites que les agents peuvent effectuer à chaque étape.
Ensuite, il calcule d'abord l'ensemble des actions autorisées $A_{step}$ en fonction de l'historique actuel $h_{joint,i}$. Puis, une action est choisie parmi les actions autorisées. Cette action $a_{step} \in A_{step}$ est ajoutée à l'historique afin d'être utilisée pour mettre à jour la politique de l'agent à l'étape suivante. Ensuite, l'algorithme MARL met à jour la politique conjointe, et donc les politiques des agents, avec l'action et l'observation actuelles.
Enfin, une analyse de la politique conjointe sous-optimale actuelle $\pi_{joint}$ satisfaisant $os_{init}$ est déclenchée périodiquement. Elle permet d'améliorer de manière itérative l'efficacité des politiques conjointes et la précision des spécifications organisationnelles déduites.
Nous pouvons noter que la restriction impliquée par $os_{init}$ dans les politiques conjointes possibles peut empêcher l'algorithme MARL de trouver une politique conjointe qui satisfait la récompense cumulative minimale attendue définie par le concepteur.

\subsection{Outil d'ingénierie}

PettingZoo est une bibliothèque qui offre une API standardisée qui simplifie le développement d'environnements avec des agents et facilite l'application des algorithmes MARL.
Nous avons développé \emph{PRAHOM PettingZoo Wrapper}\label{PettingZoo-wrapper}, un outil permettant d'automatiser la configuration de \emph{PRAHOM} pour un environnement PettingZoo donné.
Il s'agit d'un \emph{PoC} reliant des histoires communes à des spécifications $\mathcal{M}OISE^+$ afin de fournir des fonctions permettant de déduire des spécifications organisationnelles brutes ou de contraindre l'entraînement. %Pendant l'entraînement, les actions sont masquées/encouragées aux agents en fonction des spécifications organisationnelles données comme contraintes.
%
\begin{lstlisting}[language=Python, caption=PRAHOM PettingZoo Wrapper utilisation de base, label={lst:wrapper_basic_use}]
from omarl_experiments import prahom_wrapper
env=PettingZoo_env.parallel_env(render_mode="human")
specs_to_hist={"structural_specifications":{"roles":{"follower":{"23":41,"14":[74,0]}}...},"functional_specifications":{"links":{"(leader,follower,aut)":".*14.*?89"}...}...}
policy_specs_constr={"agent_0":{"structural_specifications":"roles":["follower"]}}
env=prahom_wrapper(env,action_to_specs,training_specs)
env.train("default_PPO")
trained_specs,agent_to_specs=env.prahom_specs()
\end{lstlisting}
%
Dans \autoref{lst:wrapper_basic_use}, nous avons détaillé une utilisation basique du wrapper pour augmenter un environnement PettingZoo (l. 5) avec des relations connues entre les historiques et les spécifications organisationnelles (l. 3) et les contraintes de conception que les agents doivent respecter (l. 4).
Pendant l'entraînement, \textquote{"agent\_0"} est contraint au rôle \textquote{follower}, de sorte que toutes ses actions doivent être choisies en fonction des relations entre les spécifications organisationnelles et les historiques attendus (ou des expressions historiques abrégées).
Après la formation, \emph{PRAHOM PettingZoo Wrapper} déduit les spécifications organisationnelles à partir d'historiques conjoints en 5 épisodes, ainsi que l'instanciation des agents pour chacun d'entre eux (l. 7).

Ce processus utilise d'abord les relations connues entre les historiques et les spécifications organisationnelles (l. 3) : l'historique d'un agent qui contient l'observation $14$ (\textquote{commande reçue}) et l'action $74$ (\textquote{appliquer la commande}) ou $0$ (\textquote{ne rien faire}), est un \textquote{follower}. De même, l'historique peut être lié à des spécifications organisationnelles à l'aide d'expressions régulières telles que pour les \emph{liens} (l.3).
Ensuite, il généralise plusieurs histoires conjointes en tant que nouvelles spécifications organisationnelles en s'appuyant sur leur définition générale respective.
%
Par exemple, un rôle est considéré comme déduit en mesurant la similarité entre les historiques de plusieurs manières : regroupement de séquences (avec un dendrogramme) ; K-plus proches voisins (avec une ACP des historiques) ; analyse statistique (en particulier la fréquence des actions dans diverses visualisations) ; etc.
%
Des techniques sont également utilisées pour déduire des objectifs : analyse de fréquence des observations communes d'agents ayant le même rôle ; analyse des états seuils déclenchant une amélioration de la récompense (avec un graphe de transition d'états).

À partir des rôles et des objectifs obtenus, une approche empirique permet de déduire les autres spécifications organisationnelles telles que les compatibilités, les autorisations et les obligations.
En raison de cette approche empirique et de la portée spécifique des techniques, les résultats peuvent être incomplets ou bruités. Cependant, comme les résultats sont conformes à $\mathcal{M}OISE^+$, il est possible de les utiliser dans les méthodes de conception de MAS à la lumière des spécifications organisationnelles identifiées.

% Nous commençons par définir les actions dont les spécifications sont connues En nous concentrant sur « l'adversaire principal », nous avons :

% « observations » : [self_vel, self_pos, landmark_rel_positions, other_agent_rel_positions, other_agent_velocities, leader_comm]
% « actions » : [dire_0, dire_1, dire_2, dire_3] X [aucune_action, se_déplacer_à_gauche, se_déplacer_à_droite, se_déplacer_vers_le_bas, se_déplacer_vers_le_haut]

% =============

% \subsection{Prémisses du cœur théorique de l'AOMEA}

% - MARL orienté organisation : étude générale axée sur l'intégration de l'organisation dans le MARL sous plusieurs aspects
% - DMO (Dec-POMDP MOISE+ OMARL) : une classe d'algorithmes relevant des objectifs OMARL qui utilisent MOISE+ comme modèle organisationnel et Dec-POMDP comme modèle MARL
% - PRAHOM (Partial Relations with Agent History and Organization Model) : un algorithme DMO qui permet à la fois d'obtenir les spécifications organisationnelles à partir d'agents formés et d'orienter leur formation afin de satisfaire des spécifications organisationnelles supplémentaires sous forme de contraintes.

% \subsection{AOMEA pour l'ingénierie}

% - Nous avons proposé PRAHOM (Partial Relations with Agent History and Organization Model) : un algorithme qui permet à la fois d'obtenir les spécifications organisationnelles à partir d'agents formés et de diriger leur formation afin de satisfaire des spécifications organisationnelles supplémentaires sous forme de contraintes.
% - [Algo PRAHOM...]

% - Nous avons développé PRAHOM sous forme d'enveloppe PettingZoo afin de faciliter l'application de l'approche de conception MAS proposée...
% - [Lien vers dépôt GitHub]
% - Description des fonctionnalités
% - Description des fonctionnalités
% - Description des fonctionnalités

% - L'approche générale suit le workflow suivant :
% 1) Reproduction de l'environnement cible en simulation
% 2) Définir la fonction de récompense afin que les agents cherchent à atteindre les objectifs
% 3) Ajout d'agents à former pour résoudre le problème précédemment défini
% 4) Lancer la formation en simulation avec PRAHOM
% 5) Obtention des spécifications organisationnelles brutes avec PRAHOM
% 6) Organiser ces spécifications pour produire un modèle MAS sûr
% 7) Utilisation de ce modèle pour mettre en œuvre un MAS approprié
% 8) Vérifier la viabilité et le respect des exigences en simulation
% 9) Déployer ce MAS dans le système cible

% \begin{itemize}
%     \item Questions au niveau du système multi-agent (approche centrée sur le système)
%           \begin{itemize}
%               \item Nombre d'agents, quelle hétérogénéité ?
%               \item Quel est le support commun (environnement) partagé par les agents ?
%               \item Quels sont les mécanismes de communication disponibles pour les agents ?
%               \item Quels sont les langages de communication, les ontologies et les protocoles d'interaction utilisés par les agents ?
%               \item Quelle est l'organisation au sein de laquelle les agents opèrent ? Comment est-elle établie ?
%               \item Comment les agents coordonnent-ils leurs actions ? Comment garantir un fonctionnement cohérent ?
%           \end{itemize}

%     \item Questions au niveau des agents (approche centrée sur les agents)
%           \begin{itemize}
%               \item Que représente un agent ? Quelles actions doivent être encapsulées dans un agent ?
%               \item Comment les agents représentent-ils l'environnement et l'organisation dans lesquels ils opèrent ?
%               \item Comment les agents gèrent-ils les interactions avec d'autres agents ?
%               \item Quelle est la structure interne des agents ?
%           \end{itemize}
% \end{itemize}

% ====================================================================================================






\section{Évaluation dans des environnements de jeux coopératifs}

% Évaluation
%     Afin de vérifier et de démontrer l'approche, celle-ci est appliquée à l'étude de cas suivante.
%     Étude de cas


Afin d'évaluer l'AOMEA, nous avons envisagé d'utiliser \emph{PRAHOM} dans des environnements simulés disponibles composés d'agents qui doivent atteindre un objectif avec les meilleures performances possibles grâce à diverses stratégies collectives dont certaines sont faciles à comprendre (présentées dans \autoref{fig:simulated_environments}).
Nous avons sélectionné trois environnements de type Atari, car leur rendu visuel permet d'évaluer facilement les résultats à l'aide d'observations manuelles\footnotemark[1].
Nous avons également envisagé un environnement de cyberdéfense comme première tentative d'application de \emph{PRAHOM} dans un environnement de cyberdéfense non visuel :

\footnotetext[1]{Des explications supplémentaires et les exemples discutés à l'aide de \emph{PRAHOM PettingZoo wrapper} sont disponibles à l'adresse \url{https://github.com/julien6/omarl_experiments?tab=readme-ov-file\#tutorial-predator-prey-with-communication}}


\begin{itemize}
  \item \textquote{Essaim de drones - 3e défi CAGE}~\cite{cage_challenge_3_announcement} (CYB) consiste en des agents cyberdender déployés sur des drones en réseau qui luttent contre des programmes malveillants déployés de manière malveillante. On peut s'attendre à ce que les agents \allowbreak isolent les drones compromis ;
  \item \textquote{Pistonball} (PBL)~\cite{terry2020pettingzoo} consiste en une série de pistons destinés à déplacer une balle de droite à gauche, ce qui nécessite la représentation des voisins ;
  \item \textquote{Prédateur-proie avec communication}~\cite{Lowe2017} (PPY) consiste en des prédateurs surveillés par un chef afin d'attraper les proies les plus rapides, ce qui nécessite des stratégies de chasse ;
  \item \textquote{Knights Archers Zombies}~\cite{terry2020pettingzoo} (KAZ) consiste en des chevaliers et des archers qui apprennent à tuer des zombies, ce qui nécessite un positionnement spatial efficace des agents.
\end{itemize}
%
\begin{figure}[H]
  \centering
  \includegraphics[width=0.8\linewidth]{figures/envs_4x1.png}
  \caption{Aperçu des environnements sélectionnés : CYB, PBL, PPY et KAZ}
  \label{fig:simulated_environments}
\end{figure}
%
\noindent Nous avons appliqué l'AOMEA dans trois cas :
\begin{itemize}
  \item Aucune spécification organisationnelle (NTS) : les agents doivent apprendre les stratégies collectives les plus efficaces sans aucune contrainte ni indication.
  \item Spécifications organisationnelles partiellement contraignantes (PTS) : certaines contraintes ou indications sont fournies pour aider à converger plus rapidement ou à répondre aux exigences.
  \item Spécifications organisationnelles entièrement contraignantes (FTS) : des politiques communes élaborées manuellement sont fournies, car elles servent de référence pour les politiques communes apprises.
\end{itemize}

\noindent Nous ne présentons pas ici les détails des contraintes qui ont été données dans NTS et FTS (disponibles dans le dépôt Git\footnotemark[1]).
%
\begin{figure}[h!]
  \centering
  \includegraphics[width=0.8\textwidth]{figures/prahom_learning_curve.png}
  \caption{Récompense moyenne pour chaque itération dans l'environnement PBL pour les cas NTS, PTS et FTS}
  \label{fig:prahom_learning_curve}
\end{figure}
%
\begin{figure}[h!]
  \centering
  \includegraphics[width=0.8\textwidth]{figures/prahom_pca_analysis.png}
  \caption{ACP des historiques des agents formés dans l'environnement PBL}
  \label{fig:prahom_pca_analysis}
\end{figure}
%
Nous évaluons l'impact de \emph{PRAHOM} sur les critères suivants : ratios de temps de convergence entre PTS, NTS et FTS pour atteindre une récompense cumulative seuil. La stabilité des performances montre comment les agents entraînés peuvent atteindre l'objectif de manière générale en évaluant plusieurs environnements générés avec différents paramètres. Les résultats sont présentés dans le tableau~\ref{tab:training_AOMEA_results}.
%
\input{tables/training_OMARL_results.tex}
%
D'une manière générale, nous pouvons constater que le temps de convergence est plus long pour NTS que pour PTS, qui est lui-même plus long que pour FTS. Comme prévu, l'espace de recherche diminue, ce qui réduit le temps de convergence. Par exemple, nous avons remarqué une convergence plus rapide vers une solution sous-optimale dans l'environnement PBL en fournissant des spécifications organisationnelles, comme le montre la \autoref{fig:prahom_learning_curve}. Bien que PTS converge plus rapidement que NTS vers une récompense cumulative comparable, NTS peut surpasser PTS car les politiques des agents formés sont conçues sur mesure pour résoudre le problème de manière beaucoup plus fine que ne le permettent les spécifications organisationnelles du concepteur. La faible stabilité des performances dans l'environnement CYB, plus complexe, indique que les agents formés ont plus de difficulté à trouver des stratégies générales que les agents dans les autres environnements.

Nous avons également pris en compte les critères suivants après la formation : rôles, liens et performances globales. Une analyse qualitative est présentée dans le tableau~\ref{tab:trained_AOMEA_results}
%
\input{tables/trained_OMARL_results.tex}
%
% //À FAIRE : schémas Moise+ et comparaison avec les résultats attendus
%
Dans l'environnement PBL, nous pouvons remarquer que les rôles équivalents pour les agents sont censés agir de la même manière. En effet, les historiques des agents formés sont proches, ce qui montre l'émergence d'un rôle commun. Nous générons l'ACP présentée dans \autoref{fig:prahom_pca_analysis} en exprimant les historiques des agents sous forme de vecteurs contenant les couples observation-action. Nous pouvons remarquer que l'historique de la plupart des agents se trouve dans la zone inférieure gauche (encerclée en rouge). Cela montre que la plupart des pistons semblent agir comme prévu. Nous n'observons aucune spécification organisationnelle, à l'exception des rôles qui ont été générés, car les agents ne peuvent pas communiquer. Pour l'environnement KAZ, nous pouvons remarquer deux rôles distincts : les archers ont tendance à s'éloigner des zombies, tandis que les chevaliers ont tendance à s'en approcher. Pour l'environnement PPY, nous pouvons observer que les spécifications de sortie indiquent des liens d'autorité entre le prédateur leader et les prédateurs simples afin de permettre des stratégies collectives pour encercler les proies. Enfin, l'environnement CYB montre que les communications entre les agents bleus sont bien comprises comme des liens de communication qui permettent d'isoler les drones infiltrés ou d'essayer de réparer et d'alerter les drones récemment suspects.

Pour l'environnement CYB, nous avons développé notre MAS personnalisé à l'aide d'un arbre de décision simple conçu à la main, comme préconisé dans AOMEA, à la lumière des spécifications organisationnelles que nous avons sélectionnées en supprimant les résultats bruités. Notre approche n'a pas suggéré de rôles généraux, mais des modèles de stratégie pertinents ont été identifiés. Par exemple, en ce qui concerne les liens entre les rôles des agents, nous avons remarqué que les agents qui envoient fréquemment des messages semblent être repérés comme suspects par leurs voisins. De plus, un agent cyberdéfenseur situé dans le rayon de communication d'un drone suspect a tendance à couper sa communication et à la réactiver par la suite. Même si ces informations sont peu nombreuses, le score moyen obtenu avec notre MAS est d'environ -2000, ce qui est en effet proche des 5 meilleurs scores. Cela montre que l'AOMEA est effectivement applicable au contexte de la cyberdéfense et apporte en outre des garanties de sécurité.




\section{Conclusion}

Les travaux méthodologiques sur les MAS s'appuient sur les connaissances du concepteur pour concevoir une organisation MAS adaptée, mais ne fournissent pas de moyens automatiques ou assistés pour déterminer les mécanismes organisationnels pertinents.

Les techniques MARL ont été appliquées avec succès pour former automatiquement des agents à atteindre un objectif donné sans caractérisation explicite des stratégies collectives émergentes.
L'originalité de l'AOMEA réside dans le fait qu'il enrichit un processus MARL d'un modèle organisationnel explicite dans le but méthodologique de répondre à ces questions. Nous avons d'abord exposé comment l'AOMEA est destiné à être utilisé dans l'ingénierie MAS comme outil supplémentaire pour aider au processus de conception.
Nous avons ensuite expliqué le cœur théorique de l'AOMEA en établissant des liens entre le Dec-POMDP et le $\mathcal{M}OISE^+$ à travers le processus \emph{PRAHOM}.
En outre, nous avons implémenté le \emph{PRAHOM PettingZoo wrapper} comme preuve de concept pour l'application pratique de l'AOMEA et nous avons montré qu'il permet d'obtenir certaines spécifications organisationnelles qui satisfont les contraintes de conception et permettent d'atteindre l'objectif fixé.
Enfin, nous avons appliqué notre approche dans quatre environnements PettingZoo afin d'évaluer son impact pendant et après la formation. Les résultats obtenus en termes de performances sont comparables à ceux déjà connus, ce qui démontre la viabilité de notre approche.

Même si \emph{PRAHOM} est indépendant de l'algorithme MARL car il utilise l'historique des agents pour déduire les spécifications organisationnelles, il peut être difficile de reconstruire a posteriori les comportements collectifs des agents. En effet, une perspective majeure pour améliorer \emph{PRAHOM} consiste à aller plus loin avec des techniques d'apprentissage supervisé et non supervisé, en plus des approches statistiques empiriques pour identifier des spécifications organisationnelles utiles à partir d'historiques conjoints. De plus, il est intéressant d'étudier les travaux récents sur les techniques MARL, telles que l'apprentissage hiérarchique, car ils cherchent déjà à caractériser les stratégies émergentes tout au long de l'apprentissage.
