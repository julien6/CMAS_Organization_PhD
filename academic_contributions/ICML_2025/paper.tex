%%%%%%%% ICML 2026 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2026} with \usepackage[nohyperref]{icml2026} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
\usepackage{icml2026}

% For preprint, use
% \usepackage[preprint]{icml2026}

% If accepted, instead use the following line for the camera-ready submission:
% \usepackage[accepted]{icml2026}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}


% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

% ---- Packages ----
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{amsmath, amssymb, amsfonts}
\usepackage{mathtools}
\usepackage{bm}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{hyperref}
\usepackage{cleveref}
\usepackage{enumitem}
\usepackage{xcolor}

% ---- Optional: TikZ for figures ----
\usepackage{tikz}
\usetikzlibrary{positioning, arrows.meta, calc}

% ---- Handy commands (edit later) ----
\newcommand{\wm}{\textsc{WM}}
\newcommand{\mawm}{\textsc{MAWM}}
\newcommand{\nsm}{\textsc{NS-MAWM}}
\newcommand{\DecPOMDP}{\textsc{Dec-POMDP}}
\newcommand{\RVR}{\textsc{RVR}}
\newcommand{\KL}{\mathrm{KL}}

% ---- Math shortcuts ----
\newcommand{\E}{\mathbb{E}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\I}{\mathbb{I}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Neuro-Symbolic Multi-Agent World Models}

\begin{document}

\twocolumn[
  \icmltitle{Invariant-Aware Neuro-Symbolic Multi-Agent World Models\\
    for Model-Based Multi-Agent Reinforcement Learning}

  % It is OKAY to include author information, even for blind submissions: the
  % style file will automatically remove it for you unless you've provided
  % the [accepted] option to the icml2026 package.

  % List of affiliations: The first argument should be a (short) identifier you
  % will use later to specify author affiliations Academic affiliations
  % should list Department, University, City, Region, Country Industry
  % affiliations should list Company, City, Region, Country

  % You can specify symbols, otherwise they are numbered in order. Ideally, you
  % should not use this facility. Affiliations will be numbered in order of
  % appearance and this is the preferred way.
  \icmlsetsymbol{equal}{*}

  \begin{icmlauthorlist}
    \icmlauthor{Firstname1 Lastname1}{equal,yyy}
    \icmlauthor{Firstname2 Lastname2}{equal,yyy,comp}
    \icmlauthor{Firstname3 Lastname3}{comp}
    \icmlauthor{Firstname4 Lastname4}{sch}
    \icmlauthor{Firstname5 Lastname5}{yyy}
    \icmlauthor{Firstname6 Lastname6}{sch,yyy,comp}
    \icmlauthor{Firstname7 Lastname7}{comp}
    %\icmlauthor{}{sch}
    \icmlauthor{Firstname8 Lastname8}{sch}
    \icmlauthor{Firstname8 Lastname8}{yyy,comp}
    %\icmlauthor{}{sch}
    %\icmlauthor{}{sch}
  \end{icmlauthorlist}

  \icmlaffiliation{yyy}{Department of XXX, University of YYY, Location, Country}
  \icmlaffiliation{comp}{Company Name, Location, Country}
  \icmlaffiliation{sch}{School of ZZZ, Institute of WWW, Location, Country}

  \icmlcorrespondingauthor{Firstname1 Lastname1}{first1.last1@xxx.edu}
  \icmlcorrespondingauthor{Firstname2 Lastname2}{first2.last2@www.uk}

  % You may provide any keywords that you find helpful for describing your
  % paper; these are used to populate the "keywords" metadata in the PDF but
  % will not be shown in the document
  \icmlkeywords{Machine Learning, ICML}

  \vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column listing the
% affiliations and the copyright notice. The command takes one argument, which
% is text to display at the start of the footnote. The \icmlEqualContribution
% command is standard text for equal contribution. Remove it (just {}) if you
% do not need this facility.

% Use ONE of the following lines. DO NOT remove the command.
% If you have no special notice, KEEP empty braces:
\printAffiliationsAndNotice{}  % no special notice (required even if empty)
% Or, if applicable, use the standard equal contribution text:
% \printAffiliationsAndNotice{\icmlEqualContribution}

% ============================
% Abstract
% ============================
\begin{abstract}
  % TODO: 6--10 lines.
  % Problem: MA world models under partial observability suffer from semantic drift / inconsistency.
  % Idea: inject neuro-symbolic invariants / equivariances as soft constraints (optionally gated).
  % Method: NS-MAWM training with WM loss + rule consistency regularizer.
  % Results: better long-horizon prediction, lower rule violation, improved downstream planning/RL and generalization.
  % Contributions: framework + metrics + experiments.

  Model-based reinforcement learning enables planning through learned world models, but extending these approaches to multi-agent settings is difficult due to partial observability and error accumulation in joint observation dynamics. Purely neural multi-agent world models often produce semantically inconsistent predictions, violating spatial coherence, object persistence, and interaction constraints.
  We introduce \textbf{Neuro-Symbolic Multi-Agent World Models (NS-MAWM)}, which incorporate symbolic invariants and action-conditioned equivariances as differentiable consistency constraints during world model training. These constraints act as structured inductive biases that reduce long-horizon semantic drift while preserving uncertainty through soft satisfaction. An optional gating mechanism learns when rules should be applied, avoiding over-constraining in stochastic environments.
  Experiments on grid-based multi-agent environments and a standard benchmark show that NS-MAWM improves long-horizon prediction consistency, downstream planning performance, and generalization compared to neural baselines.

\end{abstract}

% ============================
% 1. Introduction
% ============================
\section{Introduction}
\label{sec:intro}
% TODO:
% - Context: model-based RL + world models; multi-agent adds partial observability + non-stationarity + joint observation dynamics.
% - Failure mode: semantic inconsistency, long-horizon drift, hallucinated objects, broken geometry.
% - Key idea: neuro-symbolic invariants/equivariances as structured inductive biases / constraints.
% - Summary of contributions (bullets).
% - Outline.

\paragraph{Contributions.}
\begin{itemize}
  \item \textbf{TODO} Contribution 1 (method).
  \item \textbf{TODO} Contribution 2 (metric / theory).
  \item \textbf{TODO} Contribution 3 (experiments / benchmarks).
\end{itemize}

% ============================
% 2. Background and Problem Setup
% ============================
\section{Background and Problem Setup}
\label{sec:background}

\subsection{Decentralized Partially Observable Multi-Agent RL}
\label{sec:decpomdp}
% TODO:
% - Dec-POMDP definition: (S, {A_i}, T, {O_i}, Omega, R, gamma)
% - Trajectory notation, local obs o^i_t, joint action a_t, etc.
% - CTDE setting (training centralized, execution decentralized).

\subsection{Model-Based MARL and Multi-Agent World Models}
\label{sec:mawm}
% TODO:
% - MAWM definition: latent state z_t; encoder e_theta, dynamics d_theta, decoder g_theta.
% - Centralized vs factorized architectures.
% - Planning / imagination usage (Dreamer-like) or model rollout for policy learning.

\subsection{Neuro-Symbolic Constraints, Invariants, and Equivariances}
\label{sec:ns}
% TODO:
% - Define invariants vs equivariances.
% - Soft constraints (regularization) vs hard constraints (projection).
% - Motivation: reduce hypothesis space, improve consistency under partial observability.

% ============================
% 3. Method: Neuro-Symbolic MA World Models
% ============================
\section{Neuro-Symbolic Multi-Agent World Models}
\label{sec:method}

\subsection{Overview}
\label{sec:overview}
% TODO:
% - high-level pipeline figure reference
% - training objective: L_wm + lambda L_logic
% - optional gating network / applicability masking

\subsection{Multi-Agent World Model Backbone}
\label{sec:backbone}
% TODO:
% - encoder, dynamics, decoder
% - centralized attention/graph or factorized (agent + global)
% - probabilistic vs deterministic prediction

\subsection{Rule Language and Constraint Instantiation}
\label{sec:rules}
% TODO:
% - Define a rule r: preconditions -> postconditions over predicted obs/latent.
% - Local geometric rules (translation equivariance), conservation, exclusivity.
% - Applicability A_r(t,i) and violation v_r(t,i).

\subsection{Differentiable Rule Consistency Loss}
\label{sec:loss}
% TODO:
% - Define L_logic = sum_{r,i,t} A_r * g_r * v_r
% - Soft violation using 1-p(target) or KL divergence.
% - How to deal with uncertainty / partial observability (unknown cells, masks).
% - Training details.

\subsection{Learning to Apply Rules (Optional Gating)}
\label{sec:gating}
% TODO:
% - gating g_r(t,i) in [0,1]
% - features used (z_t, a_t, local cues)
% - prevents over-constraining when rule not applicable or stochastic events happen

\subsection{Algorithm}
\label{sec:algo}
% TODO: fill in algorithm steps

\begin{algorithm}[t]
  \caption{\nsm\ Training with Neuro-Symbolic Consistency}
  \label{alg:nsm}
  \begin{algorithmic}[1]
    \STATE \textbf{Input:} Dataset $\mathcal{D}$ or replay buffer, rule set $\mathcal{R}$, weight $\lambda$
    \STATE Initialize world model parameters $\theta$ (and gating parameters $\phi$, optional)
    \REPEAT
    \STATE Sample batch $\mathcal{B}=\{(o_t^{1:N}, a_t^{1:N}, o_{t+1}^{1:N})\}$ from $\mathcal{D}$
    \STATE Encode latents $z_t \leftarrow e_\theta(o_t^{1:N})$, $z_{t+1}\leftarrow e_\theta(o_{t+1}^{1:N})$
    \STATE Predict $\hat{z}_{t+1} \leftarrow d_\theta(z_t, a_t^{1:N})$
    \STATE Decode $\hat{o}_{t+1}^{1:N} \leftarrow g_\theta(\hat{z}_{t+1})$
    \STATE Compute $\mathcal{L}_{wm}$ (prediction/reconstruction and optional latent loss)
    \STATE $\mathcal{L}_{logic}\leftarrow 0$
    \FOR{each rule $r\in\mathcal{R}$ and agent $i\in\{1,\dots,N\}$}
    \STATE Compute applicability $A_r(t,i)$ via preconditions
    \STATE Compute violation $v_r(t,i)$ via postconditions on $\hat{o}_{t+1}^{i}$
    \STATE (Optional) $g_r(t,i)\leftarrow h_\phi(\text{features})$
    \STATE $\mathcal{L}_{logic} \mathrel{+}= A_r(t,i)\cdot g_r(t,i)\cdot v_r(t,i)$
    \ENDFOR
    \STATE Update $(\theta,\phi)$ by minimizing $\mathcal{L}_{wm}+\lambda\mathcal{L}_{logic}$
    \UNTIL{convergence}
  \end{algorithmic}
\end{algorithm}

% ============================
% 4. Theory / Analysis (optional but helpful)
% ============================
\section{Analysis: Why Constraints Help}
\label{sec:analysis}
% TODO:
% - interpret constraints as hypothesis-space restriction / regularization
% - discuss long-horizon drift and error compounding
% - show how rule loss reduces semantic drift
% - small proposition + sketch (keep light)

% ============================
% 5. Evaluation Protocol
% ============================
\section{Evaluation}
\label{sec:eval}

\subsection{Environments}
\label{sec:envs}
% TODO:
% - Gridworld with objects + N agents (custom)
% - One standard MARL benchmark (e.g., Overcooked-like, SMAC-like, VMAS-like)
% - Observability settings, number of agents, map sizes

\subsection{Metrics}
\label{sec:metrics}
% TODO:
% - Prediction error (1-step and H-step rollout)
% - Rule Violation Rate (RVR), Soft-RVR
% - Long-horizon semantic drift curves (RVR vs horizon)
% - Downstream return / data efficiency if doing planning/policy learning

\subsection{Baselines}
\label{sec:baselines}
% TODO:
% - neuro-only MAWM (same backbone)
% - larger backbone (capacity control)
% - ablations: rule subsets, lambda, gating vs no gating, hard vs soft (if any)
% - alternative structured models (object-centric if included)

\subsection{Implementation Details}
\label{sec:impl}
% TODO:
% - architecture sizes, optimizer, batch sizes
% - rule set size and computation cost
% - training time / hardware
% - reproducibility

\subsection{Results}
\label{sec:results}
% TODO:
% - main tables: return, prediction error, RVR
% - figures: RVR vs horizon, OOD generalization
% - qualitative rollouts / failure cases

% ============================
% 6. Related Work
% ============================
\section{Related Work}
\label{sec:related}
% TODO:
% - World models / latent dynamics in RL
% - Model-based MARL and MA world models
% - Neuro-symbolic world models / differentiable logic
% - Object-centric / relational world models
% - Symmetry/equivariance in RL and structured inductive biases

% ============================
% 7. Limitations and Future Work
% ============================
\section{Limitations and Future Work}
\label{sec:limit}
% TODO:
% - partial / incorrect rules; stochastic events
% - scalability with many rules / large N
% - transferring rules across domains
% - learned rule discovery (future)

% ============================
% 8. Conclusion
% ============================
\section{Conclusion}
\label{sec:conclusion}
% TODO:
% - recap method + key results
% - take-home message: consistency constraints improve MAWM robustness and planning

% ============================
% Acknowledgements (remove for submission if needed)
% ============================
% \section*{Acknowledgements}

% ============================
% References
% ============================
\bibliography{references}
\bibliographystyle{icml2026}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage

\appendix

\section{Rule Set Details}
\label{app:rules}
% TODO:
% - list rules formally
% - applicability masks
% - examples

\section{Additional Experimental Results}
\label{app:more}
% TODO:
% - ablations, extra environments, hyperparams

\section{Architectures and Hyperparameters}
\label{app:hparams}
% TODO:
% - full hyperparam tables

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{document}

% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
