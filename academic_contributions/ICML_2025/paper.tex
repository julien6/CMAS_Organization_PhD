%%%%%%%% ICML 2026 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2026} with \usepackage[nohyperref]{icml2026} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
\usepackage{icml2026}

% For preprint, use
% \usepackage[preprint]{icml2026}

% If accepted, instead use the following line for the camera-ready submission:
% \usepackage[accepted]{icml2026}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}


% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

% ---- Packages ----
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{amsmath, amssymb, amsfonts}
\usepackage{mathtools}
\usepackage{bm}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{hyperref}
\usepackage{cleveref}
% \usepackage{enumitem}
\usepackage[inline, shortlabels]{enumitem}
\usepackage{xcolor}
\usepackage{amsthm}

% ---- Optional: TikZ for figures ----
\usepackage{tikz}
\usetikzlibrary{positioning, arrows.meta, calc}

% ---- Handy commands (edit later) ----
\newcommand{\wm}{\textsc{WM}}
\newcommand{\mawm}{\textsc{MAWM}}
\newcommand{\nsm}{\textsc{NS-MAWM}}
\newcommand{\DecPOMDP}{\textsc{Dec-POMDP}}
\newcommand{\RVR}{\textsc{RVR}}
\newcommand{\KL}{\mathrm{KL}}

\newcommand{\probP}{\text{I\kern-0.15em P}}

% Spell-checker exceptions for acronyms and domain-specific terms
% \usepackage{spellcheck}
% \spellcheck{MAWM,NS-MAWM,gridworld,SMAC,neuro-symbolic}

% --- Tickz
\usepackage{physics}
\usepackage{tikz}
\usetikzlibrary{arrows.meta,positioning,fit,calc}
\usepackage{amsmath}
\usepackage{mathdots}
% \usepackage{yhmath}
\usepackage{cancel}
\usepackage{color}
\usepackage{siunitx}
\usepackage{array}
\usepackage{stmaryrd}
\usepackage{multirow}
% \usepackage{amssymb}
\usepackage{gensymb}
\usepackage{tabularx}
\usepackage{extarrows}
\usepackage{booktabs}
\usetikzlibrary{fadings}
\usetikzlibrary{patterns}
\usetikzlibrary{shadows.blur}
\usetikzlibrary{shapes}

% ---- Math shortcuts ----
\newcommand{\E}{\mathbb{E}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\I}{\mathbb{I}}

\usepackage{amssymb}
\usepackage{pifont}

\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%

\usepackage[english]{babel}
\addto\extrasenglish{  
    \def\figureautorefname{Figure}
    \def\tableautorefname{Table}
    \def\algorithmautorefname{Algorithm}
    \def\sectionautorefname{Section}
    \def\subsectionautorefname{Subsection}
    \def\proofoutlineautorefname{Proof Outline}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Neuro-Symbolic Multi-Agent World Models}

\begin{document}

\twocolumn[
  \icmltitle{A Neuro-Symbolic Multi-Agent World Model Framework\\
    for Model-Based Multi-Agent Reinforcement Learning}

  % It is OKAY to include author information, even for blind submissions: the
  % style file will automatically remove it for you unless you've provided
  % the [accepted] option to the icml2026 package.

  % List of affiliations: The first argument should be a (short) identifier you
  % will use later to specify author affiliations Academic affiliations
  % should list Department, University, City, Region, Country Industry
  % affiliations should list Company, City, Region, Country

  % You can specify symbols, otherwise they are numbered in order. Ideally, you
  % should not use this facility. Affiliations will be numbered in order of
  % appearance and this is the preferred way.
  \icmlsetsymbol{equal}{*}

  \begin{icmlauthorlist}
    \icmlauthor{Julien Soulé}{equal,yyy}
    \icmlauthor{Georgios Bakirtzis}{equal,yyy,comp}
    \icmlauthor{Jean-Paul Jamont}{comp}
    \icmlauthor{Firstname4 Lastname4}{sch}
    \icmlauthor{Firstname5 Lastname5}{yyy}
    \icmlauthor{Firstname6 Lastname6}{sch,yyy,comp}
    \icmlauthor{Firstname7 Lastname7}{comp}
    %\icmlauthor{}{sch}
    \icmlauthor{Firstname8 Lastname8}{sch}
    \icmlauthor{Firstname8 Lastname8}{yyy,comp}
    %\icmlauthor{}{sch}
    %\icmlauthor{}{sch}
  \end{icmlauthorlist}

  \icmlaffiliation{yyy}{Department of XXX, University of YYY, Location, Country}
  \icmlaffiliation{comp}{Company Name, Location, Country}
  \icmlaffiliation{sch}{School of ZZZ, Institute of WWW, Location, Country}

  \icmlcorrespondingauthor{Julien Soulé}{julien.soule@hotmail.fr}
  \icmlcorrespondingauthor{Firstname2 Lastname2}{first2.last2@www.uk}

  % You may provide any keywords that you find helpful for describing your
  % paper; these are used to populate the "keywords" metadata in the PDF but
  % will not be shown in the document
  \icmlkeywords{Machine Learning, ICML}

  \vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column listing the
% affiliations and the copyright notice. The command takes one argument, which
% is text to display at the start of the footnote. The \icmlEqualContribution
% command is standard text for equal contribution. Remove it (just {}) if you
% do not need this facility.

% Use ONE of the following lines. DO NOT remove the command.
% If you have no special notice, KEEP empty braces:
\printAffiliationsAndNotice{}  % no special notice (required even if empty)
% Or, if applicable, use the standard equal contribution text:
% \printAffiliationsAndNotice{\icmlEqualContribution}

% ============================
% Abstract
% ============================
\begin{abstract}
  Model-based reinforcement learning enables planning through learned WMs, but extending these approaches to multi-agent settings is challenging due to partial observability and error accumulation in joint observation dynamics. Purely neural multi-agent WMs often produce semantically inconsistent predictions, violating spatial coherence, object persistence, and interaction constraints.
  We introduce \textbf{Neuro-Symbolic Multi-Agent WMs (NS-MAWM)}, which incorporate symbolic invariants and action-conditioned equivariances as differentiable consistency constraints during WM training. These constraints act as structured inductive biases that reduce long-horizon semantic drift.
  We evaluate NS-MAWM on four representative multi-agent environments with three proposed neuro-symbolic integration strategies. Experimental results show that NS-MAWM improves long-horizon prediction consistency, planning performance, and generalization compared to a purely neural baseline.
\end{abstract}

% ============================
% 1. Introduction
% ============================
\section{Introduction}
\label{sec:intro}

Learning predictive models of environment dynamics, known as \emph{WMs} (WMs)~\cite{ha2018worldmodels}, is a central paradigm in model-based reinforcement learning (MBRL)~\cite{hafner2019learning,hafner2020dreamer,hafner2021dreamerv2}. By encoding the agent's interaction history into a latent space, these models enable imagination-based planning, policy learning, and long-horizon credit assignment~\cite{schrittwieser2020}. Recent progress in this area has shown strong results in single-agent and fully observable environments; however, their extension to multi-agent settings with partial observability remains an open frontier~\cite{Wong2023,Venugopal2024MABL,agravante-etal-2023-learning}.

In such environments, observations typically consist of multiple interacting entities governed by spatial, logical, or physical rules (for instance two agents cannot occupy the same location, objects follow deterministic transitions, etc.). Yet, most WMs treat observations as flat, unstructured vectors~\cite{ha2018worldmodels,hafner2019learning}, and learn monolithic latent dynamics end-to-end from raw pixels. This often leads to latent representations that fail to disentangle semantics (such as agent identity, object types) or to reflect the compositional structure of the environment~\cite{kipf2020contrastivelearningstructuredworld,bronstein2021geometric}. As a result, models may produce high-likelihood predictions that are logically inconsistent or physically implausible.

For instance, a trained WM might predict that two agents occupy the same grid cell, violating collision rules. Such symbolic inconsistencies are not penalized by standard likelihood or reconstruction losses, often going unnoticed during evaluation. Worse, these errors compound during long rollouts, degrading planning performance and breaking environment logic~\cite{talvitie2014modelbias,venkatraman2015improving}.

Several works have pioneered the incorporation of prior knowledge as inductive biases into WMs~\cite{kipf2020contrastivelearningstructuredworld,mondal2022eqr}. However, a general framework for integrating symbolic reasoning into WMs learning has yet to be established, especially regarding three general gaps:
%
\begin{itemize}
  \item[\textbf{G1}] \textbf{Lack of Multi-Agent-Oriented WM Structures.}
    Existing WMs neglect structural priors for multi-agent environments: explicit representation of partial observability, interaction-induced non-stationarity, and entity modularity~\cite{zhang2021worldmodelgraph, Willemsen2021MAMBPO, Venugopal2024MABL}. They assume stationary dynamics and treat agents uniformly, failing to generalize across varied configurations.

  \item[\textbf{G2}] \textbf{Lack of Neuro-Symbolic Integration in WMs.}
    While symbolic priors (action constraints, physical laws, interaction rules) enhance sample efficiency and robustness, few models incorporate them into WM learning or architecture~\cite{Xu2018semanticloss, Fan2017differentiablelearninglogicalrules, balloch2023neurosymbolicworldmodelsadapting, garcez2023neurosymbolic}. Most rely solely on black-box latent dynamics, missing opportunities to leverage symbolic knowledge as inductive bias.

  \item[\textbf{G3}] \textbf{Lack of Symbolic Coherence as an Optimization Principle.}
    Standard WM metrics focus on predictive accuracy or downstream reward, overlooking symbolic consistency. Yet reasoning in dynamic environments benefits from verifying whether predictions respect constraints~\cite{talvitie2014modelbias, balloch2023neurosymbolicworldmodelsadapting}.
\end{itemize}

Building on neuro-symbolic integration frameworks~\cite{manhaeve2018deepproblog,garcez2023neurosymbolic,Xu2018semanticloss,Fan2017differentiablelearninglogicalrules}, we propose \textbf{NS-MAWM} to augment WMs with symbolic reasoning through:
%
\begin{itemize}
  \item A \emph{structured latent space} that decomposes observations into typed entities (agents, objects) with interpretable attributes (such as position or type), enabling effective symbolic rule application.

  \item A \emph{symbolic rule formalism} that expresses environment dynamics as differentiable constraints over observations and actions, clearly specifying invariants and action-conditioned equivariances.

  \item Three neuro-symbolic integration strategies:
        \begin{enumerate*}[label={\roman*) },itemjoin={; \quad}]
          \item \textbf{Symbolic Loss Regularization}: rules are encoded as differentiable soft constraints, similarly to semantic loss or logic regularization frameworks
          \item \textbf{Symbolic Projection}: violations are corrected by projecting predictions to valid configurations, ensuring adherence to the defined rules.
          \item \textbf{Residual Symbolic Dynamics}: transitions are factorized into a symbolic process and learned residuals, allowing for a more accurate approach to learning.
        \end{enumerate*}

  \item A \emph{Rule Violation Rate (RVR)} metric that quantifies constraint violations for logical consistency evaluation.
\end{itemize}

We evaluate NS-MAWM across four discrete and continuous environments: a proposed Minecraft-like gridworld, Overcooked~\cite{Micah2020overcooked}, a Predator-Prey environment~\cite{lowe2017multi} and the SMAC environment~\cite{Ellis2023smacv2}. Our comparison includes various integration strategies alongside a vanilla WM baseline, utilizing both standard and symbolic metrics for assessment. The results demonstrate that NS-MAWM not only improves prediction accuracy but also enhances generalization to new configurations, significantly reducing symbolic violations. Among the integration strategies, for similar performance, the residual symbolic dynamics strategy shows the fastest convergence, while the combination of the symbolic loss regularization strategy and the symbolic projection one provide stronger coherence over longer time horizons.

The remainder of the paper is organized as follows: \autoref{sec:related_work} surveys related work, \autoref{sec:background} briefly reviews background concepts, \autoref{sec:method} details NS-MAWM, \autoref{sec:eval} presents experiments, and \autoref{sec:conclusion} concludes.



% ============================
% 3. Related Work
% ============================

\section{Related Work}
\label{sec:related_work}
% TODO: On cherche les travaux qui permettent d'avoir un WM qui permette une meilleure prise en compte de :
%  - l'observabilité partielle,
%  - de la non-stationnarité (autres agents qui interagissent en même temps),
%  - l'aspect multi-agent dans l'architecture du modèle,
%  - l'interpretation des observations en particulier au regard d'un environnement multi-agent complexe
%  - la possibilité d'injecter des règles symboliques
%  - la possibilité d'évaluer la cohérence au regard des règles symboliques
%  - la robustesse à des variations de l'environnement
%  - la généralisation à de nouvelles configurations d'agents/objets
%  - l'efficacité en termes de temps de calcul et de données pour converger vers une performance comparable par rapport à un WM classique (la frugalité du temps de calcul)
%  - la frugalité en termes de données (moins de données nécessaires pour atteindre une certaine performance)
%  - la scalabilité avec le nombre d'agents/objets/règles
%  - le drift sémantique sur le long terme (est-ce que le modèle reste cohérent sur le long terme ?)

We review the landscape of related works across four major axes of relevance to NS-MAWM:

\paragraph{Architectures of World Models.} Early approaches such as Dreamer~\cite{hafner2020dreamer, hafner2021dreamerv2} and PlaNet~\cite{hafner2019learning} proposed powerful black-box latent dynamics models, often using a monolithic latent space without architectural bias toward entities or agents. More recent efforts adopt a structured representation. For instance, World Model as Graph~\cite{zhang2021worldmodelgraph} and MABL~\cite{Venugopal2024MABL} propose latent modularity aligned with environment entities and topology. MAMBPO~\cite{Willemsen2021MAMBPO} incorporates explicit multi-agent structure, allowing rollouts conditioned on individual agent views.

\paragraph{Learning in Multi-Agent Environments.} Some models simply treat other agents as part of the environment, hence implicitly stationary. Mingling Foresight~\cite{Xu2022mingling} and Multi-Timescale MARL~\cite{nekoei2023dealing} improve handling of concurrent interactions via hierarchical reasoning or timescale adaptation. Opponent modeling is tackled explicitly in~\cite{Yuxiaopeng2022modelbased}, which proposes world models conditioned on latent opponent policies.

\paragraph{Symbolic Knowledge in World Models.} Symbolic priors are sparsely represented in world model literature. Several efforts inject domain knowledge as constraints, such as Semantic Loss~\cite{Xu2018semanticloss} and differentiable logic rules~\cite{Fan2017differentiablelearninglogicalrules}. DeepProbLog~\cite{manhaeve2018deepproblog} merges probabilistic logic with deep models. Recent neuro-symbolic world models~\cite{balloch2023neurosymbolicworldmodelsadapting, agravante-etal-2023-learning} propose explicit symbolic structure over latent dynamics to support adaptation and interpretability.

\paragraph{Evaluation Dimensions.} Most WMs are evaluated using reconstruction loss or control performance, but few assess coherence with symbolic rules or generalization under structural shifts. Some recent benchmarks~\cite{Duan_2025_ICCV} and surveys~\cite{Wong2023, Delvecchio2025p1157} advocate for broader evaluation metrics, including semantic consistency, rule violation rates, and frugality in data/time.

\begin{table*}[t]
  \centering
  \resizebox{\textwidth}{!}{%
    \footnotesize
    \begin{tabular}{p{4.69cm}cccccccc}
      \toprule
      \textbf{Work}                                                  & Struct. WM & MA-aware & NS-aware & Symb. Priors & Symb. Coher. & Robust & Gen.   & Frugal \\
      \midrule
      Dreamer~\cite{hafner2020dreamer}                               & \xmark     & \xmark   & \xmark   & \xmark       & \xmark       & \xmark & $\sim$ & $\sim$ \\
      WM as Graph~\cite{zhang2021worldmodelgraph}                    & \cmark     & $\sim$   & \xmark   & \xmark       & \xmark       & \xmark & $\sim$ & \xmark \\
      MABL~\cite{Venugopal2024MABL}                                  & \cmark     & \cmark   & $\sim$   & \xmark       & \xmark       & $\sim$ & \cmark & $\sim$ \\
      MAMBPO~\cite{Willemsen2021MAMBPO}                              & \cmark     & \cmark   & \xmark   & \xmark       & \xmark       & \xmark & $\sim$ & \cmark \\
      WorldCloner~\cite{balloch2023neurosymbolicworldmodelsadapting} & \cmark     & \cmark   & \cmark   & \cmark       & \cmark       & $\sim$ & \cmark & \xmark \\
      Mingling~\cite{Xu2022mingling}                                 & $\sim$     & \cmark   & \cmark   & \xmark       & \xmark       & \cmark & \cmark & \cmark \\
      OppMod~\cite{Yuxiaopeng2022modelbased}                         & \xmark     & \cmark   & \cmark   & \xmark       & \xmark       & $\sim$ & $\sim$ & \xmark \\
      DeepProbLog~\cite{manhaeve2018deepproblog}                     & \xmark     & \xmark   & \xmark   & \cmark       & \xmark       & \xmark & \xmark & \xmark \\
      NS-WM~\cite{agravante-etal-2023-learning}                      & \cmark     & \cmark   & $\sim$   & \cmark       & \cmark       & $\sim$ & $\sim$ & \xmark \\
      \bottomrule
    \end{tabular}
  }
  \caption{Comparison of representative works. Struct. WM: structured world model; MA-aware: multi-agent awareness; NS-aware: non-stationarity; Symb. Priors: symbolic priors; Symb. Coher.: symbolic coherence evaluation; Robust: robustness; Gen.: generalization; Frugal: data/time efficiency.}
  \label{tab:related_work}
\end{table*}

\autoref{tab:related_work} summarizes this landscape, highlighting how existing works address (or neglect) the critical dimensions we target. Despite significant progress, most existing world models fail to simultaneously address the three critical dimensions we target. Specifically, they often lack : (\textbf{G1}) multi-agent specific architectural bias, (\textbf{G2}) integration of symbolic priors or reasoning mechanisms, and (\textbf{G3}) coherence verification against symbolic knowledge. These gaps motivate our proposed framework to support structured, interpretable, and efficient learning in multi-agent partially observable environments with evolving dynamics.


\section{Background and basics}
\label{sec:background}

\subsection{MARL framework}
\label{sec:decpomdp}

To study cooperative MARL under partial observability,
we adopt the \textit{Decentralized Partially Observable Markov Decision Process} (Dec-POMDP)
formalism~\cite{Oliehoek2016},
which provides a principled framework for decentralized decision-making
where agents act on local observations while jointly optimizing a shared objective.
Dec-POMDPs are a strict subclass of
\textit{Partially Observable Stochastic Games} (POSGs),
specialized to fully cooperative settings with a common reward
and a jointly optimized policy~\cite{Beynier2013}.

Formally, a Dec-POMDP is a 7-tuple
$d = \langle S, \{A_i\}_{i=1}^n, T, R, \{\Omega_i\}_{i=1}^n, O, \gamma \rangle$,
where $S$ is the state space;
$A_i$ and $\Omega_i$ are the action and observation spaces of agent $i$;
$T(s,a,s') = \mathbb{P}(s' \mid s,a)$ is the transition function for joint actions
$a = (a_1,\dots,a_n)$;
$R : S \times A \times S \rightarrow \mathbb{R}$ is the shared reward;
$O(s',a,o) = \mathbb{P}(o \mid s',a)$ is the observation function;
and $\gamma \in [0,1)$ is the discount factor.

Each agent $i \in \mathcal{A} = \{1,\dots,n\}$ follows a (possibly stochastic) policy
$\pi_i : \Omega_i \rightarrow \Delta(A_i)$,
and the \textbf{joint policy} is
$\pi_{\text{joint}} = (\pi_1,\dots,\pi_n)$.
Let $H_i$ denote agent histories
$h_i = \langle (\omega_k^i, a_k^i) \rangle_{k=0}^{z}$,
and $H_{\text{joint}} = H_1 \times \dots \times H_n$ the set of joint histories.
The objective is to find $\pi_{\text{joint}}$ maximizing the expected return with either finite horizon $T$ or infinite horizon and $\gamma < 1$:
\[
  V(\pi_{\text{joint}}) =
  \mathbb{E}_{\pi_{\text{joint}}}
  \left[ \sum_{t=0}^{T} \gamma^t R(s_t, a_t, s_{t+1}) \right],
\]

\subsection{Model-Based Multi-Agent Reinforcement Learning}
\label{sec:mb-marl}

Model-Based Multi-Agent Reinforcement Learning (MB-MARL) extends model-based RL to multi-agent decision-making.
The central idea is to learn an approximate environment model
and exploit it for planning or policy optimization,
thereby improving sample efficiency and coordination
\cite{Moerland2023modelbased,Wang2022modelbasedmultiagentreinforcementlearning}.

\paragraph{Environment models.}
In cooperative settings with $n$ agents, dynamics are governed by
$P(s' \mid s, a_1,\dots,a_n)$ with shared reward $R(s,a_1,\dots,a_n)$.
MB-MARL learns approximations
$\hat{P}_\phi$ and $\hat{R}_\phi$ from interaction data.
Compared to single-agent MBRL, learning such models is more challenging
due to joint action spaces, partial observability, and learning-induced non-stationarity
\cite{Wang2022modelbasedmultiagentreinforcementlearning}.

\paragraph{Model usage.}
Following standard taxonomies \cite{Moerland2023modelbased,Wang2022modelbasedmultiagentreinforcementlearning},
learned models are used for
(i) planning (such as Model Predictive Control -- MPC),
(ii) Dyna-style learning with short imagined rollouts,
or (iii) direct policy optimization in the learned model,
which is sample-efficient but sensitive to model bias
\cite{Moerland2023modelbased}.

\paragraph{Model structure.}
MB-MARL methods differ by model structure:
centralized models learn joint dynamics using global information
(similar to CTDE but limited in scalability),
whereas decentralized or factorized models learn local or agent-specific dynamics,
possibly leveraging communication or structural assumptions
\cite{Wang2022modelbasedmultiagentreinforcementlearning}.

Under suitable assumptions, reusing data through model rollouts
can reduce environment interactions and improve sample complexity
in cooperative or discounted stochastic games
\cite{Wang2022modelbasedmultiagentreinforcementlearning,Moerland2023modelbased}.




\subsection{World Model Basics}
\label{sec:wm_basics}

WMs are a class of MBRL methods
that learn environment dynamics~\cite{ha2018worldmodels,hafner2020dreamer}.
They are particularly suited to partially observable settings such as Dec-POMDPs,
enabling planning, improved sample efficiency, and imagination-based reasoning.
Here, we present a simplified deterministic formulation while noting that
modern WMs typically rely on stochastic latent variables trained via variational inference.

Formally, at each time step $t$, let $\omega_t \in \Omega$ denote the observation,
$a_t \in A$ the executed action, and $\tilde{h}_{t-1} \in \mathcal{H}$ the recurrent hidden state summarizing the interaction history up to time $t-1$. An encoder $Enc : \Omega \rightarrow Z$ maps high-dimensional observations to a compact latent representation $z_t = Enc(\omega_t)$.
%
The temporal evolution of the latent state is modeled by a
\textbf{Recurrent Latent Dynamics Model (RLDM)}~\cite{hafner2020dreamer},
defined as
\[
  h_t = f(h_{t-1}, z_t, a_t), \qquad \hat{z}_{t+1} = g(h_t),
\]
where $f(\cdot)$ is typically implemented as a Recurrent Neural Network (RNN),
such as an Long-Short-Term Memory (LSTM)~\cite{hochreiter1997long},
and $g(\cdot)$ is a parametric mapping such as Multi-Layer Perceptron (MLP).
%
The predicted latent state is decoded by
$Dec : Z \rightarrow \Omega$ to produce the predicted observation
$\hat{\omega}_{t+1} = Dec(\hat{z}_{t+1})$.

\subsection{An Extension to Multi-Agent World Models}
\label{sec:mawm_basics}

A common extension of WMs to multi-agent settings
consists in learning a centralized latent dynamics model
over joint observations and joint actions, similar to the Centralized Training and Decentralized Execution (CTDE) paradigms.
\cite{Wang2022modelbasedmultiagentreinforcementlearning}.

Given a dataset of joint trajectories
$\mathcal{D}_{H^j} = \{ h^j \}$,
with
$h^j = \langle (\omega_t^j, a_t^j) \rangle_{t=0}^{T}$,
the objective is to learn a predictive model of joint observation dynamics,
as commonly done in model-based MARL and multi-agent world modeling
\cite{Wang2022modelbasedmultiagentreinforcementlearning,Xu2022mingling}.

We define a \textbf{Joint Observation Prediction Model} (JOPM) to capture the environment dynamics as follows:
\[
  \mathcal{T}^j :
  H^j \times \Omega^j \times A^j
  \rightarrow \mathcal{H} \times \hat{\Omega}^j ,
\]
where $\mathcal{H}$ denotes the space of recurrent hidden states,
following recurrent latent dynamics formulations in world models
\cite{ha2018worldmodels,hafner2020dreamer}.
%
At time $t$, given $\tilde{h}_{t-1} \in \mathcal{H}$,
the joint observation $\omega_t^j$ and joint action $a_t^j$,
the model outputs an updated hidden state $\tilde{h}_t$
and a predicted joint observation $\hat{\omega}_{t+1}^j$.

To reduce dimensionality, a shared encoder
$Enc^j : \Omega^j \rightarrow Z$
maps joint observations to a latent space,
$z_t = Enc^j(\omega_t^j)$
\cite{ha2018worldmodels,Xu2022mingling}.
Latent dynamics are modeled as
\[
  \tilde{h}_t = f(\tilde{h}_{t-1}, z_t, a_t^j),
  \qquad
  \hat{z}_{t+1} = g(\tilde{h}_t),
\]
and decoded via
$Dec^j : Z \rightarrow \Omega^j$,
yielding
$\hat{\omega}_{t+1}^j = Dec^j(\hat{z}_{t+1})$
.
%
The encoder--decoder pair is typically implemented using MLP or attention-based architectures
to aggregate multi-agent information and capture inter-agent dependencies
\cite{Xu2022mingling,Wang2022modelbasedmultiagentreinforcementlearning}.
This centralized formulation enables accurate joint dynamics modeling under centralized training,
but suffers from scalability and decentralized execution limitations
\cite{Wang2022modelbasedmultiagentreinforcementlearning,Moerland2023modelbased}.


\section{Neuro-Symbolic Multi-Agent WMs}
\label{sec:method}

We propose \emph{Neuro-Symbolic Multi-Agent WMs (NS-MAWM)}, a framework that augments world models with structured neuro-symbolic reasoning.%A schematic overview of NS-MAWM is provided in \autoref{fig:ns-mawm_overview}.

% \begin{figure}[t]
%   \centering
%   \include{figures/method_figure}
%   \caption{Overview of the NS-MAWM framework: ...TODO}
%   \label{fig:ns-mawm_overview}
% \end{figure}

\subsection{Structured observation latent space and unified neuro-symbolic formalism}
\label{sec:ns}

To integrate symbolic reasoning into multi-agent world modeling, we propose a formal decomposition of individual observations and define a symbolic transition operator capable of partially predicting future observations.

\paragraph{Observation representation.} We assume that the observation of an agent $i$ at time $t$ can be expressed as a feature vector:
\begin{align*}
  \omega_{i,t} = (f_{1,t}, f_{2,t}, \dots, f_{n_f,t}),
\end{align*}
where each $f_{k,t}$ is a numerical or categorical feature conveying a specific perceptual attribute (such asrelative position to another agent, object type in view, etc.), and $n_f$ is the number of features per observation.

\paragraph{Complete and partial observations.} A complete observation contains only defined values. If any feature is undefined (such asdue to occlusion or lack of knowledge), we term the observation \emph{partial}, denoted $\omega^p_t$. It consists of two subparts:
\begin{itemize}
  \item $\omega^d_t$: the sub-vector of features with defined values.
  \item $\omega^u_t$: the sub-vector of undefined (or yet-to-be-determined) features.
\end{itemize}

\paragraph{Feature determinability.} We posit that certain features are \emph{determinable} solely from the joint observation $\omega^j_t$, joint action $a^j_t$, and joint history $h^j_{t-1}$. These features are sensitive to symbolic invariants and regularities, such as spatial consistency or relative motion of agents. For example, in a gridworld, if agent $j$ moves south, and there are no obstacles, then agent $i$ observing agent $j$ to its left should later observe it in the bottom-left position.

This leads us to decompose an observation as follows:
\begin{align*}
  \omega_{i,t} = (f^d_{1,t}, \dots, f^d_{m,t}, f_{m+1,t}, \dots, f_{n_f,t}),
\end{align*}
where $f^d_{k,t} \in \mathcal{F}^d$ are determinable features from the symbolic perspective.

\paragraph{Atomic symbolic rules.} We define an atomic symbolic transition rule as a deterministic mapping:
\begin{align*}
  t^s(h^j_{t-1}, \omega^j_t, a^j_t, f^d_{i,l,t}) = f^d_{i,l,t+1},
\end{align*}
which expresses how a given determinable feature $f^d_{i,l,t}$ evolves under symbolic knowledge. For instance, the rule \texttt{agent\_move\_consistency} enforces spatial motion constraints based on known past and present information.

\paragraph{Partial symbolic transition operator.} By aggregating all applicable symbolic rules, we define a symbolic transition operator:
\begin{align*}
  \mathcal{T}^s(h^j_{t-1}, \omega^j_t, a^j_t) = \omega^{j,p}_{t+1},
\end{align*}
where $\omega^{j,p}_{t+1}$ is the predicted \emph{partial} joint observation at time $t+1$, composed of:
\begin{itemize}
  \item $\omega^{j,d}_{t+1}$: determinable features derived via symbolic rules.
  \item $\omega^{j,u}_{t+1}$: undefined features to be completed by the neural world model.
\end{itemize}

\paragraph{Matrix view.} Let $\mathcal{M}_{\omega^j_t}$ denote the observation matrix whose rows are agent-specific feature vectors. Then the symbolic transition reads:
\begin{align*}
  \mathcal{T}^s(\mathcal{M}_{h^j_{t-1}}, \mathcal{M}_{\omega^j_t}, \mathcal{M}_{a^j_t}) = \mathcal{M}_{\omega^{j,d}_{t+1}} + \mathcal{M}_{\omega^{j,u}_{t+1}} = \mathcal{M}_{\omega^{j,p}_{t+1}}.
\end{align*}
We define the determinability mask $\mathcal{M}_d = \mathrm{sgn}(|\mathcal{M}_{\omega^{j,d}_{t+1}}|)$ to indicate feature positions derived via symbolic rules. This mask allows selective symbolic/neural hybrid modeling.

This unified formalism underpins all subsequent neuro-symbolic integration strategies by enabling partial prediction and enforcement of symbolically determinable features while leaving the rest to the neural dynamics model.


\subsection{Neuro-Symbolic Integration Strategies}
\label{sec:strategies}

We present three strategies to integrate symbolic reasoning with learned world models. All strategies operate over the structured observation latent space defined in \autoref{sec:ns}, where the next joint observation $\omega^j_{t+1}$ is partially predicted by symbolic rules and completed by a neural predictor.

Let $\hat{\omega}^j_{t+1} = f_{\theta}(h^j_{t-1}, \omega^j_t, a^j_t)$ be the neural prediction of the full next joint observation and $\omega^{j,p}_{t+1} = \mathcal{T}^s(h^j_{t-1}, \omega^j_t, a^j_t)$ the partial symbolic prediction.


\subsubsection{Symbolic Consistency Regularization}
\label{sec:symbolic_loss_regularization}

This strategy adds symbolic supervision as a consistency constraint. The idea is to encourage the neural model to produce outputs that align with symbolic predictions where such knowledge is available. This is inspired by auxiliary loss strategies in hybrid models~\cite{Xu2018semanticloss, garcez2023neurosymbolic}.

Let $\mathcal{M}_d = \mathrm{sgn}(|\mathcal{M}_{\omega^{j,d}_{t+1}}|)$ be the determinability mask indicating which features are covered by symbolic rules. Let $\hat{\omega}^{j,d}_{t+1} = \mathcal{M}_d \odot \hat{\omega}^j_{t+1}$ and $\omega^{j,d}_{t+1}$ be the symbolic counterpart.

We define the symbolic consistency loss:
\begin{align*}
  \mathcal{L}_{\text{symb}} = \| \hat{\omega}^{j,d}_{t+1} - \omega^{j,d}_{t+1} \|^2,
\end{align*}
which is added to the main predictive loss:
\begin{align*}
  \mathcal{L}_{\text{total}} = \mathcal{L}_{\text{pred}} + \lambda \mathcal{L}_{\text{symb}},
\end{align*}
where $\lambda$ is a regularization coefficient. This approach nudges the model to learn symbolically consistent dynamics in a differentiable way.


\subsubsection{Symbolic Projection Strategy}
\label{sec:symbolic_projection}

This strategy uses symbolic knowledge to override parts of the neural prediction. This follows a common intervention approach in modular hybrid systems~\cite{garnelo2016deepsymbolicreinforcementlearning}. Instead of only encouraging consistency, we enforce it by projecting determinable features from the symbolic model:
\begin{align*}
  \hat{\omega}^{j}_{t+1, \text{proj}} = \mathcal{M}_d \odot \omega^{j,d}_{t+1} + (1 - \mathcal{M}_d) \odot \hat{\omega}^j_{t+1}.
\end{align*}

The projected output $\hat{\omega}^{j}_{t+1, \text{proj}}$ is guaranteed to respect all symbolic constraints. It can be used as:
\begin{itemize}
  \item Input to downstream planning or policy modules.
  \item A target for retraining the world model (bootstrapping symbolic compliance).
  \item An explicit correction layer to improve rollout fidelity.
\end{itemize}

This approach guarantees minimal symbolic violations and is especially suited for safety-critical applications or environments with known dynamics.


\subsubsection{Residual Symbolic Dynamics Strategy}
\label{sec:residual_symbolic_dynamics}

Here, we explicitly separate symbolic and sub-symbolic prediction responsibilities. The neural predictor is trained \emph{only} on features that are not determinable from symbolic knowledge. This architectural partitioning is inspired by residual learning frameworks in neurosymbolic reasoning~\cite{garcez2023neurosymbolic, balloch2023neurosymbolicworldmodelsadapting}.

Let $\mathcal{F}^d$ and $\mathcal{F}^{nd}$ denote the sets of determinable and non-determinable features. The symbolic model computes:
\begin{align*}
  \omega^{j,d}_{t+1} = \mathcal{T}^s(h^j_{t-1}, \omega^j_t, a^j_t),
\end{align*}
while the neural module $f_{\theta}^{nd}$ is trained to predict:
\begin{align*}
  \hat{\omega}^{j,nd}_{t+1} = f_{\theta}^{nd}(h^j_{t-1}, \omega^j_t, a^j_t).
\end{align*}

The final prediction is assembled by:
\begin{align*}
  \hat{\omega}^{j}_{t+1} = [\omega^{j,d}_{t+1}, \hat{\omega}^{j,nd}_{t+1}].
\end{align*}

This strategy reduces model complexity and encourages specialization: symbolic rules govern deterministic patterns, while neural components capture ambiguous or high-entropy transitions. It is particularly advantageous when the symbolic rules cover a stable core of the environment while others remain dynamic.


\subsection{Post-Prediction Symbolic Correction of Neural Features}
\label{sec:post_prediction_correction}

In addition to the integration strategies above, we introduce a symbolic correction mechanism that adjusts the raw neural prediction $\hat{\omega}^j_{t+1}$ after inference, without requiring re-training. This aims to enforce symbolic alignment and structural plausibility in the final output.

Let $\hat{\omega}^{j}_{t+1}$ be the output of the NS-MAWM predictor. We apply a symbolic post-correction operator $\mathcal{C}^{s}$:
\begin{align*}
  \tilde{\omega}^{j}_{t+1} = \mathcal{C}^{s}(\hat{\omega}^{j}_{t+1}),
\end{align*}
where $\tilde{\omega}^{j}_{t+1}$ denotes the corrected prediction.

$\mathcal{C}^{s}$ may enforce symbolic rules such as mutual exclusivity, state transitions, or presence constraints. A typical case is discretization: rounding real-valued outputs of categorical features to their nearest valid values. More generally, $\mathcal{C}^{s}$ applies domain-specific corrections to determinable features or resolves spatial/topological inconsistencies using symbolic priors.

This correction enhances coherence and interpretability, aligning predictions with what is semantically valid in the environment.

\subsection{Rule Violation Rate}
\label{sec:rvr}

To evaluate the symbolic consistency of a world model's predictions, we introduce the \textit{Rule Violation Rate (RVR)}. This metric quantifies how often the predicted observations violate the symbolic transition rules defined in \autoref{sec:ns}.

Let $\mathcal{T}^s(h^j_{t-1}, \omega^j_t, a^j_t) = \omega^{j,d}_{t+1}$ be the symbolic prediction of determinable features, and $\hat{\omega}^j_{t+1}$ the model's full predicted observation. Let $\mathcal{F}^d$ be the set of determinable features. We define a symbolic violation indicator for each determinable feature:
\begin{equation}
  v_{i,k,t} =
  \begin{cases}
    1 & \text{if } | \hat{\omega}_{i,k,t+1} - \omega_{i,k,t+1}^{d} | > \epsilon, \\
    0 & \text{otherwise},
  \end{cases}
\end{equation}
where $i$ indexes the agent, $k$ the feature, and $\epsilon$ a tolerance threshold.

The \textbf{Rule Violation Rate} over a dataset of $T$ transitions and $n$ agents is then defined as:
\begin{equation}
  \text{RVR} = \frac{1}{n T |\mathcal{F}^d|} \sum_{t=1}^T \sum_{i=1}^n \sum_{k \in \mathcal{F}^d} v_{i,k,t}.
\end{equation}

This metric captures the proportion of determinable features that are predicted inconsistently with respect to the symbolic rules. Lower RVR indicates better symbolic coherence and serves as a diagnostic signal to assess rule-alignment in both training and evaluation phases.

RVR is meaningful in contexts where symbolic consistency is not guaranteed by construction, such as purely neural world models (without symbolic correction) or the Symbolic Consistency Regularization strategy (\autoref{sec:symbolic_loss_regularization}), where symbolic guidance is applied as a soft constraint. In contrast, for the Symbolic Projection Override and Symbolic-Neural Feature Partitioning strategies, determinable features are either overwritten or fully handled by symbolic rules, and thus RVR will trivially evaluate to zero. Therefore, RVR should primarily be used as a validation metric to monitor symbolic compliance during training or for ablation studies.


% ============================
% ============================
\section{Evaluation}
\label{sec:eval}

We evaluate NS-MAWM along four axes matching the gaps in \autoref{sec:intro}:
(i) \textbf{long-horizon predictive fidelity} (model bias and compounding errors),
(ii) \textbf{symbolic coherence} (rule satisfaction),
(iii) \textbf{downstream control} (planning / policy performance),
and (iv) \textbf{generalization and robustness} (new configurations and partial rule coverage).

\subsection{Implementation}
\label{sec:impl_overview}

\footnotetext[1]{\label{fn:github}
  An anonymized implementation of NS-MAWM and full experimental details
  (including hyperparameters, architectures, and rule definitions)
  are available at \url{https://anonymous.4open.science/r/NS-MAWM-83D8/README.md}.}

\paragraph{World model backbone.}
Our framework \textit{NS-MAWM}~\hyperref[fn:github]{\footnotemark[1]} builds on a shared MAWM backbone
(\autoref{sec:mawm_basics}) and is implemented in \texttt{PyTorch}~\cite{paszke2019pytorch}.
We use a centralized Joint Observation Prediction Model (JOPM) comprising
(i) a shared encoder $Enc^j$ over joint observations,
(ii) an LSTM-based latent dynamics core~\cite{hochreiter1997long},
and (iii) a decoder $Dec^j$ predicting the next joint observation.
Joint observations are concatenated along the agent dimension and embedded using MLPs with ReLU activations.
Discrete structured features are one-hot encoded and treated as continuous outputs,
while continuous features are predicted directly.
The prediction loss is
$\mathcal{L}_{\text{pred}} = \sum_{k \in \mathcal{F}_{\text{cont}}}\|\hat{f}_{k}-f_{k}\|^2$.
All neural components are optimized using Adam~\cite{kingma2017adammethodstochasticoptimization}
with a fixed learning rate.

\paragraph{Neuro-symbolic components.}
The symbolic transition operator $\mathcal{T}^s$ (\autoref{sec:ns}) is implemented as a lightweight Python rule engine.
Each rule deterministically produces a subset of determinable features $\omega^{j,d}_{t+1}$
and a corresponding mask $\mathcal{M}_d$.
This modular design supports selective rule activation for ablation and robustness studies.
We evaluate three integration strategies (\autoref{sec:strategies}):
(i) symbolic consistency regularization via an auxiliary loss inspired by semantic loss~\cite{Xu2018semanticloss,Fan2017differentiablelearninglogicalrules},
(ii) symbolic projection at inference time,
and (iii) residual symbolic dynamics, restricting neural prediction to non-determinable features.
We further evaluate an inference-time symbolic correction operator $\mathcal{C}^s$
(\autoref{sec:post_prediction_correction}),
which enforces validity constraints (such as mutual exclusivity or spatial feasibility)
using simple projection and discretization operators~\cite{garcez2023neurosymbolic}.

\paragraph{Training data and interfaces.}
World models are trained offline from replay buffers collected using random or weakly exploratory policies.
We rely on standard multi-agent RL interfaces:
\texttt{PettingZoo}~\cite{terry2020pettingzoo},
\texttt{Overcooked-AI}~\cite{Micah2020overcooked},
and the \texttt{SMAC} benchmark~\cite{Ellis2023smacv2}.
Replay buffers store joint observations, actions, and next observations.
To assess data frugality, models are trained on multiple dataset sizes
(such as $N \in \{N_{\text{small}}, N_{\text{med}}, N_{\text{full}}\}$),
while keeping architectures and training schedules fixed across methods.

\paragraph{Hyperparameter selection.}
Neural hyperparameters (learning rate, latent dimension, LSTM size, depth)
are selected automatically using Optuna~\cite{akiba2019optuna}
by minimizing validation prediction loss.
Hyperparameter optimization is applied exclusively to neural components
and independently of symbolic rules, with identical search spaces and budgets across methods.
Selected hyperparameters are fixed for all subsequent experiments.



\subsection{Environments and rule sets}
\label{sec:envs}

We consider four representative multi-agent environments spanning discrete/continuous dynamics
and cooperative/competitive interaction patterns:

\paragraph{(E1) Gridcraft~\hyperref[fn:github-gridcraft]{\footnotemark[2]}, a Minecraft-like gridworld~footnote (discrete, partially observable).}
A grid-based environment with $n$ agents, walls/obstacles, and typed objects.
Observations are structured features (relative positions, occupancy, object presence),
making invariants and equivariances explicit.
Rules include collision avoidance, boundary constraints, object persistence, and action-conditioned motion equivariances.

\footnotetext[2]{\label{fn:github-gridcraft}
  An anonymized version is available at: \url{https://anonymous.4open.science/r/Gridcraft-006A/README.md}.}

\paragraph{(E2) Overcooked ~\cite{Micah2020overcooked} (discrete, cooperative coordination).}
We use Overcooked layouts featuring sparse rewards and strong coordination requirements.
Rules capture state-transition constraints (pickup/drop interactions), mutually exclusive object states,
and spatial movement constraints.

\paragraph{(E3) Predator--Prey~\cite{lowe2017multi} (continuous or mixed, multi-agent interaction).}
We use a standard Predator--Prey benchmark with continuous positions/velocities (or mixed discrete actions).
Rules encode kinematic constraints (bounded velocity), proximity-based capture conditions, and persistence.

\paragraph{(E4) SMAC~\cite{Ellis2023smacv2} (StarCraft Multi-Agent Challenge, partially observable).}
We evaluate on a subset of SMAC scenarios with varying team sizes.
Rules focus on movement feasibility and action-conditioned invariances
(such as if an agent does not move, its position remains consistent; cooldown-like constraints when available).

\paragraph{Rule sets.}
For each environment, we define a rule set $\mathcal{R}$~\hyperref[fn:github]{\footnotemark[1]} implementing $\mathcal{T}^s$ and the feature mask $\mathcal{M}_d$.
% TODO : donner un aperçu du nombre/type de règles par environnement

\subsection{Computing resources}
\label{sec:resources}

All experiments were conducted on an academic high-performance computing cluster, utilizing various configurations of GPU nodes. Specifically, we employed nodes equipped with NVIDIA A100 and V100 GPUs, and AMD MI210 GPUs.

\input{tables/results_overview}

\subsection{Evaluation metrics, baselines and protocol}
\label{sec:metrics_baselines_protocol}

\paragraph{Baselines.}
We compare (i) \textbf{MAWM (Neural)}, trained only with $\mathcal{L}_{\text{pred}}$;
(ii) structured WM baselines when available; and
(iii) \textbf{NS-MAWM} variants: symbolic regularization, projection, and residual dynamics.
We also report ablations removing invariants, equivariances, or the post-correction operator $\mathcal{C}^s$.

\paragraph{Predictive fidelity.}
We measure one-step prediction error on held-out data and assess compounding error
via open-loop rollouts of horizon $H$.
We report aggregated multi-step reconstruction error over $t=1..H$
and drift curves as a function of rollout horizon.

\paragraph{Symbolic coherence.}
Symbolic consistency is evaluated using the \textbf{Rule Violation Rate (RVR)}
(\autoref{sec:rvr}) on open-loop rollouts.
As projection and residual strategies enforce determinable features by construction,
RVR primarily diagnoses the neural baseline and symbolic regularization;
for enforced variants, we additionally report violations on non-determinable constraints
and pre-/post-correction rates when applicable.

\paragraph{Downstream control.}
We evaluate imagination-based control using a fixed planner (such as MPC)
operating on world-model rollouts.
Metrics include episodic return, task-dependent success rate,
and planning stability (variance across seeds),
with the planner held fixed across methods.

\paragraph{Generalization and robustness.}
Generalization is tested on out-of-distribution settings unseen during training,
including varying agent/object counts, novel layouts (gridworld and Overcooked),
and alternative SMAC scenarios.
Robustness to incomplete symbolic knowledge is assessed by randomly dropping
a fraction $\rho$ of rules (reducing $\text{Cov}(\mathcal{R})$)
during training and/or inference.

\paragraph{Protocol.}
For each environment, we collect an offline dataset with a fixed behavior policy,
train all variants on the same data split,
evaluate predictive fidelity and symbolic coherence on held-out rollouts,
and assess planning performance in the real environment.
Results are reported as mean $\pm$ standard error over $S$ seeds.


\subsection{Results}
\label{sec:results}

\autoref{tab:results_overview} summarizes the main findings.

\paragraph{Long-horizon prediction and semantic drift (G1, G3).}
Across all environments, NS-MAWM consistently reduces multi-step prediction error
relative to the neural MAWM baseline, with larger gains in more complex settings
(such as Overcooked and SMAC).
Residual and projection strategies yield the flattest error growth over rollout horizon $H$,
by explicitly separating or enforcing symbolically determinable dynamics.
These results indicate that symbolic constraints act as structured inductive biases
that mitigate long-horizon semantic drift in partially observable multi-agent settings.

\paragraph{Symbolic coherence (G2, G3).}
The neural MAWM baseline exhibits substantial symbolic violations (RVR $=0.15$--$0.23$),
showing that predictive accuracy alone does not ensure symbolic plausibility.
Symbolic consistency regularization significantly reduces RVR,
while projection and residual strategies enforce zero RVR on determinable features by construction.
This confirms that symbolic coherence must be explicitly optimized rather than treated as an implicit byproduct.

\paragraph{Downstream planning performance (G1).}
When used for imagination-based planning, NS-MAWM achieves higher episodic return
and success rate than the neural baseline across all environments
(Appendix~\ref{app:more}).
Improvements are most pronounced in sparse-reward coordination tasks such as Overcooked,
where small model inconsistencies can cause catastrophic planning failures,
demonstrating a direct link between predictive fidelity and planning reliability.

\paragraph{Generalization and robustness (G2, G3).}
NS-MAWM generalizes more robustly to out-of-distribution configurations,
including unseen layouts and varying agent counts.
Under partial rule removal (reduced $\text{Cov}(\mathcal{R})$),
performance degrades gracefully, particularly for the residual strategy.
Overall, residual symbolic dynamics provides the best trade-off between accuracy,
symbolic coherence, and computational efficiency,
while regularization combined with projection yields the strongest long-horizon consistency.


\section{Conclusion and future work}
\label{sec:conclusion}

We proposed \textbf{NS-MAWM}, a neuro-symbolic framework for multi-agent world modeling that integrates symbolic reasoning. By enforcing symbolic coherence, NS-MAWM reduces long-horizon semantic drift and improves imagination-based planning, yielding consistent gains in predictive fidelity and generalization over purely neural world models.

Despite its benefits, NS-MAWM incurs additional computational overhead,
faces scalability challenges as the number of agents, features, or symbolic rules grows,
and relies on manually specified partial symbolic transition rules.
Future work will explore:
(1) leveraging large language models (LLMs)~\cite{Brown2020gpt3,openai2023gpt4} to assist in automatic rule extraction or refinement;
(2) developing matrix-oriented formulations of symbolic transitions to reduce overhead and improve scalability;
and (3) adopting high-performance numerical frameworks such as \texttt{JAX}~\cite{jax2018github}
to enable efficient vectorization, compilation, and hardware acceleration.


\section*{Impact Statement}

This paper presents work whose goal is to advance the field of Machine Learning by improving the reliability of model-based multi-agent reinforcement learning.
By reducing semantic inconsistencies in learned WMs, this work may contribute to safer and more interpretable decision-making in multi-agent systems.
We do not identify ethical concerns specific to this work beyond those commonly associated with reinforcement learning research.


% ============================
% Acknowledgements (remove for submission if needed)
% ============================
% \section*{Acknowledgements}

% ============================
% References
% ============================
\bibliography{references}
\bibliographystyle{icml2026}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage

% \appendix

% \section{Rule Set Details}
% \label{app:rules}
% % TODO:
% % - list rules formally
% % - applicability masks
% % - examples

% \section{Additional Experimental Results}
% \label{app:more}
% % TODO:
% % - ablations, extra environments, hyperparams

% \section{Architectures and Hyperparameters}
% \label{app:hparams}
% % TODO:
% % - full hyperparam tables

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{document}
