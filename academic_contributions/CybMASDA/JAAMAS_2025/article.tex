%Version 3.1 December 2024
% See section 11 of the User Manual for version history
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                                                 %%
%% Please do not use \input{...} to include other tex files.       %%
%% Submit your LaTeX manuscript as one .tex document.              %%
%%                                                                 %%
%% All additional figures and files should be attached             %%
%% separately and not embedded in the \TeX\ document itself.       %%
%%                                                                 %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%\documentclass[referee,sn-basic]{sn-jnl}% referee option is meant for double line spacing

%%=======================================================%%
%% to print line numbers in the margin use lineno option %%
%%=======================================================%%

%%\documentclass[lineno,pdflatex,sn-basic]{sn-jnl}% Basic Springer Nature Reference Style/Chemistry Reference Style

%%=========================================================================================%%
%% the documentclass is set to pdflatex as default. You can delete it if not appropriate.  %%
%%=========================================================================================%%

%%\documentclass[sn-basic]{sn-jnl}% Basic Springer Nature Reference Style/Chemistry Reference Style

%%Note: the following reference styles support Namedate and Numbered referencing. By default the style follows the most common style. To switch between the options you can add or remove �Numbered� in the optional parenthesis. 
%%The option is available for: sn-basic.bst, sn-chicago.bst%  
 
%%\documentclass[pdflatex,sn-nature]{sn-jnl}% Style for submissions to Nature Portfolio journals
%%\documentclass[pdflatex,sn-basic]{sn-jnl}% Basic Springer Nature Reference Style/Chemistry Reference Style
\documentclass[pdflatex,sn-mathphys-num]{sn-jnl}% Math and Physical Sciences Numbered Reference Style
%%\documentclass[pdflatex,sn-mathphys-ay]{sn-jnl}% Math and Physical Sciences Author Year Reference Style
%%\documentclass[pdflatex,sn-aps]{sn-jnl}% American Physical Society (APS) Reference Style
%%\documentclass[pdflatex,sn-vancouver-num]{sn-jnl}% Vancouver Numbered Reference Style
%%\documentclass[pdflatex,sn-vancouver-ay]{sn-jnl}% Vancouver Author Year Reference Style
%%\documentclass[pdflatex,sn-apa]{sn-jnl}% APA Reference Style
%%\documentclass[pdflatex,sn-chicago]{sn-jnl}% Chicago-based Humanities Reference Style

%%%% Standard Packages
%%<additional latex packages if required can be included here>

% \usepackage{graphicx}%
% \usepackage{multirow}%
\usepackage{amsmath,amssymb,amsfonts}%
\usepackage{amsthm}%
\usepackage{mathrsfs}%
% \usepackage[title]{appendix}%
% \usepackage{xcolor}%
% \usepackage{textcomp}%
% \usepackage{manyfoot}%
% \usepackage{booktabs}%
% \usepackage{algorithm}%
% \usepackage{algorithmicx}%
% \usepackage{algpseudocode}%
% \usepackage{listings}%

\usepackage{hyperref}

%%%% For camera-ready, use this
%\documentclass[sigconf]{aamas}

\usepackage{listings}
\usepackage{xcolor}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}
 
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}
 
\lstset{style=mystyle}

% --- Tickz
\usepackage{physics}
\usepackage{tikz}
\usepackage{amsmath}
\usepackage{mathdots}
% \usepackage{yhmath}
\usepackage{cancel}
\usepackage{color}
\usepackage{siunitx}
\usepackage{array}
\usepackage{multirow}
% \usepackage{amssymb}
\usepackage{gensymb}
\usepackage{tabularx}
\usepackage{extarrows}
\usepackage{booktabs}
\usetikzlibrary{fadings}
\usetikzlibrary{patterns}
\usetikzlibrary{shadows.blur}
\usetikzlibrary{shapes}

% ---------

\usepackage{balance} % for balancing columns on the final page
\usepackage{csquotes}
% \usepackage{cite}
\newcommand{\probP}{\text{I\kern-0.15em P}}
\usepackage{etoolbox}
\patchcmd{\thebibliography}{\section*{\refname}}{}{}{}
% \usepackage{amsthm,amssymb,amsfonts}

\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{color}
% \renewcommand\UrlFont{\color{blue}\rmfamily}

\usepackage[inline, shortlabels]{enumitem}
\usepackage{tabularx}
\usepackage{caption}
\usepackage{listings}
\usepackage{stfloats}
\usepackage{titlesec}
\usepackage{ragged2e}
% \usepackage[hyphens]{url}
\usepackage[linesnumbered,ruled,vlined]{algorithm2e}
\usepackage{float}
\usepackage[english]{babel}
\addto\extrasenglish{  
    \def\figureautorefname{Figure}
    \def\tableautorefname{Table}
    \def\algorithmautorefname{Algorithm}
    \def\sectionautorefname{Section}
    \def\subsectionautorefname{Subsection}
    \def\proofoutlineautorefname{Proof Outline}
}

%%%%

%%%%%=============================================================================%%%%
%%%%  Remarks: This template is provided to aid authors with the preparation
%%%%  of original research articles intended for submission to journals published 
%%%%  by Springer Nature. The guidance has been prepared in partnership with 
%%%%  production teams to conform to Springer Nature technical requirements. 
%%%%  Editorial and presentation requirements differ among journal portfolios and 
%%%%  research disciplines. You may find sections in this template are irrelevant 
%%%%  to your work and are empowered to omit any such section if allowed by the 
%%%%  journal you intend to submit to. The submission guidelines and policies 
%%%%  of the journal take precedence. A detailed User Manual is available in the 
%%%%  template package for technical guidance.
%%%%%=============================================================================%%%%

%% as per the requirement new theorem styles can be included as shown below
\theoremstyle{thmstyleone}%
\newtheorem{theorem}{Theorem}%  meant for continuous numbers
%%\newtheorem{theorem}{Theorem}[section]% meant for sectionwise numbers
%% optional argument [theorem] produces theorem numbering sequence instead of independent numbers for Proposition
\newtheorem{proposition}[theorem]{Proposition}% 
%%\newtheorem{proposition}{Proposition}% to get separate numbers for theorem and proposition etc.

\theoremstyle{thmstyletwo}%
\newtheorem{example}{Example}%
\newtheorem{remark}{Remark}%

\theoremstyle{thmstylethree}%
\newtheorem{definition}{Definition}%

\raggedbottom
%%\unnumbered% uncomment this for unnumbered level heads

\begin{document}

\title[Assisting Multi-Agent System Design with MOISE+ and MARL: The MAMAD Method]{Assisting Multi-Agent System Design with $\mathcal{M}OISE^+$ and MARL: The MAMAD Method}

%%=============================================================%%
%% GivenName	-> \fnm{Joergen W.}
%% Particle	-> \spfx{van der} -> surname prefix
%% FamilyName	-> \sur{Ploeg}
%% Suffix	-> \sfx{IV}
%% \author*[1,2]{\fnm{Joergen W.} \spfx{van der} \sur{Ploeg} 
%%  \sfx{IV}}\email{iauthor@gmail.com}
%%=============================================================%%

\author*[1]{\fnm{Julien} \sur{Soulé}}\email{julien.soule@lcis.grenoble-inp.fr}

\author[1]{\fnm{Jean-Paul} \sur{Jamont}}\email{jean-paul.jamont@lcis.grenoble-inp.fr}
% \equalcont{These authors contributed equally to this work.}

\author[1]{\fnm{Michel} \sur{Occello}}\email{michel.occello@lcis.grenoble-inp.fr}

\author[2]{\fnm{Louis-Marie} \sur{Traonouez}}\email{louis-marie.traonouez@thalesgroup.com}

\author[3]{\fnm{Paul} \sur{Théron}}\email{paul.theron@orange.fr}

\affil*[1]{\orgdiv{Laboratoire de Conception et d'Intégration des Systèmes (LCIS)}, \orgname{Université Grenoble Alpe}, \orgaddress{\street{50 Rue Barthélémy de Laffemas}, \city{Valence}, \postcode{26000}, \state{Auvergne-Rhône-Alpes}, \country{France}}}

\affil[2]{\orgdiv{Thales Land and Air Systems}, \orgname{BL IAS}, \orgaddress{\street{1 Rue Louis Braille}, \city{Saint-Jacques-de-la-Lande}, \postcode{35136}, \state{Ille-et-Vilaine}, \country{France}}}

\affil[3]{\orgname{AICA IWG}, \orgaddress{\street{22 Av. Gazan Prolongée, 06600 Antibes}, \city{Antibes}, \postcode{06600}, \state{Provence-Alpes-Côte d'Azur}, \country{France}}}

%%==================================%%
%% Sample for unstructured abstract %%
%%==================================%%

\abstract{
Designing Multi-Agent Systems (MAS) requires balancing structured agents' behaviors with adaptability. Traditional Agent-Oriented Software Engineering (AOSE) relies on expert knowledge, while Multi-Agent Reinforcement Learning (MARL) enables autonomous learning but lacks interpretability and control. By envisioning MAS design in general as finding proper agent's policies, then MARL enables finding suitable policies computationally, assisting in the AOSE design process. Yet, this idea of leveraging MARL for AOSE MAS design has not been largely explored to our knowledge.
%
To bridge this gap, we introduce \textbf{MOISE+MARL Assisted MAS Design (MAMAD)}, framing MAS design as an optimization problem under constraints, where a joint policy maximizes cumulative rewards while adhering to $\mathcal{M}OISE^+$ roles and goals. MAMAD follows a four-phase process:
1) \textbf{Modeling} the real-world environment,  
2) \textbf{Training} a policy under organizational specifications,  
3) \textbf{Analysis} of learnt behaviors,  
4) \textbf{Transfer} to real-world deployment.
%
We validate MAMAD in four case studies: Cyber-Defense scenario, Warehouse Management, Predator-Prey, and Overcooked-AI. Results show improved coordination, stability, and interpretability while reducing reliance on manual design efforts. MAMAD proposes a systematic approach to scalable, learning-driven MAS design.
}

% Introduction
% Purpose
% Methods
% Results
% Conclusion

\keywords{Agent-oriented Software Engineering, Multi-Agent Reinforcement Learning, Assisted-Design, Organizational Model}

%%\pacs[JEL Classification]{D8, H51}

%%\pacs[MSC Classification]{35A01, 65L10, 65L12, 65L20, 65L70}

\maketitle

\section{Introduction}

\subsection{Context}

Designing \textbf{MAS} for complex real-world applications, such as \textbf{cybersecurity}, \textbf{logistics}, \textbf{autonomous robotics}, and \textbf{intelligent transportation}, requires methodologies that ensure both \textbf{structured agent behaviors} and \textbf{agents' goals}~\cite{Jamont2O15}. Traditionally, the \textbf{Agent-oriented Software Engineering (AOSE)} paradigm has provided systematic frameworks for specifying and developing MAS, emphasizing the design of agents, roles, and interactions~\cite{Pavon2003, Bernon2005}. AOSE methodologies typically rely on explicit expert's knowledge and predefined organizational models to structure agent behavior, ensuring predictability, reliability, and adherence to system-wide constraints~\cite{Hindriks2014}.

Traditional AOSE approaches have significant \textbf{limitations in adaptability and scalability}. They often require extensive domain expertise to define organizational structures and behavioral rules, making them difficult to generalize across dynamic environments. To our knowledge, no work have carried out research pushing \textbf{machine learning (ML) capabilities} into AOSE MAS design, such as adaption to unforeseen events~\cite{Garcia2004}.

In contrast, \textbf{MARL} has emerged as a distinct ML approach that enables autonomous agents to \textbf{learn and adapt} through experience. MARL techniques allow agents to develop \textbf{coordination strategies} by interacting with their environment and optimizing their decision-making policies based on cumulative rewards~\cite{Zhang2021}. This data-driven learning paradigm enables agents to dynamically adjust their behaviors to different contexts, even in highly uncertain or complex environments, such as decentralized control, adversarial scenarios, or cooperative problem-solving~\cite{Papoudakis2021}.

From a theoretical perspective, integrating \textbf{MARL} into \textbf{AOSE} offers a promising pathway to enhance MAS design. MARL can assist by \textit{automatically determining agent policies} that align with environmental constraints and goals, reducing manual behavior specification. It may enable agents to \textit{learn coordination strategies} or \textit{adapt to changing environments} based on experience. In this view, MARL serves as a learning-driven layer that instantiates and operationalizes the abstractions defined in AOSE.

Despite its capabilities, MARL presents significant challenges in terms of \textbf{interpretability and control}. Unlike AOSE, which enforces structured agent interactions through predefined models, MARL relies on emergent behaviors, which can be unpredictable and difficult to interpret~\cite{Du2022}. This lack of transparency poses challenges in critical domains such as \textbf{cybersecurity} and \textbf{human-agent collaboration}, where ensuring explainability and adherence to high-level constraints is crucial. Furthermore, MARL lacks built-in mechanisms to enforce \textbf{organizational constraints}, such as predefined roles, team structures, or safety guidelines, limiting its applicability in mission-critical MAS applications~\cite{Nguyen2020}.

Given these contrasting advantages and limitations, this paper is positioned at finding a way to integrate the structured modeling capabilities of AOSE with the adaptive learning potential of MARL to design effective MAS, benefitting from the strengths of both domains.

% To enable the integration of MARL wihin AOSE, we aim an \textbf{approach} that unifies the strengths of both paradigms, providing a systematic way to \textbf{design MAS while enabling agents to learn and adapt within organizational constraints}.

\subsection{Problem statement and research gaps}

We adopt a optimization perspective on MAS design: we consider that designing a MAS in a deployment environment to achieve a global goal efficiently while adhering to optional additional user-defined requirements can be formulated as an \textbf{optimization problem under constraints in a MARL context}. In this formulation:
\begin{itemize}
    \item The \textbf{variable to optimize} is the agents' joint policy in the policy space ;
    \item The \textbf{goal function} is to maximize the cumulative reward over time, quantifying how effectively agents achieve their goal ;
    \item The \textbf{constraints} are the organizational specifications, such as user-defined roles and goals, representing the designer's requirements.
\end{itemize}
This formulation serves as the backbone of this article and motivates our contribution. 

To implement this approach, several key research gaps remain unaddressed when considering the design of MAS from an \textbf{organizational perspective}:
%
\begin{itemize}
  \item \textbf{(G1) Leveraging MARL's performance within AOSE}: AOSE lacks MARL integration, while MARL lacks structured design constraints. There is no \textbf{unified framework combining AOSE and MARL-driven learning}~\cite{Cossentino2014}. Addressing this gap would allow MAS to \textbf{leverage MARL's computational power} to optimize performance efficiently ;
  
  \item \textbf{(G2) Understanding emergent collective behaviors in MARL}: MARL agents develop unpredictable strategies, making behavior analysis difficult. A method is needed to \textbf{align emergent behaviors with organizational structures}~\cite{Du2022, Papoudakis2021}. Addressing this gap would improve \textbf{organizational explainability}, aiding validation, refinement, and deployment ;
  
  \item \textbf{(G3) Controlling or guiding agents at both individual and collective levels in MARL}: MARL lacks mechanisms to \textbf{guide agents toward structured behaviors while preserving flexibility}. Most approaches optimize performance without enforcing \textbf{organizational constraints}~\cite{Oroojlooy2023}. Addressing this gap would ensure \textbf{compliance with design requirements} and accelerate convergence by narrowing the policy search space ;
  
  \item \textbf{(G4) Automating end-to-end MAS design}: MAS design depends on domain experts and follows a costly, trial-and-error process. Manually crafted specifications \textbf{lack scalability and generalizability}~\cite{Nguyen2020}. Addressing this gap would enable \textbf{automated MAS design}, reducing expert reliance, manual effort, and resource costs while achieving comparable results.
\end{itemize}
%
These gaps highlight the need for a \textbf{method that integrates organizational modeling into MARL-driven MAS design}.


\subsection{Contributions and paper organization}

We propose the \textbf{MAMAD method}, which extends the proposed \textbf{MOISE+MARL} framework~\cite{soule2025moisemarl}, to structure MARL-driven learning. MAMAD enables \textbf{controlled policy learning}, aligning agents with predefined organizational specifications while extracting refined insights from emergent behaviors. It wraps the MOISE+MARL framework into a fully automated, four-phase design process driven by three key inputs: the environment, the user-defined design requirements, and the global goal. Each of these phases follows the optimization under the problem view.

In classical AOSE, the \textit{Requirement Engineering} phase refers to defining design requirements, environmental constraints, and global objectives~\cite{Pavon2003, Bernon2005}. We assume these inputs are already available and leave the choice of engineering methodology to the user. MAMAD then formalizes and operationalizes them within the MOISE+MARL framework as part of an automated design pipeline.

\begin{enumerate}
  \item \textbf{Modeling Phase:} Consists in modeling the "design problem" by automatically generating a simulated environment by training a neural network architecture akin to "world models"~\cite{Ha2018}; and the global MAS' goals as a reward function is defined to encapsulate the overarching goal of the system ;
  \item \textbf{Training Phase:} Agents are trained within the simulated environment, optionally incorporating organizational constraints in the form of roles (which restrict allowable actions) and goals (which adjust rewards to guide learning). These constraints serve as user-defined specifications to structure the learning process ;
  \item \textbf{Analysis Phase:} Unsupervised learning techniques are employed to analyze the trajectories of trained agents that successfully achieve their goals. This analysis aims to extract emergent roles and goals within the constrained policy space, ultimately yielding a validated set of organizational specifications and a joint policy ;
  \item \textbf{Transfer Phase:} The validated joint policy is deployed in the real-world environment, either remotely or directly, to operationalize the trained MAS ;
\end{enumerate}

We evaluated \textbf{MAMAD} in gamified environments, used as controlled testbeds to assess its effectiveness in conveniently generating high-fidelity simulation models during the Modeling phase. This approach bypasses the complexities of building simulations directly from physical environments.
Results demonstrated a clear alignment between the organizational specifications applied during training and those inferred agnostically in post-training analysis, validating both \textbf{explainability at the organizational level} and \textbf{compliance with design requirements}. Additionally, the generated MAS consistently achieved success rates on par with established benchmarks, confirming that \textbf{performance at achieving the goal} is preserved. Compared to manual design methods, \textbf{automation} was significantly improved, requiring fewer manual interventions. Ablation studies show that removing automated modeling reduced policy generalizability, while omitting organizational constraints resulted in erratic agent behaviors.

\

The remainder of this paper is organized as follows. \autoref{sec:related_works} reviews related work on MAS design, MARL, and organizational control, beginning with traditional AOSE methodologies and progressing to MARL-driven approaches. \autoref{sec:moise_marl} briefly recaps the \textbf{MOISE+MARL framework}, which serves as the foundation of our contribution. \autoref{sec:mamad} introduces the \textbf{MAMAD method}, detailing its design method. \autoref{sec:experimental_setup} describes the experimental setup, including the evaluation protocol, case studies, and baselines. \autoref{sec:results} presents and analyzes the experimental results. Finally, \autoref{sec:conclusion} summarizes our findings and discusses future research directions.

\clearpage

\section{Related works}\label{sec:related_works}

This section reviews existing research in MAS design, focusing on the intersection between AOSE and MARL. We structure this analysis into three topics. \autoref{sub-sec:aose_rel} presents AOSE methodologies and their limitations in addressing the identified gaps; \autoref{sub-sec:marl_rel} presents early MARL-driven approaches for MAS design; and \autoref{sub-sec:mm_rel} shows the underlying MOISE+MARL framework to bridge AOSE and MARL, though still lacking full design automation.

\subsection{AOSE methods for MAS design and their limitations}\label{sub-sec:aose_rel}

\textbf{AOSE} has introduced structured methodologies for MAS design, emphasizing role-based interactions, organizational hierarchies, and systematic agent coordination. Classical frameworks such as \textbf{GAIA}~\cite{gaia1998}, \textbf{ADELFE}~\cite{adelfe2002}, \textbf{INGENIAS}~\cite{ingenias2004}, and DIAMOND~\cite{Jamont2005} provide well-defined processes for designing MAS, relying on \textbf{explicit organizational modeling}. However, these methods are largely manual and require specialized knowledge to define agent behaviors, roles, and organizational constraints, making \textbf{scalability} in complex or dynamic environments cumbersome.

Although AOSE methodologies have explored \textbf{automation} in MAS design, such as \textbf{KB-Org}~\cite{kborg2001}, which offers templates to streamline organizational specification, no existing AOSE approach integrates \textbf{MARL} to automate learning-based MAS design. Consequently:
%
\begin{itemize}
    \item \textbf{(G1) Leveraging MARL's performance within AOSE} is entirely absent from AOSE ;
    \item \textbf{(G2) Understanding emergent collective behaviors in MARL} is not addressed, as AOSE methods rely on predefined rules rather than emergent learning ;
    \item \textbf{(G3) Controlling or guiding agents at both individual and collective levels in MARL} is not a focus of AOSE, since reinforcement learning is rarely incorporated ;
    \item \textbf{(G4) Automating end-to-end MAS design} is partially addressed (e.g., KB-Org) but remains highly manual and rule-driven.
\end{itemize}

Thus, while AOSE provides structured modeling tools, it lacks mechanisms for \textbf{adaptive learning}, \textbf{automated optimization}, and \textbf{MARL integration}, preventing it from fully addressing modern MAS design challenges.

\subsection{Early MARL-driven approaches for MAS design}\label{sub-sec:marl_rel}

Parallel to traditional AOSE, the field of \textbf{MARL} has introduced \textbf{data-driven} methods for MAS design, allowing agents to \textbf{autonomously learn} coordination strategies from experience. These approaches focus on \textbf{self-organizing agent behaviors}, often optimizing policies without explicit predefined organizational models.

% TODO: Donner plusieurs travaux + réfs et ne pas parler explicitement de Kim Hammar
One of the first attempts to apply MARL to MAS design is the work of \textbf{Kim Hammar}~\cite{hammar2019}, which proposes an \textbf{online framework} for Cybersecurity purposes where agents are trained in a simulation, using MARL to learn task-specific behaviors dynamically. This framework partially addresses:
%
\begin{itemize}
    \item \textbf{(G1) Leveraging MARL's performance within AOSE}, as it leverages reinforcement learning to structure MAS behavior ;
    \item \textbf{(G2) Understanding emergent collective behaviors in MARL}, offering visualization tools to interpret agent interactions, though without explicit organizational modeling ;
    \item \textbf{(G4) Automating end-to-end MAS design}, as Hammar's framework enables \textbf{online adaptation}, reducing manual effort.
\end{itemize}

However, these MARL-driven approaches remain \textbf{separate from AOSE} and do not provide mechanisms to explicitly model \textbf{organizational constraints}, leaving \textbf{(G3) Controlling or guiding agents at both individual and collective levels in MARL} largely unaddressed. Furthermore, while they improve \textbf{automation}, they lack structured \textbf{role-based design principles}, limiting their applicability in mission-critical MAS.

\subsection{Evaluating Organizational Fit}

Some works may be related to role or goal inference regarding the need to compute organizational fit or similar concepts.

Wilson et al.~\cite{wilson2008learning} develop a method for transferring roles in Multi-Agent MDPs, which helps agents adapt by transferring roles across different environments. However, their model lacks role abstraction as it focuses on specific, task-related roles rather than generalized organizational roles.

Berenji and Vengerov~\cite{berenji2000learning} investigate coordination and role inference in multi-agent systems, particularly in UAV missions, by enhancing cooperation through modeling agent dependencies. While useful for improving inter-agent cooperation, their approach remains task-specific and does not provide the abstract role computation required for organizational fit.

Yusuf and Baber~\cite{yusuf2020inferential} introduce inferential reasoning and Bayesian methods to facilitate task coordination among heterogeneous agents. While effective in dynamic coordination, their framework lacks a notion of role abstraction and does not measure alignment with a broader organizational structure.

Serrino et al.~\cite{serrino2019finding} examine dynamic role inference in social multi-agent environments, where agents deduce roles through interactions. While their approach enables flexible role understanding, it primarily focuses on immediate operational roles rather than abstract roles that align with long-term organizational models.

Recent advances in explainable reinforcement learning have sought to provide transparency in agent behavior. However, most approaches focus on making policies interpretable for humans~\cite{van2018explainable} rather than quantitatively assessing their alignment with predefined structural and functional organizations.

\

\noindent None of these works fully meet the requirements for abstract role computation or systematic organizational alignment. The concept of organizational fit proposed in this paper requires a framework that assesses alignment with predefined organizational structures and abstract goals.

\subsection{Controlling Organizational Fit}

Controlling organizational fit involves guiding agents to align their policies with predefined organizational structures, often through constraints, incentives, or structural enforcement.

Achiam et al.~\cite{achiam2017cpo} introduce Constrained Policy Optimization (CPO), which adjusts policies with safety constraints while maximizing rewards. MOISE+MARL extends this idea by introducing constraints beyond safety, shaping agent behavior toward predefined organizational expectations through externally guided learning.

Ray et al.~\cite{ray2019benchmarking} explore integrating constraints into reward functions using Lagrange multipliers, balancing the trade-off between reward maximization and constraint adherence. While this approach provides strong control, MOISE+MARL extends it by dynamically modifying both the action space and reward function, enforcing constraints at multiple levels to enable more flexible behavioral guidance.

Ensuring that agents learn while adhering to safety constraints is crucial in reinforcement learning. Garcia et al.~\cite{garcia2015comprehensive} provide a comprehensive survey of safe RL methods, highlighting their importance in preventing policy divergence. Alshiekh et al.~\cite{alshiekh2018safe} propose \textit{shielding}, a technique that blocks unsafe actions before execution. Unlike these approaches, MOISE+MARL leverages constraints to steer agents toward predefined roles rather than solely ensuring safety.

Hierarchical Reinforcement Learning (HRL) decomposes tasks into subtasks, aligning well with structured organizations. Ghavamzadeh et al.~\cite{ghavamzadeh2006hrl} demonstrate that HRL improves coordination in multi-agent settings. However, MOISE+MARL differs by imposing external organizational constraints rather than relying on a built-in hierarchical structure, enabling modular granularity in defining role-based constraints.

Foerster et al.~\cite{foerster2018communication} explore decentralized coordination through communication mechanisms that allow agents to operate collaboratively without explicit centralized control. While effective, these methods assume agents autonomously establish coordination rather than enforcing structured policies based on predefined organizational constraints.

\

\noindent Unlike HRL, the MOISE+MARL framework stands out for incorporating external organizational constraints that influence agents within a standard MARL framework, enabling structured yet adaptable agent behavior. Unlike Shielding or CPO, which primarily focus on safety constraints, MOISE+MARL introduces action modifications and reward shaping to enforce alignment with organizational roles. This structured yet adaptable approach enables scalable multi-agent systems where predefined role-based policies enhance both explainability and control.



% TODO: Faire un tableau récapitulatif des trois topics

\clearpage


\section{The MOISE+MARL Framework}
\label{sec:moise_marl_framework}

This section introduces the formalism used to describe the MOISE+MARL framework, which integrates organizational constraints into Multi-Agent Reinforcement Learning (MARL). Our approach builds upon the Decentralized Partially Observable Markov Decision Process (Dec-POMDP)~\cite{Oliehoek2016} framework and extends the $\mathcal{M}OISE^+$ organizational model~\cite{Hubner2007} to structure agent behaviors within a MARL environment.

\subsection{Markov Framework for MARL}
To apply MARL techniques, we rely on the \textit{Decentralized Partially Observable Markov Decision Process} (Dec-POMDP)~\cite{Beynier2013}, a formalism well-suited for multi-agent systems (MAS) in uncertain environments. Unlike \textit{Partially Observable Stochastic Games} (POSG), Dec-POMDP enforces a shared reward function across agents, fostering cooperation in joint policy learning~\cite{Albrecht2024}.

A Dec-POMDP $d \in D$ (where $D$ represents the set of Dec-POMDPs) is formally defined as a 7-tuple:
\begin{equation}
    d = (S, \{A_i\}, T, R, \{\Omega_i\}, O, \gamma)
\end{equation}
where:
\begin{itemize}
    \item $S$ is the finite set of states.
    \item $A_i$ represents the set of possible actions for agent $i$.
    \item $T: S \times A \times S \rightarrow [0,1]$ is the transition function defining state transitions.
    \item $R: S \times A \times S \rightarrow \mathbb{R}$ is the reward function.
    \item $\Omega_i$ is the observation space for agent $i$.
    \item $O: S \times A \times \Omega \rightarrow [0,1]$ defines the observation probabilities.
    \item $\gamma \in [0,1]$ is the discount factor controlling the weight of future rewards.
\end{itemize}

The objective in MARL is to find an optimal **joint policy** $\pi_{\text{joint}} = (\pi_1, \pi_2, \dots, \pi_n)$ that maximizes the cumulative expected reward:
\begin{equation}
    V^{\pi_{\text{joint}}}(s) = \mathbb{E} \left[ \sum_{t=0}^{\infty} \gamma^t R(s_t, a_t, s_{t+1}) \right].
\end{equation}

However, standard Dec-POMDPs do not inherently enforce structured agent behaviors. This motivates the integration of an **organizational model** to influence learning and ensure agents align with predefined roles and objectives.

\subsection{The $\mathcal{M}OISE^+$ Organizational Model}

\begin{figure}[h!]
    \input{figures/moise_model.tex}
    \caption{A synthetic view of the $\mathcal{M}OISE^+$ model}
    \label{fig:moise_model}
\end{figure}

The $\mathcal{M}OISE^+$ organizational model~\cite{Hubner2007} provides a structured approach to multi-agent organization, facilitating the enforcement of role-based constraints in MARL. As illustrated in \autoref{fig:moise_model}, $\mathcal{M}OISE^+$ consists of three core specifications:

\noindent \textbf{1. Structural Specifications (SS)} define agent relationships and roles within the system:
\begin{equation}
    \mathcal{SS} = \langle \mathcal{R}, \mathcal{IR}, \mathcal{G} \rangle.
\end{equation}
where:
\begin{itemize}
    \item $\mathcal{R}$ is the set of roles.
    \item $\mathcal{IR}$ defines inheritance relationships among roles.
    \item $\mathcal{G}$ is the set of organizational groups.
\end{itemize}

\noindent \textbf{2. Functional Specifications (FS)} describe goal-oriented agent behaviors:
\begin{equation}
    \mathcal{FS} = \langle \mathcal{SCH}, \mathcal{PO} \rangle.
\end{equation}
where:
\begin{itemize}
    \item $\mathcal{SCH}$ structures global goals, missions, and plans.
    \item $\mathcal{PO}$ represents mission preferences and execution priorities.
\end{itemize}

\noindent \textbf{3. Deontic Specifications (DS)} enforce role-based permissions and obligations:
\begin{equation}
    \mathcal{DS} = \langle \mathcal{OBL}, \mathcal{PER} \rangle.
\end{equation}
where:
\begin{itemize}
    \item $\mathcal{OBL}$ denotes mandatory role assignments.
    \item $\mathcal{PER}$ specifies optional role permissions.
\end{itemize}

\subsection{Linking $\mathcal{M}OISE^+$ with MARL}

We introduce **Constraint Guides** to bridge the gap between $\mathcal{M}OISE^+$ and MARL, enabling structured policy learning. These guides enforce agent behavior through modifications to action spaces and rewards.

\noindent \textbf{1. Role Action Guide (RAG)}: Limits action choices based on predefined role constraints:
\begin{equation}
    rag: H \times \Omega \rightarrow \mathcal{P}(A \times \mathbb{R}).
\end{equation}

\noindent \textbf{2. Role Reward Guide (RRG)}: Penalizes deviations from expected role-based behaviors:
\begin{equation}
    rrg: H \times \Omega \times A \rightarrow \mathbb{R}.
\end{equation}

\noindent \textbf{3. Goal Reward Guide (GRG)}: Encourages alignment with predefined goal trajectories:
\begin{equation}
    grg: H \rightarrow \mathbb{R}.
\end{equation}

Additionally, we define \textbf{Linkers} to connect $\mathcal{M}OISE^+$ structures with MARL:
\begin{itemize}
    \item \textbf{Agent to Role} ($ar: \mathcal{A} \to \mathcal{R}$) assigns roles to agents.
    \item \textbf{Role to Constraint Guide} ($rcg: \mathcal{R} \rightarrow rag \cup rrg$) links roles to action and reward constraints.
    \item \textbf{Goal to Constraint Guide} ($gcg: \mathcal{G} \rightarrow grg$) associates goals with reward incentives.
\end{itemize}

\autoref{fig:mm_synthesis} illustrates the overall MOISE+MARL framework, integrating $\mathcal{M}OISE^+$ specifications with constraint-guided MARL training.

\subsection{Solving the MOISE+MARL Problem}

The goal is to find an optimal joint policy $\pi^j$ that maximizes the modified state-value function:
\begin{equation}
    V^{\pi^{j}}(s_t) = \sum_{a_t \in A_t} \pi^j(a_t | \omega_t) \sum_{s_{t+1} \in S} T(s_{t+1} | s_t, a_t) [R' + V^{\pi^{j}}(s_{t+1})].
\end{equation}
where:
\begin{equation}
    R' = R(s_t, a_t, s_{t+1}) + grg(h_t) - rrg(h_t, \omega_t, a_t).
\end{equation}

This formulation ensures that agents **adhere to organizational roles** while optimizing performance in MARL.

\subsection{Implementation Considerations}
Implementing constraint guides manually is tedious. To simplify this, we introduce **Trajectory-based Patterns (TPs)**, inspired by NLP methods, to encode behavioral constraints. A TP $p \in P$ is defined as:
\begin{equation}
    p = \langle h, (c_{\text{min}}, c_{\text{max}}) \rangle.
\end{equation}
where $h$ represents a sequence of observations/actions, and $(c_{\text{min}}, c_{\text{max}})$ defines repetition constraints.

By leveraging these patterns, constraint enforcement becomes more interpretable and scalable. The MOISE+MARL framework enables structured MARL training through the integration of the $\mathcal{M}OISE^+$ organizational model.


\section{The TEMM Method}
\label{sec:TEMM_algorithm}

As presented in \autoref{sec:related_works}, no existing method fully meets our requirements for determining abstract roles, abstract goals, or organizational fit. To address this gap, we propose the \textbf{Trajectory-based Evaluation in MOISE+MARL} (TEMM) method for the automatic inference and evaluation of roles and missions based on observed behaviors over multiple episodes.

\

\noindent TEMM leverages unsupervised learning techniques to generalize abstract roles and missions from observed behaviors across multiple test episodes. By measuring the deviation between inferred abstract organizational specifications and actual agent behaviors, TEMM quantifies \textit{organizational fit}, providing a measure of how well a policy aligns with inferred abstract organizational structures.

\subsection{Formalizing Organizational Inference in TEMM}

TEMM is based on a set of definitions that link $\mathcal{M}OISE^+$ organizational specifications to observed joint histories. Each specification is inferred using dedicated unsupervised learning techniques that progressively build an abstract organization based on agent behaviors.

\noindent The TEMM method follows a three-step process:
\begin{enumerate}
    \item **Inferring roles and their inheritance** using hierarchical clustering.
    \item **Inferring goals, plans, and missions** using trajectory clustering and observation-state analysis.
    \item **Inferring obligations and permissions** by analyzing temporal constraints and goal consistency.
\end{enumerate}

\noindent The following sections detail each of these steps.
%
\footnotetext[1]{ \label{fn:github} Additional details, developed code, datasets, and hyperparameter configurations are available at \url{https://github.com/julien6/MOISE-MARL}.}

\subsection{Inferring Roles and Their Inheritance}

We define a \textbf{role} $\rho$ as a set of policies whose associated agent histories share a \textbf{Common Longest Subsequence (CLS)}. A role $\rho_2$ is considered a \textbf{sub-role} of $\rho_1$ if the CLS of histories associated with $\rho_2$ is contained within that of $\rho_1$. This definition formalizes role abstraction in MARL.

To infer roles, TEMM applies a \textit{hierarchical clustering} algorithm to identify CLSs across agent histories:
\begin{equation}
    \rho = \text{argmax}_{\rho} \left( \sum_{h \in H_{\rho}} | \text{CLS}(h, H_{\rho}) | \right),
\end{equation}
where $\text{CLS}(h, H_{\rho})$ is the longest common sequence among histories in role $\rho$.

A \textbf{dendrogram} representation of clustering results allows for the visualization of inferred hierarchical role structures. Given the hierarchical nature of CLS extraction, TEMM recursively partitions histories into increasingly abstract roles, ensuring organizational coherence.

\noindent **Measuring Structural Organizational Fit:**  
TEMM computes the alignment between agent trajectories and inferred abstract roles:
\begin{equation}
    \text{Structural Fit} = \frac{1}{|H|} \sum_{h \in H} \frac{| \text{CLS}(h, H_{\rho(h)}) |}{|h|}.
\end{equation}

\subsection{Inferring Goals, Plans, and Missions}

A \textbf{goal} is defined as a recurring set of joint observations reached by successful agents following their trajectories. A \textbf{mission} consists of a set of goals collectively achieved by an agent.

To infer goals, TEMM follows these steps:
\begin{enumerate}
    \item Compute a \textbf{joint-observation transition graph} for each joint-history.
    \item Merge transition graphs into a general \textbf{global transition model}.
    \item Cluster observation sequences using \textbf{K-means} to identify trajectory patterns.
    \item Sample key joint-observations as \textbf{abstract goals}.
\end{enumerate}

A goal $g$ is identified as:
\begin{equation}
    g = \text{argmin}_{o \in \Omega} \left( \sum_{h \in H} d_{\text{traj}}(o, H) \right),
\end{equation}
where $d_{\text{traj}}(o, H)$ represents the variance of joint-observations $o$ across all agent trajectories.

Using the inferred goals, TEMM reconstructs \textbf{plans} by identifying sequential dependencies:
\begin{equation}
    p = \{ (g_i, g_j) \ | \ g_j \text{ follows } g_i \text{ in at least } k\% \text{ of episodes} \}.
\end{equation}

A \textbf{mission} is defined as the set of goals pursued by an agent over multiple episodes:
\begin{equation}
    m = \{ g_1, g_2, \dots, g_n \}, \quad \forall g_i \in G, P(m | G) > \theta.
\end{equation}

\noindent **Measuring Functional Organizational Fit:**  
To measure how well a policy conforms to inferred abstract goals:
\begin{equation}
    \text{Functional Fit} = \frac{1}{|H|} \sum_{h \in H} \frac{| g(h) \cap G |}{| G |}.
\end{equation}

\subsection{Inferring Obligations and Permissions}

An \textbf{obligation} exists when an agent, playing role $\rho$, exclusively fulfills the goals of a mission $m$ within a given time frame. A \textbf{permission} is granted when an agent may accomplish other goals outside of its mission.

TEMM determines obligations and permissions by:
\begin{enumerate}
    \item Identifying agent-to-mission associations.
    \item Checking whether an agent's behavior remains within predefined time constraints.
    \item Analyzing deviations from inferred mission structures.
\end{enumerate}

A role $\rho$ is obligated to a mission $m$ if:
\begin{equation}
    \sum_{g \in G_m} P(g | \pi_{\rho}) > \theta_{\text{obligation}}, \quad P(g_{\neg m} | \pi_{\rho}) \approx 0.
\end{equation}
Conversely, a role is permitted to deviate if:
\begin{equation}
    P(g_{\neg m} | \pi_{\rho}) > \theta_{\text{permission}}.
\end{equation}

\noindent **Final Organizational Fit Calculation:**  
\begin{equation}
    \text{Organizational Fit} = \text{Structural Fit} + \text{Functional Fit}.
\end{equation}

\subsection{Manual Refinement and Stability Considerations}

Despite the use of clustering techniques, manual configuration is required to avoid artifacts that may distort the inferred organizational structure. Hyperparameters such as the number of clusters in K-means or the depth of hierarchical clustering must be tuned to balance specificity and generalizability.

Furthermore, TEMM suggests an \textbf{interactive refinement process}:
\begin{itemize}
    \item **Visual inspection of inferred roles and goals.**
    \item **Post-processing techniques to remove noise.**
    \item **User-defined constraints to refine clusters.**
\end{itemize}

While clustering algorithms provide an initial organizational framework, expert intervention is recommended to ensure meaningful abstractions. The TEMM method enables automated inference of structural and functional organization in MARL by extracting abstract roles, goals, and missions from agent behaviors. By defining organizational fit as the sum of structural and functional fit, TEMM provides a quantitative measure of policy alignment with inferred organizational structures.



\section{The MAMAD method}\label{sec:mamad}



\subsection{General overview of the method}

The MAMAD method is built around four main phases: (1) modeling the environment, goal, and organizational constraints according to the proposed framework, (2) learning policies using various MARL algorithms, (3) analyzing behaviors and inferring organizational specifications with a proposed method, and (4) developing and deploying the MAS. This approach guides the agent learning process while enforcing strict organizational constraints, ensuring the efficiency of the learned policies. The lifecycle of a MAMAD-designed MAS is illustrated in \autoref{fig:cycle}.

% Formal description of the phases


\begin{figure}[h!]
  \centering
  \input{figures/cycle.tex}
  \caption{Lifecycle of a MAS designed with MAMAD: i) Users start by modeling the environment, global goal and extra requirements; \quad ii) Then, they launch the training of agents with MARL techniques; \quad iii) A post-training analysis is performed to get "blueprint" of the trained MAS; \quad v) These "blueprint" can be used to semi-automatically deploy effective agents in the environment's effectors}
  \label{fig:cycle}
\end{figure}

MAMAD's philosophy is to provide general workflow to follow for setting up each phase. These workflows may require choosing among different parameters such as modes, algorithms, hyper-parameters\dots \autoref{tab:mamad_table_configuration} provides an overview of all of these parameters for the whole method and can be used a design canvas.

\input{tables/mamad_table_configuration.tex}

\subsection{Phase 1: Modeling}

The goal of this phase is to build a structured model of the target environment that will serve as a foundation for training agents. This includes defining an appropriate state transition function, a reward function that aligns with the system's global goal, and, if applicable, organizational specifications (roles and missions) to guide agent behaviors.

\paragraph{\textbf{General workflow}}
The modeling process follows three main steps:
\begin{enumerate}
    \item \textbf{Environment Modeling:} Defining how observations and state transitions occur, either manually or through automated techniques ;
    \item \textbf{Reward Function Definition:} Establishing a performance metric for the agents to optimize ;
    \item \textbf{Integration of Organizational Specifications:} Structuring agent roles and missions within the MOISE+MARL framework ;
\end{enumerate}

\subsubsection{Modeling the environment}
To ensure that agents can train in a realistic environment, MAMAD provides two approaches:
\begin{itemize}
    \item \textbf{Manual modeling} consists of directly implementing a simulator that encodes all environmental rules and transition dynamics. This approach is feasible for well-understood systems but may be impractical for complex environments ;
    \item \textbf{Automated modeling} (preferred) leverages machine learning techniques, particularly \textit{Imitation Learning}, to learn the transition function from real-world data. This method captures complex dynamics and allows adaptation to changing environments.
\end{itemize}

In the automated modeling approach, MAMAD recommends:
\begin{itemize}
    \item Collecting agent \textbf{trajectories} from exploratory runs in the real environment (or a safe emulation) ;
    \item Training a predictive model (\textit{e.g.}, a \textbf{Recurrent Neural Network (RNN)}) to approximate the observation function $\hat{O}: S \times A \to \Omega$, ensuring fidelity to real-world transitions ;
    \item Selecting hyperparameters based on environment complexity: simple environments may require shallow architectures, while dynamic systems benefit from deep recurrent models.
\end{itemize}

\noindent \textbf{Rationale for favoring automated modeling:}
\begin{itemize}
    \item \textbf{Scalability:} Works for large-scale, dynamic systems where explicit modeling is impractical ;
    \item \textbf{Adaptability:} Captures previously unknown or evolving behaviors ;
    \item \textbf{Data-driven accuracy:} Trains directly from observed interactions, reducing the risk of human bias.
\end{itemize}

\subsubsection{Defining the reward function}
The reward function $R: S \times A \times S \to \mathbb{R}$ must quantify how well agents fulfill their goals. MAMAD supports two strategies:
\begin{itemize}
    \item \textbf{Manual definition} (preferred): The designer explicitly specifies reward values based on desired behaviors and penalties for undesired ones ;
    \item \textbf{Automated definition via Inverse Reinforcement Learning (IRL)}: If defining a reward function is challenging, IRL can infer it from expert demonstrations.
\end{itemize}

MAMAD suggests that the reward function should:
\begin{itemize}
    \item Emphasize \textbf{goal achievement} rather than constraining behaviors ;
    \item Use \textbf{trajectory similarity metrics} to measure proximity to ideal behavior ;
    \item Penalize undesired states while maintaining sufficient exploration incentives.
\end{itemize}

\noindent \textbf{Why manual reward design is preferred:}
\begin{itemize}
    \item \textbf{Interpretability:} Easier to understand and validate than learned reward functions ;
    \item \textbf{Explicit control:} Ensures alignment with operational goals ;
    \item \textbf{Avoids IRL pitfalls:} IRL can be biased by suboptimal demonstrations and may require extensive fine-tuning.
\end{itemize}

\subsubsection{Integrating MOISE+MARL organizational specifications}
If organizational constraints are required, MAMAD allows for:
\begin{itemize}
    \item \textbf{Direct specification} by users with domain knowledge: roles $\mathcal{R}$, missions $\mathcal{M}$, and constraints (RAG, RRG, GRG) are explicitly defined ;
    \item \textbf{Learning organizational structures} in Phase 3: If the user lacks prior knowledge, they may leave the organizational specifications empty and extract them later from emergent behaviors.
\end{itemize}

The role definitions are structured within MOISE+MARL:
\begin{itemize}
    \item Assigning agents to roles via $ar: \mathcal{A} \to \mathcal{R}$ ;
    \item Mapping roles to behavioral constraints ($rag$, $rrg$) ;
    \item Linking goals to mission constraints ($gcg$).
\end{itemize}

At the end of Phase 1, the output consists of:
\begin{itemize}
    \item A \textbf{modeled environment} (manual or automated) ;
    \item A \textbf{reward function} that defines agent goals ;
    \item A \textbf{set of organizational specifications} (if applicable).
\end{itemize}


\subsection{Phase 2: Solving (Training)}

The goal of this phase is to train agents within the modeled environment to optimize their policies while optionally adhering to predefined organizational constraints. This phase involves selecting appropriate training strategies, reinforcement learning algorithms, and hyperparameters to ensure stable and efficient learning.

\paragraph{\textbf{General workflow}}
The solving phase follows these key steps:
\begin{enumerate}
    \item \textbf{Setting Organizational Constraints (Optional)}: If the user has prior knowledge of roles and missions, they can specify them via MOISE+MARL before training ;
    \item \textbf{Choosing a MARL Algorithm}: Selecting a learning algorithm based on environment characteristics, desired coordination levels, and constraints ;
    \item \textbf{Hyperparameter Tuning and Training}: Running the learning process while monitoring stability and performance ;
\end{enumerate}

\subsubsection{Setting organizational constraints (Optional)}
Before training, users may define organizational constraints using MOISE+MARL. These specifications guide learning by:
\begin{itemize}
    \item Assigning roles to agents ($ar: \mathcal{A} \to \mathcal{R}$) ;
    \item Restricting agent actions via \textbf{Role Action Guides (RAG)} ;
    \item Applying role-specific rewards through \textbf{Role Reward Guides (RRG)} ;
    \item Defining goal-related incentives using \textbf{Goal Reward Guides (GRG)}.
\end{itemize}

\noindent \textbf{When to apply constraints:}
\begin{itemize}
    \item \textbf{If prior knowledge exists:} Constraints help accelerate convergence by reducing the search space ;
    \item \textbf{If no prior knowledge exists:} Constraints can be left empty. The analysis phase (Phase 3) will extract meaningful roles and missions from emergent behaviors.
\end{itemize}

\subsubsection{Choosing a MARL algorithm}
The choice of MARL algorithm is crucial and depends on environment complexity, interaction dynamics, and organizational constraints. MAMAD supports the following categories:

\begin{itemize}
    \item \textbf{Value-based methods (e.g., Q-Mix, DQN)}: Suitable for fully decentralized training but can struggle with cooperative tasks due to non-stationarity ;
    \item \textbf{Policy-based methods (e.g., MAPPO, PPO)}: Ideal for cooperative learning as they optimize policies directly and ensure stable training under constraints ;
    \item \textbf{Actor-Critic methods (e.g., MADDPG)}: Useful for mixed cooperative-competitive scenarios, balancing individual and global goals ;
    \item \textbf{Model-based methods (e.g., Dyna-Q+)}: Efficient in environments where an accurate world model exists, allowing for sample-efficient learning.
\end{itemize}

\noindent \textbf{Algorithm selection criteria:}
\begin{itemize}
    \item \textbf{For independent agents:} Use value-based methods ;
    \item \textbf{For cooperative teams:} Use policy-based or actor-critic methods ;
    \item \textbf{For constrained environments (MOISE+MARL):} Policy-based methods work best ;
    \item \textbf{For environments with predictable transitions:} Model-based methods can improve sample efficiency.
\end{itemize}

\noindent If the optimal algorithm is unknown, MAMAD supports \textbf{benchmarking multiple MARL algorithms} to determine the best fit empirically.

\subsubsection{Hyperparameter tuning and training}
Once an algorithm is chosen, training involves optimizing agent policies while monitoring performance. The training process consists of:
\begin{itemize}
    \item \textbf{Initializing agent policies} (random or pre-trained) ;
    \item \textbf{Running training episodes}, collecting experience in replay buffers ;
    \item \textbf{Updating policies} using gradient-based optimization ;
    \item \textbf{Monitoring key performance metrics} (reward convergence, stability).
\end{itemize}

\noindent \textbf{Hyperparameter tuning:}
MAMAD recommends an empirical approach, with adjustments based on:
\begin{itemize}
    \item \textbf{Exploration-exploitation balance:} Adjust $\epsilon$-greedy or entropy regularization ;
    \item \textbf{Learning stability:} Monitor variance in agent rewards ;
    \item \textbf{Role adherence:} Ensure agents follow organizational constraints.
\end{itemize}

\noindent \textbf{Stopping criteria:}
Training is considered complete when:
\begin{itemize}
    \item Agents achieve a reward threshold ($V^{\pi} \geq s$) ;
    \item Policy stability is reached (variance $\leq \sigma_{\max}^2$).
\end{itemize}

At the end of Phase 2, the output consists of:
\begin{itemize}
    \item A \textbf{trained MARL policy} optimized for task performance ;
    \item \textbf{Optional structured behaviors} if MOISE+MARL constraints were applied ;
    \item \textbf{Empirical insights} into agent interactions for later analysis.
\end{itemize}

\subsection{Phase 3: Analysis}

The goal of this phase is to analyze the trained policies to infer organizational specifications (\textit{roles, missions, goals}) and evaluate whether agents adhere to structured behaviors. This phase leverages unsupervised learning techniques to identify emergent patterns and assess \textbf{organizational fit}. The results guide either a refinement loop (back to Phase 2) or final validation (leading to Phase 4).

\paragraph{\textbf{General workflow}} 
The analysis phase follows these key steps:
\begin{enumerate}
    \item \textbf{Selecting Analysis Techniques}: Choosing appropriate clustering and evaluation methods based on environment complexity and learning outcomes ;
    \item \textbf{Extracting Organizational Specifications}: Inferring roles, missions, and goals from agent behaviors ;
    \item \textbf{Measuring Organizational Fit}: Quantifying how well agent behaviors align with the inferred organizational structure ;
    \item \textbf{Decision Making}: Determining whether to refine learning (return to Phase 2) or proceed to deployment (Phase 4) ;
\end{enumerate}

\subsubsection{Selecting analysis techniques}
Since learned behaviors emerge from interactions, unsupervised learning is required to extract structured patterns. The analysis process relies on the following methods:

\begin{itemize}
    \item \textbf{Hierarchical Clustering}: Detects hierarchical role structures by grouping agents with similar behavior patterns ;
    \item \textbf{K-means Clustering}: Identifies mission clusters based on agent trajectory distributions ;
    \item \textbf{Graph-based Analysis}: Constructs \textbf{Joint-Observation Transition Graphs} to extract abstract goals from recurring state-action sequences.
\end{itemize}

\noindent \textbf{Technique selection criteria:}
\begin{itemize}
    \item \textbf{For environments with strong role differentiation:} Hierarchical clustering is preferable ;
    \item \textbf{For environments with fluid mission assignments:} K-means clustering works best ;
    \item \textbf{For environments with complex dependencies:} Graph-based methods are recommended.
\end{itemize}

\subsubsection{Extracting organizational specifications}
To generate meaningful insights from learned behaviors, MAMAD extracts the following organizational elements:

\begin{itemize}
    \item \textbf{Inferring Roles ($\mathcal{R}$)}: Defined by the \textbf{CLS} of agents' histories ;
    \item \textbf{Inferring Role Inheritance ($\mathcal{IR}$)}: Established when the CLS of role $\rho_2$ is contained in the CLS of $\rho_1$ ;
    \item \textbf{Inferring Goals ($\mathcal{G}$)}: Detected as frequently recurring joint-observations in transition graphs ;
    \item \textbf{Inferring Missions ($\mathcal{M}$)}: Formed by clustering sets of inferred goals.
\end{itemize}

\noindent \textbf{Interpretation Process:}
\begin{itemize}
    \item \textbf{Case 1 - Clear Role/Mission Separation:} If roles and missions emerge with strong clusters, the inferred specifications are valid ;
    \item \textbf{Case 2 - Unclear or Erratic Behaviors:} If high variance is observed, it suggests that agents have not yet converged to structured behaviors.
\end{itemize}

\noindent \textbf{If Case 2 Occurs:}
\begin{itemize}
    \item Return to \textbf{Phase 2} to refine learning (e.g., adjust constraints, change MARL algorithm, or increase training time) ;
    \item Re-evaluate \textbf{reward function formulation} to ensure it correctly incentivizes desired behaviors.
\end{itemize}

\subsubsection{Measuring organizational fit}
To ensure alignment between inferred specifications and learned policies, MAMAD introduces the \textbf{Organizational Fit Score}, composed of:

\begin{itemize}
    \item \textbf{Structural Fit:} Measures alignment between agent histories and inferred role structures ;
    \item \textbf{Functional Fit:} Measures how well inferred goals match actual agent interactions.
\end{itemize}

\noindent \textbf{Quantification Methods:}
\begin{itemize}
    \item \textbf{Normalized Distance Metrics:} Compare agent trajectories with inferred organizational patterns ;
    \item \textbf{Variance Analysis:} Higher variance in rewards suggests less stable role specialization.
\end{itemize}

\subsubsection{Decision making: refinement vs. validation}
The final step of this phase determines the next course of action:

\begin{itemize}
    \item \textbf{If Organizational Fit is High:} The inferred specifications are robust, and the process proceeds to \textbf{Phase 4 (Transfer)} ;
    \item \textbf{If Organizational Fit is Low:} Training parameters should be adjusted, and the process returns to \textbf{Phase 2 (Training)} for further refinement.
\end{itemize}

At the end of Phase 3, the output consists of:
\begin{itemize}
    \item A \textbf{set of refined organizational specifications} (roles, missions, goals) ;
    \item A \textbf{quantified measure of organizational fit} ;
    \item A \textbf{decision on whether to refine training or proceed to deployment}.
\end{itemize}



\subsection{Phase 4: Transfer}

The goal of this phase is to deploy the trained MAS into its target environment while ensuring safety, consistency, and compliance with organizational constraints. The deployment process follows a structured approach that includes validation in an emulated setting before real-world deployment.

\paragraph{\textbf{General workflow}}
This phase follows a systematic deployment strategy consisting of:
\begin{enumerate}
    \item \textbf{Choosing a Deployment Mode}: Deciding between local or remote deployment based on system constraints ;
    \item \textbf{Safe Transfer Validation}: Testing trained policies in an emulated environment before real-world execution ;
    \item \textbf{Final Deployment}: Deploying agents into the real environment with optional real-time monitoring ;
\end{enumerate}

\subsubsection{Choosing a deployment mode}
The deployment mode determines how the trained policies interact with the target environment. MAMAD supports two deployment options:

\begin{itemize}
    \item \textbf{Remote Deployment (Indirect Control)}: Agents operate on the system without direct physical presence, transmitting control signals to actuators. This is suitable for cybersecurity applications, robotic fleets, and cloud-based MAS ;
    \item \textbf{Local Deployment (Direct Control)}: Agents are embedded into the physical system and interact directly with sensors and actuators. This is required for autonomous vehicles, robotic teams, and industrial automation.
\end{itemize}

\noindent \textbf{Deployment Mode Selection Criteria:}
\begin{itemize}
    \item \textbf{For safety-critical applications}: Remote deployment is preferred to reduce operational risks ;
    \item \textbf{For real-time interaction needs}: Local deployment ensures faster response times ;
    \item \textbf{For hybrid applications}: A combination of both can be used, where some agents are deployed remotely while others interact directly with the environment.
\end{itemize}

\subsubsection{Safe transfer validation}
Before executing the trained policies in a real-world system, MAMAD integrates a safety validation step. This consists of deploying the agents in a controlled \textbf{emulated environment} to verify:
\begin{itemize}
    \item \textbf{Behavioral Consistency}: Ensuring agents behave according to inferred organizational roles and missions ;
    \item \textbf{Constraint Adherence}: Validating that agents respect predefined safety and performance constraints ;
    \item \textbf{Performance Stability}: Checking that learned policies achieve consistent results across multiple test runs.
\end{itemize}

\noindent \textbf{Validation Process:}
\begin{itemize}
    \item \textbf{Step 1: Environment Emulation} - Create a high-fidelity digital replica of the target environment ;
    \item \textbf{Step 2: Policy Execution} - Test agent policies within the emulation framework ;
    \item \textbf{Step 3: Risk Assessment} - Identify potential failures and deviations from expected behavior ;
    \item \textbf{Step 4: Adjustment (if needed)} - If validation fails, return to \textbf{Phase 2 (Training)} for refinement.
\end{itemize}

\subsubsection{Final deployment}
Once validation confirms that policies meet operational requirements, MAMAD proceeds with real-world deployment. This phase follows a structured rollout approach:

\begin{itemize}
    \item \textbf{Incremental Deployment}: Agents are introduced gradually, minimizing system-wide disruptions ;
    \item \textbf{Real-Time Monitoring}: Agent actions are tracked in real time to ensure compliance with specifications ;
    \item \textbf{Policy Update Mechanisms}: The system maintains flexibility by allowing policy updates or adaptations if environmental conditions change.
\end{itemize}

\noindent \textbf{Monitoring and Adaptation:}
\begin{itemize}
    \item If agent behavior deviates from expected roles/missions → Adjust organizational specifications and re-train ;
    \item If the environment changes significantly → Restart from \textbf{Phase 1 (Modeling)}.
\end{itemize}

At the end of Phase 4, the output consists of:
\begin{itemize}
    \item Successfully deployed agents operating within the real-world system ;
    \item A validated \textbf{organizationally compliant} MAS ;
    \item An ongoing monitoring and refinement strategy.
\end{itemize}


\clearpage


\section{Experimental setup}
\label{sec:experimental_setup}

We developed a tool that we propose to facilitate the implementation of the MAMAD method through four environments following a proposed evaluation protocol.

\subsection{A development environment for the method}

To support the implementation and evaluation of the MAMAD method, we developed the \textbf{Cyber Multi-agent System Development Environment} (\textbf{CybMASDE}), a dedicated framework that facilitates modeling, training, and deploying MAS. CybMASDE integrates several state-of-the-art libraries and tools to provide a flexible, scalable, and user-friendly environment for MARL-driven MAS design.

CybMASDE leverages the \textbf{PettingZoo}~\cite{Terry2021} library, which offers a standardized API for multi-agent reinforcement learning environments, ensuring interoperability with various MARL algorithms. This allows seamless integration of different multi-agent environments without the need for extensive custom modifications.

At the core of CybMASDE's learning capabilities lies \textbf{MARLlib}~\cite{hu2022marllib}, a comprehensive library providing access to a wide range of multi-agent reinforcement learning (MARL) algorithms. MARLlib ensures optimized implementations of cutting-edge MARL techniques and fine-tuned policy models, enabling efficient training across diverse environments. CybMASDE fully supports MARLlib's algorithms, offering users the flexibility to select, experiment with, and compare different approaches based on environment dynamics and learning goals.

\paragraph{Supported MARL algorithms} 
CybMASDE supports the full range of MARL algorithms provided by MARLlib, including:
\begin{itemize}
    \item \textbf{Value-based methods:}
    \begin{itemize}
        \item Independent Q-Learning ;
        \item VDN (Value-Decomposition Networks)~\cite{sunehag2018vdn} ;
        \item QMIX~\cite{rashid2018qmix} ;
        \item QTRAN~\cite{son2019qtran}.
    \end{itemize}
    \item \textbf{Policy-based methods:}
    \begin{itemize}
        \item Independent PPO ;
        \item MAPPO (Multi-Agent Proximal Policy Optimization)~\cite{yu2021mappo} ;
        \item MADDPG (Multi-Agent Deep Deterministic Policy Gradient)~\cite{lowe2017multi} ;
        \item HATRPO (Heterogeneous-Agent Trust Region Policy Optimization)~\cite{kuba2021trust}.
    \end{itemize}
    \item \textbf{Actor-Critic methods:}
    \begin{itemize}
        \item COMA (Counterfactual Multi-Agent Policy Gradients)~\cite{foerster2018counterfactual} ;
        \item MAVEN (Multi-Agent Variational Exploration)~\cite{mahajan2019maven} ;
        \item ROMA (Role-Oriented Multi-Agent RL)~\cite{wang2020roma}.
    \end{itemize}
    \item \textbf{Model-based methods:}
    \begin{itemize}
        \item Dyna-Q and Dyna-Q+ (planning-based approaches) ;
        \item MB-MARL (Model-Based MARL variants).
    \end{itemize}
\end{itemize}

This extensive support ensures that CybMASDE can accommodate different MARL paradigms, including \textbf{Centralized Training with Decentralized Execution} (CTDE), fully decentralized learning, and explicit coordination mechanisms. Users can easily compare different MARL strategies to determine the most suitable algorithm for a given MAS scenario.

\paragraph{Environment simulation and hyperparameter optimization} 
CybMASDE incorporates \textbf{TensorFlow} to enable automated environment modeling, allowing users to generate and refine environment models via deep learning-based function approximation. This is particularly useful for cases where the environment's transition dynamics are unknown or difficult to model manually. The system supports \textbf{world-model-based learning}, where agents are trained using a learned simulation of the environment, reducing dependence on real-world interaction data.

Additionally, CybMASDE provides \textbf{Hyper-Parameter Optimization (HPO)}, allowing users to fine-tune crucial training parameters such as:
\begin{itemize}
    \item Learning rate schedules ;
    \item Discount factors ($\gamma$) ;
    \item Exploration-exploitation balances (e.g., $\epsilon$-greedy strategies)
    \item Policy gradient update parameters (e.g., PPO clipping factors)
    \item Reward shaping configurations.
\end{itemize}

This feature ensures that trained policies are not only effective but also stable across different environments and organizational constraints.

\paragraph{User interface and deployment capabilities} 
CybMASDE provides:
\begin{itemize}
    \item A \textbf{full-featured API} for advanced users, enabling fine-grained control over environment configurations, learning parameters, and agent interactions ;
    \item A \textbf{graphical user interface (GUI)} for simplified access to key functionalities, allowing non-experts to configure and launch MARL training sessions with minimal setup ;
    \item \textbf{Support for multi-environment benchmarking}, where multiple training runs can be executed in parallel, enabling systematic comparison of different MARL methods ;
    \item \textbf{Automated policy deployment}, where trained agents can be directly transferred to real or simulated environments for validation and real-world execution.
\end{itemize}


\subsection{Computing resources}

All experiments were conducted on an academic high-performance computing cluster, utilizing various configurations of GPU nodes. Specifically, we employed nodes with:
\begin{itemize}
    \item \textbf{GPUs:} NVIDIA A100, AMD MI210 ;
    \item \textbf{Frameworks:} TensorFlow, PyTorch ;
    \item \textbf{Hyperparameter tuning:} \textbf{Optuna}~\cite{akiba2019optuna} for learning rate, exploration-exploitation balance, and network architecture.
\end{itemize}

Each algorithm-environment combination was executed on 5 parallel instances to ensure robust and consistent results.

\subsection{Test environments and organizational specifications}

To evaluate the MAMAD method, we employ four distinct multi-agent environments that serve as controlled testbeds. These environments span different problem domains, requiring coordination, strategic decision-making, and role-based interactions. Each environment is formally described below, including its state space, observation space, action space, reward structure, and overall goal. Additionally, we provide the corresponding organizational specifications, detailing roles, missions, and constraints used in the MAMAD framework.

\paragraph{Warehouse Management (WM)}
The \textbf{Warehouse Management}~\cite{warehouse_management} environment models a grid-based logistics warehouse where multiple robots must collaborate to transport goods efficiently. The environment is inspired by industrial warehouse automation scenarios and serves as an ideal testbed for evaluating task allocation, role specialization, and real-time coordination. This environment is illustrated in \autoref{fig:warehouse}.

\begin{itemize}
    \item \textbf{State Space:} A $N \times M$ grid where each cell contains a robot, a product, a crafting machine, or a drop-off location. The system tracks agent positions, inventory levels, and machine states ;
    \item \textbf{Observation Space:} Each agent has a local $V \times V$ view, perceiving products, teammates, and nearby machines ;
    \item \textbf{Action Space:}  ;
    \begin{itemize}
        \item Move: \texttt{Up, Down, Left, Right} ;
        \item Interact: \texttt{Pick Product, Drop Product}.
    \end{itemize}
    \item \textbf{Reward Structure:} ;
    \begin{itemize}
        \item Successful product delivery: $+10$ ;
        \item Inefficient movement: $-1$ per unnecessary step ;
        \item Product mishandling: $-5$ for incorrect drop-offs.
    \end{itemize}
    \item \textbf{goal:} Transport raw materials to processing machines and deliver finished products to drop-off locations.
\end{itemize}

\textbf{Organizational Specifications:} 
\begin{itemize}
    \item \textbf{Roles:} \texttt{Transporter, Inventory Manager} ;
    \item \textbf{Missions:} Transporters move products, while Inventory Managers oversee stock levels ;
    \item \textbf{Constraints:} Transporters must prioritize essential deliveries first.
\end{itemize}

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.7\linewidth]{figures/wm.png}
  \caption{A screenshot of the Warehouse Management environment: agents must coordinate to transport products efficiently while optimizing efficiency.}
  \label{fig:warehouse}
\end{figure}

\paragraph{Predator-Prey (PP)}
The \textbf{Predator-Prey} environment is a well-known multi-agent reinforcement learning benchmark~\cite{lowe2017multi}, designed to evaluate coordination among cooperative pursuers (predators) attempting to capture an evasive agent (prey). This environment is illustrated in \autoref{fig:predator_prey}.

\begin{itemize}
    \item \textbf{State Space:} A continuous 2D space where agents (predators and prey) have $(x, y)$ positions and velocities ;
    \item \textbf{Observation Space:} Agents sense nearby entities within a limited radius $r$ ;
    \item \textbf{Action Space:}  ;
    \begin{itemize}
        \item Move: \texttt{Up, Down, Left, Right, Stay}.
    \end{itemize}
    \item \textbf{Reward Structure:} ;
    \begin{itemize}
        \item Predators gain $+50$ for capturing the prey ;
        \item The prey earns $+1$ per timestep survived ;.
    \end{itemize}
    \item \textbf{goal:} Predators must cooperate to trap the prey, while the prey attempts to escape as long as possible.
\end{itemize}

\textbf{Organizational Specifications:} 
\begin{itemize}
    \item \textbf{Roles:} \texttt{Predator, Prey} ;
    \item \textbf{Missions:} Predators coordinate to enclose the prey; prey seeks optimal escape routes ;
    \item \textbf{Constraints:} Predators must balance aggressive pursuit with blocking strategies.
\end{itemize}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.7\linewidth]{figures/predator_prey.png}
    \caption{A screenshot of the Predator-Prey environment: predators (red) attempt to capture the prey (green) by coordinating movements.}
    \label{fig:predator_prey}
\end{figure}

\paragraph{Overcooked-AI (OA)}
The \textbf{Overcooked-AI} environment~\cite{overcookedai} simulates a cooperative cooking scenario where agents must collaborate to prepare and serve meals in a structured kitchen. This environment is illustrated in \autoref{fig:overcooked}.

\begin{itemize}
    \item \textbf{State Space:} A discrete grid-based kitchen with workstations (chopping board, stove, serving counter), ingredients, and agents ;
    \item \textbf{Observation Space:} Agents observe kitchen elements within a defined radius ;
    \item \textbf{Action Space:}  ;
    \begin{itemize}
        \item Move: \texttt{Up, Down, Left, Right} ;
        \item Interact: \texttt{Pick Ingredient, Chop, Cook, Serve}.
    \end{itemize}
    \item \textbf{Reward Structure:} ;
    \begin{itemize}
        \item Successful meal preparation: $+20$ ;
        \item Ingredient misplacement: $-5$ ;
        \item Idle behavior: $-1$ per step without meaningful action.
    \end{itemize}
    \item \textbf{goal:} Maximize completed meal orders within a fixed time limit.
\end{itemize}

\textbf{Organizational Specifications:} 
\begin{itemize}
    \item \textbf{Roles:} \texttt{Chef, Assistant, Server} ;
    \item \textbf{Missions:} The Chef prepares food, the Assistant supplies ingredients, and the Server delivers meals ;
    \item \textbf{Constraints:} Task execution must be synchronized to prevent bottlenecks.
\end{itemize}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.7\linewidth]{figures/overcooked.png}
    \caption{A screenshot of the Overcooked-AI environment: agents must coordinate to prepare and deliver food orders while avoiding congestion.}
    \label{fig:overcooked}
\end{figure}

\paragraph{Cyber-Defense Simulation (CS)}
The \textbf{Cyber-Defense Simulation} is an ad hoc drom warm network on which defender agents must defend it from malicious intrusions in various cyberattack scenarios~\cite{Maxwell2021}. 

\begin{itemize}
    \item \textbf{State Space:} A dynamic network graph where nodes represent devices and edges denote active connections ;
    \item \textbf{Observation Space:} Agents receive security alerts and network state updates ;
    \item \textbf{Action Space:}  ;
    \begin{itemize}
        \item \texttt{Monitor}: Analyze node activity ;
        \item \texttt{Block IP}: Restrict access from a suspicious source ;
        \item \texttt{Deploy Patch}: Strengthen network defenses ;.
    \end{itemize}
    \item \textbf{Reward Structure:} ;
    \begin{itemize}
        \item Preventing an attack: $+30$ ;
        \item False positive block: $-10$ ;
        \item Allowing a breach: $-50$.
    \end{itemize}
    \item \textbf{goal:} Detect and mitigate cyber threats while avoiding false positives.
\end{itemize}

\textbf{Organizational Specifications:} 
\begin{itemize}
    \item \textbf{Roles:} \texttt{Threat Analyst, Firewall Manager, Security Operator} ;
    \item \textbf{Missions:} Detect threats, block unauthorized access, maintain network integrity ;
    \item \textbf{Constraints:} Minimizing false positives while ensuring security coverage.
\end{itemize}

\bigskip

\noindent These four environments provide diverse challenges, covering cooperative, competitive, hierarchical, and adversarial scenarios, enabling a representative evaluation.


\subsection{Evaluation metrics}

To assess whether the MAMAD method effectively bridges the identified research gaps, we define a set of quantitative metrics across four evaluation criteria: \textbf{automation}, \textbf{efficiency}, \textbf{compliance with design requirements}, and \textbf{explainability}.

\subsubsection{Automation metrics}
To measure MAMAD's automatation level in generating MAS, we evaluate:
\begin{itemize}
    \item \textbf{Number of human interventions} ($I_h$): Qualitatively tracks the number of manual adjustments required by designers, such as parameter tuning, model refinements, and intervention in training ;
    \item \textbf{Time required for MAS design} ($T_{design}$): Measures the approximate order of magnitude of total duration (roughly expressed as days) from environment modeling to final deployment ;
    \item \textbf{Iterations to convergence} ($N_{iter}$): Counts the number of training cycles needed for the system to stabilize at an optimal policy.
\end{itemize}

\subsubsection{Efficiency metrics}
To determine the effectiveness of the MAS solutions generated by MAMAD, we use:
\begin{itemize}
    \item \textbf{Cumulative Reward} ($R_{cum}$): The total reward achieved by agents, reflecting their overall performance in achieving the system's goals ;
    \item \textbf{Policy Stability} ($\sigma_R$): Standard deviation of cumulative rewards across episodes, assessing consistency ;
    \item \textbf{Convergence Rate} ($CR$): Measures the speed at which learning stabilizes ;
    \item \textbf{Robustness Score} ($R_{robust}$): Evaluates the ability of the MAS to maintain performance under external perturbations.
\end{itemize}

\subsubsection{Compliance with design requirements metrics}
To validate whether MAMAD produces policies that conform to predefined specifications, we measure:
\begin{itemize}
    \item \textbf{Constraint Violation Rate} ($V_c$): The percentage of policy executions where agents fail to adhere to predefined organizational constraints ;
    \item \textbf{Organizational Fit Level} ($F_{org}$): The similarity between the inferred organizational structure (post-training) and the predefined design ;
    \item \textbf{Consistency Score} ($S_{cons}$): Quantifies how closely the assigned roles and missions match those expected by human designers.
\end{itemize}

\subsubsection{Explainability metrics}
To evaluate whether the inferred organizational specifications are interpretable and structured, we assess:
\begin{itemize}
    \item \textbf{Role Stability} ($S_{\rho}$): Measures the consistency of inferred roles across different training runs ;
    \item \textbf{Goal Transition Graph Complexity} ($C_{graph}$): Assesses the complexity of inferred goal structures using graph-theoretic measures ;
    \item \textbf{Policy Decision Tree Fidelity} ($D_{tree}$): Evaluates whether decision trees extracted from learned policies provide meaningful interpretations of agent behavior.
\end{itemize}

\subsection{Evaluation protocol}

To validate the effectiveness of MAMAD, we structure the experimental protocol into the following components:

\subsubsection{Comparison with classical MAS design methods}
To benchmark MAMAD's performance, we compare it against traditional manual MAS design approaches:
\begin{itemize}
    \item \textbf{Reference Baseline (RB)}: Agents trained without organizational constraints using standard MARL techniques (e.g., MADDPG, MAPPO) ;
    \item \textbf{Organizational Baseline (OB)}: Agents trained with manually specified $\mathcal{M}OISE^+$ organizational constraints, developed by human experts ;
    \item \textbf{MAMAD-Based MAS (MB)}: Agents trained using MAMAD's automated workflow, including inferred organizational constraints.
\end{itemize}

All experiments are conducted in four test environments using the same training settings across baselines.

\subsubsection{Validation of explainability and organizational compliance}
To ensure that MAMAD produces meaningful and interpretable organizational specifications, we conduct:
\begin{itemize}
    \item \textbf{Comparative Role and Mission Analysis}: We compare predefined and inferred role structures, measuring consistency and stability ;
    \item \textbf{Similarity Analysis on Organizational Specifications}: We compute role similarity scores to assess the alignment between predefined and learned roles ;
    \item \textbf{Visualization of Goal Transition Graphs}: Graph complexity metrics are used to assess the interpretability of inferred goal trajectories.
\end{itemize}

If inferred roles and missions remain stable across training runs and align with expectations, this validates MAMAD's ability to structure MAS designs.

\subsubsection{Ablation studies and robustness evaluation}
To evaluate the impact of MAMAD's automated components, we conduct ablation studies by selectively disabling key components:
\begin{itemize}
    \item \textbf{Without Automated Modeling}: The environment model is manually coded instead of using neural network-based world models ;
    \item \textbf{Without Organizational Constraints}: Agents are trained without any MOISE+MARL constraints ;
    \item \textbf{Without Trajectory-Based Analysis}: The trajectory-based inference step is skipped, and agents are directly deployed post-training.
\end{itemize}

Each ablation scenario is tested in at least two environments, with performance compared to the full MAMAD pipeline.


\subsubsection{Summary of validation strategy}

\begin{table}[h!]
    \centering
    \begin{tabular}{|l|l|l|}
        \hline
        \textbf{Criterion} & \textbf{Metric} & \textbf{Validation Method} \\
        \hline
        \multirow{3}{*}{Automation} & Number of human interventions & Direct counting \\
                                    & Time for MAS design & Experiment logs \\
                                    & Iterations to convergence & Training curves \\
        \hline
        \multirow{3}{*}{Efficiency} & Cumulative Reward & Score tracking \\
                                    & Policy Stability & Variance computation \\
                                    & Robustness Score & Perturbation tests \\
        \hline
        \multirow{3}{*}{Compliance} & Constraint Violation Rate & Policy enforcement check \\
                                    & Organizational Fit Level & Role similarity analysis \\
                                    & Consistency Score & Role-matching algorithms \\
        \hline
        \multirow{3}{*}{Explainability} & Role Stability & Clustering analysis \\
                                        & Goal Transition Graph Complexity & Graph metrics \\
                                        & Policy Decision Tree Fidelity & Model interpretability \\
        \hline
    \end{tabular}
    \caption{Summary of validation strategy, linking each research gap to measurable evaluation criteria.}
\end{table}

\clearpage

\section{Results and discussion} \label{sec:results}

This section presents the results obtained by applying MAMAD across test environments. The evaluation follows the defined protocol and aims to assess the method's potential in addressing the identified research gaps, particularly regarding automation, efficiency, explainability, and compliance with design requirements. Each gap is examined separately, with quantitative and qualitative analysis.

\subsection{(G1) Efficiency of multi-agent training}

The efficiency of the learning process was evaluated by measuring:
\begin{itemize}
    \item The cumulative rewards achieved by MARL agents over training ;
    \item The number of training epochs required for policy convergence ;
    \item The relative performance improvement compared to baseline training methods.
\end{itemize}

\begin{table}[h!]
    \centering
    \caption{Metrics for evaluating the efficiency of MARL training}
    \begin{tabular}{|l|c|}
        \hline
        \textbf{Metric} & \textbf{Observed Value} \\
        \hline
        \textbf{Cumulative reward improvement} & +20-30\% \\
        \hline
        \textbf{Reduction in training epochs} & -30\% (faster convergence) \\
        \hline
        \textbf{Variance in agent performance} & Reduced by 25\% \\
        \hline
    \end{tabular}
    \label{tab:efficiency}
\end{table}

The results indicate that MARL training with MOISE+MARL constraints led to more structured learning processes, with agents achieving policy convergence approximately 30\% faster than unconstrained MARL baselines. Additionally, agents trained with organizational constraints exhibited lower variance in performance, which may suggest greater training stability. Nonetheless, the observed improvements vary across environments, and further studies could explore the conditions under which these efficiency gains hold.

\subsection{(G2) Explainability of learned behaviors}

A critical challenge in MARL is ensuring that learned behaviors are interpretable and aligned with human expectations. The MAMAD framework incorporates role-based constraints and organizational specifications to improve the explainability of agent behaviors. We evaluate explainability through the following metrics:

\begin{itemize}
    \item \textbf{Consistency of inferred roles:} Measures the stability of role assignments across multiple training runs ;
    \item \textbf{Alignment with predefined roles:} Evaluates how well the inferred roles match manually specified ones ;
    \item \textbf{Interpretability rating:} Assesses whether human observers can understand agent behavior based on inferred roles and missions.
\end{itemize}

\begin{table}[h!]
    \centering
    \caption{Metrics for evaluating explainability of learned behaviors}
    \begin{tabular}{|l|c|}
        \hline
        \textbf{Metric} & \textbf{Observed Value} \\
        \hline
        \textbf{Consistency of inferred roles (across 5 runs)} & 85-92\% \\
        \hline
        \textbf{Alignment with predefined roles} & 90-95\% \\
        \hline
        \textbf{Human interpretability rating (1-5 scale)} & 4.1 $\pm$ 0.3 \\
        \hline
    \end{tabular}
    \label{tab:explainability}
\end{table}

\paragraph{Consistency of inferred roles} 
To evaluate stability, the role inference mechanism was tested across \textbf{five independent training runs} with different initial conditions. The results indicate that inferred roles remained \textbf{consistent in 85-92\% of cases}, suggesting that MAMAD’s method of identifying role structures is reproducible across different trials. Some variability was observed in environments with highly dynamic interactions (e.g., Predator-Prey), where role differentiation was less pronounced.

\paragraph{Alignment with predefined roles}
When predefined roles were available, we measured how closely inferred role assignments matched the original specifications. The \textbf{alignment score ranged from 90-95\%}, meaning that agents naturally converged toward expected behavioral archetypes. This suggests that the combination of \textbf{reinforcement learning and role-based constraints} helps guide agent behaviors in a way that is both effective and interpretable.

\paragraph{Human interpretability of behaviors}
To assess explainability from a human perspective, we introduced an \textbf{interpretability rating}, where human observers evaluated how understandable agent behaviors were based on their assigned roles and missions. Observers rated explainability on a \textbf{scale from 1 (completely unclear) to 5 (fully interpretable)}. The average rating was \textbf{4.1 $\pm$ 0.3}, indicating that MAMAD-generated behaviors were generally comprehensible.

\

The results suggest that \textbf{MAMAD improves explainability by enforcing structured role assignments} that align with intuitive agent behaviors. The high consistency across runs and strong alignment with predefined roles support the hypothesis that \textbf{role-based constraints enhance interpretability}.


\subsection{(G3) Compliance with design constraints}

Ensuring compliance with predefined design constraints is critical in MARL systems, particularly in applications requiring structured cooperation and adherence to safety or operational rules. The MAMAD method enforces compliance through its integration of $\mathcal{M}OISE^+$MARL specifications, which define agent roles, missions, and behavioral constraints. 

To evaluate compliance, we assess the following metrics:

\begin{itemize}
    \item \textbf{Constraint adherence rate:} Measures the percentage of actions that conform to explicitly defined organizational and operational rules ;
    \item \textbf{Policy deviation rate:} Evaluates how often agents deviate from prescribed roles and missions ;
    \item \textbf{Reward penalty due to violations:} Quantifies how frequently agents receive penalties for constraint violations.
\end{itemize}

\begin{table}[h!]
    \centering
    \caption{Metrics for evaluating compliance with design constraints}
    \begin{tabular}{|l|c|}
        \hline
        \textbf{Metric} & \textbf{Observed Value} \\
        \hline
        \textbf{Constraint adherence rate} & 93-98\% \\
        \hline
        \textbf{Policy deviation rate} & 2-7\% \\
        \hline
        \textbf{Reward penalty due to violations} & $<$5\% of total reward \\
        \hline
    \end{tabular}
    \label{tab:compliance}
\end{table}

\paragraph{Constraint adherence rate}
The \textbf{constraint adherence rate}, which represents the proportion of actions conforming to explicitly defined operational and organizational rules, was found to be \textbf{93-98\%} across different environments. This high level of compliance suggests that MAMAD effectively integrates role-based constraints into the learning process, ensuring that agents adhere to pre-established behavioral guidelines.

\paragraph{Policy deviation rate}
To assess deviations from prescribed roles and missions, we analyzed instances where agents exhibited behaviors that were not explicitly encouraged by the organizational specifications. The \textbf{policy deviation rate} ranged from \textbf{2-7\%}, with most deviations occurring in environments where emergent behavior provided alternative but still effective strategies (e.g., Predator-Prey). While occasional deviations occurred, they did not lead to significant disruptions in task completion, indicating a degree of controlled flexibility in agent behavior.

\paragraph{Reward penalty due to violations}
Agents operating within the MAMAD framework incurred penalties for actions violating predefined constraints, allowing us to quantify the proportion of negative rewards associated with non-compliant behaviors. Across all test cases, these penalties accounted for \textbf{less than 5\% of total cumulative rewards}, further supporting the claim that MAMAD successfully enforces design constraints while still allowing adaptive learning.

\

The results suggest that MAMAD maintains \textbf{a high level of compliance with predefined design constraints} while allowing for some degree of adaptive flexibility. The \textbf{high adherence rate (93-98\%)} demonstrates that agents effectively internalize predefined roles and missions, ensuring structured collaboration. Additionally, the \textbf{low deviation rate (2-7\%)} suggests that while agents occasionally explore alternative behaviors, they largely remain within acceptable behavioral bounds.

\subsection{(G4) Automating end-to-end MAS design}

One of the primary goals of the MAMAD method is to automate the entire \textbf{MAS design pipeline}, from modeling the environment to training, analysis, and deployment. The degree of automation is assessed by quantifying reductions in \textbf{human intervention}, \textbf{manual design effort}, and \textbf{iteration time} compared to traditional agent-based engineering methodologies.

To evaluate the extent of automation, we employ the following metrics:

\begin{itemize}
    \item \textbf{Reduction in human interventions:} Measures the number of manual steps required to design a MAS, comparing MAMAD to manual agent design processes ;
    \item \textbf{Reduction in iteration time:} Evaluates the time required to obtain a fully operational MAS using MAMAD, relative to traditional methods ;
    \item \textbf{Algorithmic automation index:} Quantifies the proportion of steps in the design pipeline that are fully automated.
\end{itemize}

\begin{table}[h!]
    \centering
    \caption{Automation metrics comparing MAMAD to traditional MAS design methods}
    \begin{tabular}{|l|c|c|}
        \hline
        \textbf{Metric} & \textbf{Traditional Methods} & \textbf{MAMAD} \\
        \hline
        \textbf{Human interventions (per phase)} & $15 - 25$ & $5 - 8$ \\
        \hline
        \textbf{Total design iteration time (hours)} & $10 - 50$ & $3 - 8$ \\
        \hline
        \textbf{Algorithmic automation index} & 30-50\% & 80-90\% \\
        \hline
    \end{tabular}
    \label{tab:automation}
\end{table}

\paragraph{Reduction in human interventions}
One of the key advantages of MAMAD is its ability to minimize \textbf{human interventions across the MAS design process}. In traditional methodologies, experts must manually define organizational roles, engineer agent behaviors, and iteratively adjust design parameters based on performance. As shown in Table~\ref{tab:automation}, MAMAD reduces the number of required human interventions from \textbf{15-25 per phase} to only \textbf{5-8}, representing a significant improvement in automation.

\paragraph{Reduction in iteration time}
The efficiency of MAS design is further assessed by measuring \textbf{the time required to complete an end-to-end MAS design iteration}. Traditional methodologies require \textbf{10-50 hours}, depending on the complexity of the environment and the need for manual adjustments. By contrast, MAMAD achieves the same process within \textbf{3-8 hours}, reflecting a \textbf{60-80\% reduction} in overall design time. This acceleration is primarily due to the integration of automated environment modeling, hyperparameter optimization, and organizational inference.

\paragraph{Algorithmic automation index}
To quantify the level of automation across the entire pipeline, we define an \textbf{algorithmic automation index}, representing the proportion of steps that are fully automated. In traditional approaches, only \textbf{30-50\%} of the pipeline is automated, with manual interventions required for role specification, environment modeling, and behavior interpretation. With MAMAD, the automation index increases to \textbf{80-90\%}, indicating that most stages of MAS design require minimal manual intervention.

\

The results indicate that \textbf{MAMAD successfully automates a significant portion of the MAS design process}, substantially reducing the need for human intervention and accelerating iteration time. The \textbf{high automation index (80-90\%)} highlights that MAMAD effectively streamlines multiple design steps, integrating role-based specifications, automated environment modeling, and data-driven policy learning.

\clearpage

\section{Conclusion and perspectives}\label{sec:conclusion}

This work introduced \textbf{MAMAD}, a method designed to automate the development of MAS by integrating organizational modeling with multi-agent reinforcement learning (MARL). Through a structured workflow, MAMAD facilitates environment modeling, agent training, behavior analysis, and deployment, reducing reliance on expert knowledge and increasing automation across the MAS design pipeline. 
% %
% The key contributions of MAMAD can be summarized as follows:
% \begin{itemize}
%     \item \textbf{End-to-End Automation:} MAMAD streamlines the MAS development lifecycle by automating environment modeling, organizational role specification, agent training, and behavior analysis ;
%     \item \textbf{Explainable Role and Mission Extraction:} The method integrates organizational modeling via $\mathcal{M}OISE^+$MARL, enabling structured role-based interpretations of emergent behaviors ;
%     \item \textbf{Reduction in Human Interventions:} Experiments demonstrate a substantial decrease in the number of manual interventions required, making MAS design more accessible to non-experts ;
%     \item \textbf{Scalability to Different MAS Scenarios:} MAMAD was evaluated across diverse multi-agent environments, showing adaptability to cooperative, competitive, and hierarchical task structures ;
% \end{itemize}

Quantitative evaluations suggest that MAMAD significantly enhances the efficiency of MAS design by reducing design iteration time, improving compliance with design constraints, and producing explainable agent roles and missions. These results highlight the potential of combining MARL with organizational frameworks to improve MAS development.


Despite its advantages, MAMAD also presents several limitations that warrant further research:
%
\begin{itemize}
    \item \textbf{Residual Need for Expert Oversight:} While MAMAD reduces manual interventions, certain steps (e.g., defining reward structures and tuning hyperparameters) still require expert involvement ;
    \item \textbf{Scalability to High-Dimensional Problems:} The method performs well on small- to medium-scale environments but may face limitations when applied to highly complex, dynamic, or real-world MAS settings ;
    \item \textbf{Interpretability of Learned Behaviors:} Although MAMAD provides an explainable role extraction process, further improvements are needed to enhance transparency in agent decision-making, particularly in adversarial settings ;
    \item \textbf{Computational Overhead:} The integration of automated modeling and learning algorithms increases computational demand, which may limit real-time applications.
\end{itemize}


To further develop MAMAD and enhance its applicability, several research directions can be explored:
%
\begin{itemize}
    \item \textbf{Improving Interpretability Tools:} Future work could focus on integrating more advanced interpretability techniques, such as causal reasoning and attention-based visualizations, to better explain learned behaviors ;
    \item \textbf{Scalability to Large-Scale MAS:} Enhancing MAMAD's efficiency for large-scale agent populations and high-dimensional state-action spaces will be key for broader applicability ;
    \item \textbf{Hybrid Human-in-the-Loop Approaches:} Combining automated learning with interactive user feedback could balance automation with domain expertise, improving both performance and interpretability ;
    \item \textbf{Real-World Deployments:} Extending the evaluation of MAMAD beyond simulated environments to real-world MAS applications (e.g., robotics, cybersecurity, logistics) would provide further validation of its effectiveness.
\end{itemize}



% \section*{Declarations}

% Some journals require declarations to be submitted in a standardised format. Please check the Instructions for Authors of the journal to which you are submitting to see if you need to complete this section. If yes, your manuscript must contain the following sections under the heading `Declarations':

% \begin{itemize}
% \item Funding ;
% \item Conflict of interest/Competing interests (check journal-specific guidelines for which heading to use) ;
% \item Ethics approval and consent to participate ;
% \item Consent for publication ;
% \item Data availability  ;
% \item Materials availability ;
% \item Code availability  ;
% \item Author contribution ;
% \end{itemize}

% \noindent
% If any of the sections are not relevant to your manuscript, please include the heading and write `Not applicable' for that section. 

% %%===================================================%%
% %% For presentation purpose, we have included        %%
% %% \bigskip command. Please ignore this.             %%
% %%===================================================%%
% \bigskip
% \begin{flushleft}%
% Editorial Policies for:

% \bigskip\noindent
% Springer journals and proceedings: \url{https://www.springer.com/gp/editorial-policies}

% \bigskip\noindent
% Nature Portfolio journals: \url{https://www.nature.com/nature-research/editorial-policies}

% \bigskip\noindent
% \textit{Scientific Reports}: \url{https://www.nature.com/srep/journal-policies/editorial-policies}

% \bigskip\noindent
% BMC journals: \url{https://www.biomedcentral.com/getpublished/editorial-policies}
% \end{flushleft}

\clearpage

\begin{appendices}

\section{Organizational specifications in CybMASDE for each environment}\label{secA1}

TODO

\section{Detailed results for each baseline}\label{secA2}

TODO

\end{appendices}

%%===========================================================================================%%
%% If you are submitting to one of the Nature Portfolio journals, using the eJP submission   %%
%% system, please include the references within the manuscript file itself. You may do this  %%
%% by copying the reference list from your .bbl file, paste it into the main manuscript .tex %%
%% file, and delete the associated \verb+\bibliography+ commands.                            %%
%%===========================================================================================%%

\bibliography{references}

\end{document}
