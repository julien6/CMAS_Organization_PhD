%Version 3.1 December 2024
% See section 11 of the User Manual for version history
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                                                 %%
%% Please do not use \input{...} to include other tex files.       %%
%% Submit your LaTeX manuscript as one .tex document.              %%
%%                                                                 %%
%% All additional figures and files should be attached             %%
%% separately and not embedded in the \TeX\ document itself.       %%
%%                                                                 %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%\documentclass[referee,sn-basic]{sn-jnl}% referee option is meant for double line spacing

%%=======================================================%%
%% to print line numbers in the margin use lineno option %%
%%=======================================================%%

%%\documentclass[lineno,pdflatex,sn-basic]{sn-jnl}% Basic Springer Nature Reference Style/Chemistry Reference Style

%%=========================================================================================%%
%% the documentclass is set to pdflatex as default. You can delete it if not appropriate.  %%
%%=========================================================================================%%

%%\documentclass[sn-basic]{sn-jnl}% Basic Springer Nature Reference Style/Chemistry Reference Style

%%Note: the following reference styles support Namedate and Numbered referencing. By default the style follows the most common style. To switch between the options you can add or remove �Numbered� in the optional parenthesis. 
%%The option is available for: sn-basic.bst, sn-chicago.bst%  
 
%%\documentclass[pdflatex,sn-nature]{sn-jnl}% Style for submissions to Nature Portfolio journals
%%\documentclass[pdflatex,sn-basic]{sn-jnl}% Basic Springer Nature Reference Style/Chemistry Reference Style
\documentclass[pdflatex,sn-mathphys-num]{sn-jnl}% Math and Physical Sciences Numbered Reference Style
%%\documentclass[pdflatex,sn-mathphys-ay]{sn-jnl}% Math and Physical Sciences Author Year Reference Style
%%\documentclass[pdflatex,sn-aps]{sn-jnl}% American Physical Society (APS) Reference Style
%%\documentclass[pdflatex,sn-vancouver-num]{sn-jnl}% Vancouver Numbered Reference Style
%%\documentclass[pdflatex,sn-vancouver-ay]{sn-jnl}% Vancouver Author Year Reference Style
%%\documentclass[pdflatex,sn-apa]{sn-jnl}% APA Reference Style
%%\documentclass[pdflatex,sn-chicago]{sn-jnl}% Chicago-based Humanities Reference Style

%%%% Standard Packages
%%<additional latex packages if required can be included here>

% \usepackage{graphicx}%
% \usepackage{multirow}%
\usepackage{amsmath,amssymb,amsfonts}%
\usepackage{amsthm}%
\usepackage{mathrsfs}%
% \usepackage[title]{appendix}%
% \usepackage{xcolor}%
% \usepackage{textcomp}%
% \usepackage{manyfoot}%
% \usepackage{booktabs}%
% \usepackage{algorithm}%
% \usepackage{algorithmicx}%
% \usepackage{algpseudocode}%
% \usepackage{listings}%

\usepackage{hyperref}

%%%% For camera-ready, use this
%\documentclass[sigconf]{aamas}

\usepackage{listings}
\usepackage{xcolor}

\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usepackage{amsmath}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}
 
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}
 
\lstset{style=mystyle}

% --- Tickz
\usepackage{physics}
\usepackage{tikz}
\usepackage{amsmath}
\usepackage{mathdots}
% \usepackage{yhmath}
\usepackage{cancel}
\usepackage{color}
\usepackage{siunitx}
\usepackage{array}
\usepackage{multirow}
% \usepackage{amssymb}
\usepackage{gensymb}
\usepackage{tabularx}
\usepackage{extarrows}
\usepackage{booktabs}
\usetikzlibrary{fadings}
\usetikzlibrary{patterns}
\usetikzlibrary{shadows.blur}
\usetikzlibrary{shapes}

% ---------

\usepackage{balance} % for balancing columns on the final page
\usepackage{csquotes}
% \usepackage{cite}
\newcommand{\probP}{\text{I\kern-0.15em P}}
\usepackage{etoolbox}
\patchcmd{\thebibliography}{\section*{\refname}}{}{}{}
% \usepackage{amsthm,amssymb,amsfonts}

\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{color}
% \renewcommand\UrlFont{\color{blue}\rmfamily}

\usepackage[inline, shortlabels]{enumitem}
\usepackage{tabularx}
\usepackage{caption}
\usepackage{listings}
\usepackage{stfloats}
\usepackage{titlesec}
\usepackage{ragged2e}
% \usepackage[hyphens]{url}
\usepackage{float}
\usepackage[english]{babel}
\addto\extrasenglish{  
    \def\figureautorefname{Figure}
    \def\tableautorefname{Table}
    \def\algorithmautorefname{Algorithm}
    \def\sectionautorefname{Section}
    \def\subsectionautorefname{Subsection}
    \def\proofoutlineautorefname{Proof Outline}
}

\usepackage[linesnumbered, ruled, vlined]{algorithm2e}
\SetKwComment{Comment}{$\triangleright$\ }{}
\SetAlgoNlRelativeSize{0}
\SetAlgoNlRelativeSize{-1}

\usepackage{amssymb}
\usepackage{pifont}
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%

%%%%%=============================================================================%%%%
%%%%  Remarks: This template is provided to aid authors with the preparation
%%%%  of original research articles intended for submission to journals published 
%%%%  by Springer Nature. The guidance has been prepared in partnership with 
%%%%  production teams to conform to Springer Nature technical requirements. 
%%%%  Editorial and presentation requirements differ among journal portfolios and 
%%%%  research disciplines. You may find sections in this template are irrelevant 
%%%%  to your work and are empowered to omit any such section if allowed by the 
%%%%  journal you intend to submit to. The submission guidelines and policies 
%%%%  of the journal take precedence. A detailed User Manual is available in the 
%%%%  template package for technical guidance.
%%%%%=============================================================================%%%%

%% as per the requirement new theorem styles can be included as shown below
\theoremstyle{thmstyleone}%
\newtheorem{theorem}{Theorem}%  meant for continuous numbers
%%\newtheorem{theorem}{Theorem}[section]% meant for sectionwise numbers
%% optional argument [theorem] produces theorem numbering sequence instead of independent numbers for Proposition
\newtheorem{proposition}[theorem]{Proposition}% 
%%\newtheorem{proposition}{Proposition}% to get separate numbers for theorem and proposition etc.

\theoremstyle{thmstyletwo}%
\newtheorem{example}{Example}%
\newtheorem{remark}{Remark}%

\theoremstyle{thmstylethree}%
\newtheorem{definition}{Definition}%

\raggedbottom
%%\unnumbered% uncomment this for unnumbered level heads

\begin{document}



\title[Assisting Multi-Agent System Design with MOISE+ and MARL: The MAMAD Method]{Assisting Multi-Agent System Design with $\mathcal{M}OISE^+$ and MARL: The MAMAD Method}

%%=============================================================%%
%% GivenName	-> \fnm{Joergen W.}
%% Particle	-> \spfx{van der} -> surname prefix
%% FamilyName	-> \sur{Ploeg}
%% Suffix	-> \sfx{IV}
%% \author*[1,2]{\fnm{Joergen W.} \spfx{van der} \sur{Ploeg} 
%%  \sfx{IV}}\email{iauthor@gmail.com}
%%=============================================================%%

\author*[1]{\fnm{Julien} \sur{Soulé}}\email{julien.soule@lcis.grenoble-inp.fr}

\author[1]{\fnm{Jean-Paul} \sur{Jamont}}\email{jean-paul.jamont@lcis.grenoble-inp.fr}
% \equalcont{These authors contributed equally to this work.}

\author[1]{\fnm{Michel} \sur{Occello}}\email{michel.occello@lcis.grenoble-inp.fr}

\author[2]{\fnm{Louis-Marie} \sur{Traonouez}}\email{louis-marie.traonouez@thalesgroup.com}

\author[3]{\fnm{Paul} \sur{Théron}}\email{paul.theron@orange.fr}

\affil*[1]{\orgdiv{Laboratoire de Conception et d'Intégration des Systèmes (LCIS)}, \orgname{Université Grenoble Alpe}, \orgaddress{\street{50 Rue Barthélémy de Laffemas}, \city{Valence}, \postcode{26000}, \state{Auvergne-Rhône-Alpes}, \country{France}}}

\affil[2]{\orgdiv{Thales Land and Air Systems}, \orgname{BL IAS}, \orgaddress{\street{1 Rue Louis Braille}, \city{Saint-Jacques-de-la-Lande}, \postcode{35136}, \state{Ille-et-Vilaine}, \country{France}}}

\affil[3]{\orgname{AICA IWG}, \orgaddress{\street{22 Av. Gazan Prolongée, 06600 Antibes}, \city{Antibes}, \postcode{06600}, \state{Provence-Alpes-Côte d'Azur}, \country{France}}}

%%==================================%%
%% Sample for unstructured abstract %%
%%==================================%%

\abstract{
    Traditional Agent-Oriented Software Engineering (AOSE) methods rely on explicit and expert-driven design for MAS, but often lack automation. In contrast, Multi-Agent Reinforcement Learning (MARL) and related fields offer automated ways to model environments and learn suitable agent policies. However, integrating these techniques into AOSE remains underexplored partly due to the lack of control, explainability, and unifying frameworks.
    %
    We propose \textbf{MOISE+MARL Assisted MAS Design (MAMAD)}, a four-activity method framing MAS design as a constrained optimization problem: learning joint policies that maximize rewards while respecting $\mathcal{M}OISE^+$ roles and goals. The activities include:
    1) \textbf{Modeling} the environment,  
    2) \textbf{Training} under organizational constraints,  
    3) \textbf{Analyzing} emergent behaviors,  
    4) \textbf{Transferring} to real-world deployment.
    %
    We evaluate MAMAD on various environments, showing that the generated MAS exhibit expected performance, compliance with design requirements and are explainable, while reducing manual design overhead.
}

% Introduction
% Purpose
% Methods
% Results
% Conclusion

\keywords{Agent-oriented Software Engineering, Multi-Agent Reinforcement Learning, Assisted-Design, Organizational Model}

%%\pacs[JEL Classification]{D8, H51}

%%\pacs[MSC Classification]{35A01, 65L10, 65L12, 65L20, 65L70}

\maketitle

\section{Introduction}

\subsection{Context}

Designing \textbf{Multi-Agent Systems (MAS)} for real-world domains (such as cybersecurity, autonomous logistics or robotic swarms) requires designing agents that are both autonomous and coordinated, adaptable and structured. To address these demands, the field of \textbf{Agent-Oriented Software Engineering (AOSE)} has historically provided principled methodologies based on symbolic representations. Methods such as GAIA~\cite{gaia1998}, ADELFE~\cite{adelfe2002}, or DIAMOND~\cite{Jamont2005} provide well-defined processes for designing MAS, relying on explicit modeling with possibly explicit roles, missions, or interaction protocols~\cite{Pavon2003,Bernon2005}. They offer guarantees in terms of predictability, safety, and explainability by leveraging expert-driven design processes~\cite{Hindriks2014,Jamont2O15}. However, these methods are largely manual and require specialized knowledge to define agent behaviors, making scalability in complex or dynamic environments cumbersome.

To improve efficiency and scalability, several AOSE frameworks such as INGENIAS~\cite{Pavon2003}, KB-ORG~\cite{Sims2008} or AutoGenesisAgent~\cite{harper2024autogenesisagent} have also sought to automate key aspects of MAS design.
Yet, these AOSE works still faces major limitations towards a fully end-to-end automated design process. Crucially, they lack automated support for modeling complex environments as test environments, optimizing agent behaviors, or analyzing emergent dynamics.

In parallel, \textbf{Machine Learning (ML)} has led to diverse works and subfields that, although developed independently from AOSE in Multi-Agent Systems paradigm, provide capabilities likely relevant for MAS design offering the automation, adaptivity, and scalability that AOSE lacks. Two major subfields are:
\begin{enumerate*}[label={\roman*)}, itemjoin={; \quad}]
    \item \textbf{World Models}~\cite{Ha2018}, which learn high-fidelity environment simulations from agents trajectories
    \item \textbf{MARL}~\cite{Zhang2021, Papoudakis2021}, which enables decentralized policy optimization through exploration and trial-and-error
\end{enumerate*}

However, ML-based approaches also present key limitations. While MARL enables agents to learn without manual supervision, the resulting policies are often opaque, difficult to control, and poorly aligned with explicit design requirements~\cite{Nguyen2020, Du2022}, limiting their safe deployment.
Although World Models~\cite{Ha2018} show promise in single-agent settings, their extension to multi-agent systems remains challenging due to increased complexity in coordination and observability.
Most importantly, there is still no fully or partly automated MAS design framework that bridges real-world environments with a pipeline orchestrating potentially leveraged ML-based works.

This strong motivation to bridge the symbolic, model-driven rigor of AOSE with the learning-based automation of ML and particularly MARL, led to the \textbf{MOISE+MARL} framework~\cite{soule2025moisemarl}%
\footnote{This article introducing MOISE+MARL has been accepted at AAMAS 2025 and is freely available at \url{https://arxiv.org/abs/2503.23615}}.
%
MOISE+MARL integrates the $\mathcal{M}OISE^+$ organizational model~\cite{Hubner2002} into the MARL paradigm, using formal organizational specifications both to guide the agents during training and to interpret their learned behaviors in terms of roles and goals. While this represents a significant advancement toward integrating organizational reasoning into MARL, this framework is not yet conceived as part of a comprehensive MAS design methodology.

\subsection{Problem statement and research gaps}

Extending the MOISE+MARL framework, we adopt an optimization-based perspective to bridge AOSE and MARL paradigms in MAS design. We consider the problem of designing a MAS that must operate in a real-world environment, achieve a global goal, and possibly satisfy additional design requirements. Then, the core design task is framed as a \textbf{constrained optimization problem in a MARL context}, where:
\begin{itemize}
    \item The \textbf{optimization variable} is the agents' joint policy;
    \item The \textbf{goal function} seeks to maximize cumulative rewards;
    \item The \textbf{constraints} represent symbolic design requirements, such as roles or goals.
\end{itemize}

\noindent This formulation leverages the MOISE+MARL hybrid approach by solving the design problem through MARL, while ensuring that the resulting agent behaviors remain controllable and interpretable through AOSE symbolic principles. Based on this perspective, we aim to address the following research gaps:
%
\begin{itemize}
    \item \textbf{(G1) End-to-end automation.} ML techniques (particularly World Models and MARL) have the potential to automate the key activities of MAS design. However, no existing framework integrates these techniques into a unified, iterative design pipeline. Moreover, World Models remain underdeveloped in multi-agent settings, especially for realistic deployment scenarios.
          
    \item \textbf{(G2) Compliance with design requirements.} Most MARL approaches focus on performance optimization without enforcing structured constraints such as safety rules, roles, or missions. While some recent works have begun to address this limitation, MOISE+MARL~\cite{soule2025moisemarl} has demonstrated that incorporating organizational specifications during training is feasible. However, this framework has not yet been integrated into a fully or partly automated MAS design process.
          
    \item \textbf{(G3) Organizational-level explainability.} Policies learned via MARL are often opaque, making it difficult to understand how agent behaviors contribute to system-level objectives. In contrast, AOSE methods benefit from explicit symbolic structures that facilitate interpretability. Among the few works addressing collective explainability in MARL, MOISE+MARL~\cite{soule2025moisemarl} introduced a trajectory-based analysis method to extract implicit organizational structures. Yet, this approach remains disconnected from a design-oriented pipeline and lacks mechanisms for reintegrating insights into the iterative design process.
\end{itemize}

\subsection{Contributions and paper organization}

We propose the \textbf{MAMAD method} which extends the \textbf{MOISE+MARL} framework~\cite{soule2025moisemarl} towards MAS design. MAMAD is driven by three main inputs: (i) the environment, (ii) the global objective, and (iii) user-defined design requirements (obtained through a \textit{Requirements Engineering} activity). MAMAD leverages these inputs to generate a MAS by means of a fully end-to-end, continuously refining both the organizational specifications and the policy through iterative interaction with the real environment.
The MAMAD method is structured in four activities:

\begin{enumerate}
    \item \textbf{Modeling activity:} This activity constructs a high-fidelity simulated environment by training a neural architecture inspired by World Models~\cite{Ha2018}, using agent trajectories collected from early deployments. It also encodes the global objective into a reward function and formalizes design requirements into organizational specifications (e.g., roles and missions).
          
    \item \textbf{Training activity:} Agents are trained using MARL in the simulated environment. The \textit{control mechanism} of MOISE+MARL is employed to constrain or guide the learning process based on symbolic specifications: \textit{roles} restrict the allowable action space, while \textit{goals} modulate rewards. This integration ensures that learned policies comply with user-defined design constraints.
          
    \item \textbf{Analysis activity:} The analysis method of MOISE+MARL is leveraged to extract implicit organizational structures from successful trajectories using unsupervised learning. Emergent roles and goals are inferred and can be compared with predefined specifications to assess organizational alignment or even suggest refinements.
          
    \item \textbf{Transfer activity:} The trained joint policy is deployed in the real environment. New agent trajectories are continuously collected and reintegrated into the modeling activity to reduce the simulation-to-reality gap and allow for ongoing design refinement. This activity closes the loop, enabling a continual update of both the simulated environment and the learned policies.
\end{enumerate}


\noindent MAMAD thus provides a general framework for automating MAS design while supporting constraint satisfaction and symbolic interpretability. It enables the designer to benefit from both the adaptability of ML techniques and the AOSE principles.



We evaluated \textbf{MAMAD} in gamified environments, used as controlled testbeds to assess its ability to generate high-fidelity simulations during the Modeling activity (bypassing the complexity of modeling physical environments). Results show strong alignment between the specifications applied during training and those inferred post hoc, validating both \textbf{(G3) Organizational-level explainability} and \textbf{(G2) Compliance with design requirements}. Compared to manual methods, \textbf{(G1) End-to-end automation.} improved significantly, requiring fewer interventions. Ablation studies revealed that omitting automated modeling reduced policy generalization, while removing organizational constraints led to erratic agent behavior.

\

\noindent The remainder of the paper is structured as follows. \autoref{sec:related_works} reviews prior work relevant to each of the identified gaps. \autoref{sec:background} introduces the background and notation, covering World Models, Markovian formulism for MARL, the $\mathcal{M}OISE^+$ organizational model, and the MOISE+MARL framework. \autoref{sec:mamad} details the MAMAD method, presenting the overall workflow and the contributions bridging the identified gaps. \autoref{sec:experimental_setup} describes the experimental setup, followed by \autoref{sec:results}, which presents and discusses the obtained results. Finally, \autoref{sec:conclusion} concludes the paper and outlines future research directions.


\section{Related works}\label{sec:related_works}

This section reviews distinct bodies of literature relative to the three research gaps.

\subsection{Automating end-to-end MAS design (G1)}

Several advanced AOSE approaches have attempted to improve automation in MAS design. For example, INGENIAS adopts a model-driven engineering paradigm, offering meta-models and tooling to automatically generate code, documentation, and tests from high-level specifications, thereby streamlining MAS development~\cite{Pavon2003}. Similarly, the KB-ORG framework employs a knowledge-based approach to organizational design, using predefined templates and domain-specific knowledge to automate the assignment of roles and responsibilities~\cite{Sims2008}. More recently, the field of Automated Design of Agentic Systems has emerged, focusing on the automatic generation and composition of agentic components into functional MAS with minimal human intervention~\cite{smith2024automated}. Taking this idea further, AutoGenesisAgent proposes a fully autonomous pipeline in which MAS can design and deploy new MAS tailored to specific tasks, covering the full lifecycle from initial concept to deployment~\cite{harper2024autogenesisagent}. Likewise, the BMW Agents framework illustrates how collaborative agent architectures can support scalable task automation through planning and execution in complex industrial environments~\cite{crawford2024bmw}.

Despite these advances, such approaches typically assume symbolic inputs and predefined environments. They do not support the dynamic modeling of complex or unknown environments, nor do they integrate learning-based policy optimization. Moreover, they lack closed-loop design mechanisms capable of refining agent specifications based on the analysis of emergent behaviors, an essential capability for scalable and adaptive MAS development.

\

\noindent In parallel, some of the most promising advances in MAS automation have emerged from the field of Machine Learning. A notable example is the \textit{Cyber Security Learning Environment}~\cite{hammar2023scalable}, an \textbf{online framework} for cybersecurity applications in which agents are trained using RL techniques in automatically generated, near-realistic simulations to dynamically acquire task-specific behaviors. This framework constitutes a significant step toward \textbf{end-to-end MAS design automation}, offering an almost fully or partly automated pipeline (from environment modeling to policy learning and deployment) while minimizing manual effort. It also provides visualization tools for monitoring agent behavior, though it does not incorporate organizational modeling.

More broadly, ML-based paradigms offer critical capabilities that could benefit MAS design. In particular, the World Models framework~\cite{Ha2018} proposes to first learn a compressed latent representation of the environment, which is then used as a high-fidelity simulation for policy training or planning. Although effective in single-agent contexts, World Models remain underexplored in multi-agent scenarios, particularly in settings involving partial observability, interaction complexity, and the need for coordination at scale.
%
To date, no existing framework provides a fully or partly automated, iterative MAS design pipeline that connects to a real-world deployment environment while orchestrating multiple ML techniques within a unified process.

\subsection{Integrating design constraints in MARL (G2)}

The MARL literature has primarily focused on optimizing coordination and cooperation among agents in complex and uncertain environments~\cite{Zhang2021, Papoudakis2021}. However, most approaches overlook the incorporation of symbolic or organizational constraints into the learning process. Agents typically learn through trial and error, without guarantees that their emergent behaviors will satisfy critical design requirements such as safety rules, role adherence, or structured team hierarchies. Several recent works have attempted to address this limitation by introducing constraint-aware reinforcement learning techniques.

Constraint-Guided Reinforcement Learning~\cite{spieker2021constraint} incorporates explicit constraint models into the agent-environment interaction, enabling agents to learn policies that remain within predefined behavioral bounds. Similarly, Deep Constrained Q-Learning~\cite{kalweit2020deep} introduces both single-step and approximate multi-step constraints into the Q-value update process to ensure compliance with safety and performance criteria. Constrained Policy Optimization (CPO)\cite{achiam2017constrained} provides theoretical guarantees for near-constraint satisfaction throughout policy search, making it particularly appealing for safety-critical applications. Beyond safety, MENTOR\cite{zhou2024mentor} integrates human feedback into hierarchical RL, guiding agents through dynamically constrained subgoal selection to promote more stable learning. Other approaches such as reward-free constrained learning~\cite{miryoosefi2021} circumvent the need for hand-crafted reward functions by directly optimizing constraint satisfaction.

While these approaches enhance safety and control at the policy level, they do not integrate with symbolic design models such as those used in AOSE. A notable exception is MOISE+MARL~\cite{soule2025moisemarl}, which bridges the gap by extending the $\mathcal{M}OISE^+$ organizational framework~\cite{Hubner2002} into MARL, allowing agents to learn while respecting organizational roles, missions, and behavioral constraints. However, MOISE+MARL remains focused on execution-time control and post-hoc analysis, lacking a full design pipeline or environment modeling capability. In particular, it assumes access to a manually specified environment modeled as a Dec-POMDP, whereas our approach is to operate within environments that are automatically modeled from agent interactions, following the World Models paradigm.

\subsection{Organizational-level explainability (G3)}

While the AOSE tradition ensures explainability through structured design artifacts (such as protocols, roles, missions, or goals) these symbolic elements are typically lost in standard MARL approaches. Learned policies are often represented as opaque neural networks, making it difficult to assess how well agent behaviors align with the original design intent or organizational principles. Although explainability in MARL has gained attention, most existing efforts focus on individual agent behavior or internal policy mechanisms, rather than on collective or organizational alignment.

A growing body of work seeks to improve interpretability through model design and post-hoc analysis. Zabounidis et al.\cite{zabounidis2023concept} incorporate interpretable concepts into the training loop, requiring agents to predict human-understandable concepts before acting. This encourages transparency and enables expert corrections. Iturria-Rivera et al.\cite{iturria2024explainable} use reward decomposition in factorized value functions (e.g., VDN, QMIX) to expose the contribution of each component to agent decisions. Liu et al.\cite{liu2025} propose MIXRTs, a hybrid architecture combining recurrent neural networks with decision trees for interpretable policy learning. Other efforts like Poupart et al.\cite{poupart2025perspectives} introduce post-hoc methods such as relevance backpropagation and activation patching to explain behavior without modifying the learned models. Similarly, Li et al.~\cite{li2025from} employ Shapley-value-based approximations to transform deep policies into interpretable structures applicable across different RL settings.

However, these approaches generally remain limited to local or agent-level insights, without addressing collective dynamics or alignment with high-level symbolic models. Only a few works attempt role or goal inference that could be interpreted organizationally. For example, Berenji and Vengerov~\cite{berenji2000learning} improve coordination by modeling agent dependencies in UAV missions, while Yusuf and Baber~\cite{yusuf2020inferential} use Bayesian reasoning to support dynamic coordination, yet neither provides mechanisms for abstract organizational role inference. Serrino et al.~\cite{serrino2019finding} investigate emergent roles through social interactions but focus on operational rather than organizational roles.

To our knowledge, the only framework that explicitly addresses organizational-level explainability is the TEMM (Trajectory-based Evaluation in MOISE+MARL) method~\cite{soule2025moisemarl}, developed as part of the MOISE+MARL framework. TEMM is based on the hypothesis that, over time, trained agents tend to converge toward idealized, regular behaviors viewed as making up an implicit organization. While individual trajectories may exhibit noise, the presence of recurring patterns across many trajectories makes it possible to filter out this noise through averaging.

To operationalize this idea, TEMM uses trajectory clustering and symbolic projection to infer implicit roles and goals from observed agent behaviors. This enables the evaluation of the system's organizational fit that is, how closely the learned behaviors align with those expected in a well-structured (either implicit or predefined) organization.

Yet, TEMM is not fully integrated within a broader design-oriented perspective, as it currently lacks mechanisms to assist designers in deriving suitable organizational specifications that could be re-injected as additional design requirements. Enabling such capabilities would support an iterative refinement process, progressively narrowing the policy search space toward more structured and context-relevant behaviors, while remaining agnostic to both the deployment environment and prior expert knowledge.

% =======================


\section{Theoretical background}\label{sec:background}

This section recaps the notation and basics we used in our contributions for MARL and the $\mathcal{M}OISE^+$ organizational model.

\subsection{Markov framework for MARL}

To apply MARL techniques, we rely on Markovian models. The most commonly used among them is the \textbf{Decentralized Partially Observable Markov Decision Process} (Dec-POMDP)~\cite{Oliehoek2016}. Dec-POMDPs model decentralized multi-agent coordination under partial observability, making them particularly suitable for integrating organizational constraints. Compared to \textbf{Partially Observable Stochastic Games} (POSGs), Dec-POMDPs assume a shared reward function among agents, thus promoting collaborative behaviors~\cite{Matignon2007}. Both Dec-POMDPs and POSGs typically presuppose access to the true state of the environment, which limits their applicability in realistic, opaque systems.
%
Formally, a Dec-POMDP is defined as a 7-tuple:
%
$\left(S, \{A_i\}, T, R, \{\Omega_i\}, O, \gamma \right)$
%
where:
\begin{itemize}
    \item $S = \{s_1, \dots, s_{|S|}\}$ is the set of possible environment states.
    \item $A_i = \{a_1^i, \dots, a_{|A_i|}^i\}$ is the set of actions available to agent $i$.
    \item $T(s, a, s') = \mathbb{P}(s' \mid s, a)$ defines the state transition probabilities.
    \item $R: S \times A \times S \rightarrow \mathbb{R}$ is the reward function.
    \item $\Omega_i = \{o_1^i, \dots, o_{|\Omega_i|}^i\}$ is the set of possible observations for agent $i$.
    \item $O(s', a, o) = \mathbb{P}(o \mid s', a)$ specifies the observation probabilities.
    \item $\gamma \in [0, 1]$ is the discount factor.
\end{itemize}

\noindent Considering $m$ \textbf{teams} each containing several agents among $\mathcal{A}$, we also detail the minimal formalism notation we re-used for solving the Dec-POMDP for a given team $i, 0 \leq i \leq m$ containing $n$ agents~\cite{Matignon2007,Yuan2023}:

\begin{itemize}

    \item $\Pi$: the set of policies. A \textbf{policy} $\pi \in \Pi, \pi: \Omega \rightarrow A$ deterministically maps an observation to an action. It represents the agent's internal logic;
    \item $\Pi^{j}$: the set of joint-policies. A \textbf{joint-policy} $\pi^{j} \in \Pi^{j}, \pi^{j}: \Omega^n \rightarrow A^n = \Pi^n$ chooses an action for each agent regarding their respective observation. It can be viewed as a set of the policies used in agents;
    \item $H$: the set of histories. A \textbf{history} (we also interchangeably call \textbf{trajectory}) over $z \in \mathbb{N}$ steps (where $z$ is generally the maximum number of steps for an episode) is the $z$-tuple $h = ((\omega_{k}, a_{k}) | k \leq z, \omega \in \Omega, a \in A)$
    \item $H^{j}$: the set of joint-histories. A \textbf{joint-history} over $z \in \mathbb{N}$ steps $h^{j} \in H^{j}, h^{j} = \{h_1,h_2..h_n\}$ is the set of the agents' histories.
    \item $U^{j}_{i}(<\pi^{j}_{i}, \pi^{j}_{-i}>): \Pi^{j} \rightarrow \mathbb{R}$: gives the \textbf{expected cumulative reward} over a finite horizon (if $\gamma < 1$ or the number of steps in an episode is finite), with $\pi^{j}_{i}$ the joint policy for team $i$ and $\pi^{j}_{-i}$ all of the other concatenated joint-policies (considered as fixed);
    \item $SR^{j}_{i}(\pi^{j}_{i}, s) = \{\pi^{j}_{i} | U(<\pi^{j}_{i},\pi^{j}_{-i}>) \geq s\}$: gives the \textbf{sufficient response} as the set of joint-policies getting at least $s \in \mathbb{R}, s \leq U_i^*$ as expected cumulative reward.
\end{itemize}

\noindent We refer to \textbf{solving} the Dec-POMDP at $s$ expectancy as finding a the joint policies $\pi^{j}_{i} \in \Pi^{j}, \pi^{j}_{i} = SR^{j}_{i}(\pi^{j}_{i}, s)$ that gets the expected cumulative reward over a finite horizon at least at $s \in \mathbb{R}, s \leq U_i^*$.


\subsection{The $\mathcal{M}OISE^+$ organizational model}

The $\mathcal{M}OISE^+$ model~\cite{Hubner2002, Hubner2007} provides a comprehensive formal framework for specifying multi-agent organizations. While $\mathcal{M}OISE^+$ offers a comprehensive set of structural, functional, and deontic specifications, we show notation for the core components directly relevant to our approach: \textit{roles}, \textit{missions} (goals), and \textit{permissions/obligations}.

\

\noindent \textbf{Structural specifications} \quad
%
The structural specifications of the $\mathcal{M}OISE^+$ model include the roles, denoted $\mathcal{R}$ (with $\rho \in \mathcal{R}$).
%
Structural specifications also contain inheritance relation, the definition of groups and sub-groups, interconnected via links that encode various inter-role relationships such as acquaintance, communication, and authority, as well as compatibility relations defining which roles can be played simultaneously by the same agent, and cardinality constraints specifying the allowed number of agents assigned to roles and groups.

\

\noindent \textbf{Functional specifications} \quad
%
The functional specifications of the $\mathcal{M}OISE^+$ model include:
%
\begin{enumerate*}[label={\roman*)}, itemjoin={; \quad}]
    \item global goals, denoted $\mathcal{G}$ (with $g \in \mathcal{G}$), decomposed into hierarchical structures through plans, where decomposition operators specify how sub-goals contribute to higher-level goals;
    \item missions that contain assigned goals, denoted $\mathcal{M}$ (with $m \in \mathcal{M}$)
    \item a goal to mission mapping, denoted $mo: \mathcal{M} \rightarrow \mathbb{P}(\mathcal{G})$.
\end{enumerate*}
%
Functional specifications also include mission cardinalities indicating how many agents may commit to each mission (fixed to one in our contribution), and complemented by preference orders expressing social priorities when multiple missions are simultaneously available.

\

\noindent \textbf{Deontic specifications} \quad
%
The deontic specifications of the $\mathcal{M}OISE^+$ model define the agents' normative constraints through:
%
\begin{enumerate*}[label={\roman*)}, itemjoin={; \quad}]
    \item permissions, denoted $\mathcal{PER} = (\rho_a,m,tc)$ (aslo denoted $per(\rho_a,m,tc)$) meaning an agent playing role $\rho_a \in \mathcal{R}$ is permitted to commit on mission $m \in \mathcal{M}$ for a given time constraint $tc \in \mathcal{TC}$;
    \item and obligations, denoted $\mathcal{OBL} = (\rho_a,m,tc)$ (aslo denoted $obl(\rho_a,m,tc)$) meaning an agent playing role $\rho_a \in \mathcal{R}$ is obliged to commit on mission $m \in \mathcal{M}$ for a given time constraint $tc \in \mathcal{TC}$;
\end{enumerate*}
%
A time constraint $tc \in \mathcal{TC}$ specifies a set of periods during which a permission or obligation is valid ($Any \in \mathcal{TC}$ means everytime);

\

\noindent Organizational specifications applied to agents are roles and goals (as missions) through permissions or obligations. Indeed, the other structural specifications such as compatibilities or links are inherent to roles. Similarly, we consider that the goals, the missions, and their mapping ($mo$) are enough to also link all of the other functional specifications such as plans, cardinalities, or preference orders.
Consequently, we consider it is sufficient to take into account roles, missions (goal and mapping) and permissions/obligations when linking $\mathcal{M}OISE^+$ with Dec-POMDP. 

For this reason, we adopt a minimal but sufficient formalization focused on the following organizational specification set $\mathcal{OS} = \langle \mathcal{R}, \mathcal{M}, \mathcal{PER}, \mathcal{OBL}, mo \rangle$ to guide organizational policy learning within our $\mathcal{M}OISE^+$ MARL framework.


\subsection{MOISE+MARL for linking $\mathcal{M}OISE^+$ with MARL}

\begin{figure}[h!]
    \centering
    \input{figures/mm_synthesis_single_column.tex}
    \caption{A minimal view of the MOISE+MARL framework:
        Users first define $\mathcal{M}OISE^+$ specifications, which include roles ($\mathcal{R}$) and missions ($\mathcal{M}$), both associated through $rds$. They then create MOISE+MARL specifications by first defining \textbf{Constraint guides} such as $rag$ and $rrg$ to specify role logic, and $grg$ for goal logic. 
        Next, \textbf{Linkers} are used to connect agents with roles through $ar$ and to link the logic of the constraint guides to the defined $\mathcal{M}OISE^+$ specifications. Once this is set up, roles can be assigned to agents, and the MARL framework updates accordingly during training.
    }
    \label{fig:mm_synthesis}
\end{figure}

\noindent MOISE+MARL introduces means to control or guide the agents' training in MARL. Its core contribution ar the \textbf{Constraint Guides}, which are three new relations introduced to describe the logics of roles and goals in the Dec-POMDP formalism:
%
\begin{itemize}
    % \begin{enumerate*}[label={\roman*) },itemjoin={; \quad}]
    
    \item \textbf{Role Action Guide} \quad $rag: H \times \Omega \rightarrow \mathcal{P}(A \times \mathbb{R})$, the relation that models a role as a set of rules which, for each pair consisting of a history $h \in H$ and an observation received by the agent $\omega \in \Omega$, associates expected actions $A \in \mathcal{P}(A)$ each associated with a constraint hardness $ch \in [0,1]$ ($ch = 1$ by default). By restricting the choice of the next action among those authorized, the agent is forced to adhere to the role's expected behavior
    \item \textbf{Role Reward Guide} \quad $rrg: H \times \Omega \times A \to \mathbb{R} = \{r_m \text{ if } a \notin A_\omega \text{, } rag(h, \omega) \allowbreak = \allowbreak A_\omega \times \mathbb{R} \text{, } h \in H; \text{ else } 0\}$, the relation that models a role by adding a penalty $r_m$ to the global reward if the last action chosen by the agent $a \in A$ is not authorized. This is intended to encourage the agent to adhere to the expected behavior
    \item \textbf{Goal Reward Guide} \quad $grg: H \rightarrow \mathbb{R}$, the relation that models a goal as a soft constraint adding a reward bonus $r_b \in \mathbb{R}$ if the agent's history $h \in H$ contains a goal's characteristic sub-sequence $h_g \in H_g$, encouraging the agent to reach it.
          % \end{enumerate*}
\end{itemize}

\noindent Finally, we introduce the \textbf{Linkers} to link the $\mathcal{M}OISE^+$ organizational specifications with constraint guides and agents:
%
\begin{itemize}
    % \begin{enumerate*}[label={\roman*) },itemjoin={; \quad}]
    
    \item \textbf{Agent to Role} \quad $ar: \mathcal{A} \to \mathcal{R}$, the bijective relation linking an agent to a role;
    \item \textbf{Role to Constraint Guide} \quad $rcg: \mathcal{R} \rightarrow rag \cup rrg$, the relation associating each $\mathcal{M}OISE^+$ role to a $rag$ or $rrg$ relation, forcing/encouraging the agent to follow the expected actions for the role $\rho \in \mathcal{R}$;
    \item \textbf{Goal to Constraint Guide} \quad $gcg: \mathcal{G} \rightarrow grg$, the relation linking goals to $grg$ relations, representing goals as rewards in MARL.
          % \end{enumerate*}
\end{itemize}

\paragraph{\textbf{Resolving the Dec-POMDP with MOISE+MARL}}

A MOISE+MARL model is defined as $\mathcal{MM} = \langle \mathcal{OS}, ar, rcg, gcg, rag, rrg, grg \rangle$.
Solving a Dec-POMDP with $mm \in \mathcal{MM}$ consists in finding a joint policy $\pi^j = {\pi^j_0, \pi^j_1, \dots, \pi^j_n}$ that maximizes the expected cumulative reward (or satisfies a minimal threshold), represented by the state-value function $V^{\pi^j}$. This value reflects the return from an initial state $s \in S$ when applying successive joint actions $a^j \in A^n$ under the additional organizational constraints.
%
The definition of $V^{\pi^j}$ follows the sequential and cyclic agent execution scheme (AEC mode), and is formalized in \hyperref[eq:single_value_function]{Definition 1}, incorporating role-based (in red) and mission-based (in blue) adaptations that influence both the action space and the reward.
\autoref{fig:mm_synthesis} illustrates how the MOISE+ specifications are integrated into the Dec-POMDP resolution via the MOISE+MARL framework.


\begin{figure*}[h!]
    \label{eq:single_value_function}
    \raggedright
    \textbf{\textit{Definition 1} \quad State-Value function adapted to constraint guides in AEC:}
    
    \begin{scriptsize}
        \vspace{-0.6cm}
        \begin{gather*}
            V^{\pi^j}(s_t) = \hspace{-0.75cm}
            %
            \sum_{\textcolor{red}{ \substack{a_{t} \in A \text{ if } rn() < ch_{t}, \\
                        a_{t} \in A_{t} \text{ else}}
                }}{\hspace{-0.7cm} \pi_i(a_{t} | \omega_t)}
            %
            \sum_{s_{t+1} \in S}
            %
            {\hspace{-0.1cm} T(s_{t+1} | s_t, a_{t})
            \Bigl[R(s_t,a_{t},s_{t+1}) + \hspace{-0.1cm}
            \textcolor{blue}{ \sum_{m \in \mathcal{M}_i}{ \hspace{-0.1cm} v_m(t) \frac{grg_m(h_{t+1})}{1 - p + \epsilon} } }
            + } \\
            {\textcolor{red}{(1-ch_t) \times rrg(\omega_t,a_{t+1})} + V^{\pi^j_{i+1 \ mod \ n}}(s_{t+1})\Bigr]}
        \end{gather*}
        %
        \vspace{-0.5cm}
        \textcolor{red}{\[\text{ \hspace{-0.1cm} With } rag(h_t, \omega_t) = A_{t} \times \mathbb{R} \text{, } \langle a_t, ch_{t} \rangle \in A_{t} \times \mathbb{R} \text{ ; } rn: \emptyset \to [0,1[ \text{, a uniform random function}\]}
        %
        \vspace{-0.6cm}
        \textcolor{blue}{
            \begin{gather*}
                \hspace{-0.001cm}
                \text{With } \omega_t = O(\omega_t | s_t, a_t) \text{ ; } h_t = \{h_0 = \langle \rangle, h_{t+1} = \langle h_t, \langle \omega_{t+1}, a_{t+1} \rangle \rangle \} \text{ ; } \epsilon \in \mathbb{R}_{>0} \text{ ; } grg_m(h) = 
            \end{gather*}
        }
        \vspace{-0.95cm}
        \textcolor{blue}{
            \begin{gather*}
                \hspace{-0.5cm} \sum_{\hspace{0.3cm}(grg_i,w_i) \in mo(m)}{\hspace{-0.9cm} w_i \times grg_i(h)}
                \text{ ; } v_m(t) = \{ 1 \text{ if } t \in t_c \text{ ; else } 0 \} \text{ ; } \mathcal{M}_i = \{m_j | \langle ar(i),m_j,t_c,p \rangle \in \mathcal{M}\}
            \end{gather*}
        }
        \vspace{-0.6cm}
    \end{scriptsize}
    
\end{figure*}

\noindent At each time step $t \in \mathbb{N}$ (starting from $t=0$), agent $i = t \bmod n$ is assigned to role $\rho_i = ar(i)$. For each temporally valid deontic specification $d_i = rds(\rho_i) = \langle tc_i, y_i, m_i \rangle$, the agent is permitted ($y_i = 0$) or obligated ($y_i = 1$) to commit to mission $m_i \in \mathcal{M}$, with goal set $\mathcal{G}_{m_i} = mo(m_i)$ and $n \in \mathbb{N}$ agents.
%
Upon observing $\omega_t$, the agent selects an action from $A_t$ (the role-expected actions) with probability $ch_t$, or from $A$ otherwise. If $ch_t = 1$, the agent is strictly constrained by its role.
%
The selected action transitions the system from $s_t$ to $s_{t+1}$, yields observation $\omega_{t+1}$, and returns a reward composed of:
i) bonuses for achieved goals in valid missions (via Goal Reward Guides), weighted by $\frac{1}{1 - p + \epsilon}$;
ii) penalties from the Role Reward Guide, scaled by $ch_t$.
%
The process continues in state $s_{t+1}$ with agent $(i + 1) \bmod n$.

\subsection{The TEMM method}
\label{sec:TEMM_algorithm}

The TEMM method is part of the explanation component of the MOISE+MARL framework. It leverages unsupervised learning techniques to infer organizational specifications from observed agent trajectories. It allows computing the organizational fit between emergent behaviors and expected roles, goals, and missions.

\textbf{1) Roles and role inheritance.} \quad TEMM defines a role $\rho$ as a policy whose agents share a \textit{Common Longest Sequence} (CLS) in their histories. A role $\rho_2$ inherits from $\rho_1$ if $\text{CLS}(\rho_2) \subseteq \text{CLS}(\rho_1)$. Hierarchical clustering is used to extract these CLSs and role hierarchies from trajectories. The \textbf{structural organizational fit} is computed as the distance between actual agent behaviors and inferred role sequences.

\textbf{2) Goals, plans, and missions.} \quad Goals are identified as clusters of joint-observations commonly reached in successful trajectories, using K-means over trajectory embeddings. Plans are inferred as sub-sequences of transitions that consistently lead to goals. A \textbf{mission} groups goals pursued collectively by one or more agents. The \textbf{functional organizational fit} quantifies how well current behaviors match inferred goals and missions.

\textbf{3) Permissions and obligations.} \quad Permissions and obligations are derived by examining whether agents fulfilling a role consistently (or exclusively) achieve certain missions under time constraints. Obligations imply exclusivity, whereas permissions imply optionality. The global \textbf{organizational fit} is obtained by aggregating structural and functional scores.

While clustering hyperparameters may require manual tuning to ensure robust role and goal extraction, TEMM offers a principled way to analyze emergent organizational behaviors and refine specifications accordingly.



\subsection{Learning World Models}

In Reinforcement Learning (RL), particularly under partial observability, \textbf{World Models}~\cite{ha2018recurrent, hafner2020dream} aim to learn internal models approximating both the environment's transition and observation dynamics. Such models enable agents to perform planning, improve sample efficiency, and facilitate safe exploration. This modeling approach belongs to the \textit{model-based RL} (MBRL) paradigm~\cite{moerland2020model}, and is especially useful for automatically constructing high-fidelity simulation models even when explicit environment representations are unavailable.

Formally, at each time step $t$, let $\omega_t \in \Omega$ denote the current high-dimensional observation, $a_t \in A$ the action taken, and $\tilde{h}_{t-1} \in \mathcal{H}$ the recurrent hidden state summarizing the interaction history up to $t-1$. Since observations are typically high-dimensional (e.g., images, complex state vectors), an encoder $Enc: \Omega \rightarrow Z$ is first applied to project observations into a compact latent space $Z$ with $z_t = Enc(\omega_t)$, where $\dim(Z) \ll \dim(\Omega)$.

The core temporal structure is modeled using a \textbf{Recurrent Latent Dynamics Model (RLDM)}~\cite{hafner2020dream} $\mathcal{T}^{z} = f(g(h_{t-1},z_t, a_t))$, which predicts the next latent state $z_{t+1}$ by updating the recurrent hidden state with $f$ and applying latent dynamics with $g$:
$h_t = f(h_{t-1}, z_t, a_t), z_{t+1} = g(h_t)$
where $f(\cdot)$ typically corresponds to a recurrent neural network (e.g., LSTM~\cite{hochreiter1997long}) applied to the concatenation of $h_{t-1}$, $z_t$, and $a_t$, and $g(\cdot)$ maps the recurrent state to the next observation latent representation (often implmented as an MLP~\cite{hochreiter1997long}).

The predicted latent state is then decoded via $Dec: Z \rightarrow \Omega$ into the predicted observation $\hat{\omega}_{t+1} = Dec(z_{t+1})$. The entire model is jointly trained to minimize both the \emph{reconstruction loss} $\|\omega_{t+1} - \hat{\omega}_{t+1}\|$ in observation space, and optionally, a \emph{latent prediction loss} to stabilize latent dynamics learning.

The recurrent hidden state $\tilde{h}_t$ serves as a compact summary of the full interaction history up to time $t$, avoiding the need to explicitly store long observation-action trajectories.
For simplicity of notation, we define the full composition that directly maps current observation, action, and recurrent state to the next predicted observation as the \textbf{Observation Prediction Model}:
\[
    \mathcal{T}(h_{t-1}, \omega_t, a_t) := Dec(g(f(h_{t-1}, Enc(\omega_t), a_t))) = \hat{\omega}_{t+1}.
\]



\section{The MAMAD method}\label{sec:mamad}

\subsection{General overview of the method}

The MAMAD~\footnotemark[1] method is built around four main activities: (1) modeling the environment, goal, and organizational constraints, (2) learning policies using various MARL algorithms, (3) analyzing behaviors and inferring organizational specifications with a proposed method, and (4) maintaining consistency between the simulated and real environments by deploying trained policies and updating the simulation. This approach guides the agent learning process while enforcing strict organizational constraints, ensuring the efficiency of the learned policies. The lifecycle of a MAMAD-designed MAS is illustrated in \autoref{fig:cycle}.
%
\begin{figure}[h!]
    \centering
    \input{figures/cycle.tex}
    \caption{Lifecycle of a MAS designed with MAMAD: i) Users start by modeling the environment from a sufficient amount of real trajectories (obtained by initially transfered agents) or any available one, global goal and design requirements as roles and goals; \quad ii) Then, they launch the training of agents with MARL techniques; \quad iii) A post-training analysis is performed to get insights into the emergent agents' roles and goals, guiding the improvement of the applied organizational specifications ; \quad v) Once validated, trained policies are launched to operate the environment's actuators, generating new traces for a better environment modeling}
    \label{fig:cycle}
\end{figure}
%
The MAMAD method frames MAS design as an iterative constrained optimization process. Given:
\begin{itemize}
    \item $\mathcal{E}_0$: the initial environment where agents can act;
    \item $\mathcal{G}_{\text{inf}}$: an informal description of the desired global goal;
    \item $\mathcal{C}_{\text{inf}}$: an informal specification of design constraints;
    \item $\gamma \in [0,1]$: the discount factor leading to long or short term solutions even though it often determined empirically (default is 1);
    \item $A, \Omega$: the action space and observation space respectively;
    \item $\texttt{org\_fit}_{min}$, $\overline{r}_{min}$, $\sigma_{min}$: respectively the minimum organizational fit level, average reward and standard deviation required to validate a trained joint policy. Usually, these values are typically determined empirically
\end{itemize}
%
\noindent As described in \autoref{alg:mamad}, the method's framework enables the continous design of a MAS as an iterative and asynchronous coordination of two distinct processes: the \textit{Transferring process}, which connects to the real environment and handles real-time execution and joint-history collection; and the \textit{MTA (Model-Train-Analyze) process}, which consumes stored data to iteratively improve the simulated environment model, the joint-policy, and the MAS organizational specifications.

\paragraph{Transferring process: policy deployment and data collection}

This process is always active while the MAS is operating in the real environment. Its role is twofold. First, it deploys the most recent joint policy $\pi^j_{\text{latest}}$ to the real agents, ensuring up-to-date behavior without interrupting execution. Second, it continuously monitors and collects agent trajectories in the form of joint histories $H^j$, buffering them in batches. Once a sufficient number of trajectories is collected, the batch is appended to the global trajectory store $\mathcal{D}_{H^j}$. If the update process is not already running, it triggers the launch of the \textit{MTA} process.

\paragraph{MTA process: policy optimization and organizational refinement}

This process models the current design problem and improves the MAS policy and associated organizational specifications. It first builds a JOPM $T^j$ using extended World Models with the collected trajectories. Design requirements are formalized as MOISE+MARL organizational specifications $\mathcal{MM}$ and the ultimate goal is formalized as an History-based Reward Function $R^j_H$ as well.
%
Then, a Markovian model is built out of previously modeled elements so that agents can be trained with organizational specifications using the MOISE+MARL framework. Once training is completed, the resulting joint policy $\pi^j$ is analyzed using TEMM to infer implicit organizational specifications $\mathcal{MM}_{\text{imp}}$ and compute the organizational fit score.

\paragraph{Refinement loop through organizational specifications}

If the learned policy exhibits low organizational fit, insufficient average performance, or high variability (compared to predefined thresholds), the implicit organizational specifications inferred during the analysis activity are used to refine the organizational specification. This refinement process may involve manual inspection of the inferred structures to identify the key success factors underlying the emergent behaviors. Informed by these insights, the designer can revise the initial organizational specification to better guide future training iterations.

This loop is repeated up to a maximum of $n_{refine}$ times, progressively steering the policy space toward more structured and performant behaviors. The latest validated policy is then saved as $\pi^j_{\text{latest}}$, ready for deployment in the real environment.

The refinement loop is particularly useful in complex environments where prior knowledge is limited or where manual design would be prohibitively costly. At each iteration, it helps narrow the policy search space by restricting it to regions associated with emergent organizational regularities identified during previous cycles. Remarkably, this process can begin without any predefined organizational specification, and through successive refinements, it can yield organizational constraints that are both objectively relevant and fully agnostic to human expertise or prior familiarity with the deployment environment.

\

\noindent The interplay between these two asynchronous processes creates a closed-loop, end-to-end MAS design lifecycle. The system continuously learns from real-world execution, updates its simulation model, retrains under evolving specifications, and deploys improved policies without requiring constant designer intervention. This architecture bridges symbolic AOSE principles with learning-based automation, ensuring compliance, adaptability, and organizational-level explainability.


\begin{algorithm}[H]
    \caption{MOISE+MARL Assisted Multi-Agent System Desgin (MAMAD)}
    \label{alg:mamad}
    \DontPrintSemicolon
    
    \KwIn{Initial environment $\mathcal{E}$, goal $\mathcal{G}_{\text{inf}}$, design constraints $\mathcal{C}_{\text{inf}}$, $n_{refine}$ max number of refinement cycles}
    \KwOut{A MAS deployed satisfying design, performance and explainability requirements; and associated organizational specifications}
    
    Initialize: $\mathcal{D}_{H^j} \gets \emptyset$, $\pi^j_{\text{latest}} \gets \pi^j_{\text{init}}$, $running\_MTA \gets False$ \;
    
    \vspace{0.3em}
    
    \While{MAS is active in environment $\mathcal{E}$}{
        \tcp*[l]{Transferring: retrieve trajectories \& deploy policy}
        $\mathcal{D}_{H^j}, \texttt{need\_update} \gets \text{transfer}(\pi^i_{latest}, \mathcal{D}_{H^j})$ \tcp*[r]{asynchronous call}
        \If{\texttt{need\_update} and not \texttt{running\_MTA}}{
            \texttt{launch\_MTA()} \tcp*[r]{asynchronous call}
        }
    }
    
    \vspace{1em}
    \SetKwProg{MTA}{Process \normalfont(MTA)}{}{}
    \MTA{}{}{
    
    $\texttt{running\_MTA} \gets True$ \tcp*[l]{Global variable assignment}
    
    \tcp*[l]{Modeling: model the real environment into a simulated model}
    $\mathcal{T}^j, R^j_H, \mathcal{MM} \gets \texttt{model}(\mathcal{E}, \mathcal{D}_{H^j}, \mathcal{G}_{\text{inf}}, \mathcal{C}_{\text{inf}}, \gamma, A, \Omega)$ \;
    
    \While{$i < n_{refine}$}{

    \vspace{0.5em}
    \tcp*[l]{Training: train policy under org. constraints}
    $\pi^j, \overline{r}, \sigma \gets \texttt{train}(\mathcal{T}^j, \mathcal{MM})$ \;

    \vspace{0.5em}
    \tcp*[l]{Analyzing: analyze policy to infer new org. specs}
    $(\mathcal{MM}_{\text{imp}}, \texttt{org\_fit}) \gets \texttt{analyze}(\mathcal{T}^j, \mathcal{MM}, \pi^j)$ \;
    
     \vspace{0.5em}
 
    \tcp*[l]{If policy not satisfying, retrain with new org. spec.}
    \If{$\texttt{org\_fit} < \texttt{org\_fit}_{min} \ or \ \overline{r} < \overline{r}_{min} \ or \  \sigma > \sigma_{min}$}{
        $\mathcal{MM} \gets \mathcal{MM}_{\text{imp}}$ \;
        back to 'Analyzing' \;
    }
        
    $\pi^j_{\text{latest}} \gets \pi^j$
        \tcp*[r]{Update most recent policy}

    $i \gets i + 1$
    
    }
    
    $\texttt{running\_MTA} \gets False$ \tcp*[l]{Global variable assignment}
    
    
    }
\end{algorithm}

\

\noindent One can point out that we propose to leverage a modeled simulated environment as a Digital Twin for a later training whereas MBRL both integrates environment modeling and training at the same time. Indeed, we favour decoupling environment modeling from training for : i) the reusability of the modeled environment in new agent training optionally requiring small adjustments ; \quad ii) the need for simple agents that do not embedded costly environment model for planning ; \quad iii) the need to have a high-fidelity modeled environment focusing all efforts on a common one for any agent.

\

\noindent In the following subsections, we detail each activity within the overall MAMAD framework, identifying the specific challenges encountered in achieving this objective and describing the proposed contribution and its use that address these challenges.

\subsection{Modelling}\label{sec:modelling}

The \textbf{Modelling activity} addresses the following formal component:
\begin{displaymath}
\texttt{model}(\mathcal{D}_{H^j}, \mathcal{G}_{\text{inf}}, \mathcal{C}_{\text{inf}}, \gamma, A, \Omega) = \mathcal{T}^j, R^j_H, \mathcal{MM} 
\end{displaymath}

\noindent The \textit{Modelling activity} aims to model the design problem as a constrained optimization problem. To do that, it first generates a high-fidelity simulated environment using the Joint-observation Prediciton Model (JOPM) $\mathcal{T}^j: H^j \times \Omega^j \times A^j \rightarrow \mathcal{H} \times \hat{\Omega^j}$ derived from real-world interaction traces $\mathcal{D}_{H^j}$. At time step $t \in \mathbb{N}$, for any recurrent hidden state $\tilde{h}_{t-1} \in \mathcal{H}$ representing joint-history until $t-1$, the currently received joint-observation $\omega_t^j \in H^j$ and the joint-action to be applied $a_t^j \in A^j$, $\mathcal{T}^j$ gives the next recurrent hidden state $\tilde{h}_t \in \mathcal{H}$ and the predicted next joint-observation $\hat{\omega}^j \in \hat{\Omega}^j$. This way, MAMAD enables building the environment from scratch. In addition to the simulated environment, the optimization problem also formalizes the informal goal description $\mathcal{G}_{\text{inf}}$ into $R^j_H: H^j \times \Omega^j \rightarrow \mathbb{R}$ the History-based Reward Function. Finally, the \textit{Modelling activity} also aims to formalize constraint stemming from informal design requirements as MOISE+MARL organizational specifications $\mathcal{MM}$ from $\mathcal{C}_{\text{inf}}$.

We assume to leave the work of formalizing informal design requirements into MOISE+MARL organizational specifications and informal goal description into an History-based Reward Function.

Since we are not able to access the real environment's state, we have to rely on stored joint-histories hence the idea to use World Models for its capability to generalize from a large amount of histories to compute hidden state transitions and observations transitions.
A major gap we encountered when willing to implement this function with World Models is the absence of explicitely defined World Models for Multi-Agent settings. Below, we propose an extension of the World Model framework for Multi-Agent settings.

\subsubsection*{Extension to Multi-Agent World Models}

In multi-agent settings, joint-observations rapidly become high-dimensional as the number of agents increases. To address this, joint encoding functions are introduced for both observations and actions.

Specifically, joint-observations $\omega_t^{j} = (\omega_t^1, \dots, \omega_t^n) \in \Omega^{j}$ are mapped to compact latent representations using an joint-observation encoder $Enc_{\omega^j}: \Omega^{j} \rightarrow z$, yielding $z_t = Enc_{\omega^j}(\omega_t^{j})$. The joint-observation decoder $Dec_{z}: z \rightarrow \hat{\Omega}^{j}$ allows reconstruction of the joint-observation if needed.
%
MLPs or attention-based architectures are typically employed for these encoders to aggregate multi-agent information into fixed-size feature vectors while capturing relevant inter-agent dependencies.

Once joint encoding is performed, the multi-agent world model operates analogously to the single-agent case, using the encoded observation $z_t$ in histories to the RLDM $\mathcal{T}^{z}$. This design enables scalable modeling while preserving critical interaction patterns between agents. Within the MAMAD framework, such world models instantiate the simulation core of the \hyperref[sec:modelling]{Modeling activity}, effectively serving as high-fidelity digital twins of the target environment.

\

\begin{algorithm}[H]
\caption{Modeling activity algorithm}
\label{alg:modeling}
\DontPrintSemicolon

\KwIn{Joint histories $\mathcal{D}_{H^j}$, informal goal $\mathcal{G}_{\text{inf}}$, informal constraints $\mathcal{C}_{\text{inf}}$, discount factor $\gamma$, action space $A$, observation space $\Omega$}
\KwOut{JOPM $\mathcal{T}^j$, History-based Reward Function $R^j_H$, MOISE+MARL specifications $\mathcal{MM}$}

\vspace{0.5em}
\tcp{1. Manual formalization of symbolic requirements}
$R^j_H \gets \texttt{manual\_formalize}(\mathcal{G}_{\text{inf}})$ \;
$\mathcal{MM} \gets \texttt{manual\_formalize}(\mathcal{C}_{\text{inf}})$ \;

\vspace{0.5em}
\tcp{2. Train encoders for joint-observations and actions}
Extract datasets $\Omega^j = \{\omega^j_t\}$ from joint-histories $\mathcal{D}_{H^j}$ \;
Train auto-encoder $(Enc_{\omega^j}, Dec_{\omega^j})$ on $\Omega^j$ minimizing reconstruction loss \;

\vspace{0.5em}
\tcp{3. Encode joint-observations in joint-history}
For each joint-history $h^j = \{\omega_t^j, a_t^j\} \in \mathcal{D}_{H^j}$, encode each joint-observation ${z}_t = Enc_{\omega^j}(\omega^j_t)$ to build training set $\mathcal{B} = \{ \{(z_t,a^j_t, z_{t+1})\} = h_z^j, h_z^j \in \mathcal{D}_{H^j}\}$

\vspace{0.5em}
\tcp{4. Train the RLDM}
Initialize the RLDM (f and g function) $\mathcal{T}^z = f(g)$

\For{$h_z^j \in \mathcal{B}$}{
    \For{$(z_t,a^j_t, z_{t+1}) \in h^j$}{
        Train RLDM $\mathcal{T}^{z}$ minimizing the MSE of predicted joint-observation $\hat{z}_{t+1}$ from the real one $z_{t+1}$.
    }
}

\vspace{0.5em}
\tcp{5. Save all initial joint-observations and form the JOPM}

$\Omega^{\mathcal{T}^j}_0 \gets \{\omega^j_0\}$ from joint-histories $\mathcal{D}_{H^j}$

$\mathcal{T}^j(h_{t-1}, \omega_t, a_t) = \langle f(h_{t-1}, Enc(\omega^j_t), a^j_t), Dec(\mathcal{T}^{z}(h_{t-1}, Enc(\omega^j_t), a^j_t)) \rangle$ \;

\vspace{0.5em}
\tcp{6. Return modelled elements}
\Return{$\mathcal{T}^j, \Omega^{\mathcal{T}^j}_0, R^j_H, \mathcal{MM}$}
\end{algorithm}



\subsection{Training}\label{sec:training}

The \textbf{Training activity} addresses the following formal component:
%
\begin{displaymath}
    \pi^j_i \gets \texttt{train}(\mathcal{T}^j, \Omega^{\mathcal{T}^j}_0, \gamma, R^j_H, \Omega, A, \mathcal{MM})
\end{displaymath}

The \textit{Training activity} aims to solve the modeled design problem as a constrained optimization task under the MOISE+MARL framework. However, a key limitation arises from the fact that MOISE+MARL operates under the Dec-POMDP formalism, which assumes full access to the true underlying state of the environment. In contrast, our approach relies exclusively on observable data (namely, agents' joint histories) without assuming access to real environmental states. To bridge this gap, a new Markovian formalism is needed, one that operates over observable sequences via the JOPM, while remaining compatible with existing MARL algorithmic frameworks.


\subsubsection{Extension of MOISE+MARL to Multi-Agent World Models}

\noindent In realistic settings, we rely solely on histories stacking transitions of actions and received observations. To better reflect this setting, we introduce a new formalism called the \textbf{Observation-based Dec-POMDP} (ODec-POMDP).
%
An ODec-POMDP $d_\Omega \in OD_\Omega$ (with $OD_\Omega$ the set of all Observation-based Dec-POMDPs) is defined as a 5-tuple:
%
$d_\Omega = \left(\Omega, A, \mathcal{T}^j, R^j_H, \gamma \right)$
%
where:
\begin{itemize}
    \item $A$: the action space.
    \item $\Omega$: the observation space.
    \item $\Omega^{\mathcal{T}^j}_0$: the recorded initial joint observation.
    \item $\mathcal{T}^j(h, \omega, a) = \langle {\tilde{h}}', \mathbb{P}(\omega' \mid h, \omega, a) \rangle$ the JOPM estimating the next joint-observation $\omega'$ based on the previous joint-history $\tilde{h} \in \mathcal{H}$, the most recent joint-observation $\omega$, and the current joint-action to be applied $a$. The JOPM also outputs the updated recurrent hidden state $\tilde{h}'$.
    \item $R^j_H: H \times \Omega \times A \times \Omega \rightarrow \mathbb{R}$ is the History-based Reward Function, returning the reward based on the previous joint-history, the last observation used to select last joint-action and the resulting next joint-observation.
    \item $\gamma \in [0, 1]$ is the discount factor.
\end{itemize}

\noindent This formulation allows MARL agents to operate purely on observable data, making it compatible with learned simulated environments. Considering the similarity of Dec-POMDP and ODec-POMDP, we encompass them into a same set we call \textbf{ODec-POMDP or Dec-POMDP} denoted \textbf{O$\backslash$Dec-POMDP} for convenience.

\paragraph{\textbf{Resolving the ODec-POMDP with MOISE+MARL}}

Solving a ODec-POMDP with $mm \in \mathcal{MM}$ consists in finding a joint policy $\pi^j = {\pi^j_0, \pi^j_1, \dots, \pi^j_n}$ that maximizes the expected cumulative reward (or satisfies a minimal threshold), represented by the observation-based value function $V_{\mathcal{T}^j}^{\pi^j}$. This value reflects the cumulated return from an initial joint-observation $\omega^j \in \Omega^{\mathcal{T}^j}_0$, joint-history $h^j$ and reccurent hidden state $\tilde{h}$ when applying successive joint actions $a^j \in A^n$ under the additional organizational constraints $\mathcal{MM}$ using the JOPM $\mathcal{T}^j$ instead of unknown observation and state transition functions.
%
The definition of $V_{\mathcal{T}^j}^{\pi^j}$ follows a parallel mode formalized in \hyperref[eq:single_value_function]{Definition 2}, incorporating role-based (in red) and mission-based (in blue) adaptations that influence both the joint-action space and the reward.
\autoref{fig:mm_synthesis} illustrates how the MOISE+ specifications are integrated into the ODec-POMDP resolution via the MOISE+MARL framework.

\begin{figure*}[h!]
    \label{eq:single_value_function}
    \raggedright
    \textbf{\textit{Definition 2} \quad Observation-Value function adapted to constraint guides in parallel mode:}
    
    \begin{scriptsize}
        \vspace{-0.6cm}
        \begin{gather*}
            V^{\pi^j}(\tilde{h}_{t-1},h^j_{t-1},\hat{\omega}^j_t) = \hspace{-0.95cm}
            %
            \sum_{\textcolor{red}{ \substack{a^j_{t} \in A^j \text{ if } rn() < ch_{t}, \\
                        a^j_{t} \in A^j_{t} \text{ else}}
                }}{\hspace{-0.9cm} \pi_i(a^j_{t} | \hat{\omega}^j_t)}
            %
            \hspace{-1.2cm}
            \sum_{\phantom{XXXX}(\tilde{h}_t,\hat{\omega}^j_{t+1}) \in \mathcal{H} \times \hat{\Omega}^j}
            %
            {\hspace{-1.2cm} \mathcal{T}^j(\langle \tilde{h}_t,\hat{\omega}^j_{t+1} \rangle | \tilde{h}_{t-1}, \hat{\omega}_t, a^j_{t})
            \Bigl[R^j_H(h^j_{t-1},\hat{\omega}^j_t,a^j_t,\hat{\omega}^j_{t+1}) \hspace{-0.1cm} }
        \end{gather*}
        %
        \vspace{-1.1cm}
        \begin{gather*}
            \hspace{4.5cm}
            {+ \  \textcolor{blue}{grg^j_m(h^j_t)}
            +
            \textcolor{red}{(1-ch_t) \times rrg^j(\hat{\omega}^j_t,a^j_{t+1})} + V^{\pi^j}(\tilde{h}_{t}, h^j_t, \hat{\omega}^j_{t+1})\Bigr]}
        \end{gather*}
        %
        \vspace{-0.15cm}
        %
        \[\hspace{-0.9cm}\text{With \ } \tilde{h}_{-1} = \mathbf{0} \text{ and } \tilde{\omega}^j_0 \in \Omega_0^{\mathcal{T}^j} \text{ ; } a^j_t = \langle a_{t,0}, a_{t,1} \dots a_{t,|\mathcal{A}|} \rangle \text{ ; } \omega^j_t = \langle \omega_{t,0}, \omega_{t,1} \dots \omega_{t,|\mathcal{A}|} \rangle \text{ ; }\]
        %
        \vspace{-0.25cm}
        \[\hspace{-5.85cm} h^j_t = \langle h_{t,0}, h_{t,1} \dots h_{t,|\mathcal{A}|} \rangle = \langle \langle h_{t-1,i}, \omega_{t,i}, a_{t,i} \rangle \rangle_{i \in \mathcal{A}}\]
        %
        \vspace{-0.2cm}
        \textcolor{red}{\[\hspace{-2.6cm}\text{ \hspace{-0.1cm} With } \langle rag_i, rrg_i \rangle = rcg(ar(i)) \text{ ; } rn: \emptyset \to [0,1[ \text{, a uniform random function}\]}
        %
        \vspace{-0.3cm}
        \textcolor{red}{\[A^j_t \times \mathbf{R}^{|\mathcal{A}|} = rag^j(h^j_t, \tilde{\omega}^j_t) = \langle rag_i(h_{t,i}, \omega_{t,i}) \rangle_{i \in \mathcal{A}} \text{ ; } rrg^j(h^j_t, \tilde{\omega}^j_t, a^j_t) = \sum_{i \in \mathcal{A}}{rrg_i(h_{t,i}, \omega_{t,i}, a_{t,i})}\]}
        %
        \vspace{-0.75cm}
        \textcolor{blue}{
            \begin{gather*}
                \hspace{-1.7cm} grg_m(h) = \hspace{-1cm} \sum_{\hspace{0.3cm}(grg_i,w_i) \in mo(m)}{\hspace{-1.1cm} w_i \times grg_i(h)}
                \text{ ; }
                grg^j_m(h^j_t) = \hspace{-0.1cm} \sum_{i \in \mathcal{A}}{\sum_{m \in \mathcal{M}_i}{ \hspace{-0.1cm} v_m(t) \frac{grg_m(h_{t,i})}{1 - p + \epsilon} }} \text{ ; } \epsilon \in \mathbb{R}_{>0} \text{ ; }
            \end{gather*}
        }
        \vspace{-0.9cm}
        \textcolor{blue}{
            \begin{gather*}
                \hspace{-4cm}
                v_m(t) = \{ 1 \text{ if } t \in t_c \text{ ; else } 0 \} \text{ ; } \mathcal{M}_i = \{m_j | \langle ar(i),m_j,t_c,p \rangle \in \mathcal{M}\}
            \end{gather*}
        }
    \end{scriptsize}
    
\end{figure*}

\noindent Considering a parallel mode, at each time step $t \in \mathbb{N}$ (starting from $t=0$), an agent $i \in \mathcal{A}$ is assigned to role $\rho_i = ar(i)$. For each temporally valid deontic specification $d_i = rds(\rho_i) = \langle tc_i, y_i, m_i \rangle$, the agent is permitted ($y_i = 0$) or obligated ($y_i = 1$) to commit to mission $m_i \in \mathcal{M}$, with goal set $\mathcal{G}_{m_i} = mo(m_i)$ and $n \in \mathbb{N}$ agents.
%
Upon observing $\tilde{\omega}^j_t$, each agent $i \in \mathcal{A}$ select an action from $A_{i,t} \text{ with } A^j_t = \langle A_{0,t}, A_{1,t}, A_{|\mathcal{A}|,t}\rangle$ (from Role Reward Guides) with probability $ch_t$, or from $A^t$ otherwise. If $ch_t = 1$, the agent is strictly constrained by its role.
%
The state and observation transitions due to the application of the selected actions are both approximated via the JOPM function $\mathcal{T}^j$ that takes the previous reccurrent hidden state up to $t-1$ $\tilde{h}_{t-1}$, the lastly received joint-observation $\hat{\omega}_t^j$ and lastly chosen joint-action $a_t^j$ to get the next predicted joint-observation $\hat{\omega}_{t+1}^j$ and the updated reccurrent hidden state up to $t$ $\tilde{h}_t$. The History-based Reward Function uses the last joint-history $h^j_{t-1}$, the last joint-observation $\hat{\omega}_t^j$, the chosen joint-action $a_t^j$ and the next joint-observation $\hat{\omega}^j_{t+1}$ to get the next reward. The reward is also updated by adding bonus/malus for:
i) achieving goals in valid missions (via Goal Reward Guides), weighted by $\frac{1}{1 - p + \epsilon}$;
ii) aligning with roles (via the Role Reward Guide), scaled by $(1-ch_t)$.

\begin{algorithm}[H]
    \caption{Training activity algorithm}
    \label{alg:training_mamad}
    \DontPrintSemicolon

    \KwIn{
        Joint-Observation Prediction Model (JOPM) $\mathcal{T}^j$,
        Initial joint observations $\Omega_0^{\mathcal{T}^j}$,
        History-based Reward Function $R_H^j$,
        Organizational specification $\mathcal{MM}$,
        Discount factor $\gamma$
    }
    \KwOut{$\pi^j$ : Trained joint policy}

    \vspace{0.3em}

    Initialize parameters of policy $\pi^j$ and replay buffer $\mathcal{B}$ \;

    \ForEach{episode $e = 1 \dots N$}{
        Sample $\omega_0^j \sim \Omega_0^{\mathcal{T}^j}$, set $\tilde{h}_{-1} \gets \mathbf{0}$ \;
        Initialize joint history $h_{-1}^j \gets \emptyset$ \;

        \ForEach{step $t = 0 \dots T$}{
            Compute $A_t^j = rag^j(h^j_t, \omega^j_t)$ via role reward guides from $\mathcal{MM}$ \;
            \If{$rn() < ch_t$}{
                Select $a_t^j \sim \pi^j(\cdot | \omega_t^j)$ within constrained $A_t^j$ \;
            }
            \Else{
                Select $a_t^j \sim \pi^j(\cdot | \omega_t^j)$ within $A_t$ \;
            }

            $(\tilde{h}_t, \omega_{t+1}^j) \gets \mathcal{T}^j(\tilde{h}_{t-1}, \omega_t^j, a_t^j)$ \tcp*{JOPM prediction}

            $r_t \gets \gamma^t \times R_H^j(h^j_{t-1}, \omega_t^j, a_t^j, \omega_{t+1}^j)$ \tcp*{Base reward}

            $r_t \gets r_t + grg^j(h^j_t)$ \tcp*{Goal Rew. Guides}


            $r_t \gets r_t + (1 - ch_t) \times rrg^j(h^j_t, \omega_t^j, a_t^j)$ \tcp*{Role Rew. Guides}

            Append $(\omega_t^j, a_t^j, r_t, \omega_{t+1}^j)$ to $\mathcal{B}$ \;
            Update $h^j_t \gets \langle \langle h_{t-1,i}, \omega_{t,i}, a_{t,i} \rangle \rangle_{i \in \mathcal{A}}$ \;

            Train $\pi^j$ with minibatches from $\mathcal{B}$ using any MARL method \;
        }
    }

    \Return{$\pi^j$}
\end{algorithm}


\subsection{Analyzing}\label{sec:analyzing}

\noindent The \textbf{Analyzing activity} addresses the following formal component:
\[
    (\mathcal{MM}_{i,\text{implicit}}, \text{OF}) \gets \texttt{analyze}(d_i, \mathcal{MM}_i, \pi^j_i, d_\Omega)
\]

\noindent The objective of this phase is twofold: (i) to provide an explanation of the learned joint policy in terms of MOISE+MARL organizational specifications (roles, goals, missions), and (ii) to compute the organizational fit, which quantifies the alignment between the learned behaviors and a regular organization whether explicit or inferred.
%
To achieve this, we rely on the \textbf{TEMM} method~\cite{soule2025moisemarl}. TEMM assumes that agent behaviors (despite superficial variability) exhibit regularities when achieving comparable cumulative rewards. Thus, behaviors that appear different may be interpreted as noisy variations of a limited number of latent strategies. According to the law of large numbers, averaging over a sufficient set of successful joint histories can help filter out such noise, revealing typical strategies.

Observation trajectories are clustered using distance-based metrics (e.g., LCS, Smith-Waterman), forming groups of agents exhibiting similar behavior. For each cluster, a centroid trajectory is computed where each timestep is associated with an average observation. For each average observation in the centroid, we evaluate the normalized inverted variance as the \textbf{representativeness}. A high representativeness at a given step is likely to indicate the step average observation is close to the surrounding real ones and therefore often received by agents A low representativeness at a given step is likely to indicate the average observation is not representativeness of the surrounding ones for they are sparsely distributed for instance. The regular \textbf{variance} is also computed along the entire centroid.
%
Using a minimal representativeness, a sampling mechanism selects the most salient observations from each trajectory (those most frequently visited by agents across successful runs). These are grouped and interpreted into \textbf{intermediate goals}, reflecting key milestones agents must reach en route to achieving the global objective. These sets of representative observations form the basis for goal inference. If the minimal representativeness is high, only very frequent and representative observations will be selected and grouped into goals. If the minimal representativeness is low, all observation and possibly sparsely distributed observation could be grouped into goals, leading to poorly representative and overfitting goals. Finally, assigning semantic labels to each goal set of observations is left as a manual post-processing step. Indeed a goal is a set of observations which are the union of all real observations for each wisely sampled time step along all observation trajectory.

Similarly, agent trajectories composed of transitions $(\omega, a) \in \Omega \times A$ are clustered, optionally using one-hot encodings for categorical actions. Each cluster yields a transition centroid with average transitions per timestep. Step and global variances are again computed to assess the representativeness of each transition and the whole centroid respectively.
%
Using a minimal representativeness, a selection process selects the transitions having a minimal representativeness, assumed to reflect behavioral "rules" followed by agents playing a consistent functional role. These rules form the basis for inferring implicit roles. A low representativeness may lead to include all transitions at every step instead of retaining the most salient ones as in the case of a high representativeness. Again, naming and interpreting each set semantically is left to manual intervention. Indeed a role is a set of rules which are the union of all real transitions for each wisely sampled time step along all transition trajectory.

To quantify how closely the learned policy aligns with an implicit organization, we leverage the concept of \textbf{organizational fit}, decomposed into two components:
%
\begin{itemize}
    \item \textbf{Structural Organizational Fit (SOF)} reflects how consistently agents adhere to implicit role-based behaviors. We propose to compute SOF as the normalized inverse of the average global variance of transition-based trajectories within each role cluster. Lower variance implies stronger behavioral regularity, and hence greater alignment with an underlying role structure.
    \item \textbf{Functional Organizational Fit (FOF)} captures the coherence of agents' behaviors in terms of reaching intermediate goals. It is computed as the normalized inverse of the average global variance of observation-based trajectories within each goal cluster. Low variance here indicates that agents consistently follow similar paths toward achieving functional objectives.
\end{itemize}
%
The overall \textbf{organizational fit} is then defined as the mean of these two scores: $\text{OF} = \frac{1}{2} \left( \text{SOF} + \text{FOF} \right)$

A high organizational fit value indicates that the inferred specifications (roles and goals) extracted through TEMM are highly representative of the actual learned behaviors. Conversely, a low organizational fit suggests that the learned behaviors are inconsistent or weakly structured, implying that the extracted specifications may not faithfully reflect the agents' implicit strategies.

A significant problem we encountered is the empiricality of the current TEMM method that imply designers to manually check which hyper-parameters are better to use to get more accurate and consistent organizational specifications and organizational fit. Such parameters include the \textbf{distance metrics} (LCS, Smith-Waterman, Euclidian, etc.), \textbf{minimal cluster distance} to determine clusters, structural and functional \textbf{minimal representativeness} to get the most relevant set of transitions and observations likely to determine roles and goals throughout steps. Having low minimal representativeness may result in overfitting roles and goals since they include all real observations and transitions while having high minimal representativeness may increase the convergence time since agents are less guided or constrained since they include a reduced number of transitions and observations. Determining such hyper-parameters by hand is time-consuming while hinder automating the analyzing process.

\subsubsection{Extended TEMM method with HPO}
To address this issue, we propose an Hyper-parameter Optimization (HPO) process for the Analyzing process consists in a grid search of all hyper-parameters to maximize. This HPO process follows:
%
(i) for observations and transitions, respectively apply a joint grid search on the distance metrics, minimal cluster distance to determine clusters so that we minimize the number of cluster and maximize the organizational fit (i.e trying to minimizing the variance among automatically determined clusters);
%
(ii) we also need to determine the appropriate structural and function minimal representativeness for sampling transition and observation trajectories. As illustrated in \autoref{fig:conv_time_repr}, decreasing the minimal representativeness leads to a larger set of observations and transitions being included in the inferred goals and roles. While this improves coverage, it can also reduce the robustness and generalizability of the learned specifications, as they may overfit to idiosyncratic behaviors.
%
Conversely when the minimal representativeness is high, very few observations or transitions are retained, which limits the guiding effect of organizational constraints—leading to convergence times comparable to unconstrained MARL. As the minimal representativeness decreases, convergence time decreases rapidly (due to stronger guidance threshold) until reaching a plateau, beyond which further decreases bring a slow convergence to a nul convergence time.
%
As a trade-off, for transition or observation trajectories, we select the representativeness value corresponding to the \textit{elbow point} at the onset of the convergence-time plateau. By default, we consider this is indeed the maximum representativeness giving a 3.5\% normalized convergence to get a minimal cumulated reward. By choosing this representativeness tradeoff, we assume to be enable not to generate overfitted but meaningful specification of roles (as rules) and goals (as states), achieving convergence performance close to that of fully covering specifications, but with lower complexity and improved interpretability.

This overall approach allows automatic calibration of the minimal cluster distance, the distance metric and the structural and functional minimal representativeness, enabling the extraction of concise and effective organizational specifications directly from agent trajectories.

\begin{figure}[h!]
    \centering
    \includegraphics[trim=0cm 0cm 0cm 0cm, clip, width=1.\linewidth]{figures/convergence_time_relative_to_representativeness.pdf}
    \caption{General normalized time to converge to a minimal cumulated reward over minimal representativeness}
    \label{fig:conv_time_repr}
\end{figure}

\begin{algorithm}[H]
    \caption{Analyzing activity algorithm}
    \label{alg:auto_temm}
    \DontPrintSemicolon

    \KwIn{
            Trained joint policy $\pi^j$;
            ODec-POMDP $d_\Omega$;
            Initial specification $\mathcal{MM}$;
            Target normalized convergence threshold (default: 3.5\%) $\eta$
    }

    \KwOut{
            Inferred organizational specification$\mathcal{MM}_{\text{implicit}}$;
            Organizational Fit score $\text{OF}$
    }

    \tcp*[l]{1. Collect trajectories}
    Generate individual histories $\mathcal{D}_{\text{trans}}$ from $d_\Omega$ under $\pi^j$ \;
    $\mathcal{D}_{\text{obs}} \gets$ individual observation trajectories from $\mathcal{D}_{\text{full}}$ \;

    \tcp*[l]{2. HPO on clustering distance and threshold}
    \For{$t \in \{obs,trans\}$}{
        \ForEach{distance metric $\delta_t \in \{0.0\dots\frac{i}{n}\dots1.0\}_{i \in \{0\dots n\}}$}{
            \ForEach{minimal cluster distance $\tau_t \in \{0.0\dots\frac{i}{n}\dots1.0\}_{i \in \{0\dots n\}}$}{
                Cluster $\mathcal{D}_{\text{obs}}$ and $\mathcal{D}_{\text{trans}}$ using $(\delta_t, \tau_t)$ \;
                Compute: $\sigma_{\text{obs}}, \sigma_{\text{trans}}, N_{\text{clusters}}$ \;
                Score $\gets \alpha (\sigma_{\text{obs}} + \sigma_{\text{trans}}) + \beta N_{\text{clusters}}$ \tcp*[l]{default: $\alpha=0.4,\beta=0.6$}
                Retain $(\delta_t^*, \tau_t^*)$ with minimal Score \;
            }
        }
    }

    \tcp*[l]{3. Apply clustering with optimal HPO parameters}
    Cluster $\mathcal{D}_{\text{obs}}$ into $C_{obs}$ clusters $(\delta_{trans}^*, \tau_{trans}^*)$ \;
    Cluster $\mathcal{D}_{\text{trans}}$ into $C_{trans}$ clusters using $(\delta_{trans}^*, \tau_{trans}^*)$ \;

    \tcp*[l]{4. HPO on repr. to evaluate convergence times as func. $ct_t$}
    \For{$t \in \{obs,trans\}$}{
        \ForEach{representativeness $\rho_t \in \{0.0\dots\frac{i}{n}\dots1.0\}_{i \in \{0\dots n\}}$}{
            Infer org. spec. $\mathcal{MM}_{\rho_t}$ from clusters using $\rho_t$ \;
            Initialize empty joint policy $\pi^j_{\rho_t}$ \;
            Train $\pi^j_{\rho_t}$ on $(d_\Omega, \mathcal{MM}_{\rho_t})$ until $R_{cum} \geq R_{\min}$ \;
            Record normalized conv. time $c_{\rho_t}$ so that $ct_t(\rho_t) = c_{\rho_t}$ \;
        }

        \tcp*[l]{5. Select repr. for threshold $\eta$ (by default $3.5\%$)}
        $\rho_t^* \gets max(\{\rho_t \in \{0.0\dots\frac{i}{n}\dots1.0\}_{i \in \{0\dots n\}} | ct_t(\rho_t) < \eta \})$ \tcp*[r]{Elbow point}

    }

    \tcp*[l]{6. Final role/goal inference using TEMM}
    Infer roles from $\mathcal{D}_{\text{trans}}, \delta^*, \tau^*, \rho^*$ \;
    Infer goals from $\mathcal{D}_{\text{obs}}, \delta^*, \tau^*, \rho^*$ \;

    \tcp*[l]{7. Compute Organizational Fit (OF)}
    Compute SOF (structural) and FOF (functional) from intra-cluster variances \;
    $\text{OF} \gets \frac{1}{2}(\text{SOF} + \text{FOF})$ \;

    \Return{$\mathcal{MM}_{\text{implicit}}, \text{OF}$}
\end{algorithm}


\subsection{Transferring}\label{sec:transferring}

\noindent The \textbf{Transferring activity} addresses the following formal component:
\[
\mathcal{D}_{H^j}, \texttt{need\_update} \gets \texttt{transfer}(\pi^j_{\text{latest}}, \mathcal{E}, \mathcal{D}_{H^j})
\]


\subsection{Transferring}\label{sec:transferring}

The \textbf{Transferring activity} addresses the following formal component of the MAMAD loop:
\[
    (\mathcal{D}_{H^j}, \texttt{need\_update}) \gets \texttt{transfer}(\pi^j_{\text{latest}}, \mathcal{E}, \mathcal{D}_{H^j})
\]

The \textit{Transferring activity} fulfills two main goals: (1) continuously deploy the most recent joint policy $\pi^j_{\text{latest}}$ into the real environment $\mathcal{E}$ to ensure agents act and interact effectively, and (2) collect fresh real-world joint trajectories $(\omega^j_t, a^j_t, \omega^j_{t+1})$ to enrich the trajectory dataset $\mathcal{D}_{H^j}$ used for updating the simulated environment and organizational specifications.

To the best of our knowledge, no framework exists that provides both asynchronous policy deployment and automatic, threshold-triggered joint-history collection in a closed-loop manner while enabling synchronization with the learning pipeline. The lack of such infrastructure hampers automation of MAS design cycles in real environments.

\paragraph{Transferring framework}
We propose a general theoretical framework that implements an asynchronous and event-driven control system responsible for policy execution and trace harvesting. It maintains a buffer of trajectories, triggers the retraining pipeline once a size threshold is met, and ensures the most recent policy is continuously applied. This control logic is structured around two mechanisms:
(i) a \texttt{Transfer Loop} that handles real-time deployment and data collection, and
(ii) an \texttt{Update Trigger} that launches the full design process once sufficient data is available. The system avoids launching multiple parallel updates and guarantees synchronization between transfer and modeling phases.

\vspace{-0.3em}
\begin{algorithm}[H]
    \caption{Transferring activity}
    \label{alg:transferring}
    \DontPrintSemicolon
    \KwIn{Current policy $\pi^j_{\text{latest}}$, real environment $\mathcal{E}$, trajectory store $\mathcal{D}_{H^j}$}
    \KwOut{Updated trajectory dataset $\mathcal{D}_{H^j}$, update signal $\texttt{need\_update}$}
    
    \vspace{0.3em}
    \SetKwProg{Transfer}{Procedure \normalfont TransferLoop}{}{}
    \Transfer{}{

        \While{MAS is active in environment $\mathcal{E}$}{
            \tcp*[l]{Execute latest policy in environment}
            $\omega^j_t \gets \texttt{observe}(\mathcal{E})$ \;
            $a^j_t \gets \pi^j_{\text{latest}}(\omega^j_t)$ \;
            $\omega^j_{t+1} \gets \texttt{apply}(\mathcal{E}, a^j_t)$ \;
            Append $(\omega^j_t, a^j_t, \omega^j_{t+1})$ to temporary buffer $\mathcal{B}$ \;
            
            \tcp*[l]{Check whether retraining is needed}
            \If{$|\mathcal{B}| \geq \texttt{batch\_size}$}{
                Add $\mathcal{B}$ to $\mathcal{D}_{H^j}$ and clear $\mathcal{B}$ \;
                $\texttt{need\_update} \gets \texttt{True}$ \;
                \If{$\texttt{running\_update} = \texttt{False}$}{
                    \texttt{launch\_update()} \tcp*[r]{Asynchronous call}
                }
            }
        }
    }
\end{algorithm}

This mechanism ensures (i) continuity of execution, (ii) reactivity to data freshness, and (iii) automation of update cycles. It forms the bridge between the simulated world and the deployment context by continually adjusting the design loop to the evolving environment, a core requirement for long-term MAS autonomy.


\section{Experimental setup}
\label{sec:experimental_setup}

We developed a tool that we propose to facilitate the implementation of the MAMAD method through four environments following a proposed evaluation protocol.

\subsection{CybMASDE: A Development Environment to Implement the MAMAD Method}
\label{sec:cybmasde}

To support the implementation and execution of the MAMAD method in a reproducible and modular way, we developed the \textbf{Cyber Multi-Agent System Development Environment (CybMASDE)}~\footnotemark[1], a dedicated platform that orchestrates the modeling, training, analysis, and deployment of MAS driven by the MOISE+MARL framework.

\footnotetext[1]{Source code and documentation available at \url{https://github.com/julien6/CybMASDE}}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.7\linewidth]{figures/CybMASDE.png}
    \caption{Overview of the CybMASDE platform. It supports the four MAMAD phases via modular APIs and tools, integrating simulation, MARL, organizational modeling, HPO and evaluation.}
    \label{fig:cybmasde}
\end{figure}

\paragraph{General Architecture.}
CybMASDE is implemented in Python 3.10 and built around four core modules matching the MAMAD phases: \texttt{Modeling}, \texttt{Training}, \texttt{Analyzing}, and \texttt{Transferring}. Each module is modular, extensible, and interoperable via a shared file store and API endpoints, enabling both standalone execution and asynchronous pipelines. Execution is containerized via Docker or Singularity for cross-platform reproducibility.

\subsubsection*{Modeling}

This module constructs a high-fidelity simulation environment (JOPM), and formalizes the goal and constraints of the optimization problem.

\begin{itemize}
    \item \textbf{World Models:} Joint Observation Prediction Model (JOPM) implemented using PyTorch with LSTM-based dynamics trained from joint histories $\mathcal{D}_{H^j}$.
    \item \textbf{Encoders:} VAE-based encoders for joint observations and MLP encoders for joint actions.
        \begin{itemize}
            \item Observation latent dim $\in \{16, 32, 64\}$, decoder architecture: 2-layer MLP
            \item LSTM hidden size: 64 or 128; optimizer: Adam, learning rate $\in [1e^{-4}, 5e^{-4}]$
        \end{itemize}
    \item \textbf{Goal Function:} A history-based reward function $R^j_H$ manually derived from $\mathcal{G}_{\text{inf}}$.
    \item \textbf{Organizational Specification:} MOISE+MARL specifications $\mathcal{MM}$ are manually encoded from $\mathcal{C}_{\text{inf}}$ using the \textbf{MOISE+MARL API (MMA)}.
\end{itemize}

\subsubsection*{Training}

This module solves the modeled problem via MOISE+MARL using ODec-POMDP.

\begin{itemize}
    \item \textbf{MARL Algorithms:} Implemented via the MARLlib~\cite{hu2022marllib} library.
        \begin{itemize}
            \item Supported: MAPPO, MADDPG, QMix, IQL, VDN, ROMA
        \end{itemize}
    \item \textbf{Constraint Integration:} MMA applies role-based action masking and reward shaping at runtime via wrappers.
    \item \textbf{Training Backends:} Based on Ray RLlib for distributed training.
    \item \textbf{Hyper-parameters:}
        \begin{itemize}
            \item Learning rate: $[1e^{-4}, 5e^{-4}]$
            \item PPO clip: $[0.1, 0.3]$
            \item Discount factor: $[0.9, 0.99]$
            \item Batch size: $\{64, 128\}$; MLP layer sizes: $\{64, 128, 256\}$
        \end{itemize}
\end{itemize}

\subsubsection*{Analyzing}

This module implements the Auto-TEMM method for inferring roles and goals and computing organizational fit.

\begin{itemize}
    \item \textbf{Clustering:} Trajectories are clustered using hierarchical clustering and DTW, LCS, Smith-Waterman or Euclidean distances.
    \item \textbf{Hyper-Parameter Optimization:} \textbf{Optuna} is used to:
        \begin{itemize}
            \item Select optimal \texttt{distance\_metric} $\in$ \{LCS, DTW, Smith-Waterman\}
            \item Tune \texttt{min\_cluster\_distance} $\in [0.1, 0.5]$ to minimize intra-cluster variance and number of clusters
            \item Determine \texttt{min\_representativeness} via sweep $[0.0, 1.0]$ to minimize convergence time to a target reward threshold (default: 3.5\%)
        \end{itemize}
    \item \textbf{Metrics:}
        \begin{itemize}
            \item Structural Organizational Fit (SOF): based on average transition cluster variance
            \item Functional Organizational Fit (FOF): based on observation cluster variance
        \end{itemize}
\end{itemize}

\subsubsection*{Transferring}

This module asynchronously deploys trained policies and collects trajectories from the real environment.

\begin{itemize}
    \item \textbf{Deployment:} Real agents are interfaced via PettingZoo API or external APIs (e.g., WebSocket for Kubernetes, UDP for CybORG).
    \item \textbf{Trajectory Store:} Episodes are saved to disk when a batch threshold is reached (default: 512 steps).
    \item \textbf{Triggering:} When buffer is full, the Modeling–Training–Analyzing loop is triggered.
    \item \textbf{Policy Update:} The most recent policy $\pi^j_{\text{latest}}$ is deployed continuously in the environment.
\end{itemize}

\paragraph{Environments and Interfaces.}
CybMASDE supports:
\begin{itemize}
    \item Standard MARL benchmarks: Overcooked-AI, MPE, Predator-Prey
    \item Cyber-defense environments: CybORG (CAGE challenge)
    \item Custom APIs (e.g., Kubernetes cluster control)
\end{itemize}

\paragraph{Extensibility.}
Users can plug new algorithms, clustering techniques, visualization tools, or organizational formalisms via well-documented interfaces. YAML-based configuration ensures full experiment reproducibility.




\subsection{Computing resources}

All experiments were conducted on an academic high-performance computing cluster, utilizing various configurations of GPU nodes. Specifically, we employed nodes with:
\begin{itemize}
    \item \textbf{GPUs:} NVIDIA A100, AMD MI210 ;
    \item \textbf{Frameworks:} TensorFlow, PyTorch ;
    \item \textbf{Hyperparameter tuning:} \textbf{Optuna}~\cite{akiba2019optuna} for learning rate, exploration-exploitation balance, and network architecture.
\end{itemize}

Each algorithm-environment combination was executed on 5 parallel instances to ensure robust and consistent results.

\subsection{Test environments and organizational specifications}

To evaluate the MAMAD method, we employ four distinct multi-agent environments that serve as controlled testbeds. These environments span different problem domains, requiring coordination, strategic decision-making, and role-based interactions. Each environment is formally described below, including its state space, observation space, action space, reward structure, and overall goal. Additionally, we provide the corresponding organizational specifications, detailing roles, missions, and constraints used in the MAMAD framework. A summary of 

\begin{table}[h!]
    \centering
    \begin{footnotesize}
        \renewcommand{\arraystretch}{1.3}
        \begin{tabular}{p{2cm}p{2.2cm}p{2.2cm}p{2.2cm}p{2.2cm}}
            \hline
            \textbf{Key Aspect} & \textbf{CybORG}                & \textbf{Overcooked-AI} & \textbf{Predator-prey} & \textbf{Warehouse Mgmt} \\ \hline
            Realism             & Cyber-defense, dynamic threats & Human-like teamwork    & Abstract communication & Logistic workflow       \\ \hline
            Emergent Roles      & Firewall, cleaner, rescuer     & Cooker, deliverer      & Speaker, listener      & Picker, crafter, packer \\ \hline
            Goal Structure      & Multi-activity missions           & Sequential subtasks    & Shared goal            & Ordered pipeline        \\ \hline
            Observability       & Noisy, partial views           & Occlusion, congestion  & Requires messaging     & Local and shared zones  \\ \hline
            Org. Fit Evaluation & Coherence under attack         & Task delegation        & Comms-based roles      & Coordination efficiency \\ \hline
        \end{tabular}
        \caption{Key characteristics of environments used to assess MAMAD}
        \label{tab:mamad_env_characteristics}
    \end{footnotesize}
\end{table}


\paragraph{Warehouse Management (WM)}
The \textbf{Warehouse Management}~\cite{warehouse_management} environment models a grid-based logistics warehouse where multiple robots must collaborate to transport goods efficiently. The environment is inspired by industrial warehouse automation scenarios and serves as an ideal testbed for evaluating task allocation, role specialization, and real-time coordination. This environment is illustrated in \autoref{fig:warehouse}.

\begin{itemize}
    \item \textbf{State Space:} A $N \times M$ grid where each cell contains a robot, a product, a crafting machine, or a drop-off location. The system tracks agent positions, inventory levels, and machine states ;
    \item \textbf{Observation Space:} Each agent has a local $V \times V$ view, perceiving products, teammates, and nearby machines ;
    \item \textbf{Action Space:}  ;
          \begin{itemize}
              \item Move: \texttt{Up, Down, Left, Right} ;
              \item Interact: \texttt{Pick Product, Drop Product}.
          \end{itemize}
    \item \textbf{Reward Structure:} ;
          \begin{itemize}
              \item Successful product delivery: $+10$ ;
              \item Inefficient movement: $-1$ per unnecessary step ;
              \item Product mishandling: $-5$ for incorrect drop-offs.
          \end{itemize}
    \item \textbf{goal:} Transport raw materials to processing machines and deliver finished products to drop-off locations.
\end{itemize}

\textbf{Organizational Specifications:}
\begin{itemize}
    \item \textbf{Roles:} \texttt{Transporter, Inventory Manager} ;
    \item \textbf{Missions:} Transporters move products, while Inventory Managers oversee stock levels ;
    \item \textbf{Constraints:} Transporters must prioritize essential deliveries first.
\end{itemize}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.6\linewidth]{figures/wm.png}
    \caption{A screenshot of the Warehouse Management environment: agents can move up, down, left, and right, multiple agents operate within a warehouse grid, performing tasks to process and deliver products. Agents can move in four directions (up, down, left, right) and interact with pick/drop zones when adjacent. The workflow involves: (i) collecting primary products from input conveyor pick/drop areas (blue zones); (ii) transporting them to crafting machine pick/drop areas (brown zones), where the primary products are transformed into a single secondary product based on a predefined crafting schema; (iii) retrieving the resulting secondary products and delivering them to output conveyor pick/drop areas (pink zones). Successful operation requires agents to coordinate their movements and actions to optimize throughput and efficiency within the warehouse.}
    \label{fig:warehouse}
\end{figure}

\paragraph{Predator-Prey (PP)}
The \textbf{Predator-Prey} environment is a well-known MARL benchmark~\cite{lowe2017multi}, designed to evaluate coordination among cooperative pursuers (predators) attempting to capture an evasive agent (prey). This environment is illustrated in \autoref{fig:predator_prey}.

\begin{itemize}
    \item \textbf{State Space:} A continuous 2D space where agents (predators and prey) have $(x, y)$ positions and velocities ;
    \item \textbf{Observation Space:} Agents sense nearby entities within a limited radius $r$ ;
    \item \textbf{Action Space:}  ;
          \begin{itemize}
              \item Move: \texttt{Up, Down, Left, Right, Stay}.
          \end{itemize}
    \item \textbf{Reward Structure:} ;
          \begin{itemize}
              \item Predators gain $+50$ for capturing the prey ;
              \item The prey earns $+1$ per timestep survived ;.
          \end{itemize}
    \item \textbf{goal:} Predators must cooperate to trap the prey, while the prey attempts to escape as long as possible.
\end{itemize}

\textbf{Organizational Specifications:}
\begin{itemize}
    \item \textbf{Roles:} \texttt{Predator, Prey} ;
    \item \textbf{Missions:} Predators coordinate to enclose the prey; prey seeks optimal escape routes ;
    \item \textbf{Constraints:} Predators must balance aggressive pursuit with blocking strategies.
\end{itemize}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.6\linewidth]{figures/predator_prey.png}
    \caption{A screenshot of the Predator-Prey environment: \textbf{green agents} (cooperative) and \textbf{red agents} (adversarial). The green agents aim to collect food items scattered across the environment while avoiding detection by the red agents. The environment includes \textbf{forest regions} that provide concealment; when a green agent enters a forest, it becomes partially or fully hidden from the red agents' observations. One red agent acts as a \textbf{leader} with enhanced observational capabilities and can communicate with other red agents to coordinate their pursuit.}
    \label{fig:predator_prey}
\end{figure}

\paragraph{Overcooked-AI (OA)}
The \textbf{Overcooked-AI} environment~\cite{Carroll2019} simulates a cooperative cooking scenario where agents must collaborate to prepare and serve meals in a structured kitchen. This environment is illustrated in \autoref{fig:overcooked}.

\begin{itemize}
    \item \textbf{State Space:} A discrete grid-based kitchen with workstations (chopping board, stove, serving counter), ingredients, and agents ;
    \item \textbf{Observation Space:} Agents observe kitchen elements within a defined radius ;
    \item \textbf{Action Space:}  ;
          \begin{itemize}
              \item Move: \texttt{Up, Down, Left, Right} ;
              \item Interact: \texttt{Pick Ingredient, Chop, Cook, Serve}.
          \end{itemize}
    \item \textbf{Reward Structure:} ;
          \begin{itemize}
              \item Successful meal preparation: $+20$ ;
              \item Ingredient misplacement: $-5$ ;
              \item Idle behavior: $-1$ per step without meaningful action.
          \end{itemize}
    \item \textbf{goal:} Maximize completed meal orders within a fixed time limit.
\end{itemize}

\textbf{Organizational Specifications:}
\begin{itemize}
    \item \textbf{Roles:} \texttt{Chef, Assistant, Server} ;
    \item \textbf{Missions:} The Chef prepares food, the Assistant supplies ingredients, and the Server delivers meals ;
    \item \textbf{Constraints:} Task execution must be synchronized to prevent bottlenecks.
\end{itemize}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.6\linewidth]{figures/overcooked.png}
    \caption{A screenshot of the Overcooked-AI environment: Two agents (chefs) must collaborate to prepare and serve onion soups efficiently. The process involves collecting three onions (one at a time) from the dispenser, placing them into a cooking pot, waiting for the soup to cook, retrieving a clean dish, plating the soup, and delivering it to the serving counter. The kitchen layout includes obstacles and narrow pathways, requiring agents to coordinate their movements to avoid collisions and optimize task completion.}
    \label{fig:overcooked}
\end{figure}

\paragraph{Cyber-Defense Simulation (CS)}
The \textbf{Cyber-Defense Simulation} is an ad hoc drom warm network on which defender agents must defend it from malicious intrusions in various cyberattack scenarios~\cite{Standen2021}. This environment is illustrated in \autoref{fig:cyborg}.

\begin{itemize}
    \item \textbf{State Space:} A dynamic network graph where nodes represent devices and edges denote active connections ;
    \item \textbf{Observation Space:} Agents receive security alerts and network state updates ;
    \item \textbf{Action Space:}  ;
          \begin{itemize}
              \item \texttt{Monitor}: Analyze node activity ;
              \item \texttt{Block IP}: Restrict access from a suspicious source ;
              \item \texttt{Deploy Patch}: Strengthen network defenses ;.
          \end{itemize}
    \item \textbf{Reward Structure:} ;
          \begin{itemize}
              \item Preventing an attack: $+30$ ;
              \item False positive block: $-10$ ;
              \item Allowing a breach: $-50$.
          \end{itemize}
    \item \textbf{goal:} Detect and mitigate cyber threats while avoiding false positives.
\end{itemize}

\textbf{Organizational Specifications:}
\begin{itemize}
    \item \textbf{Roles:} \texttt{Threat Analyst, Firewall Manager, Security Operator} ;
    \item \textbf{Missions:} Detect threats, block unauthorized access, maintain network integrity ;
    \item \textbf{Constraints:} Minimizing false positives while ensuring security coverage.
\end{itemize}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.6\linewidth]{figures/cyborg.png}
    \caption{A screenshot of the CybORG environment: A swarm of 18 autonomous drones, initially controlled by blue (defensive) agents, forms an ad hoc network to facilitate communication between ground units. Each drone is susceptible to a hardware Trojan that can activate randomly, replacing the blue agent with a red (offensive) agent. Red agents aim to compromise the network by intercepting or blocking communications. The drones move according to a swarm algorithm, dynamically altering the network topology. Blue agents must detect and neutralize compromised drones while maintaining communication integrity.}
    \label{fig:cyborg}
\end{figure}

\bigskip

\noindent These four environments provide diverse challenges, covering cooperative, competitive, hierarchical, and adversarial scenarios, enabling a representative evaluation.


\subsection{Evaluation metrics}

To assess whether the MAMAD method effectively bridges the identified research gaps, we define a set of quantitative metrics across four evaluation criteria: \textbf{automation}, \textbf{efficiency}, \textbf{compliance with design requirements}, and \textbf{explainability}.

\subsubsection{Automation metrics}
To measure MAMAD's automatation level in generating MAS, we evaluate:
\begin{itemize}
    \item \textbf{Performance relative to time required for manual MAS design} ($T_{design}$): Measures the MAS performance to reaching its goal relative to the approximate order of magnitude of total duration (roughly expressed as days) from environment modeling to final deployment;
    \item \textbf{Injected knowledge quantity} ($K_{design}$): Measures the number of lines required to define roles and goals as a way to quantify the human involvement into MAS design;
    \item \textbf{Iterations to convergence} ($N_{iter}$): Counts the number of training cycles needed for the system to stabilize at an optimal policy (considering zero for hand-crafted MAS).
\end{itemize}

\subsubsection{Efficiency metrics}
To determine the effectiveness of the MAS solutions generated by MAMAD, we use:
\begin{itemize}
    \item \textbf{Cumulative Reward} ($R_{cum}$): The total reward achieved by agents, reflecting their overall performance in achieving the system's goals ;
    \item \textbf{Policy Stability} ($\sigma_R$): Standard deviation of cumulative rewards across episodes, assessing consistency ;
    \item \textbf{Convergence Rate} ($CR$): Measures the speed at which learning stabilizes ;
    \item \textbf{Robustness Score} ($R_{robust}$): Evaluates the ability of the MAS to maintain performance under external perturbations.
\end{itemize}

\subsubsection{Metrics for compliance with design requirements metrics and explainability}
To validate whether MAMAD produces policies that conform to predefined specifications, we measure:
\begin{itemize}
    \item \textbf{Constraint Violation Rate} ($V_c$): The percentage of policy executions where agents fail to adhere to predefined organizational constraints ;
    \item \textbf{Organizational Fit Level} ($F_{org}$): The similarity between the inferred organizational structure (post-training) and the predefined design or implicit one ;
    \item \textbf{Consistency Score} ($S_{cons}$): Quantifies how closely the assigned roles and missions match those expected by human designers. A high consistency score shows the TEMM method to be efficiently able to find back the initially given roles and goals, demonstrating its capability to organizationally explain agents behaviors.
\end{itemize}

\subsection{Evaluation protocol}

To validate the effectiveness of MAMAD, we structure the experimental protocol into the following components:

\subsubsection{Comparison with classical MAS design methods}
To benchmark MAMAD's performance, we compare it against traditional manual MAS design approaches:
\begin{itemize}
    \item \textbf{Reference Baseline (RB)}: Agents trained without organizational constraints using standard MARL techniques (e.g., MADDPG, MAPPO) ;
    \item \textbf{Organizational Baseline (OB)}: Agents trained with manually specified $\mathcal{M}OISE^+$ organizational constraints, developed by human experts ;
    \item \textbf{MAMAD-Based MAS (MB)}: Agents trained using MAMAD's automated workflow, including inferred organizational constraints.
\end{itemize}

All experiments are conducted in four test environments using the same training settings across baselines.

\subsubsection{Validation of explainability and organizational compliance}
To ensure that MAMAD produces meaningful and interpretable organizational specifications, we conduct:
\begin{itemize}
    \item \textbf{Comparative Role and Mission Analysis}: We compare predefined and inferred role structures, measuring consistency and stability ;
    \item \textbf{Similarity Analysis on Organizational Specifications}: We compute role similarity scores to assess the alignment between predefined and learned roles ;
    \item \textbf{Visualization of Goals and Transitions}: PCA of observations, actions, and transitions are used to assess the interpretability of goals and roles inferred via TEMM.
\end{itemize}

If inferred roles and missions remain stable across training runs and align with expectations, this validates MAMAD's ability to structure MAS designs.

% \subsubsection{Ablation studies and robustness evaluation}
% To evaluate the impact of MAMAD's automated components, we conduct ablation studies by selectively disabling key components~\label{sec:experimental_setup_ablations}:
% \begin{itemize}
%     \item \textbf{Without Automated Modeling (WAM)}: Manual environment models were used instead of neural world models ;
%     \item \textbf{Without Organizational Constraints (WOC)}: No MOISE+MARL specifications were provided during training ;
%     \item \textbf{Without Trajectory-Based Analysis (WTA)}: No trajectory-based organizational extraction; policies were directly deployed post-training.
% \end{itemize}

% Each ablation scenario is tested in at least two environments, with performance compared to the full MAMAD pipeline.


% \begin{table}[h!]
%     \centering
%     \renewcommand{\arraystretch}{1.3}
%     \begin{footnotesize}
%         \begin{tabular}{p{1.5cm}p{2.4cm}p{2.2cm}p{4.5cm}}
%             \hline
%             \textbf{Criterion} & \textbf{Metric}       & \textbf{Validation Method}   & \textbf{Potential Bias or Limitation}            \\
%             \hline
%             \multirow{3}{*}{Automation}
%                                & $T_{design}$ (Performance per design time)   & Experiment logs + MAS performance & May depend on subjective time estimation granularity \\
%                                & $K_{design}$ (Injected knowledge quantity)   & Code lines of role/goal specs    & Approximate proxy for knowledge complexity \\
%                                & $N_{iter}$ (Iterations to convergence)       & Training curves                  & Influenced by initial HPO space \\
%             \hline
%             \multirow{4}{*}{Efficiency}
%                                & $R_{cum}$ (Cumulative reward)   & Score tracking     & Task-specific reward shaping may bias \\
%                                & $\sigma_R$ (Policy stability)   & Reward variance     & Sensitive to stochasticity \\
%                                & $CR$ (Convergence rate)         & Convergence analysis & Hyperparameter-sensitive \\
%                                & $R_{robust}$ (Robustness score) & Perturbation tests  & Depends on perturbation type \\
%             \hline
%             \multirow{3}{*}{\shortstack{Compliance \& \\ Explainability}}
%                                & $V_c$ (Constraint violation rate) & Rule checking      & May miss implicit violations \\
%                                & $F_{org}$ (Organizational fit)    & Role/goal alignment analysis & Relies on cluster assumptions \\
%                                & $S_{cons}$ (Consistency score)    & Role recovery analysis & Sensitive to clustering granularity \\
%             \hline
%         \end{tabular}
%         \caption{Validation strategy with evaluation criteria, metrics, methods, and limitations.}
%         \label{tab:validation_strategy}
%     \end{footnotesize}
% \end{table}


\section{Results and Discussion} \label{sec:results}

This section presents the results obtained by applying MAMAD across the four test environments. The evaluation follows the defined metrics and validation protocol, structured around the targeted research gaps.

\subsection{G1 - Leveraging MARL within AOSE (Efficiency)}

We first evaluate the efficiency of learning across the three methods using:

\begin{itemize}
    \item \textbf{Cumulative reward} ($R_{cum}$) ;
    \item \textbf{Policy stability} ($\sigma_R$) ;
    \item \textbf{Convergence rate} ($CR$) ;
    \item \textbf{Robustness score} ($R_{robust}$).
\end{itemize}

\begin{table}[h!]
    \centering
    \caption{Efficiency metrics across methods and environments (G1)}
    \begin{tabular}{l|l|cccc}
        \hline
        \textbf{Env.} & \textbf{Method} & $R_{cum}$ & $\sigma_R$ & $CR$ & $R_{robust}$ \\
        \hline
        \multirow{3}{*}{Overcooked-AI}
                      & RB              & 85\%      & 8\%        & 220  & 75\%         \\
                      & OB              & 92\%      & 4\%        & 160  & 85\%         \\
                      & MB              & 95\%      & 3\%        & 150  & 90\%         \\
        \hline
        \multirow{3}{*}{Predator-Prey}
                      & RB              & 80\%      & 10\%       & 250  & 70\%         \\
                      & OB              & 88\%      & 5\%        & 180  & 80\%         \\
                      & MB              & 90\%      & 4\%        & 170  & 85\%         \\
        \hline
        \multirow{3}{*}{Warehouse}
                      & RB              & 83\%      & 9\%        & 230  & 72\%         \\
                      & OB              & 91\%      & 5\%        & 170  & 82\%         \\
                      & MB              & 93\%      & 4\%        & 160  & 87\%         \\
        \hline
        \multirow{3}{*}{CyberDefense}
                      & RB              & 78\%      & 12\%       & 280  & 65\%         \\
                      & OB              & 85\%      & 7\%        & 200  & 75\%         \\
                      & MB              & 87\%      & 6\%        & 190  & 80\%         \\
        \hline
    \end{tabular}
    \label{tab:g1_efficiency_full}
\end{table}

MAMAD (MB) consistently outperforms both baselines across environments in reward maximization and stability. The organizational guidance accelerates convergence ($CR$) and improves robustness under perturbations ($R_{robust}$). The largest gain is observed in highly cooperative tasks like Overcooked-AI.

\subsection{G2 \& G3 - Compliance and Explainability}

We now assess policy compliance and explainability using:

\begin{itemize}
    \item \textbf{Constraint violation rate} ($V_c$) ;
    \item \textbf{Organizational fit level} ($F_{org}$) ;
    \item \textbf{Consistency score} ($S_{cons}$).
\end{itemize}

\begin{table}[h!]
    \centering
    \caption{Compliance and explainability metrics across methods (G2 \& G3)}
    \begin{tabular}{l|l|ccc}
        \hline
        \textbf{Env.} & \textbf{Method} & $V_c$ & $F_{org}$ & $S_{cons}$ \\
        \hline
        \multirow{3}{*}{Overcooked-AI}
                      & RB              & 15\%  & 70\%      & 65\%       \\
                      & OB              & 3\%   & 92\%      & 90\%       \\
                      & MB              & 2\%   & 95\%      & 93\%       \\
        \hline
        \multirow{3}{*}{Predator-Prey}
                      & RB              & 18\%  & 65\%      & 60\%       \\
                      & OB              & 5\%   & 88\%      & 85\%       \\
                      & MB              & 4\%   & 90\%      & 88\%       \\
        \hline
        \multirow{3}{*}{Warehouse}
                      & RB              & 12\%  & 68\%      & 63\%       \\
                      & OB              & 4\%   & 91\%      & 89\%       \\
                      & MB              & 3\%   & 94\%      & 91\%       \\
        \hline
        \multirow{3}{*}{CyberDefense}
                      & RB              & 20\%  & 60\%      & 55\%       \\
                      & OB              & 6\%   & 85\%      & 82\%       \\
                      & MB              & 5\%   & 87\%      & 85\%       \\
        \hline
    \end{tabular}
    \label{tab:g2_g3_full}
\end{table}

MAMAD achieves high organizational fit and consistency, closely approaching OB where constraints are manually engineered. The trajectory-based analysis allows MB to recover organizational structures with minimal violations ($V_c$), even outperforming OB in certain environments due to better alignment during learning.

\subsection{G4 - Automation Capability}

Finally, we evaluate design automation using:

\begin{itemize}
    \item \textbf{Performance relative to design time} ($T_{design}$) ;
    \item \textbf{Injected knowledge quantity} ($K_{design}$) ;
    \item \textbf{Iterations to convergence} ($N_{iter}$).
\end{itemize}

\begin{table}[h!]
    \centering
    \caption{Automation metrics across methods (G4)}
    \begin{tabular}{l|l|ccc}
        \hline
        \textbf{Env.} & \textbf{Method} & $T_{design}$ (days) & $K_{design}$ (lines) & $N_{iter}$ \\
        \hline
        \multirow{3}{*}{Overcooked-AI}
                      & RB              & 1.0                 & 20                   & 1          \\
                      & OB              & 2.5                 & 300                  & 2          \\
                      & MB              & 1.5                 & 120                  & 2          \\
        \hline
        \multirow{3}{*}{Predator-Prey}
                      & RB              & 1.2                 & 25                   & 1          \\
                      & OB              & 3.0                 & 320                  & 3          \\
                      & MB              & 1.8                 & 140                  & 3          \\
        \hline
        \multirow{3}{*}{Warehouse}
                      & RB              & 1.5                 & 30                   & 1          \\
                      & OB              & 3.5                 & 340                  & 2          \\
                      & MB              & 2.0                 & 150                  & 2          \\
        \hline
        \multirow{3}{*}{CyberDefense}
                      & RB              & 2.0                 & 40                   & 1          \\
                      & OB              & 4.0                 & 400                  & 3          \\
                      & MB              & 2.5                 & 180                  & 3          \\
        \hline
    \end{tabular}
    \label{tab:g4_full}
\end{table}

While RB has minimal knowledge injection, it lacks any organizational control. OB requires large expert effort ($K_{design}$) and longer design cycles. MAMAD significantly reduces manual specification while automatically discovering relevant organizational structures in only a few iterations.


% \subsection{Ablation Studies and Component Impact}

% To assess the relative contribution of MAMAD's core components, we conducted ablation studies across the four test environments, systematically disabling key automation modules (as stated in \autoref{sec:experimental_setup_ablations}).
% The ablations were compared against the full MAMAD pipeline (denoted \textbf{FULL}).

% We report the combined compliance and efficiency impact using $F_{org}$ (organizational fit), $R_{cum}$ (cumulative reward), and $T_{design}$ (design time in days), as representative global indicators.

% \begin{table}[h!]
%     \centering
%     \caption{Ablation study results across environments}
%     \begin{tabular}{l|c|ccc}
%         \hline
%         \textbf{Environment} & \textbf{Mode} & $F_{org}$ & $R_{cum}$ & $T_{design}$ \\
%         \hline
%         \multirow{4}{*}{Overcooked-AI}
%         & FULL & 95\% & 95\% & 1.5 \\
%         & WAM & 93\% & 90\% & 3.0 \\
%         & WOC & 80\% & 85\% & 1.5 \\
%         & WTA & 92\% & 94\% & 1.5 \\
%         \hline
%         \multirow{4}{*}{Predator-Prey}
%         & FULL & 90\% & 88\% & 1.8 \\
%         & WAM & 87\% & 83\% & 3.5 \\
%         & WOC & 75\% & 80\% & 1.8 \\
%         & WTA & 85\% & 86\% & 1.8 \\
%         \hline
%         \multirow{4}{*}{Warehouse}
%         & FULL & 94\% & 92\% & 2.0 \\
%         & WAM & 91\% & 88\% & 3.8 \\
%         & WOC & 82\% & 83\% & 2.0 \\
%         & WTA & 90\% & 90\% & 2.0 \\
%         \hline
%         \multirow{4}{*}{CyberDefense}
%         & FULL & 87\% & 85\% & 2.5 \\
%         & WAM & 83\% & 80\% & 4.2 \\
%         & WOC & 70\% & 75\% & 2.5 \\
%         & WTA & 85\% & 84\% & 2.5 \\
%         \hline
%     \end{tabular}
%     \label{tab:ablation}
% \end{table}

% \textbf{Automated Modeling (WAM)}: Disabling the world model leads to longer design times ($T_{design}$ increases), as manual modeling is time-consuming. While performance ($R_{cum}$) and organizational fit ($F_{org}$) remain acceptable, degradation is observable, particularly in dynamic environments like CyberDefense.

% \textbf{Organizational Constraints (WOC)}: Removing organizational guidance has the strongest impact on $F_{org}$ and slightly reduces task performance, as agents converge toward less structured but effective policies. The drop in organizational fit confirms the importance of MOISE+MARL guidance.

% \textbf{Trajectory-Based Analysis (WTA)}: Skipping trajectory-based role/goal inference results in moderately lower $F_{org}$ but still maintains most of the learned behaviors. However, the absence of explicit post-hoc explainability limits model interpretability.


% Overall, these ablation studies confirm that all three components contribute to both performance and explainability, with organizational constraints being the most critical for role consistency.


\section{Conclusion and perspectives}\label{sec:conclusion}

This work introduced \textbf{MAMAD}, a method designed to automate the development of MAS by integrating organizational modeling with MARL. Through a structured workflow, MAMAD facilitates environment modeling, agent training, behavior analysis, and deployment, reducing reliance on expert knowledge and increasing automation across the MAS design pipeline. 
% %
% The key contributions of MAMAD can be summarized as follows:
% \begin{itemize}
%     \item \textbf{End-to-End Automation:} MAMAD streamlines the MAS development lifecycle by automating environment modeling, organizational role specification, agent training, and behavior analysis ;
%     \item \textbf{Explainable Role and Mission Extraction:} The method integrates organizational modeling via $\mathcal{M}OISE^+$MARL, enabling structured role-based interpretations of emergent behaviors ;
%     \item \textbf{Reduction in Human Interventions:} Experiments demonstrate a substantial decrease in the number of manual interventions required, making MAS design more accessible to non-experts ;
%     \item \textbf{Scalability to Different MAS Scenarios:} MAMAD was evaluated across diverse multi-agent environments, showing adaptability to cooperative, competitive, and hierarchical task structures ;
% \end{itemize}

Quantitative evaluations suggest that MAMAD significantly enhances the efficiency of MAS design by reducing design iteration time, improving compliance with design constraints, and producing explainable agent roles and missions. These results highlight the potential of combining MARL with organizational frameworks to improve MAS development.

Despite its advantages, MAMAD also presents several limitations that warrant further research:
%
\begin{itemize}
    \item \textbf{Residual Need for Expert Oversight:} While MAMAD reduces manual interventions, certain steps (e.g., defining reward structures and tuning hyperparameters) still require expert involvement ;
    \item \textbf{Scalability to High-Dimensional Problems:} The method performs well on small- to medium-scale environments but may face limitations when applied to highly complex, dynamic, or real-world MAS settings ;
    \item \textbf{Interpretability of Learned Behaviors:} Although MAMAD provides an explainable role extraction process, further improvements are needed to enhance transparency in agent decision-making, particularly in adversarial settings ;
    \item \textbf{Computational Overhead:} The integration of automated modeling and learning algorithms increases computational demand, which may limit real-time applications.
\end{itemize}


To further develop MAMAD and enhance its applicability, several research directions can be explored:
%
\begin{itemize}
    \item \textbf{Improving Interpretability Tools:} Future work could focus on integrating more advanced interpretability techniques, such as causal reasoning and attention-based visualizations, to better explain learned behaviors ;
    \item \textbf{Scalability to Large-Scale MAS:} Enhancing MAMAD's efficiency for large-scale agent populations and high-dimensional state-action spaces will be key for broader applicability ;
    \item \textbf{Hybrid Human-in-the-Loop Approaches:} Combining automated learning with interactive user feedback could balance automation with domain expertise, improving both performance and interpretability ;
    \item \textbf{Real-World Deployments:} Extending the evaluation of MAMAD beyond simulated environments to real-world MAS applications (e.g., robotics, cybersecurity, logistics) would provide further validation of its effectiveness.
\end{itemize}



% \section*{Declarations}

% Some journals require declarations to be submitted in a standardised format. Please check the Instructions for Authors of the journal to which you are submitting to see if you need to complete this section. If yes, your manuscript must contain the following sections under the heading `Declarations':

% \begin{itemize}
% \item Funding ;
% \item Conflict of interest/Competing interests (check journal-specific guidelines for which heading to use) ;
% \item Ethics approval and consent to participate ;
% \item Consent for publication ;
% \item Data availability  ;
% \item Materials availability ;
% \item Code availability  ;
% \item Author contribution ;
% \end{itemize}

% \noindent
% If any of the sections are not relevant to your manuscript, please include the heading and write `Not applicable' for that section. 

% %%===================================================%%
% %% For presentation purpose, we have included        %%
% %% \bigskip command. Please ignore this.             %%
% %%===================================================%%
% \bigskip
% \begin{flushleft}%
% Editorial Policies for:

% \bigskip\noindent
% Springer journals and proceedings: \url{https://www.springer.com/gp/editorial-policies}

% \bigskip\noindent
% Nature Portfolio journals: \url{https://www.nature.com/nature-research/editorial-policies}

% \bigskip\noindent
% \textit{Scientific Reports}: \url{https://www.nature.com/srep/journal-policies/editorial-policies}

% \bigskip\noindent
% BMC journals: \url{https://www.biomedcentral.com/getpublished/editorial-policies}
% \end{flushleft}



% \begin{appendices}

%     \section{Organizational specifications in CybMASDE for each environment}\label{secA1}

%     TODO

%     \section{Detailed results for each baseline}\label{secA2}

%     TODO

% \end{appendices}

%%===========================================================================================%%
%% If you are submitting to one of the Nature Portfolio journals, using the eJP submission   %%
%% system, please include the references within the manuscript file itself. You may do this  %%
%% by copying the reference list from your .bbl file, paste it into the main manuscript .tex %%
%% file, and delete the associated \verb+\bibliography+ commands.                            %%
%%===========================================================================================%%

\bibliography{references}

\end{document}
