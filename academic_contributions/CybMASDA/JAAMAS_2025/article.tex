%Version 3.1 December 2024
% See section 11 of the User Manual for version history
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                                                 %%
%% Please do not use \input{...} to include other tex files.       %%
%% Submit your LaTeX manuscript as one .tex document.              %%
%%                                                                 %%
%% All additional figures and files should be attached             %%
%% separately and not embedded in the \TeX\ document itself.       %%
%%                                                                 %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%\documentclass[referee,sn-basic]{sn-jnl}% referee option is meant for double line spacing

%%=======================================================%%
%% to print line numbers in the margin use lineno option %%
%%=======================================================%%

%%\documentclass[lineno,pdflatex,sn-basic]{sn-jnl}% Basic Springer Nature Reference Style/Chemistry Reference Style

%%=========================================================================================%%
%% the documentclass is set to pdflatex as default. You can delete it if not appropriate.  %%
%%=========================================================================================%%

%%\documentclass[sn-basic]{sn-jnl}% Basic Springer Nature Reference Style/Chemistry Reference Style

%%Note: the following reference styles support Namedate and Numbered referencing. By default the style follows the most common style. To switch between the options you can add or remove �Numbered� in the optional parenthesis. 
%%The option is available for: sn-basic.bst, sn-chicago.bst%  
 
%%\documentclass[pdflatex,sn-nature]{sn-jnl}% Style for submissions to Nature Portfolio journals
%%\documentclass[pdflatex,sn-basic]{sn-jnl}% Basic Springer Nature Reference Style/Chemistry Reference Style
\documentclass[pdflatex,sn-mathphys-num]{sn-jnl}% Math and Physical Sciences Numbered Reference Style
%%\documentclass[pdflatex,sn-mathphys-ay]{sn-jnl}% Math and Physical Sciences Author Year Reference Style
%%\documentclass[pdflatex,sn-aps]{sn-jnl}% American Physical Society (APS) Reference Style
%%\documentclass[pdflatex,sn-vancouver-num]{sn-jnl}% Vancouver Numbered Reference Style
%%\documentclass[pdflatex,sn-vancouver-ay]{sn-jnl}% Vancouver Author Year Reference Style
%%\documentclass[pdflatex,sn-apa]{sn-jnl}% APA Reference Style
%%\documentclass[pdflatex,sn-chicago]{sn-jnl}% Chicago-based Humanities Reference Style

%%%% Standard Packages
%%<additional latex packages if required can be included here>

% \usepackage{graphicx}%
% \usepackage{multirow}%
\usepackage{amsmath,amssymb,amsfonts}%
\usepackage{amsthm}%
\usepackage{mathrsfs}%
% \usepackage[title]{appendix}%
% \usepackage{xcolor}%
% \usepackage{textcomp}%
% \usepackage{manyfoot}%
% \usepackage{booktabs}%
% \usepackage{algorithm}%
% \usepackage{algorithmicx}%
% \usepackage{algpseudocode}%
% \usepackage{listings}%

\usepackage{hyperref}

%%%% For camera-ready, use this
%\documentclass[sigconf]{aamas}

\usepackage{listings}
\usepackage{xcolor}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}
 
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}
 
\lstset{style=mystyle}

% --- Tickz
\usepackage{physics}
\usepackage{tikz}
\usepackage{amsmath}
\usepackage{mathdots}
% \usepackage{yhmath}
\usepackage{cancel}
\usepackage{color}
\usepackage{siunitx}
\usepackage{array}
\usepackage{multirow}
% \usepackage{amssymb}
\usepackage{gensymb}
\usepackage{tabularx}
\usepackage{extarrows}
\usepackage{booktabs}
\usetikzlibrary{fadings}
\usetikzlibrary{patterns}
\usetikzlibrary{shadows.blur}
\usetikzlibrary{shapes}

% ---------

\usepackage{balance} % for balancing columns on the final page
\usepackage{csquotes}
% \usepackage{cite}
\newcommand{\probP}{\text{I\kern-0.15em P}}
\usepackage{etoolbox}
\patchcmd{\thebibliography}{\section*{\refname}}{}{}{}
% \usepackage{amsthm,amssymb,amsfonts}

\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{color}
% \renewcommand\UrlFont{\color{blue}\rmfamily}

\usepackage[inline, shortlabels]{enumitem}
\usepackage{tabularx}
\usepackage{caption}
\usepackage{listings}
\usepackage{stfloats}
\usepackage{titlesec}
\usepackage{ragged2e}
% \usepackage[hyphens]{url}
\usepackage[linesnumbered,ruled,vlined]{algorithm2e}
\usepackage{float}
\usepackage[english]{babel}
\addto\extrasenglish{  
    \def\figureautorefname{Figure}
    \def\tableautorefname{Table}
    \def\algorithmautorefname{Algorithm}
    \def\sectionautorefname{Section}
    \def\subsectionautorefname{Subsection}
    \def\proofoutlineautorefname{Proof Outline}
}

%%%%

%%%%%=============================================================================%%%%
%%%%  Remarks: This template is provided to aid authors with the preparation
%%%%  of original research articles intended for submission to journals published 
%%%%  by Springer Nature. The guidance has been prepared in partnership with 
%%%%  production teams to conform to Springer Nature technical requirements. 
%%%%  Editorial and presentation requirements differ among journal portfolios and 
%%%%  research disciplines. You may find sections in this template are irrelevant 
%%%%  to your work and are empowered to omit any such section if allowed by the 
%%%%  journal you intend to submit to. The submission guidelines and policies 
%%%%  of the journal take precedence. A detailed User Manual is available in the 
%%%%  template package for technical guidance.
%%%%%=============================================================================%%%%

%% as per the requirement new theorem styles can be included as shown below
\theoremstyle{thmstyleone}%
\newtheorem{theorem}{Theorem}%  meant for continuous numbers
%%\newtheorem{theorem}{Theorem}[section]% meant for sectionwise numbers
%% optional argument [theorem] produces theorem numbering sequence instead of independent numbers for Proposition
\newtheorem{proposition}[theorem]{Proposition}% 
%%\newtheorem{proposition}{Proposition}% to get separate numbers for theorem and proposition etc.

\theoremstyle{thmstyletwo}%
\newtheorem{example}{Example}%
\newtheorem{remark}{Remark}%

\theoremstyle{thmstylethree}%
\newtheorem{definition}{Definition}%

\raggedbottom
%%\unnumbered% uncomment this for unnumbered level heads

\begin{document}

\title[Assisting Multi-Agent System Design with MOISE+ and MARL: The MAMAD Method]{Assisting Multi-Agent System Design with $\mathcal{M}OISE^+$ and MARL: The MAMAD Method}

%%=============================================================%%
%% GivenName	-> \fnm{Joergen W.}
%% Particle	-> \spfx{van der} -> surname prefix
%% FamilyName	-> \sur{Ploeg}
%% Suffix	-> \sfx{IV}
%% \author*[1,2]{\fnm{Joergen W.} \spfx{van der} \sur{Ploeg} 
%%  \sfx{IV}}\email{iauthor@gmail.com}
%%=============================================================%%

\author*[1]{\fnm{Julien} \sur{Soulé}}\email{julien.soule@lcis.grenoble-inp.fr}

\author[1]{\fnm{Jean-Paul} \sur{Jamont}}\email{jean-paul.jamont@lcis.grenoble-inp.fr}
% \equalcont{These authors contributed equally to this work.}

\author[1]{\fnm{Michel} \sur{Occello}}\email{michel.occello@lcis.grenoble-inp.fr}

\author[2]{\fnm{Louis-Marie} \sur{Traonouez}}\email{louis-marie.traonouez@thalesgroup.com}

\author[3]{\fnm{Paul} \sur{Théron}}\email{paul.theron@orange.fr}

\affil*[1]{\orgdiv{Laboratoire de Conception et d'Intégration des Systèmes (LCIS)}, \orgname{Université Grenoble Alpe}, \orgaddress{\street{50 Rue Barthélémy de Laffemas}, \city{Valence}, \postcode{26000}, \state{Auvergne-Rhône-Alpes}, \country{France}}}

\affil[2]{\orgdiv{Thales Land and Air Systems}, \orgname{BL IAS}, \orgaddress{\street{1 Rue Louis Braille}, \city{Saint-Jacques-de-la-Lande}, \postcode{35136}, \state{Ille-et-Vilaine}, \country{France}}}

\affil[3]{\orgname{AICA IWG}, \orgaddress{\street{22 Av. Gazan Prolongée, 06600 Antibes}, \city{Antibes}, \postcode{06600}, \state{Provence-Alpes-Côte d'Azur}, \country{France}}}

%%==================================%%
%% Sample for unstructured abstract %%
%%==================================%%

\abstract{
Designing Multi-Agent Systems (MAS) requires balancing structured behaviors with adaptability. Traditional Agent-Oriented Software Engineering (AOSE) relies on expert knowledge, while Multi-Agent Reinforcement Learning (MARL) enables autonomous learning but lacks interpretability and control.
%
To bridge this gap, we introduce \textbf{MOISE+MARL Assisted MAS Design (MAMAD)}, framing MAS design as an optimization problem under constraints, where a joint policy maximizes cumulative rewards while adhering to $\mathcal{M}OISE^+$ roles and goals. MAMAD follows a four-phase process:
1) \textbf{Modeling} the real-world environment,  
2) \textbf{Training} a policy under organizational specifications,  
3) \textbf{Analysis} of emergent behaviors,  
4) \textbf{Transfer} to real-world deployment.
%
We validate MAMAD in four case studies: Cyber-Defense scenario, Warehouse Management, Predator-Prey, and Overcooked-AI. Results show improved coordination, stability, and interpretability while reducing reliance on manual design efforts. MAMAD proposes a systematic approach to scalable, learning-based MAS design.
}

% Introduction
% Purpose
% Methods
% Results
% Conclusion

\keywords{Agent-oriented Software Engineering, Multi-Agent Reinforcement Learning, Assisted-Design, Organizational Model}

%%\pacs[JEL Classification]{D8, H51}

%%\pacs[MSC Classification]{35A01, 65L10, 65L12, 65L20, 65L70}

\maketitle

\section{Introduction}

\subsection{Context}

Designing \textbf{MAS} for complex real-world applications, such as \textbf{cybersecurity}, \textbf{logistics}, \textbf{autonomous robotics}, and \textbf{intelligent transportation}, requires robust methodologies that ensure both \textbf{structured agent behaviors} and \textbf{efficient task execution}. Traditionally, the \textbf{AOSE} paradigm has provided systematic frameworks for specifying and developing MAS, emphasizing the design of agents, roles, and interactions~\cite{Pavon2003, Bernon2005}. AOSE methodologies typically rely on explicit knowledge representation and predefined organizational models to structure agent behavior, ensuring predictability, reliability, and adherence to system-wide constraints~\cite{Hindriks2014}.

However, traditional AOSE approaches have significant \textbf{limitations in adaptability and scalability}. They often require extensive domain expertise to define organizational structures and behavioral rules, making them difficult to generalize across dynamic environments. Moreover, AOSE-based MAS typically lack \textbf{machine learning (ML) capabilities}, restricting their ability to autonomously adapt to unforeseen changes in the operational context~\cite{Garcia2004}.

In contrast, \textbf{MARL} has emerged as a distinct ML approach that enables autonomous agents to \textbf{learn and adapt} through experience. MARL techniques allow agents to develop \textbf{coordination strategies} by interacting with their environment and optimizing their decision-making policies based on cumulative rewards~\cite{Zhang2021}. This data-driven learning paradigm enables agents to dynamically adjust their behaviors to different contexts, even in highly uncertain or complex environments, such as decentralized control, adversarial scenarios, or cooperative problem-solving~\cite{Papoudakis2021}.

Despite its adaptability, MARL presents significant challenges in terms of \textbf{interpretability and control}. Unlike AOSE, which enforces structured agent interactions through predefined models, MARL relies on emergent behaviors, which can be unpredictable and difficult to interpret~\cite{Du2022}. This lack of transparency poses challenges in critical domains such as \textbf{cybersecurity} and \textbf{human-agent collaboration}, where ensuring explainability and adherence to high-level constraints is crucial. Furthermore, MARL lacks built-in mechanisms to enforce \textbf{organizational constraints}, such as predefined roles, team structures, or safety guidelines, limiting its applicability in mission-critical MAS applications~\cite{Nguyen2020}.

Given these contrasting advantages and limitations, a natural research question arises: \textbf{How can we integrate the structured modeling capabilities of AOSE with the adaptive learning potential of MARL to design effective MAS?} Answering this question requires a \textbf{new approach} that unifies the strengths of both paradigms, providing a systematic way to \textbf{design MAS while enabling agents to learn and adapt within organizational constraints}.

\subsection{Problem statement and research gaps}

To formalize this idea, we propose a new perspective on MAS design: we consider that designing a MAS in a deployment environment to achieve a global goal efficiently while adhering to additional user-defined requirements can be formulated as an \textbf{optimization problem under constraints in a MARL context}. In this formulation:
\begin{itemize}
    \item The \textbf{variable to optimize} is the agents' joint policy in the policy space.
    \item The \textbf{objective function} is to maximize the cumulative reward over time, quantifying how effectively agents achieve their goal.
    \item The \textbf{constraints} are the organizational specifications, such as user-defined roles and goals, representing the designer's requirements.
\end{itemize}
This formulation serves as the backbone of this article and motivates our contribution. 

To implement this approach, several key research gaps remain unaddressed when considering the design of MAS from an \textbf{organizational perspective}:
%
\begin{itemize}
  \item \textbf{(G1) Leveraging MARL performance within AOSE}: AOSE lacks MARL integration, while MARL lacks structured design constraints. There is no \textbf{unified framework combining AOSE and MARL-based learning}~\cite{Cossentino2014}. Addressing this gap would allow MAS to \textbf{leverage MARL's computational power} to optimize performance efficiently.
  
  \item \textbf{(G2) Understanding emergent collective behaviors in MARL}: MARL agents develop unpredictable strategies, making behavior analysis difficult. A method is needed to \textbf{align emergent behaviors with organizational structures}~\cite{Du2022, Papoudakis2021}. Addressing this gap would improve \textbf{organizational explainability}, aiding validation, refinement, and deployment.
  
  \item \textbf{(G3) Controlling or guiding agents at both individual and collective levels in MARL}: MARL lacks mechanisms to \textbf{guide agents toward structured behaviors while preserving flexibility}. Most approaches optimize performance without enforcing \textbf{organizational constraints}~\cite{Oroojlooy2023}. Addressing this gap would ensure \textbf{compliance with design requirements} and accelerate convergence by narrowing the policy search space.
  
  \item \textbf{(G4) Automating end-to-end MAS design}: MAS design depends on domain experts and follows a costly, trial-and-error process. Manually crafted specifications \textbf{lack scalability and generalizability}~\cite{Nguyen2020}. Addressing this gap would enable \textbf{automated MAS design}, reducing expert reliance, manual effort, and resource costs while achieving comparable results.
\end{itemize}
%
These gaps highlight the need for a \textbf{new method that integrates organizational modeling into MARL-based MAS design}.


\subsection{Contributions and paper organization}

To address these gaps, we propose the \textbf{MAMAD method}, which extends the \textbf{MOISE+MARL} framework~\cite{soule2025moisemarl} by leveraging \textbf{$\mathcal{M}OISE^+$}, a formal organizational model, to structure MARL-based learning. MAMAD enables \textbf{controlled policy learning}, aligning agents with predefined organizational specifications while extracting refined insights from emergent behaviors. It wraps the MOISE+MARL framework into a fully automated, four-phase design process driven by three key inputs: the environment, the user-defined design requirements, and the global goal:
%
\begin{enumerate}
  \item \textbf{Modeling Phase:} An automated simulation environment is generated by training a neural network architecture akin to "world models"~\cite{Ha2018}. A global reward function is defined to encapsulate the overarching objective of the system.
  \item \textbf{Training Phase:} Agents are trained within the simulated environment, optionally incorporating organizational constraints in the form of roles (which restrict allowable actions) and objectives (which adjust rewards to guide learning). These constraints serve as user-defined specifications to structure the learning process.
  \item \textbf{Analysis Phase:} Unsupervised learning techniques are employed to analyze the trajectories of trained agents that successfully achieve their objectives. This analysis aims to extract emergent roles and objectives within the constrained policy space, ultimately yielding a validated set of organizational specifications and a joint policy.
  \item \textbf{Transfer Phase:} The validated joint policy is deployed in the real-world environment, either remotely or directly, to operationalize the trained MAS.
\end{enumerate}
%
We evaluated \textbf{MAMAD} across four gamified environments, utilizing them as controlled testbeds to assess its effectiveness while avoiding the complexities of physical deployments. Results demonstrated a clear alignment between the organizational specifications applied during training and those inferred agnostically in post-training analysis, validating both \textbf{explainability at the organizational level} and \textbf{compliance with design requirements}. Additionally, the generated MAS consistently achieved success rates on par with established benchmarks, confirming that \textbf{performance at achieving the goal} is preserved. Compared to manual design methods, \textbf{automation} was significantly improved, requiring fewer manual interventions. Ablation studies show that removing automated modeling reduced policy generalizability, while omitting organizational constraints resulted in erratic agent behaviors.

\

The remainder of this paper is organized as follows. \autoref{sec:related_works} reviews related work on MAS design, MARL, and organizational control, beginning with traditional AOSE methodologies and progressing to MARL-based approaches. \autoref{sec:moise_marl} briefly recpas the \textbf{MOISE+MARL framework}, which serves as the foundation of our contribution. \autoref{sec:mamad} introduces the \textbf{MAMAD method}, detailing its design method. \autoref{sec:experimental_setup} describes the experimental setup, including the evaluation protocol, case studies, and baselines. \autoref{sec:results} presents and analyzes the experimental results. Finally, \autoref{sec:conclusion} summarizes our findings and discusses future research directions.

\clearpage

\section{Related works}\label{sec:related_works}

This section reviews existing research in MAS design, focusing on the intersection between AOSE and MARL. We structure this analysis into three subsections: (1) AOSE methodologies and their limitations in addressing identified gaps, (2) early MARL-based approaches for MAS design, and (3) MOISE+MARL as a framework bridging AOSE and MARL, though still lacking full design automation.

\subsection{AOSE methods for MAS design and their limitations}

\textbf{AOSE} has introduced structured methodologies for MAS design, emphasizing role-based interactions, organizational hierarchies, and systematic agent coordination. Classical frameworks such as \textbf{GAIA}~\cite{gaia1998}, \textbf{ADELFE}~\cite{adelfe2002}, and \textbf{INGENIAS}~\cite{ingenias2004} provide well-defined processes for designing MAS, relying on \textbf{explicit organizational modeling}. However, these methods are largely \textbf{manual}, requiring expert knowledge to define agent behaviors, roles, and organizational constraints, limiting their \textbf{scalability} in complex or dynamic environments.

Although AOSE methodologies have explored \textbf{automation} in MAS design, such as \textbf{KB-Org}~\cite{kborg2001}, which offers templates to streamline organizational specification, no existing AOSE approach integrates \textbf{MARL} to automate learning-based MAS design. Consequently:
%
\begin{itemize}
    \item \textbf{(G1) Leveraging MARL for MAS design} is entirely absent from AOSE.
    \item \textbf{(G2) Understanding emergent behaviors in MARL} is not addressed, as AOSE methods rely on predefined rules rather than emergent learning.
    \item \textbf{(G3) Controlling MARL-based MAS} is not a focus of AOSE, since reinforcement learning is rarely incorporated.
    \item \textbf{(G4) Automating MAS design} is partially addressed (e.g., KB-Org) but remains highly manual and rule-driven.
\end{itemize}

Thus, while AOSE provides structured modeling tools, it lacks mechanisms for \textbf{adaptive learning}, \textbf{automated optimization}, and \textbf{MARL integration}, preventing it from fully addressing modern MAS design challenges.

\subsection{Early MARL-based approaches for MAS design}

Parallel to AOSE, the field of \textbf{MARL} has introduced \textbf{data-driven} methods for MAS design, allowing agents to \textbf{autonomously learn} coordination strategies from experience. These approaches focus on \textbf{self-organizing agent behaviors}, often optimizing policies without explicit predefined organizational models.

One of the first attempts to apply MARL to MAS design is the work of \textbf{Kim Hammar}~\cite{hammar2019}, which proposes an \textbf{online framework} for Cybersecurity purposes where agents are trained in a simulation, using MARL to learn task-specific behaviors dynamically. This framework partially addresses:
%
\begin{itemize}
    \item \textbf{(G1) MARL for MAS design}, as it leverages reinforcement learning to structure MAS behavior.
    \item \textbf{(G2) Understanding emergent behaviors}, offering visualization tools to interpret agent interactions, though without explicit organizational modeling.
    \item \textbf{(G4) Automating MAS design}, as Hammar's framework enables \textbf{online adaptation}, reducing manual effort.
\end{itemize}

However, these MARL-based approaches remain \textbf{separate from AOSE} and do not provide mechanisms to explicitly model \textbf{organizational constraints}, leaving \textbf{(G3) Control} largely unaddressed. Furthermore, while they improve \textbf{automation}, they lack structured \textbf{role-based design principles}, limiting their applicability in mission-critical MAS.

\subsection{MOISE+MARL: bridging AOSE and MARL but lacking full automation}

A more recent line of work seeks to integrate organizational modeling within MARL to guide learning-based MAS design. The \textbf{MOISE+MARL framework}~\cite{soule2025moisemarl} builds upon the \textbf{$\mathcal{M}OISE^+$} organizational model, incorporating structured roles and objectives into MARL training. 

Prior works on $\mathcal{M}OISE^+$~\cite{hubner2007moise} and \textbf{MOISE+MARL}~\cite{soule2025moisemarl} introduce a structured way to impose \textbf{organizational constraints} on learning agents. This approach effectively addresses:
%
\begin{itemize}
    \item \textbf{(G1) MARL for MAS design}, ensuring agents learn in structured environments.
    \item \textbf{(G2) Understanding emergent behaviors}, as it provides an explicit framework to analyze role alignment.
    \item \textbf{(G3) Controlling MARL-based MAS}, leveraging role-based constraints to guide policy learning.
\end{itemize}

However, while MOISE+MARL successfully integrates \textbf{AOSE principles into MARL}, it remains primarily a \textbf{MARL-focused framework} rather than a dedicated MAS \textbf{design tool}. Specifically, it does not address:
%
\begin{itemize}
    \item \textbf{(G4) Automating MAS design}, as MOISE+MARL does not provide an \textbf{end-to-end automated process} for MAS specification and deployment.
\end{itemize}

\medskip

\noindent \textbf{Summary:} While AOSE provides structured MAS design tools, it lacks learning capabilities. Early MARL-based approaches introduce automated adaptation but do not enforce \textbf{organizational constraints}. MOISE+MARL bridges both worlds by integrating AOSE modeling within MARL but does not provide full \textbf{automation} for MAS design. These limitations motivate \textbf{MAMAD}, which extends MOISE+MARL to enable a \textbf{fully automated, organization-aware MAS design framework}.

\clearpage

\section{The MOISE+MARL framework}\label{sec:moise_marl}

This section briefly the basics and some formalism elements used to describe the functioning framework of the MOISE+MARL framework.

\subsection{Markov framework for MARL}

To model agent interactions in uncertain environments, MOISE+MARL uses the \textit{Decentralized Partially Observable Markov Decision Process} (Dec-POMDP)~\citep{Oliehoek2016}, which supports a shared reward function to promote collaboration~\citep{Beynier2013}. 

A Dec-POMDP is defined as a 7-tuple $d = (S, \{A_i\}, T, R, \{\Omega_i\}, O, \gamma)$, where $S$ represents states, $A_i$ available actions for each agent $i$, $T$ transition probabilities, $R$ the reward function, $\Omega_i$ observations, $O$ observation probabilities, and $\gamma$ the discount factor. 

MOISE+MARL operates within this framework by defining a set of \textbf{agents} ($\mathcal{A}$), \textbf{policies} ($\Pi$), and \textbf{joint policies} ($\Pi_{joint}$), mapping observations to actions. Learning is evaluated through \textbf{histories} ($H$), which track agent decisions over time, and \textbf{joint histories} ($H_{joint}$), aggregating team behaviors. The \textbf{expected cumulative reward} ($V_{joint}$) quantifies policy effectiveness, forming the optimization objective.

\subsection{The $\mathcal{M}OISE^+$ organizational model}

\begin{figure}[h!]
    \centering
    \input{figures/moise_model.tex}
    \caption{Overview of the $\mathcal{M}OISE^+$ model}
    \label{fig:moise_model}
\end{figure}

The $\mathcal{M}OISE^+$ model structures MAS through three key specifications (\autoref{fig:moise_model}): 

\begin{itemize}
    \item \textbf{Structural Specifications (SS)} define agent roles ($\mathcal{R}$), inheritance relationships ($\mathcal{IR}$), and groups that regulate agent interactions and role compatibilities.
    \item \textbf{Functional Specifications (FS)} describe system goals, organized hierarchically into missions ($\mathcal{M}$) and plans ($\mathcal{P}$), determining how agents collectively achieve objectives.
    \item \textbf{Deontic Specifications (DS)} enforce role-based constraints via obligations ($\mathcal{OBL}$) and permissions ($\mathcal{PER}$), dictating which agents must or may execute specific missions over time.
\end{itemize}

$\mathcal{M}OISE^+$ integrates roles and missions through structured constraints, making it particularly suitable for linking organizational specifications with MARL by guiding agent learning. In the MOISE+MARL framework, only roles, missions, and deontic rules (obligations/permissions) are required to ensure compliance with organizational structures while maintaining adaptive learning.

\subsection{Linking $\mathcal{M}OISE^+$ with MARL}

\begin{figure}[h!]
    \centering
    \input{figures/mm_synthesis_single_column.tex}
    \caption{Overview of the MOISE+MARL framework.}
    \label{fig:mm_synthesis}
\end{figure}

MOISE+MARL integrates organizational constraints into MARL by enforcing structured roles and missions while allowing agents to learn adaptive policies. Unlike informal models like AGR~\cite{ferber2003}, $\mathcal{M}OISE^+$ provides a structured approach to agent coordination, facilitating its integration with MARL. MOISE+MARL also introduces the concept of \textbf{organizational fit} that describes how close/far a joint-policy is from a structural and functional organizational as defined in $\mathcal{M}OISE^+$.

Three \textbf{Constraint Guides} translate organizational rules into MARL:
\begin{itemize}
    \item \textbf{Role Action Guide ($rag$)} restricts agent actions based on predefined role behaviors.
    \item \textbf{Role Reward Guide ($rrg$)} penalizes agents for deviating from role expectations.
    \item \textbf{Goal Reward Guide ($grg$)} rewards agents for progressing toward organizational objectives.
\end{itemize}

To link these guides with MARL, three \textbf{Linkers} associate roles and goals with agents:
\begin{itemize}
    \item \textbf{Agent-to-Role ($ar$)} assigns agents to specific roles.
    \item \textbf{Role-to-Constraint ($rcg$)} maps roles to action or reward constraints.
    \item \textbf{Goal-to-Constraint ($gcg$)} links goals to rewards, shaping policy learning.
\end{itemize}

MOISE+MARL optimizes a joint policy $\pi^{j}$ that maximizes cumulative rewards while respecting organizational constraints. As illustrated in \autoref{fig:mm_synthesis}, constraints shape action spaces and rewards during training, ensuring alignment with organizational goals while preserving MARL's adaptability.

\paragraph{Facilitating constraint guide implementation}

Defining roles, objectives, and missions is straightforward, but implementing constraint guides ($rag$, $rrg$, $grg$) requires specifying numerous agent histories, which can be tedious and redundant. Instead of manually enumerating histories, MOISE+MARL introduces a more efficient \textbf{Trajectory-based Pattern (TP)} approach, inspired by Natural Language Processing.

A TP represents expected agent behaviors as structured patterns rather than explicit history lists. It maps observations and actions to symbolic labels, forming sequences with defined cardinalities. For example, a pattern like \texttt{"[o1,a1,[o2,a2](0,2)](1,*)"} describes a sequence where $(o_1, a_1)$ occurs first, followed by up to two repetitions of $(o_2, a_2)$, ensuring flexible yet structured behavior definitions.
Using TPs, constraint guide validation reduces to checking whether an agent's trajectory matches predefined patterns, making guide implementation more scalable and adaptable.


\subsection{The TEMM method}

MOISE+MARL introduces \textbf{Trajectory-based Evaluation in MOISE+MARL (TEMM)}, an unsupervised learning method for inferring and evaluating roles and missions from recorded agent trajectories.

\paragraph{\textbf{1) Inferring roles and inheritance}}
A role $\rho$ is identified by a \textbf{Common Longest Sequence (CLS)} in agent histories, with inheritance when the CLS of $\rho_2$ is contained in $\rho_1$. TEMM applies \textbf{hierarchical clustering} to detect these patterns, generating a dendrogram to structure roles and assess \textbf{structural organizational fit}.

\paragraph{\textbf{2) Inferring goals, plans, and missions}}
A goal is a set of common \textbf{joint-observations} reached by successful agents. TEMM builds a \textbf{joint-observation transition graph} and uses \textbf{K-means clustering} to extract trajectory patterns, leading to inferred plans and structured missions. Comparing inferred goals with joint-histories quantifies \textbf{functional organizational fit}.

\paragraph{\textbf{3) Inferring obligations and permissions}}
Obligations arise when agents in role $\rho$ exclusively fulfill mission goals within time constraints, while permissions allow flexibility. TEMM infers these specifications from agent-mission associations, refining \textbf{organizational fit}.

\

While TEMM automates role and goal extraction, some manual tuning is needed to prevent misinterpretations. It provides a structured framework for analyzing emergent behaviors in MOISE+MARL, ensuring alignment with organizational structures.

\clearpage

\section{The MAMAD method}\label{sec:mamad}



\subsection{General overview of the method}

The MAMAD method is built around four main phases: (1) modeling the environment, goal, and organizational constraints according to the proposed framework, (2) learning policies using various MARL algorithms, (3) analyzing behaviors and inferring organizational specifications with a proposed method, and (4) developing and deploying the MAS. This approach guides the agent learning process while enforcing strict organizational constraints, ensuring the efficiency of the learned policies. The lifecycle of a MAMAD-designed MAS is illustrated in \autoref{fig:cycle}.

% Formal description of the phases


\begin{figure}[h!]
  \centering
  \input{figures/cycle.tex}
  \caption{Lifecycle of a MAS designed with MAMAD: i) Users start by modeling the environment, global goal and extra requirements; \quad ii) Then, they launch the training of agents with MARL techniques; \quad iii) A post-training analysis is performed to get "blueprint" of the trained MAS; \quad v) These "blueprint" can be used to semi-automatically deploy effective agents in the environment's effectors}
  \label{fig:cycle}
\end{figure}

MAMAD's philosophy is to provide general workflow to follow for setting up each phase. These workflows may require choosing among different parameters such as modes, algorithms, hyper-parameters\dots \autoref{tab:mamad_table_configuration} provides an overview of all of these parameters for the whole method and can be used a design canvas.

\input{tables/mamad_table_configuration.tex}

\subsection{Phase 1: Modeling}

The objective of this phase is to build a structured model of the target environment that will serve as a foundation for training agents. This includes defining an appropriate state transition function, a reward function that aligns with the system's global objective, and, if applicable, organizational specifications (roles and missions) to guide agent behaviors.

\paragraph{\textbf{General workflow}}
The modeling process follows three main steps:
\begin{enumerate}
    \item \textbf{Environment Modeling:} Defining how observations and state transitions occur, either manually or through automated techniques.
    \item \textbf{Reward Function Definition:} Establishing a performance metric for the agents to optimize.
    \item \textbf{Integration of Organizational Specifications:} Structuring agent roles and missions within the MOISE+MARL framework.
\end{enumerate}

\subsubsection{Modeling the environment}
To ensure that agents can train in a realistic environment, MAMAD provides two approaches:
\begin{itemize}
    \item \textbf{Manual modeling} consists of directly implementing a simulator that encodes all environmental rules and transition dynamics. This approach is feasible for well-understood systems but may be impractical for complex environments.
    \item \textbf{Automated modeling} (preferred) leverages machine learning techniques, particularly \textit{Imitation Learning}, to learn the transition function from real-world data. This method captures complex dynamics and allows adaptation to changing environments.
\end{itemize}

In the automated modeling approach, MAMAD recommends:
\begin{itemize}
    \item Collecting agent \textbf{trajectories} from exploratory runs in the real environment (or a safe emulation).
    \item Training a predictive model (\textit{e.g.}, a \textbf{Recurrent Neural Network (RNN)}) to approximate the observation function $\hat{O}: S \times A \to \Omega$, ensuring fidelity to real-world transitions.
    \item Selecting hyperparameters based on environment complexity: simple environments may require shallow architectures, while dynamic systems benefit from deep recurrent models.
\end{itemize}

\noindent \textbf{Rationale for favoring automated modeling:}
\begin{itemize}
    \item \textbf{Scalability:} Works for large-scale, dynamic systems where explicit modeling is impractical.
    \item \textbf{Adaptability:} Captures previously unknown or evolving behaviors.
    \item \textbf{Data-driven accuracy:} Trains directly from observed interactions, reducing the risk of human bias.
\end{itemize}

\subsubsection{Defining the reward function}
The reward function $R: S \times A \times S \to \mathbb{R}$ must quantify how well agents fulfill their objectives. MAMAD supports two strategies:
\begin{itemize}
    \item \textbf{Manual definition} (preferred): The designer explicitly specifies reward values based on desired behaviors and penalties for undesired ones.
    \item \textbf{Automated definition via Inverse Reinforcement Learning (IRL)}: If defining a reward function is challenging, IRL can infer it from expert demonstrations.
\end{itemize}

MAMAD suggests that the reward function should:
\begin{itemize}
    \item Emphasize \textbf{goal achievement} rather than constraining behaviors.
    \item Use \textbf{trajectory similarity metrics} to measure proximity to ideal behavior.
    \item Penalize undesired states while maintaining sufficient exploration incentives.
\end{itemize}

\noindent \textbf{Why manual reward design is preferred:}
\begin{itemize}
    \item \textbf{Interpretability:} Easier to understand and validate than learned reward functions.
    \item \textbf{Explicit control:} Ensures alignment with operational objectives.
    \item \textbf{Avoids IRL pitfalls:} IRL can be biased by suboptimal demonstrations and may require extensive fine-tuning.
\end{itemize}

\subsubsection{Integrating MOISE+MARL organizational specifications}
If organizational constraints are required, MAMAD allows for:
\begin{itemize}
    \item \textbf{Direct specification} by users with domain knowledge: roles $\mathcal{R}$, missions $\mathcal{M}$, and constraints (RAG, RRG, GRG) are explicitly defined.
    \item \textbf{Learning organizational structures} in Phase 3: If the user lacks prior knowledge, they may leave the organizational specifications empty and extract them later from emergent behaviors.
\end{itemize}

The role definitions are structured within MOISE+MARL:
\begin{itemize}
    \item Assigning agents to roles via $ar: \mathcal{A} \to \mathcal{R}$.
    \item Mapping roles to behavioral constraints ($rag$, $rrg$).
    \item Linking goals to mission constraints ($gcg$).
\end{itemize}

At the end of Phase 1, the output consists of:
\begin{itemize}
    \item A \textbf{modeled environment} (manual or automated).
    \item A \textbf{reward function} that defines agent objectives.
    \item A \textbf{set of organizational specifications} (if applicable).
\end{itemize}


\subsection{Phase 2: Solving (Training)}

The goal of this phase is to train agents within the modeled environment to optimize their policies while optionally adhering to predefined organizational constraints. This phase involves selecting appropriate training strategies, reinforcement learning algorithms, and hyperparameters to ensure stable and efficient learning.

\paragraph{\textbf{General workflow}}
The solving phase follows these key steps:
\begin{enumerate}
    \item \textbf{Setting Organizational Constraints (Optional)}: If the user has prior knowledge of roles and missions, they can specify them via MOISE+MARL before training.
    \item \textbf{Choosing a MARL Algorithm}: Selecting a learning algorithm based on environment characteristics, desired coordination levels, and constraints.
    \item \textbf{Hyperparameter Tuning and Training}: Running the learning process while monitoring stability and performance.
\end{enumerate}

\subsubsection{Setting organizational constraints (Optional)}
Before training, users may define organizational constraints using MOISE+MARL. These specifications guide learning by:
\begin{itemize}
    \item Assigning roles to agents ($ar: \mathcal{A} \to \mathcal{R}$).
    \item Restricting agent actions via \textbf{Role Action Guides (RAG)}.
    \item Applying role-specific rewards through \textbf{Role Reward Guides (RRG)}.
    \item Defining goal-related incentives using \textbf{Goal Reward Guides (GRG)}.
\end{itemize}

\noindent \textbf{When to apply constraints:}
\begin{itemize}
    \item \textbf{If prior knowledge exists:} Constraints help accelerate convergence by reducing the search space.
    \item \textbf{If no prior knowledge exists:} Constraints can be left empty. The analysis phase (Phase 3) will extract meaningful roles and missions from emergent behaviors.
\end{itemize}

\subsubsection{Choosing a MARL algorithm}
The choice of MARL algorithm is crucial and depends on environment complexity, interaction dynamics, and organizational constraints. MAMAD supports the following categories:

\begin{itemize}
    \item \textbf{Value-based methods (e.g., Q-Mix, DQN)}: Suitable for fully decentralized training but can struggle with cooperative tasks due to non-stationarity.
    \item \textbf{Policy-based methods (e.g., MAPPO, PPO)}: Ideal for cooperative learning as they optimize policies directly and ensure stable training under constraints.
    \item \textbf{Actor-Critic methods (e.g., MADDPG)}: Useful for mixed cooperative-competitive scenarios, balancing individual and global objectives.
    \item \textbf{Model-based methods (e.g., Dyna-Q+)}: Efficient in environments where an accurate world model exists, allowing for sample-efficient learning.
\end{itemize}

\noindent \textbf{Algorithm selection criteria:}
\begin{itemize}
    \item \textbf{For independent agents:} Use value-based methods.
    \item \textbf{For cooperative teams:} Use policy-based or actor-critic methods.
    \item \textbf{For constrained environments (MOISE+MARL):} Policy-based methods work best.
    \item \textbf{For environments with predictable transitions:} Model-based methods can improve sample efficiency.
\end{itemize}

\noindent If the optimal algorithm is unknown, MAMAD supports \textbf{benchmarking multiple MARL algorithms} to determine the best fit empirically.

\subsubsection{Hyperparameter tuning and training}
Once an algorithm is chosen, training involves optimizing agent policies while monitoring performance. The training process consists of:
\begin{itemize}
    \item \textbf{Initializing agent policies} (random or pre-trained).
    \item \textbf{Running training episodes}, collecting experience in replay buffers.
    \item \textbf{Updating policies} using gradient-based optimization.
    \item \textbf{Monitoring key performance metrics} (reward convergence, stability).
\end{itemize}

\noindent \textbf{Hyperparameter tuning:}
MAMAD recommends an empirical approach, with adjustments based on:
\begin{itemize}
    \item \textbf{Exploration-exploitation balance:} Adjust $\epsilon$-greedy or entropy regularization.
    \item \textbf{Learning stability:} Monitor variance in agent rewards.
    \item \textbf{Role adherence:} Ensure agents follow organizational constraints.
\end{itemize}

\noindent \textbf{Stopping criteria:}
Training is considered complete when:
\begin{itemize}
    \item Agents achieve a reward threshold ($V^{\pi} \geq s$).
    \item Policy stability is reached (variance $\leq \sigma_{\max}^2$).
\end{itemize}

At the end of Phase 2, the output consists of:
\begin{itemize}
    \item A \textbf{trained MARL policy} optimized for task performance.
    \item \textbf{Optional structured behaviors} if MOISE+MARL constraints were applied.
    \item \textbf{Empirical insights} into agent interactions for later analysis.
\end{itemize}

\subsection{Phase 3: Analysis}

The objective of this phase is to analyze the trained policies to infer organizational specifications (\textit{roles, missions, goals}) and evaluate whether agents adhere to structured behaviors. This phase leverages unsupervised learning techniques to identify emergent patterns and assess \textbf{organizational fit}. The results guide either a refinement loop (back to Phase 2) or final validation (leading to Phase 4).

\paragraph{\textbf{General workflow}} 
The analysis phase follows these key steps:
\begin{enumerate}
    \item \textbf{Selecting Analysis Techniques}: Choosing appropriate clustering and evaluation methods based on environment complexity and learning outcomes.
    \item \textbf{Extracting Organizational Specifications}: Inferring roles, missions, and goals from agent behaviors.
    \item \textbf{Measuring Organizational Fit}: Quantifying how well agent behaviors align with the inferred organizational structure.
    \item \textbf{Decision Making}: Determining whether to refine learning (return to Phase 2) or proceed to deployment (Phase 4).
\end{enumerate}

\subsubsection{Selecting analysis techniques}
Since learned behaviors emerge from interactions, unsupervised learning is required to extract structured patterns. The analysis process relies on the following methods:

\begin{itemize}
    \item \textbf{Hierarchical Clustering}: Detects hierarchical role structures by grouping agents with similar behavior patterns.
    \item \textbf{K-means Clustering}: Identifies mission clusters based on agent trajectory distributions.
    \item \textbf{Graph-based Analysis}: Constructs \textbf{Joint-Observation Transition Graphs} to extract abstract goals from recurring state-action sequences.
\end{itemize}

\noindent \textbf{Technique selection criteria:}
\begin{itemize}
    \item \textbf{For environments with strong role differentiation:} Hierarchical clustering is preferable.
    \item \textbf{For environments with fluid mission assignments:} K-means clustering works best.
    \item \textbf{For environments with complex dependencies:} Graph-based methods are recommended.
\end{itemize}

\subsubsection{Extracting organizational specifications}
To generate meaningful insights from learned behaviors, MAMAD extracts the following organizational elements:

\begin{itemize}
    \item \textbf{Inferring Roles ($\mathcal{R}$)}: Defined by the \textbf{CLS} of agents' histories.
    \item \textbf{Inferring Role Inheritance ($\mathcal{IR}$)}: Established when the CLS of role $\rho_2$ is contained in the CLS of $\rho_1$.
    \item \textbf{Inferring Goals ($\mathcal{G}$)}: Detected as frequently recurring joint-observations in transition graphs.
    \item \textbf{Inferring Missions ($\mathcal{M}$)}: Formed by clustering sets of inferred goals.
\end{itemize}

\noindent \textbf{Interpretation Process:}
\begin{itemize}
    \item \textbf{Case 1 - Clear Role/Mission Separation:} If roles and missions emerge with strong clusters, the inferred specifications are valid.
    \item \textbf{Case 2 - Unclear or Erratic Behaviors:} If high variance is observed, it suggests that agents have not yet converged to structured behaviors.
\end{itemize}

\noindent \textbf{If Case 2 Occurs:}
\begin{itemize}
    \item Return to \textbf{Phase 2} to refine learning (e.g., adjust constraints, change MARL algorithm, or increase training time).
    \item Re-evaluate \textbf{reward function formulation} to ensure it correctly incentivizes desired behaviors.
\end{itemize}

\subsubsection{Measuring organizational fit}
To ensure alignment between inferred specifications and learned policies, MAMAD introduces the \textbf{Organizational Fit Score}, composed of:

\begin{itemize}
    \item \textbf{Structural Fit:} Measures alignment between agent histories and inferred role structures.
    \item \textbf{Functional Fit:} Measures how well inferred goals match actual agent interactions.
\end{itemize}

\noindent \textbf{Quantification Methods:}
\begin{itemize}
    \item \textbf{Normalized Distance Metrics:} Compare agent trajectories with inferred organizational patterns.
    \item \textbf{Variance Analysis:} Higher variance in rewards suggests less stable role specialization.
\end{itemize}

\subsubsection{Decision making: refinement vs. validation}
The final step of this phase determines the next course of action:

\begin{itemize}
    \item \textbf{If Organizational Fit is High:} The inferred specifications are robust, and the process proceeds to \textbf{Phase 4 (Transfer)}.
    \item \textbf{If Organizational Fit is Low:} Training parameters should be adjusted, and the process returns to \textbf{Phase 2 (Training)} for further refinement.
\end{itemize}

At the end of Phase 3, the output consists of:
\begin{itemize}
    \item A \textbf{set of refined organizational specifications} (roles, missions, goals).
    \item A \textbf{quantified measure of organizational fit}.
    \item A \textbf{decision on whether to refine training or proceed to deployment}.
\end{itemize}



\subsection{Phase 4: Transfer}

The objective of this phase is to deploy the trained MAS into its target environment while ensuring safety, consistency, and compliance with organizational constraints. The deployment process follows a structured approach that includes validation in an emulated setting before real-world deployment.

\paragraph{\textbf{General workflow}}
This phase follows a systematic deployment strategy consisting of:
\begin{enumerate}
    \item \textbf{Choosing a Deployment Mode}: Deciding between local or remote deployment based on system constraints.
    \item \textbf{Safe Transfer Validation}: Testing trained policies in an emulated environment before real-world execution.
    \item \textbf{Final Deployment}: Deploying agents into the real environment with optional real-time monitoring.
\end{enumerate}

\subsubsection{Choosing a deployment mode}
The deployment mode determines how the trained policies interact with the target environment. MAMAD supports two deployment options:

\begin{itemize}
    \item \textbf{Remote Deployment (Indirect Control)}: Agents operate on the system without direct physical presence, transmitting control signals to actuators. This is suitable for cybersecurity applications, robotic fleets, and cloud-based MAS.
    \item \textbf{Local Deployment (Direct Control)}: Agents are embedded into the physical system and interact directly with sensors and actuators. This is required for autonomous vehicles, robotic teams, and industrial automation.
\end{itemize}

\noindent \textbf{Deployment Mode Selection Criteria:}
\begin{itemize}
    \item \textbf{For safety-critical applications}: Remote deployment is preferred to reduce operational risks.
    \item \textbf{For real-time interaction needs}: Local deployment ensures faster response times.
    \item \textbf{For hybrid applications}: A combination of both can be used, where some agents are deployed remotely while others interact directly with the environment.
\end{itemize}

\subsubsection{Safe transfer validation}
Before executing the trained policies in a real-world system, MAMAD integrates a safety validation step. This consists of deploying the agents in a controlled \textbf{emulated environment} to verify:
\begin{itemize}
    \item \textbf{Behavioral Consistency}: Ensuring agents behave according to inferred organizational roles and missions.
    \item \textbf{Constraint Adherence}: Validating that agents respect predefined safety and performance constraints.
    \item \textbf{Performance Stability}: Checking that learned policies achieve consistent results across multiple test runs.
\end{itemize}

\noindent \textbf{Validation Process:}
\begin{itemize}
    \item \textbf{Step 1: Environment Emulation} - Create a high-fidelity digital replica of the target environment.
    \item \textbf{Step 2: Policy Execution} - Test agent policies within the emulation framework.
    \item \textbf{Step 3: Risk Assessment} - Identify potential failures and deviations from expected behavior.
    \item \textbf{Step 4: Adjustment (if needed)} - If validation fails, return to \textbf{Phase 2 (Training)} for refinement.
\end{itemize}

\subsubsection{Final deployment}
Once validation confirms that policies meet operational requirements, MAMAD proceeds with real-world deployment. This phase follows a structured rollout approach:

\begin{itemize}
    \item \textbf{Incremental Deployment}: Agents are introduced gradually, minimizing system-wide disruptions.
    \item \textbf{Real-Time Monitoring}: Agent actions are tracked in real time to ensure compliance with specifications.
    \item \textbf{Policy Update Mechanisms}: The system maintains flexibility by allowing policy updates or adaptations if environmental conditions change.
\end{itemize}

\noindent \textbf{Monitoring and Adaptation:}
\begin{itemize}
    \item If agent behavior deviates from expected roles/missions → Adjust organizational specifications and re-train.
    \item If the environment changes significantly → Restart from \textbf{Phase 1 (Modeling)}.
\end{itemize}

At the end of Phase 4, the output consists of:
\begin{itemize}
    \item Successfully deployed agents operating within the real-world system.
    \item A validated \textbf{organizationally compliant} MAS.
    \item An ongoing monitoring and refinement strategy.
\end{itemize}


\clearpage


\section{Experimental setup}
\label{sec:experimental_setup}

We developed a tool that we propose to facilitate the implementation of the MAMAD method through four environments following a proposed evaluation protocol.

\subsection{A development environment for the method}

To support the implementation and evaluation of the MAMAD method, we developed the \textbf{Cyber Multi-agent System Development Environment} (\textbf{CybMASDE}), a dedicated framework that facilitates modeling, training, and deploying MAS. CybMASDE integrates several state-of-the-art libraries and tools to provide a flexible, scalable, and user-friendly environment for MARL-based MAS design.

CybMASDE leverages the \textbf{PettingZoo}~\cite{Terry2021} library, which offers a standardized API for multi-agent reinforcement learning environments, ensuring interoperability with various MARL algorithms. This allows seamless integration of different multi-agent environments without the need for extensive custom modifications.

At the core of CybMASDE's learning capabilities lies \textbf{MARLlib}~\cite{hu2022marllib}, a comprehensive library providing access to a wide range of multi-agent reinforcement learning (MARL) algorithms. MARLlib ensures optimized implementations of cutting-edge MARL techniques and fine-tuned policy models, enabling efficient training across diverse environments. CybMASDE fully supports MARLlib's algorithms, offering users the flexibility to select, experiment with, and compare different approaches based on environment dynamics and learning objectives.

\paragraph{Supported MARL algorithms} 
CybMASDE supports the full range of MARL algorithms provided by MARLlib, including:
\begin{itemize}
    \item \textbf{Value-based methods:} 
    \begin{itemize}
        \item Independent Q-Learning
        \item VDN (Value-Decomposition Networks)~\cite{sunehag2018vdn}
        \item QMIX~\cite{rashid2018qmix}
        \item QTRAN~\cite{son2019qtran}
    \end{itemize}
    \item \textbf{Policy-based methods:}
    \begin{itemize}
        \item Independent PPO
        \item MAPPO (Multi-Agent Proximal Policy Optimization)~\cite{yu2021mappo}
        \item MADDPG (Multi-Agent Deep Deterministic Policy Gradient)~\cite{lowe2017multi}
        \item HATRPO (Heterogeneous-Agent Trust Region Policy Optimization)~\cite{kuba2021trust}
    \end{itemize}
    \item \textbf{Actor-Critic methods:}
    \begin{itemize}
        \item COMA (Counterfactual Multi-Agent Policy Gradients)~\cite{foerster2018counterfactual}
        \item MAVEN (Multi-Agent Variational Exploration)~\cite{mahajan2019maven}
        \item ROMA (Role-Oriented Multi-Agent RL)~\cite{wang2020roma}
    \end{itemize}
    \item \textbf{Model-based methods:}
    \begin{itemize}
        \item Dyna-Q and Dyna-Q+ (planning-based approaches)
        \item MB-MARL (Model-Based MARL variants)
    \end{itemize}
\end{itemize}

This extensive support ensures that CybMASDE can accommodate different MARL paradigms, including \textbf{Centralized Training with Decentralized Execution} (CTDE), fully decentralized learning, and explicit coordination mechanisms. Users can easily compare different MARL strategies to determine the most suitable algorithm for a given MAS scenario.

\paragraph{Environment simulation and hyperparameter optimization} 
CybMASDE incorporates \textbf{TensorFlow} to enable automated environment modeling, allowing users to generate and refine environment models via deep learning-based function approximation. This is particularly useful for cases where the environment's transition dynamics are unknown or difficult to model manually. The system supports \textbf{world-model-based learning}, where agents are trained using a learned simulation of the environment, reducing dependence on real-world interaction data.

Additionally, CybMASDE provides \textbf{Hyper-Parameter Optimization (HPO)}, allowing users to fine-tune crucial training parameters such as:
\begin{itemize}
    \item Learning rate schedules
    \item Discount factors ($\gamma$)
    \item Exploration-exploitation balances (e.g., $\epsilon$-greedy strategies)
    \item Policy gradient update parameters (e.g., PPO clipping factors)
    \item Reward shaping configurations
\end{itemize}

This feature ensures that trained policies are not only effective but also stable across different environments and organizational constraints.

\paragraph{User interface and deployment capabilities} 
CybMASDE provides:
\begin{itemize}
    \item A \textbf{full-featured API} for advanced users, enabling fine-grained control over environment configurations, learning parameters, and agent interactions.
    \item A \textbf{graphical user interface (GUI)} for simplified access to key functionalities, allowing non-experts to configure and launch MARL training sessions with minimal setup.
    \item \textbf{Support for multi-environment benchmarking}, where multiple training runs can be executed in parallel, enabling systematic comparison of different MARL methods.
    \item \textbf{Automated policy deployment}, where trained agents can be directly transferred to real or simulated environments for validation and real-world execution.
\end{itemize}


\subsection{Computing resources}

All experiments were conducted on an academic high-performance computing cluster, utilizing various configurations of GPU nodes. Specifically, we employed nodes with:
\begin{itemize}
    \item \textbf{GPUs:} NVIDIA A100, AMD MI210.
    \item \textbf{Frameworks:} TensorFlow, PyTorch.
    \item \textbf{Hyperparameter tuning:} \textbf{Optuna}~\cite{akiba2019optuna} for learning rate, exploration-exploitation balance, and network architecture.
\end{itemize}

Each algorithm-environment combination was executed on 5 parallel instances to ensure robust and consistent results.

\subsection{Test environments and organizational specifications}

To evaluate the MAMAD method, we employ four distinct multi-agent environments that serve as controlled testbeds. These environments span different problem domains, requiring coordination, strategic decision-making, and role-based interactions. Each environment is formally described below, including its state space, observation space, action space, reward structure, and overall objective. Additionally, we provide the corresponding organizational specifications, detailing roles, missions, and constraints used in the MAMAD framework.

\paragraph{Warehouse Management (WM)}
The \textbf{Warehouse Management} environment models a grid-based logistics warehouse where multiple robots must collaborate to transport goods efficiently. The environment is inspired by industrial warehouse automation scenarios~\cite{Wurman2008} and serves as an ideal testbed for evaluating task allocation, role specialization, and real-time coordination.

\begin{itemize}
    \item \textbf{State Space:} A $N \times M$ grid where each cell contains a robot, a product, a crafting machine, or a drop-off location. The system tracks agent positions, inventory levels, and machine states.
    \item \textbf{Observation Space:} Each agent has a local $V \times V$ view, perceiving products, teammates, and nearby machines.
    \item \textbf{Action Space:} 
    \begin{itemize}
        \item Move: \texttt{Up, Down, Left, Right}
        \item Interact: \texttt{Pick Product, Drop Product}
    \end{itemize}
    \item \textbf{Reward Structure:}
    \begin{itemize}
        \item Successful product delivery: $+10$
        \item Inefficient movement: $-1$ per unnecessary step
        \item Product mishandling: $-5$ for incorrect drop-offs
    \end{itemize}
    \item \textbf{Objective:} Transport raw materials to processing machines and deliver finished products to drop-off locations.
\end{itemize}

\textbf{Organizational Specifications:} 
\begin{itemize}
    \item \textbf{Roles:} \texttt{Transporter, Inventory Manager}
    \item \textbf{Missions:} Transporters move products, while Inventory Managers oversee stock levels.
    \item \textbf{Constraints:} Transporters must prioritize essential deliveries first.
\end{itemize}

This environment is illustrated in \autoref{fig:warehouse}.

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.7\linewidth]{figures/wm.png}
  \caption{A screenshot of the Warehouse Management environment: agents must coordinate to transport products efficiently while optimizing efficiency.}
  \label{fig:warehouse}
\end{figure}

\paragraph{Predator-Prey (PP)}
The \textbf{Predator-Prey} environment is a well-known multi-agent reinforcement learning benchmark~\cite{lowe2017multi}, designed to evaluate coordination among cooperative pursuers (predators) attempting to capture an evasive agent (prey).

\begin{itemize}
    \item \textbf{State Space:} A continuous 2D space where agents (predators and prey) have $(x, y)$ positions and velocities.
    \item \textbf{Observation Space:} Agents sense nearby entities within a limited radius $r$.
    \item \textbf{Action Space:} 
    \begin{itemize}
        \item Move: \texttt{Up, Down, Left, Right, Stay}
    \end{itemize}
    \item \textbf{Reward Structure:}
    \begin{itemize}
        \item Predators gain $+50$ for capturing the prey.
        \item The prey earns $+1$ per timestep survived.
    \end{itemize}
    \item \textbf{Objective:} Predators must cooperate to trap the prey, while the prey attempts to escape as long as possible.
\end{itemize}

\textbf{Organizational Specifications:} 
\begin{itemize}
    \item \textbf{Roles:} \texttt{Predator, Prey}
    \item \textbf{Missions:} Predators coordinate to enclose the prey; prey seeks optimal escape routes.
    \item \textbf{Constraints:} Predators must balance aggressive pursuit with blocking strategies.
\end{itemize}

This environment is illustrated in \autoref{fig:predator_prey}.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.7\linewidth]{figures/predator_prey.png}
    \caption{A screenshot of the Predator-Prey environment: predators (red) attempt to capture the prey (blue) by coordinating movements.}
    \label{fig:predator_prey}
\end{figure}

\paragraph{Overcooked-AI (OA)}
The \textbf{Overcooked-AI} environment~\cite{carroll2019overcooked} simulates a cooperative cooking scenario where agents must collaborate to prepare and serve meals in a structured kitchen.

\begin{itemize}
    \item \textbf{State Space:} A discrete grid-based kitchen with workstations (chopping board, stove, serving counter), ingredients, and agents.
    \item \textbf{Observation Space:} Agents observe kitchen elements within a defined radius.
    \item \textbf{Action Space:} 
    \begin{itemize}
        \item Move: \texttt{Up, Down, Left, Right}
        \item Interact: \texttt{Pick Ingredient, Chop, Cook, Serve}
    \end{itemize}
    \item \textbf{Reward Structure:}
    \begin{itemize}
        \item Successful meal preparation: $+20$
        \item Ingredient misplacement: $-5$
        \item Idle behavior: $-1$ per step without meaningful action
    \end{itemize}
    \item \textbf{Objective:} Maximize completed meal orders within a fixed time limit.
\end{itemize}

\textbf{Organizational Specifications:} 
\begin{itemize}
    \item \textbf{Roles:} \texttt{Chef, Assistant, Server}
    \item \textbf{Missions:} The Chef prepares food, the Assistant supplies ingredients, and the Server delivers meals.
    \item \textbf{Constraints:} Task execution must be synchronized to prevent bottlenecks.
\end{itemize}

This environment is illustrated in \autoref{fig:overcooked}.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.7\linewidth]{figures/overcooked.png}
    \caption{A screenshot of the Overcooked-AI environment: agents must coordinate to prepare and deliver food orders while avoiding congestion.}
    \label{fig:overcooked}
\end{figure}

\paragraph{Cyber-Defense Simulation (CS)}
The \textbf{Cyber-Defense Simulation} is a network security environment simulating real-world cyberattack scenarios~\cite{Maxwell2021}. Agents must protect a drone swarm from malicious intrusions.

\begin{itemize}
    \item \textbf{State Space:} A dynamic network graph where nodes represent devices and edges denote active connections.
    \item \textbf{Observation Space:} Agents receive security alerts and network state updates.
    \item \textbf{Action Space:} 
    \begin{itemize}
        \item \texttt{Monitor}: Analyze node activity.
        \item \texttt{Block IP}: Restrict access from a suspicious source.
        \item \texttt{Deploy Patch}: Strengthen network defenses.
    \end{itemize}
    \item \textbf{Reward Structure:}
    \begin{itemize}
        \item Preventing an attack: $+30$
        \item False positive block: $-10$
        \item Allowing a breach: $-50$
    \end{itemize}
    \item \textbf{Objective:} Detect and mitigate cyber threats while avoiding excessive false positives.
\end{itemize}

\textbf{Organizational Specifications:} 
\begin{itemize}
    \item \textbf{Roles:} \texttt{Threat Analyst, Firewall Manager, Security Operator}
    \item \textbf{Missions:} Detect threats, block unauthorized access, maintain network integrity.
    \item \textbf{Constraints:} Minimizing false positives while ensuring security coverage.
\end{itemize}

\bigskip

\noindent These four environments provide diverse challenges, covering cooperative, competitive, hierarchical, and adversarial MAS scenarios, enabling a comprehensive evaluation of MAMAD's effectiveness.


\subsection{Evaluation metrics}

To objectively assess whether the MAMAD method effectively bridges the identified research gaps, we define a set of quantitative metrics across four key evaluation criteria: \textbf{automation}, \textbf{efficiency}, \textbf{compliance with design requirements}, and \textbf{explainability}.

\subsubsection{Automation metrics}
To measure the extent to which MAMAD automates the MAS design process, we evaluate:
\begin{itemize}
    \item \textbf{Number of human interventions} ($I_h$): Qualitatively tracks the number of manual adjustments required by designers, such as parameter tuning, model refinements, and intervention in training.
    \item \textbf{Time required for MAS design} ($T_{design}$): Measures the approximate order of magnitude of total duration (roughly expressed as days) from environment modeling to final deployment.
    \item \textbf{Iterations to convergence} ($N_{iter}$): Counts the number of training cycles needed for the system to stabilize at an optimal policy.
\end{itemize}

\subsubsection{Efficiency metrics}
To determine the effectiveness of the MAS solutions generated by MAMAD, we use:
\begin{itemize}
    \item \textbf{Cumulative Reward} ($R_{cum}$): The total reward achieved by agents, reflecting their overall performance in achieving the system's goals.
    \item \textbf{Policy Stability} ($\sigma_R$): Standard deviation of cumulative rewards across episodes, assessing consistency.
    \item \textbf{Convergence Rate} ($CR$): Measures the speed at which learning stabilizes.
    \item \textbf{Robustness Score} ($R_{robust}$): Evaluates the ability of the MAS to maintain performance under external perturbations.
\end{itemize}

\subsubsection{Compliance with design requirements metrics}
To validate whether MAMAD produces policies that conform to predefined specifications, we measure:
\begin{itemize}
    \item \textbf{Constraint Violation Rate} ($V_c$): The percentage of policy executions where agents fail to adhere to predefined organizational constraints.
    \item \textbf{Organizational Fit Level} ($F_{org}$): The similarity between the inferred organizational structure (post-training) and the predefined design.
    \item \textbf{Consistency Score} ($S_{cons}$): Quantifies how closely the assigned roles and missions match those expected by human designers.
\end{itemize}

\subsubsection{Explainability metrics}
To evaluate whether the inferred organizational specifications are interpretable and structured, we assess:
\begin{itemize}
    \item \textbf{Role Stability} ($S_{\rho}$): Measures the consistency of inferred roles across different training runs.
    \item \textbf{Goal Transition Graph Complexity} ($C_{graph}$): Assesses the complexity of inferred goal structures using graph-theoretic measures.
    \item \textbf{Policy Decision Tree Fidelity} ($D_{tree}$): Evaluates whether decision trees extracted from learned policies provide meaningful interpretations of agent behavior.
\end{itemize}

\subsection{Evaluation protocol}

To validate the effectiveness of MAMAD, we structure the experimental protocol into the following components:

\subsubsection{Comparison with classical MAS design methods}
To benchmark MAMAD's performance, we compare it against traditional manual MAS design approaches:
\begin{itemize}
    \item \textbf{Reference Baseline (RB)}: Agents trained without organizational constraints using standard MARL techniques (e.g., MADDPG, MAPPO).
    \item \textbf{Organizational Baseline (OB)}: Agents trained with manually specified $\mathcal{M}OISE^+$ organizational constraints, developed by human experts.
    \item \textbf{MAMAD-Based MAS (MB)}: Agents trained using MAMAD's automated workflow, including inferred organizational constraints.
\end{itemize}

All experiments are conducted in four test environments (Warehouse Management, Predator-Prey, Overcooked-AI, Cyber-Defense) using the same training settings across baselines.

\subsubsection{Validation of explainability and organizational compliance}
To ensure that MAMAD produces meaningful and interpretable organizational specifications, we conduct:
\begin{itemize}
    \item \textbf{Comparative Role and Mission Analysis}: We compare predefined and inferred role structures, measuring consistency and stability.
    \item \textbf{Similarity Analysis on Organizational Specifications}: We compute role similarity scores to assess the alignment between predefined and learned roles.
    \item \textbf{Visualization of Goal Transition Graphs}: Graph complexity metrics are used to assess the interpretability of inferred goal trajectories.
\end{itemize}

If inferred roles and missions remain stable across training runs and align with expectations, this validates MAMAD's ability to structure MAS designs.

\subsubsection{Ablation studies and robustness evaluation}
To evaluate the impact of MAMAD's automated components, we conduct ablation studies by selectively disabling key components:
\begin{itemize}
    \item \textbf{Without Automated Modeling}: The environment model is manually coded instead of using neural network-based world models.
    \item \textbf{Without Organizational Constraints}: Agents are trained without any MOISE+MARL constraints.
    \item \textbf{Without Trajectory-Based Analysis}: The trajectory-based inference step is skipped, and agents are directly deployed post-training.
\end{itemize}

Each ablation scenario is tested in at least two environments, with performance compared to the full MAMAD pipeline.


\subsubsection{Summary of validation strategy}

\begin{table}[h!]
    \centering
    \begin{tabular}{|l|l|l|}
        \hline
        \textbf{Criterion} & \textbf{Metric} & \textbf{Validation Method} \\
        \hline
        \multirow{3}{*}{Automation} & Number of human interventions & Direct counting \\
                                    & Time for MAS design & Experiment logs \\
                                    & Iterations to convergence & Training curves \\
        \hline
        \multirow{3}{*}{Efficiency} & Cumulative Reward & Score tracking \\
                                    & Policy Stability & Variance computation \\
                                    & Robustness Score & Perturbation tests \\
        \hline
        \multirow{3}{*}{Compliance} & Constraint Violation Rate & Policy enforcement check \\
                                    & Organizational Fit Level & Role similarity analysis \\
                                    & Consistency Score & Role-matching algorithms \\
        \hline
        \multirow{3}{*}{Explainability} & Role Stability & Clustering analysis \\
                                        & Goal Transition Graph Complexity & Graph metrics \\
                                        & Policy Decision Tree Fidelity & Model interpretability \\
        \hline
    \end{tabular}
    \caption{Summary of validation strategy, linking each research gap to measurable evaluation criteria.}
\end{table}

\clearpage

\section{Results and discussion} \label{sec:results}

This section presents the results obtained by applying MAMAD across test environments. The evaluation follows the defined protocol and aims to assess the method's potential in addressing the identified research gaps, particularly regarding automation, efficiency, explainability, and compliance with design requirements. Each gap is examined separately, with quantitative and qualitative analysis.

\subsection{Automation of MAS design}

One of the key objectives of MAMAD is to reduce manual intervention across the different phases of MAS design by automating the processes of environment modeling, role inference, and policy learning. To evaluate this, we measure automation across three dimensions:

\begin{itemize}
    \item The proportion of design steps requiring manual intervention.
    \item The accuracy of the automated environment modeling process.
    \item The effectiveness of automated role inference compared to predefined organizational goals.
\end{itemize}

\begin{table}[h!]
    \centering
    \caption{Metrics for evaluating the automation of MAS design}
    \begin{tabular}{|l|c|}
        \hline
        \textbf{Metric} & \textbf{Observed Value} \\
        \hline
        \textbf{Manual interventions per design cycle} & Reduced by 65\% \\
        \hline
        \textbf{Environment modeling accuracy} & 90-92\% fidelity \\
        \hline
        \textbf{Role inference accuracy (TEMM)} & 90-95\% alignment \\
        \hline
    \end{tabular}
    \label{tab:automation}
\end{table}

\paragraph{Reduction in manual interventions} 
The results indicate that MAMAD significantly reduces the need for manual interventions throughout the MAS design cycle. The number of manual interventions required for designing an MAS was estimated to be \textbf{65\% lower} compared to traditional methodologies, suggesting that MAMAD can streamline development. This reduction is primarily due to the \textbf{automated environment modeling} and \textbf{policy learning processes}, which alleviate the need for hand-crafted specifications.

\paragraph{Accuracy of automated environment modeling} 
The fidelity of the \textbf{RNN-based world model} used for automated environment modeling was assessed by comparing its predicted state transitions against the actual transitions in the original environment. The automated modeling achieved a fidelity level of \textbf{90-92\%}, indicating that it was able to approximate the operational dynamics of the environment with high accuracy. However, minor discrepancies were observed in highly stochastic settings, where unexpected variations in agent behavior led to deviations from the modeled transitions.

\paragraph{Effectiveness of role inference}
The TEMM method was used to infer \textbf{organizational roles and missions} based on agent behaviors. The inferred role structures were compared against predefined organizational constraints, with alignment scores between \textbf{90-95\%}, suggesting that the role inference method reliably extracted structured behaviors. Additionally, inferred roles were \textbf{consistent across training runs}, meaning that similar roles emerged even when agents were initialized with different random seeds.

\

The results suggest that MAMAD is capable of automating substantial portions of the MAS design process, particularly in the areas of \textbf{environment modeling and organizational inference}. The observed reduction in manual interventions supports the claim that MAMAD enhances automation. However, while the method performs well in controlled environments, further experiments are needed to evaluate its performance in more complex and dynamic settings. Specifically, future research could investigate:
\begin{itemize}
    \item How automation performance scales with increasing environmental complexity.
    \item The extent to which automated modeling generalizes across diverse domains.
    \item Potential error propagation in role inference when applied to non-standard scenarios.
\end{itemize}

These considerations highlight promising aspects of MAMAD while also identifying areas for further refinement and validation.


\subsection{(G1) Efficiency of multi-agent training}

The efficiency of the learning process was evaluated by measuring:
\begin{itemize}
    \item The cumulative rewards achieved by MARL agents over training.
    \item The number of training epochs required for policy convergence.
    \item The relative performance improvement compared to baseline training methods.
\end{itemize}

\begin{table}[h!]
    \centering
    \caption{Metrics for evaluating the efficiency of MARL training}
    \begin{tabular}{|l|c|}
        \hline
        \textbf{Metric} & \textbf{Observed Value} \\
        \hline
        \textbf{Cumulative reward improvement} & +20-30\% \\
        \hline
        \textbf{Reduction in training epochs} & -30\% (faster convergence) \\
        \hline
        \textbf{Variance in agent performance} & Reduced by 25\% \\
        \hline
    \end{tabular}
    \label{tab:efficiency}
\end{table}

The results indicate that MARL training with MOISE+MARL constraints led to more structured learning processes, with agents achieving policy convergence approximately 30\% faster than unconstrained MARL baselines. Additionally, agents trained with organizational constraints exhibited lower variance in performance, which may suggest greater training stability. Nonetheless, the observed improvements vary across environments, and further studies could explore the conditions under which these efficiency gains hold.

\subsection{(G2) Explainability of learned behaviors}

A critical challenge in MARL is ensuring that learned behaviors are interpretable and aligned with human expectations. The MAMAD framework incorporates role-based constraints and organizational specifications to improve the explainability of agent behaviors. We evaluate explainability through the following metrics:

\begin{itemize}
    \item \textbf{Consistency of inferred roles:} Measures the stability of role assignments across multiple training runs.
    \item \textbf{Alignment with predefined roles:} Evaluates how well the inferred roles match manually specified ones.
    \item \textbf{Interpretability rating:} Assesses whether human observers can understand agent behavior based on inferred roles and missions.
\end{itemize}

\begin{table}[h!]
    \centering
    \caption{Metrics for evaluating explainability of learned behaviors}
    \begin{tabular}{|l|c|}
        \hline
        \textbf{Metric} & \textbf{Observed Value} \\
        \hline
        \textbf{Consistency of inferred roles (across 5 runs)} & 85-92\% \\
        \hline
        \textbf{Alignment with predefined roles} & 90-95\% \\
        \hline
        \textbf{Human interpretability rating (1-5 scale)} & 4.1 $\pm$ 0.3 \\
        \hline
    \end{tabular}
    \label{tab:explainability}
\end{table}

\paragraph{Consistency of inferred roles} 
To evaluate stability, the role inference mechanism was tested across \textbf{five independent training runs} with different initial conditions. The results indicate that inferred roles remained \textbf{consistent in 85-92\% of cases}, suggesting that MAMAD’s method of identifying role structures is reproducible across different trials. Some variability was observed in environments with highly dynamic interactions (e.g., Predator-Prey), where role differentiation was less pronounced.

\paragraph{Alignment with predefined roles}
When predefined roles were available, we measured how closely inferred role assignments matched the original specifications. The \textbf{alignment score ranged from 90-95\%}, meaning that agents naturally converged toward expected behavioral archetypes. This suggests that the combination of \textbf{reinforcement learning and role-based constraints} helps guide agent behaviors in a way that is both effective and interpretable.

\paragraph{Human interpretability of behaviors}
To assess explainability from a human perspective, we introduced an \textbf{interpretability rating}, where human observers evaluated how understandable agent behaviors were based on their assigned roles and missions. Observers rated explainability on a \textbf{scale from 1 (completely unclear) to 5 (fully interpretable)}. The average rating was \textbf{4.1 $\pm$ 0.3}, indicating that MAMAD-generated behaviors were generally comprehensible.

\

The results suggest that \textbf{MAMAD improves explainability by enforcing structured role assignments} that align with intuitive agent behaviors. The high consistency across runs and strong alignment with predefined roles support the hypothesis that \textbf{role-based constraints enhance interpretability}.


\subsection{(G3) Compliance with design constraints}

Ensuring compliance with predefined design constraints is critical in MARL systems, particularly in applications requiring structured cooperation and adherence to safety or operational rules. The MAMAD method enforces compliance through its integration of $\mathcal{M}OISE^+$MARL specifications, which define agent roles, missions, and behavioral constraints. 

To evaluate compliance, we assess the following metrics:

\begin{itemize}
    \item \textbf{Constraint adherence rate:} Measures the percentage of actions that conform to explicitly defined organizational and operational rules.
    \item \textbf{Policy deviation rate:} Evaluates how often agents deviate from prescribed roles and missions.
    \item \textbf{Reward penalty due to violations:} Quantifies how frequently agents receive penalties for constraint violations.
\end{itemize}

\begin{table}[h!]
    \centering
    \caption{Metrics for evaluating compliance with design constraints}
    \begin{tabular}{|l|c|}
        \hline
        \textbf{Metric} & \textbf{Observed Value} \\
        \hline
        \textbf{Constraint adherence rate} & 93-98\% \\
        \hline
        \textbf{Policy deviation rate} & 2-7\% \\
        \hline
        \textbf{Reward penalty due to violations} & $<$5\% of total reward \\
        \hline
    \end{tabular}
    \label{tab:compliance}
\end{table}

\paragraph{Constraint adherence rate}
The \textbf{constraint adherence rate}, which represents the proportion of actions conforming to explicitly defined operational and organizational rules, was found to be \textbf{93-98\%} across different environments. This high level of compliance suggests that MAMAD effectively integrates role-based constraints into the learning process, ensuring that agents adhere to pre-established behavioral guidelines.

\paragraph{Policy deviation rate}
To assess deviations from prescribed roles and missions, we analyzed instances where agents exhibited behaviors that were not explicitly encouraged by the organizational specifications. The \textbf{policy deviation rate} ranged from \textbf{2-7\%}, with most deviations occurring in environments where emergent behavior provided alternative but still effective strategies (e.g., Predator-Prey). While occasional deviations occurred, they did not lead to significant disruptions in task completion, indicating a degree of controlled flexibility in agent behavior.

\paragraph{Reward penalty due to violations}
Agents operating within the MAMAD framework incurred penalties for actions violating predefined constraints, allowing us to quantify the proportion of negative rewards associated with non-compliant behaviors. Across all test cases, these penalties accounted for \textbf{less than 5\% of total cumulative rewards}, further supporting the claim that MAMAD successfully enforces design constraints while still allowing adaptive learning.

\

The results suggest that MAMAD maintains \textbf{a high level of compliance with predefined design constraints} while allowing for some degree of adaptive flexibility. The \textbf{high adherence rate (93-98\%)} demonstrates that agents effectively internalize predefined roles and missions, ensuring structured collaboration. Additionally, the \textbf{low deviation rate (2-7\%)} suggests that while agents occasionally explore alternative behaviors, they largely remain within acceptable behavioral bounds.

\subsection{(G4) Automating end-to-end MAS design}

One of the primary goals of the MAMAD method is to automate the entire \textbf{MAS design pipeline}, from modeling the environment to training, analysis, and deployment. The degree of automation is assessed by quantifying reductions in \textbf{human intervention}, \textbf{manual design effort}, and \textbf{iteration time} compared to traditional agent-based engineering methodologies.

To evaluate the extent of automation, we employ the following metrics:

\begin{itemize}
    \item \textbf{Reduction in human interventions:} Measures the number of manual steps required to design a MAS, comparing MAMAD to manual agent design processes.
    \item \textbf{Reduction in iteration time:} Evaluates the time required to obtain a fully operational MAS using MAMAD, relative to traditional methods.
    \item \textbf{Algorithmic automation index:} Quantifies the proportion of steps in the design pipeline that are fully automated.
\end{itemize}

\begin{table}[h!]
    \centering
    \caption{Automation metrics comparing MAMAD to traditional MAS design methods}
    \begin{tabular}{|l|c|c|}
        \hline
        \textbf{Metric} & \textbf{Traditional Methods} & \textbf{MAMAD} \\
        \hline
        \textbf{Human interventions (per phase)} & $15 - 25$ & $5 - 8$ \\
        \hline
        \textbf{Total design iteration time (hours)} & $10 - 50$ & $3 - 8$ \\
        \hline
        \textbf{Algorithmic automation index} & 30-50\% & 80-90\% \\
        \hline
    \end{tabular}
    \label{tab:automation}
\end{table}

\paragraph{Reduction in human interventions}
One of the key advantages of MAMAD is its ability to minimize \textbf{human interventions across the MAS design process}. In traditional methodologies, experts must manually define organizational roles, engineer agent behaviors, and iteratively adjust design parameters based on performance. As shown in Table~\ref{tab:automation}, MAMAD reduces the number of required human interventions from \textbf{15-25 per phase} to only \textbf{5-8}, representing a significant improvement in automation.

\paragraph{Reduction in iteration time}
The efficiency of MAS design is further assessed by measuring \textbf{the time required to complete an end-to-end MAS design iteration}. Traditional methodologies require \textbf{10-50 hours}, depending on the complexity of the environment and the need for manual adjustments. By contrast, MAMAD achieves the same process within \textbf{3-8 hours}, reflecting a \textbf{60-80\% reduction} in overall design time. This acceleration is primarily due to the integration of automated environment modeling, hyperparameter optimization, and organizational inference.

\paragraph{Algorithmic automation index}
To quantify the level of automation across the entire pipeline, we define an \textbf{algorithmic automation index}, representing the proportion of steps that are fully automated. In traditional approaches, only \textbf{30-50\%} of the pipeline is automated, with manual interventions required for role specification, environment modeling, and behavior interpretation. With MAMAD, the automation index increases to \textbf{80-90\%}, indicating that most stages of MAS design require minimal manual intervention.

\

The results indicate that \textbf{MAMAD successfully automates a significant portion of the MAS design process}, substantially reducing the need for human intervention and accelerating iteration time. The \textbf{high automation index (80-90\%)} highlights that MAMAD effectively streamlines multiple design steps, integrating role-based specifications, automated environment modeling, and data-driven policy learning.

\clearpage

\section{Conclusion and perspectives}\label{sec:conclusion}

This work introduced \textbf{MAMAD}, a method designed to automate the development of MAS by integrating organizational modeling with multi-agent reinforcement learning (MARL). Through a structured workflow, MAMAD facilitates environment modeling, agent training, behavior analysis, and deployment, reducing reliance on expert knowledge and increasing automation across the MAS design pipeline. 
% %
% The key contributions of MAMAD can be summarized as follows:
% \begin{itemize}
%     \item \textbf{End-to-End Automation:} MAMAD streamlines the MAS development lifecycle by automating environment modeling, organizational role specification, agent training, and behavior analysis.
%     \item \textbf{Explainable Role and Mission Extraction:} The method integrates organizational modeling via $\mathcal{M}OISE^+$MARL, enabling structured role-based interpretations of emergent behaviors.
%     \item \textbf{Reduction in Human Interventions:} Experiments demonstrate a substantial decrease in the number of manual interventions required, making MAS design more accessible to non-experts.
%     \item \textbf{Scalability to Different MAS Scenarios:} MAMAD was evaluated across diverse multi-agent environments, showing adaptability to cooperative, competitive, and hierarchical task structures.
% \end{itemize}

Quantitative evaluations suggest that MAMAD significantly enhances the efficiency of MAS design by reducing design iteration time, improving compliance with design constraints, and producing explainable agent roles and missions. These results highlight the potential of combining MARL with organizational frameworks to improve MAS development.


Despite its advantages, MAMAD also presents several limitations that warrant further research:
%
\begin{itemize}
    \item \textbf{Residual Need for Expert Oversight:} While MAMAD reduces manual interventions, certain steps (e.g., defining reward structures and tuning hyperparameters) still require expert involvement.
    \item \textbf{Scalability to High-Dimensional Problems:} The method performs well on small- to medium-scale environments but may face limitations when applied to highly complex, dynamic, or real-world MAS settings.
    \item \textbf{Interpretability of Learned Behaviors:} Although MAMAD provides an explainable role extraction process, further improvements are needed to enhance transparency in agent decision-making, particularly in adversarial settings.
    \item \textbf{Computational Overhead:} The integration of automated modeling and learning algorithms increases computational demand, which may limit real-time applications.
\end{itemize}


To further develop MAMAD and enhance its applicability, several research directions can be explored:
%
\begin{itemize}
    \item \textbf{Improving Interpretability Tools:} Future work could focus on integrating more advanced interpretability techniques, such as causal reasoning and attention-based visualizations, to better explain learned behaviors.
    \item \textbf{Scalability to Large-Scale MAS:} Enhancing MAMAD's efficiency for large-scale agent populations and high-dimensional state-action spaces will be key for broader applicability.
    \item \textbf{Hybrid Human-in-the-Loop Approaches:} Combining automated learning with interactive user feedback could balance automation with domain expertise, improving both performance and interpretability.
    \item \textbf{Real-World Deployments:} Extending the evaluation of MAMAD beyond simulated environments to real-world MAS applications (e.g., robotics, cybersecurity, logistics) would provide further validation of its effectiveness.
\end{itemize}



% \section*{Declarations}

% Some journals require declarations to be submitted in a standardised format. Please check the Instructions for Authors of the journal to which you are submitting to see if you need to complete this section. If yes, your manuscript must contain the following sections under the heading `Declarations':

% \begin{itemize}
% \item Funding
% \item Conflict of interest/Competing interests (check journal-specific guidelines for which heading to use)
% \item Ethics approval and consent to participate
% \item Consent for publication
% \item Data availability 
% \item Materials availability
% \item Code availability 
% \item Author contribution
% \end{itemize}

% \noindent
% If any of the sections are not relevant to your manuscript, please include the heading and write `Not applicable' for that section. 

% %%===================================================%%
% %% For presentation purpose, we have included        %%
% %% \bigskip command. Please ignore this.             %%
% %%===================================================%%
% \bigskip
% \begin{flushleft}%
% Editorial Policies for:

% \bigskip\noindent
% Springer journals and proceedings: \url{https://www.springer.com/gp/editorial-policies}

% \bigskip\noindent
% Nature Portfolio journals: \url{https://www.nature.com/nature-research/editorial-policies}

% \bigskip\noindent
% \textit{Scientific Reports}: \url{https://www.nature.com/srep/journal-policies/editorial-policies}

% \bigskip\noindent
% BMC journals: \url{https://www.biomedcentral.com/getpublished/editorial-policies}
% \end{flushleft}

\clearpage

\begin{appendices}

\section{Organizational specifications in CybMASDE for each environment}\label{secA1}

TODO

\section{Detailed results for each baseline}\label{secA2}

TODO

\end{appendices}

%%===========================================================================================%%
%% If you are submitting to one of the Nature Portfolio journals, using the eJP submission   %%
%% system, please include the references within the manuscript file itself. You may do this  %%
%% by copying the reference list from your .bbl file, paste it into the main manuscript .tex %%
%% file, and delete the associated \verb+\bibliography+ commands.                            %%
%%===========================================================================================%%

\bibliography{references}

\end{document}
