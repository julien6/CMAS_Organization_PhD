%Version 3.1 December 2024
% See section 11 of the User Manual for version history
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                                                 %%
%% Please do not use \input{...} to include other tex files.       %%
%% Submit your LaTeX manuscript as one .tex document.              %%
%%                                                                 %%
%% All additional figures and files should be attached             %%
%% separately and not embedded in the \TeX\ document itself.       %%
%%                                                                 %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%\documentclass[referee,sn-basic]{sn-jnl}% referee option is meant for double line spacing

%%=======================================================%%
%% to print line numbers in the margin use lineno option %%
%%=======================================================%%

%%\documentclass[lineno,pdflatex,sn-basic]{sn-jnl}% Basic Springer Nature Reference Style/Chemistry Reference Style

%%=========================================================================================%%
%% the documentclass is set to pdflatex as default. You can delete it if not appropriate.  %%
%%=========================================================================================%%

%%\documentclass[sn-basic]{sn-jnl}% Basic Springer Nature Reference Style/Chemistry Reference Style

%%Note: the following reference styles support Namedate and Numbered referencing. By default the style follows the most common style. To switch between the options you can add or remove �Numbered� in the optional parenthesis. 
%%The option is available for: sn-basic.bst, sn-chicago.bst%  
 
%%\documentclass[pdflatex,sn-nature]{sn-jnl}% Style for submissions to Nature Portfolio journals
%%\documentclass[pdflatex,sn-basic]{sn-jnl}% Basic Springer Nature Reference Style/Chemistry Reference Style
\documentclass[pdflatex,sn-mathphys-num]{sn-jnl}% Math and Physical Sciences Numbered Reference Style
%%\documentclass[pdflatex,sn-mathphys-ay]{sn-jnl}% Math and Physical Sciences Author Year Reference Style
%%\documentclass[pdflatex,sn-aps]{sn-jnl}% American Physical Society (APS) Reference Style
%%\documentclass[pdflatex,sn-vancouver-num]{sn-jnl}% Vancouver Numbered Reference Style
%%\documentclass[pdflatex,sn-vancouver-ay]{sn-jnl}% Vancouver Author Year Reference Style
%%\documentclass[pdflatex,sn-apa]{sn-jnl}% APA Reference Style
%%\documentclass[pdflatex,sn-chicago]{sn-jnl}% Chicago-based Humanities Reference Style

%%%% Standard Packages
%%<additional latex packages if required can be included here>

% \usepackage{graphicx}%
% \usepackage{multirow}%
\usepackage{amsmath,amssymb,amsfonts}%
\usepackage{amsthm}%
\usepackage{mathrsfs}%
% \usepackage[title]{appendix}%
% \usepackage{xcolor}%
% \usepackage{textcomp}%
% \usepackage{manyfoot}%
% \usepackage{booktabs}%
% \usepackage{algorithm}%
% \usepackage{algorithmicx}%
% \usepackage{algpseudocode}%
% \usepackage{listings}%

\usepackage{hyperref}

%%%% For camera-ready, use this
%\documentclass[sigconf]{aamas}

\usepackage{listings}
\usepackage{xcolor}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}
 
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}
 
\lstset{style=mystyle}

% --- Tickz
\usepackage{physics}
\usepackage{tikz}
\usepackage{amsmath}
\usepackage{mathdots}
% \usepackage{yhmath}
\usepackage{cancel}
\usepackage{color}
\usepackage{siunitx}
\usepackage{array}
\usepackage{multirow}
% \usepackage{amssymb}
\usepackage{gensymb}
\usepackage{tabularx}
\usepackage{extarrows}
\usepackage{booktabs}
\usetikzlibrary{fadings}
\usetikzlibrary{patterns}
\usetikzlibrary{shadows.blur}
\usetikzlibrary{shapes}

% ---------

\usepackage{balance} % for balancing columns on the final page
\usepackage{csquotes}
% \usepackage{cite}
\newcommand{\probP}{\text{I\kern-0.15em P}}
\usepackage{etoolbox}
\patchcmd{\thebibliography}{\section*{\refname}}{}{}{}
% \usepackage{amsthm,amssymb,amsfonts}

\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{color}
% \renewcommand\UrlFont{\color{blue}\rmfamily}

\usepackage[inline, shortlabels]{enumitem}
\usepackage{tabularx}
\usepackage{caption}
\usepackage{listings}
\usepackage{stfloats}
\usepackage{titlesec}
\usepackage{ragged2e}
% \usepackage[hyphens]{url}
\usepackage{float}
\usepackage[english]{babel}
\addto\extrasenglish{  
    \def\figureautorefname{Figure}
    \def\tableautorefname{Table}
    \def\algorithmautorefname{Algorithm}
    \def\sectionautorefname{Section}
    \def\subsectionautorefname{Subsection}
    \def\proofoutlineautorefname{Proof Outline}
}

\usepackage[linesnumbered, ruled, vlined]{algorithm2e}
\SetKwComment{Comment}{$\triangleright$\ }{}
\SetAlgoNlRelativeSize{0}
\SetAlgoNlRelativeSize{-1}

\usepackage{amssymb}
\usepackage{pifont}
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%

%%%%%=============================================================================%%%%
%%%%  Remarks: This template is provided to aid authors with the preparation
%%%%  of original research articles intended for submission to journals published 
%%%%  by Springer Nature. The guidance has been prepared in partnership with 
%%%%  production teams to conform to Springer Nature technical requirements. 
%%%%  Editorial and presentation requirements differ among journal portfolios and 
%%%%  research disciplines. You may find sections in this template are irrelevant 
%%%%  to your work and are empowered to omit any such section if allowed by the 
%%%%  journal you intend to submit to. The submission guidelines and policies 
%%%%  of the journal take precedence. A detailed User Manual is available in the 
%%%%  template package for technical guidance.
%%%%%=============================================================================%%%%

%% as per the requirement new theorem styles can be included as shown below
\theoremstyle{thmstyleone}%
\newtheorem{theorem}{Theorem}%  meant for continuous numbers
%%\newtheorem{theorem}{Theorem}[section]% meant for sectionwise numbers
%% optional argument [theorem] produces theorem numbering sequence instead of independent numbers for Proposition
\newtheorem{proposition}[theorem]{Proposition}% 
%%\newtheorem{proposition}{Proposition}% to get separate numbers for theorem and proposition etc.

\theoremstyle{thmstyletwo}%
\newtheorem{example}{Example}%
\newtheorem{remark}{Remark}%

\theoremstyle{thmstylethree}%
\newtheorem{definition}{Definition}%

\raggedbottom
%%\unnumbered% uncomment this for unnumbered level heads

\begin{document}



\title[Assisting Multi-Agent System Design with MOISE+ and MARL: The MAMAD Method]{Assisting Multi-Agent System Design with $\mathcal{M}OISE^+$ and MARL: The MAMAD Method}

%%=============================================================%%
%% GivenName	-> \fnm{Joergen W.}
%% Particle	-> \spfx{van der} -> surname prefix
%% FamilyName	-> \sur{Ploeg}
%% Suffix	-> \sfx{IV}
%% \author*[1,2]{\fnm{Joergen W.} \spfx{van der} \sur{Ploeg} 
%%  \sfx{IV}}\email{iauthor@gmail.com}
%%=============================================================%%

\author*[1]{\fnm{Julien} \sur{Soulé}}\email{julien.soule@lcis.grenoble-inp.fr}

\author[1]{\fnm{Jean-Paul} \sur{Jamont}}\email{jean-paul.jamont@lcis.grenoble-inp.fr}
% \equalcont{These authors contributed equally to this work.}

\author[1]{\fnm{Michel} \sur{Occello}}\email{michel.occello@lcis.grenoble-inp.fr}

\author[2]{\fnm{Louis-Marie} \sur{Traonouez}}\email{louis-marie.traonouez@thalesgroup.com}

\author[3]{\fnm{Paul} \sur{Théron}}\email{paul.theron@orange.fr}

\affil*[1]{\orgdiv{Laboratoire de Conception et d'Intégration des Systèmes (LCIS)}, \orgname{Université Grenoble Alpe}, \orgaddress{\street{50 Rue Barthélémy de Laffemas}, \city{Valence}, \postcode{26000}, \state{Auvergne-Rhône-Alpes}, \country{France}}}

\affil[2]{\orgdiv{Thales Land and Air Systems}, \orgname{BL IAS}, \orgaddress{\street{1 Rue Louis Braille}, \city{Saint-Jacques-de-la-Lande}, \postcode{35136}, \state{Ille-et-Vilaine}, \country{France}}}

\affil[3]{\orgname{AICA IWG}, \orgaddress{\street{22 Av. Gazan Prolongée, 06600 Antibes}, \city{Antibes}, \postcode{06600}, \state{Provence-Alpes-Côte d'Azur}, \country{France}}}

%%==================================%%
%% Sample for unstructured abstract %%
%%==================================%%

\abstract{
    Traditional Agent-Oriented Software Engineering (AOSE) methods rely on explicit and expert-driven design for Multi-Agent Systems (MAS), but often lack automation. In contrast, Multi-Agent Reinforcement Learning (MARL) and related fields offer automated ways to model environments and learn suitable agent policies. However, integrating these techniques into AOSE remains underexplored partly due to the lack of control, explainability, and unifying frameworks.
    %
    We propose \textbf{MOISE+MARL Assisted MAS Design (MAMAD)}, a four-phase method framing MAS design as a constrained optimization problem: learning joint policies that maximize rewards while respecting $\mathcal{M}OISE^+$ roles and goals. The phases include:
    1) \textbf{Modeling} the environment,  
    2) \textbf{Training} under organizational constraints,  
    3) \textbf{Analyzing} emergent behaviors,  
    4) \textbf{Transferring} to real-world deployment.
    %
    We evaluate MAMAD on various environments, showing that the generated MAS exhibit expected performance, compliance with design requirements and are explainable, while reducing manual design overhead.
}

% Introduction
% Purpose
% Methods
% Results
% Conclusion

\keywords{Agent-oriented Software Engineering, Multi-Agent Reinforcement Learning, Assisted-Design, Organizational Model}

%%\pacs[JEL Classification]{D8, H51}

%%\pacs[MSC Classification]{35A01, 65L10, 65L12, 65L20, 65L70}

\maketitle



\section{Introduction}

\subsection{Context}

Designing \textbf{Multi-Agent Systems (MAS)} for real-world domains (such as cybersecurity, autonomous logistics or robotic swarms) requires designing agents that are both autonomous and coordinated, adaptable and structured. To address these demands, the field of \textbf{Agent-Oriented Software Engineering (AOSE)} has historically provided principled methodologies based on symbolic representations. \textbf{GAIA}~\cite{gaia1998}, \textbf{ADELFE}~\cite{adelfe2002}, and \textbf{DIAMOND}~\cite{Jamont2005} provide well-defined processes for designing MAS, relying on explicit modeling with possibly explicit roles, missions, or interaction protocols~\cite{Pavon2003,Bernon2005}. They offer guarantees in terms of predictability, safety, and explainability by leveraging expert-driven design processes~\cite{Hindriks2014,Jamont2O15}. However, these methods are largely manual and require specialized knowledge to define agent behaviors, making scalability in complex or dynamic environments cumbersome.

To improve efficiency and scalability, several AOSE frameworks have also sought to automate key aspects of MAS design. INGENIAS leverages model-driven engineering to generate code and documentation from high-level specifications~\cite{Pavon2003}, while KB-ORG uses organizational templates and domain knowledge to automate the assignment of roles and responsibilities~\cite{Sims2008}. More recent efforts explore the automatic generation and composition of agent components~\cite{smith2024automated}, as seen in the Automated Design of Agentic Systems field. AutoGenesisAgent goes further by enabling MAS to autonomously design and deploy other MAS tailored to specific tasks~\cite{harper2024autogenesisagent}.

Despite these efforts towards automation, these AOSE works still faces major limitations towards a fully end-to-end automated design process. Crucially, they lack automated support for modeling complex environments as test environments, optimizing agent behaviors, or analyzing emergent dynamics across large-scale interactions. These gaps hinder their application in real-world systems where constant adaptation and scalable intelligence are key.

In parallel, \textbf{Machine Learning (ML)} has led to diverse works and subfields that, although developed independently from AOSE in Multi-Agent Systems paradigm, provide capabilities likely relevant for MAS design offering the automation, adaptivity, and scalability that AOSE lacks. Two major subfields are:
\begin{enumerate*}[label={\roman*)}, itemjoin={; \quad}]
    \item \textbf{World Models}~\cite{Ha2018}, which learn high-fidelity environment simulations from agents trajectories
    \item \textbf{Multi-Agent Reinforcement Learning (MARL)}~\cite{Zhang2021, Papoudakis2021}, which enables decentralized policy optimization through exploration and trial-and-error
\end{enumerate*}

However, ML-based approaches also present key limitations. While MARL enables agents to learn without manual supervision, the resulting policies are often opaque, difficult to control, and poorly aligned with explicit design requirements~\cite{Nguyen2020, Du2022}, limiting their safe deployment.
Although World Models~\cite{Ha2018} show promise in single-agent settings, their extension to multi-agent systems remains challenging due to increased complexity in coordination and observability.
Most importantly, there is still no fully automated MAS design framework that bridges real-world environments with a pipeline orchestrating potentially leveraged ML-based works, leaving a major gap for practical deployment.

In light of this contrast, there is a strong motivation to explore how the symbolic, model-based rigor of AOSE could be complemented by the learning-based automation of ML. Rather than viewing these paradigms as incompatible, we argue that they can be unified within a hybrid methodology, benefitting from both domains.


\subsection{Problem statement and research gaps}

In order to gather AOSE and MARL domains we adopt an optimization perspective on MAS design.
We consider the design of a MAS operating in a real-world environment and expected to achieve a global objective, possibly under additional organizational requirements. The core of this problem is formulated as a \textbf{constrained optimization process in a MARL setting}, where:
\begin{itemize}
    \item The \textbf{optimization variable} is the agents' joint policy;
    \item The \textbf{objective function} maximizes cumulative rewards over time;
    \item The \textbf{constraints} encode high-level requirements.
\end{itemize}

Based on this core formulation, we aim to bridge AOSE gaps by ML techniques while bridging ML gaps with AOSE reversely:

\begin{itemize}
    \item \textbf{(G1) End-to-end automation.} Traditional AOSE methods require human expertise for each phase of MAS design, from environment modeling to behavior specification. ML techniques, particularly World Models and MARL, have the potential to automate these phases. However, no existing framework integrates these techniques into a unified MAS design pipeline. Moreover, World Models remain underdeveloped in multi-agent settings, especially in realistic deployment scenarios. Bridging this gap would allow scalable, self-adapting MAS design based on data rather than expert rules.
    \item \textbf{(G2) Compliance with design requirements.} Most MARL approaches focus on performance optimization without enforcing design-time requirements such as safety rules, organizational roles, or mission structures. Recent work like MOISE+MARL~\cite{soule2025moisemarl} shows the feasibility of introducing structured constraints into policy learning, but remains isolated. Addressing this gap requires a general approach to express and enforce such constraints across training.

    \item \textbf{(G3) Organizational-level explainability.} ML-generated policies are often black boxes, making it difficult to interpret how agent behaviors fulfill the intended organization. In AOSE, behavior is derived from structured roles and missions, enabling clear traceability. Extending such explainability to MARL-trained policies would support trust and validation in critical applications. Yet, few works address explainability from a collective, organizational point of view.
\end{itemize}

Together, these gaps outline the need for a method that both \textit{automates} MAS design and \textit{preserves} organizational structure and explainability. Our proposed method, MAMAD, aims to close these gaps by combining formal AOSE models and reinforcement learning into a unified, iterative process.

\subsection{Contributions and paper organization}

We propose the \textbf{MAMAD method}, an extension of the \textbf{MOISE+MARL} framework~\cite{soule2025moisemarl}
% \footnote{This article introducing MOISE+MARL has been accepted for AAMAS 2025 and is freely available at \url{https://arxiv.org/abs/2503.23615}}
, to structure learning in MARL. MOISE+MARL enables controlled policy learning, aligning agent behaviors with predefined organizational specifications while extracting insights from emergent behaviors. MAMAD wraps MOISE+MARL within a fully automated, four-phase design process driven by three inputs: the environment, user-defined requirements, and the global objective.

In classical AOSE, the \textit{Requirement Engineering} phase involves specifying design requirements, environmental constraints, and global objectives~\cite{Pavon2003, Bernon2005}. We assume these are obtained via a chosen engineering methodology. MAMAD operationalizes them with the MOISE+MARL framework as part of an automated design workflow.

\begin{enumerate}
    \item \textbf{Modeling Phase:} Automatically builds a simulated environment by training a neural architecture inspired by \textbf{World Models}~\cite{Ha2018}, and defines the MAS global goal (decomposable into intermediary subgoals) through a reward function.
    \item \textbf{Training Phase:} Agents learn in the simulated environment, optionally guided by organizational constraints: roles (limiting allowable actions) and goals (shaping rewards). These structure the learning process as user-defined specifications.
    \item \textbf{Analysis Phase:} Unsupervised learning techniques analyze successful trajectories to extract emergent roles and goals within the constrained policy space, yielding validated specifications and policies.
    \item \textbf{Transfer Phase:} The validated joint policy is deployed in the real-world environment to operationalize the trained MAS.
\end{enumerate}

We evaluated \textbf{MAMAD} in gamified environments, used as controlled testbeds to assess its ability to generate high-fidelity simulations during the Modeling phase (bypassing the complexity of modeling physical environments). Results show strong alignment between the specifications applied during training and those inferred post hoc, validating both \textbf{(G3) Organizational-level explainability} and \textbf{(G2) Compliance with design requirements}. Compared to manual methods, \textbf{(G1) End-to-end automation.} improved significantly, requiring fewer interventions. Ablation studies revealed that omitting automated modeling reduced policy generalization, while removing organizational constraints led to erratic agent behavior.

\

The rest of the paper is organized as follows. \autoref{sec:related_works} detail each gap reviewing related works for each of them. \autoref{sec:mamad} presents the MAMAD method by detailing the algorithmic design workflow. Each phase introduces contributions bridging targeted gaps and their use.\autoref{sec:experimental_setup} describes the experimental setup. \autoref{sec:results} reports and analyzes the results. \autoref{sec:conclusion} concludes the paper and outlines future directions.



\section{Related works}\label{sec:related_works}

This section reviews existing research through targeted gaps. \autoref{sub-sec:rel_aose_automate_marl} surveys AOSE methodologies and recent efforts to automate various aspects of MAS design (G4). It also discusses the current limitations in integrating MARL into AOSE, while considering approaches based on Reinforcement Learning (RL) that contribute to MAS development (G1).
\autoref{sub-sec:rel_control} presents existing methods for guiding or constraining the learning process in MARL and RL (G2).
\autoref{sub-sec:rel_evaluation} reviews works focused on enhancing the explainability of learned agent policies (G3).

An overview of the reviewed literature is provided in \autoref{tab:related_works}.

\begin{table*}[h!]
    \centering
    \caption{Coverage of related works with respect to the targeted gaps.}
    \label{tab:related_works}
    \begin{tabular}{p{7.5cm}cccc}
        \toprule
        \textbf{Reference / Approach}                                                                                              & \textbf{G1} & \textbf{G2} & \textbf{G3} & \textbf{G4} \\
        \midrule
        GAIA~\cite{gaia1998}, ADELFE~\cite{adelfe2002}, DIAMOND~\cite{Jamont2005}                                                  & \xmark      & \xmark      & \xmark      & $\sim$      \\
        KB-ORG~\cite{Sims2008}                                                                                                     & \xmark      & \xmark      & \xmark      & $\sim$      \\
        ADAS~\cite{smith2024automated}, AutoGenesisAgent~\cite{harper2024autogenesisagent}                                         & \xmark      & \xmark      & \xmark      & \cmark      \\
        BMW Agents~\cite{crawford2024bmw}                                                                                          & \xmark      & \xmark      & $\sim$      & \cmark      \\
        Rahwan et al.~\cite{rahwan2006integrating}                                                                                 & $\sim$      & \xmark      & \xmark      & $\sim$      \\
        Gómez-Rodríguez et al.~\cite{gomez2011modeling}                                                                            & $\sim$      & \xmark      & \xmark      & $\sim$      \\
        CyberSec MARL framework~\cite{hammar2023scalable}                                                                          & $\sim$      & $\sim$      & $\sim$      & $\sim$      \\
        Spieker~\cite{spieker2021constraint}                                                                                       & \xmark      & \cmark      & $\sim$      & \xmark      \\
        Kalweit et al.~\cite{kalweit2020deep}                                                                                      & \xmark      & \cmark      & $\sim$      & \xmark      \\
        Achiam et al.~\cite{achiam2017constrained}                                                                                 & \xmark      & \cmark      & $\sim$      & \xmark      \\
        Zhou et al.~\cite{zhou2024mentor}                                                                                          & \xmark      & $\sim$      & $\sim$      & \xmark      \\
        Miryoosefi et al.~\cite{miryoosefi2021}                                                                                    & \xmark      & $\sim$      & \xmark      & \xmark      \\
        Zabounidis et al.~\cite{zabounidis2023concept}                                                                             & \xmark      & \xmark      & \cmark      & \xmark      \\
        Iturria-Rivera et al.~\cite{iturria2024explainable}                                                                        & \xmark      & \xmark      & \cmark      & \xmark      \\
        Liu et al.~\cite{liu2025}                                                                                                  & \xmark      & \xmark      & \cmark      & \xmark      \\
        Poupart et al.~\cite{poupart2025perspectives}                                                                              & \xmark      & \xmark      & $\sim$      & \xmark      \\
        Li et al.~\cite{li2025from}                                                                                                & \xmark      & \xmark      & \cmark      & \xmark      \\
        Wilson~\cite{Williams2004}, Berenji~\cite{Zhang2013}, Yusuf~\cite{yusuf2020inferential}, Serrino~\cite{serrino2019finding} & $\sim$      & $\sim$      & $\sim$      & \xmark      \\
        \bottomrule
    \end{tabular}
\end{table*}


\subsection{Automation and analogous MARL-driven works in AOSE}\label{sub-sec:rel_aose_automate_marl}

AOSE has introduced structured methodologies for MAS design, emphasizing role-based interactions, organizational hierarchies, and systematic agent coordination. Classical frameworks such as \textbf{GAIA}~\cite{gaia1998}, \textbf{ADELFE}~\cite{adelfe2002}, and \textbf{DIAMOND}~\cite{Jamont2005} provide well-defined processes for designing MAS, relying on explicit organizational modeling. However, these methods are largely manual and require specialized knowledge to define agent behaviors, roles, and organizational constraints, making scalability in complex or dynamic environments cumbersome.

To enhance efficiency and scalability, various methodologies and frameworks have been proposed to automate different aspects of MAS design:

\textbf{Model-Driven Engineering in AOSE:} INGENIAS adopts a model-driven engineering approach, providing meta-models and tools that facilitate the automatic generation of code, documentation, and tests from high-level specifications, thereby streamlining the development process of MAS~\cite{Pavon2003}.

\textbf{Knowledge-Based Organizational Design:} The KB-ORG framework introduces a knowledge-based approach to automate the organizational design of MAS. By leveraging predefined templates and organizational knowledge, KB-ORG assists in the systematic assignment of roles and responsibilities to agents, reducing the manual overhead in organizational specification~\cite{Sims2008}.

\textbf{Automated Design of Agentic Systems:} The emerging field of Automated Design of Agentic Systems focuses on the automatic creation of agent-based system designs. This includes the generation of novel building blocks and their composition into functional systems, aiming to minimize human intervention in the design phase~\cite{smith2024automated}.

\textbf{Self-Generating Multi-Agent Systems:} AutoGenesisAgent presents a framework where multi-agent systems are capable of autonomously designing and deploying other multi-agent systems tailored for specific tasks. This self-generative capability encompasses the entire lifecycle from initial concept to deployment, highlighting a significant step towards fully automated MAS design~\cite{harper2024autogenesisagent}.

\textbf{Collaborative Frameworks for Task Automation:} The BMW Agents framework emphasizes task automation through multi-agent collaboration. It outlines a flexible agent engineering approach that supports planning and execution across various domains, ensuring scalability and adaptability in complex industrial applications~\cite{crawford2024bmw}.

\

Parallel to traditional AOSE methodologies, the field of \textbf{MARL} enables agents to \textbf{autonomously learn} coordination strategies from experience. Even though, MARL/RL is not necessarly intended for AOSE, its capability for generating \textbf{self-organizing agent behaviors} may participate in it.
A notable project in that direction is the \textit{Cyber Security Learning Environment}~\cite{hammar2023scalable}, which is an \textbf{online framework} for cybersecurity purposes where an agent is trained in a simulation to learn task-specific behaviors dynamically. This framework partially enables \textbf{automating end-to-end MAS design} since it follows an automated pipeline spanning from the environment modeling to training and deployment with minimal manual effort. It also offers visualization tools to interpret agent interactions, though without explicit organizational modeling.

Despite these advancements, challenges remain in seamlessly integrating AOSE and MARL. Aligning learning algorithms with organizational constraints and ensuring that emergent behaviors adhere to system specifications are ongoing research areas. Future work is directed towards developing standardized frameworks that encapsulate both design-time organizational models and run-time learning mechanisms, fostering the development of robust and adaptable multi-agent systems.


\subsection{Control or guidance in MARL}\label{sub-sec:rel_control}

In MARL, guiding or constraining the learning process is essential to ensure that agents develop behaviors aligning with safety, fairness, and task-specific requirements. Various methodologies have been proposed to incorporate such constraints into the learning framework.

\textbf{Constraint-Guided Reinforcement Learning.} Spieker~\cite{spieker2021constraint} introduces Constraint-Guided Reinforcement Learning, which integrates constraint models into the agent-environment interaction. This approach allows agents to learn optimal policies within specified behavioral constraints, enhancing safety and reliability during training and deployment.

\textbf{Constrained Q-Learning.} Kalweit et al.~\cite{kalweit2020deep} propose Deep Constrained Q-Learning, an off-policy algorithm that restricts the action space during the Q-value update process. By incorporating both single-step and approximate multi-step constraints, this method ensures that the learned policies adhere to predefined safety and performance criteria.

\textbf{Constrained Policy Optimization.} Achiam et al.~\cite{achiam2017constrained} develop Constrained Policy Optimization (CPO), a policy search algorithm that enforces constraints throughout the training process. CPO provides theoretical guarantees for near-constraint satisfaction at each iteration, making it suitable for applications requiring strict adherence to safety standards.

\textbf{Hierarchical Guidance with Human Feedback.} Zhou et al.~\cite{zhou2024mentor} present MENTOR, a Hierarchical RL framework that incorporates human feedback and dynamic distance constraints. This approach guides the agent in selecting subgoals that are neither too easy nor too difficult, facilitating a more stable and efficient learning process in complex tasks.

\textbf{Reward-Free Constrained Learning.} Miryoosefi et al.~\cite{miryoosefi2021} propose a reward-free approach to constrained RL. By focusing on constraint satisfaction without relying on explicit reward signals, this method addresses scenarios where defining a suitable reward function is challenging or impractical.

These methodologies highlight the importance of incorporating constraints and guidance mechanisms into MARL to ensure that agents learn behaviors that are not only effective but also align with predefined safety and performance standards.


\subsection{Explainability in MARL}\label{sub-sec:rel_evaluation}

Explainability and interpretability in MARL aim to make the behavior of agents transparent, facilitating trust, debugging, and compliance with safety standards.

\textbf{Concept-Based Interpretability.} Zabounidis et al.~\cite{zabounidis2023concept} introduce a method that incorporates interpretable concepts provided by domain experts into MARL models. By requiring the model to predict these concepts before making decisions, the approach enhances transparency and allows for expert intervention to correct mispredictions, improving both interpretability and performance.

\textbf{Reward Decomposition Techniques.} Iturria-Rivera et al.~\cite{iturria2024explainable} propose an explainable MARL framework that utilizes reward decomposition in value function factorization-based algorithms like VDN and QMIX. This method provides insights into how individual components of the reward function contribute to the overall decision-making process, enhancing the interpretability of agent behaviors.

\textbf{Interpretable Model Architectures.} Liu et al.~\cite{liu2025} present MIXRTs, an architecture that combines recurrent neural networks with soft decision trees to create interpretable MARL models. This design allows for explicit representation of decision processes and clarifies each agent's contribution to team performance, addressing the opacity of traditional deep learning models.

\textbf{Post-Hoc Explanation Methods.} Poupart et al.~\cite{poupart2025perspectives} discuss various post-hoc techniques for interpreting MARL models, such as relevance backpropagation and activation patching. These methods aim to extract explanations from trained models without altering their architectures, providing insights into agent behaviors and emergent phenomena.

\textbf{Model-Agnostic Interpretability.} Li et al.~\cite{li2025from} propose a model-agnostic approach that leverages Shapley values to transform complex deep RL policies into transparent representations. This technique bridges the gap between explainability and interpretability, offering stable and interpretable policies applicable to both on-policy and off-policy algorithms.

\textbf{Role and Goal Inference.} Several works explore role or goal inference as a means to assess organizational fit. Wilson et al.~\cite{Williams2004} propose role transfer in Multi-Agent MDPs to enhance adaptability, though limited to task-specific roles. Berenji and Vengerov~\cite{Zhang2013} model agent dependencies to improve coordination in UAV missions, but without abstract role computation. Yusuf and Baber~\cite{yusuf2020inferential} use Bayesian inference for dynamic task coordination, yet lack explicit organizational alignment. Serrino et al.~\cite{serrino2019finding} infer roles through interaction in social environments, focusing on operational rather than organizational roles.



% \section{Theoretical background}\label{sec:background}

% This section recaps the notation and basics we used in our contributions for MARL and the $\mathcal{M}OISE^+$ organizational model.

% \subsection{Markov framework for MARL}

% To apply MARL techniques, we rely on the \textit{Decentralized Partially Observable Markov Decision Process} (Dec-POMDP) \cite{Oliehoek2016}. Dec-POMDPs model decentralized multi-agent coordination under partial observability, making them well suited for integrating organizational constraints. Unlike \textit{Partially Observable Stochastic Games} (POSG), the Dec-POMDP features a common reward function, promoting collaboration~\cite{Matignon2007}.

% A Dec-POMDP $d \in D$ (with $D$ the set of Dec-POMDP) is a 7-tuple $d = (S,\{A_i\},T,R,\{\Omega_i\},O,\gamma)$ , where:
% \begin{itemize}
%     \item $S = \{s_1, ..s_{|S|}\}$: The set of the possible states.
%     \item $A_{i} = \{a_{1}^{i},..,a_{|A_{i}|}^{i}\}$: The set of the possible actions for agent $i$.
%     \item $T$ so that $T(s,a,s') = \probP{(s'|s,a)}$ : The set of conditional transition probabilities between states
%     \item $R: S \times A \times S \rightarrow \mathbb{R}$: The reward function
%     \item $\Omega_{i} = \{o_{1}^{i},..,o_{|\Omega_{i}|}^{i}\}$: The set of observations for agent $ag_i$
%     \item $O$ so that $O(s',a,o) = \probP{(o|s',a)}$ : The set of conditional observation probabilities.
%     \item $\gamma \in [0,1]$, the discount factor
% \end{itemize}

% Considering $m$ \textbf{teams} (also referred as \textbf{groups}) each containing several agents among $\mathcal{A}$, we also detail the minimal formalism notation we re-used for solving the Dec-POMDP for a given team $i, 0 \leq i \leq m$ containing $n$ agents~\cite{Matignon2007,Yuan2023}:

% \begin{itemize}

%     \item $\Pi$: the set of policies. A \textbf{policy} $\pi \in \Pi, \pi: \Omega \rightarrow A$ deterministically maps an observation to an action. It represents the agent's internal logic;
%     \item $\Pi^{j}$: the set of joint-policies. A \textbf{joint-policy} $\pi^{j} \in \Pi^{j}, \pi^{j}: \Omega^n \rightarrow A^n = \Pi^n$ chooses an action for each agent regarding their respective observation. It can be viewed as a set of the policies used in agents;
%     \item $H$: the set of histories. A \textbf{history} (we also interchangeably call \textbf{trajectory}) over $z \in \mathbb{N}$ steps (where $z$ is generally the maximum number of steps for an episode) is the $z$-tuple $h = ((\omega_{k}, a_{k}) | k \leq z, \omega \in \Omega, a \in A)$
%     \item $H^{j}$: the set of joint-histories. A \textbf{joint-history} over $z \in \mathbb{N}$ steps $h^{j} \in H^{j}, h^{j} = \{h_1,h_2..h_n\}$ is the set of the agents' histories.
%     \item $U^{j}_{i}(<\pi^{j}_{i}, \pi^{j}_{-i}>): \Pi^{j} \rightarrow \mathbb{R}$: gives the \textbf{expected cumulative reward} over a finite horizon (if $\gamma < 1$ or the number of steps in an episode is finite), with $\pi^{j}_{i}$ the joint policy for team $i$ and $\pi^{j}_{-i}$ all of the other concatenated joint-policies (considered as fixed);
%     \item $BR^{j}_{i}(\pi^{j}_{i}) = argmax_{\pi^{j}_{i}}(U(<\pi^{j}_{i},\pi^{j}_{-i}>))$: gives the \textbf{best response} $\pi^{j,*}_{i}$ in the sense that the team cannot change any of the policies in the joint-policy $\pi^{j,*}_{i}$ to get a better expected cumulative reward than $U_i^* = U^{j}_{i}(<\pi^{j,*}_{i}, \pi^{j}_{-i}>)$;
%     \item $SR^{j}_{i}(\pi^{j}_{i}, s) = \{\pi^{j}_{i} | U(<\pi^{j}_{i},\pi^{j}_{-i}>) \geq s\}$: gives the \textbf{sufficient response} as the set of joint-policies getting at least $s \in \mathbb{R}, s \leq U_i^*$ as expected cumulative reward.
% \end{itemize}

% \noindent We refer to \textbf{solving} the Dec-POMDP for the team $i$ as finding a joint policy $\pi^{j}_{i} \in \Pi^{j}, \pi^{j}_{i} = BR^{j}_{i}(\pi^{j}_{i})$ that maximize the expected cumulative reward over a finite horizon.
% We refer to \textbf{sub-optimally solving} the Dec-POMDP at $s$ expectancy as finding a the joint policies $\pi^{j}_{i} \in \Pi^{j}, \pi^{j}_{i} = SR^{j}_{i}(\pi^{j}_{i}, s)$ that gets the expected cumulative reward over a finite horizon at least at $s \in \mathbb{R}, s \leq U_i^*$.

% % We refer to \textbf{solving the Dec-POMDP} as the search for a joint policy $\pi^{j} \in \Pi^{j}$ such that $\pi^{j}s)$, achieving at least an expected cumulative reward of $s$, where $s \in \mathbb{R}$.

% \subsection{The $\mathcal{M}OISE^+$ Organizational Model}

% The $\mathcal{M}OISE^+$ model~\cite{Hubner2002, Hubner2007} provides a comprehensive formal framework for specifying multi-agent organizations. While $\mathcal{M}OISE^+$ offers a comprehensive set of structural, functional, and deontic specifications, we show notation for the core components directly relevant to our approach: \textit{roles}, \textit{missions} (goals), and \textit{permissions/obligations}.

% \paragraph{\textbf{Structural specifications}}
% %
% The structural specifications of the $\mathcal{M}OISE^+$ model include the roles, denoted $\mathcal{R}$ (with $\rho \in \mathcal{R}$).
% %
% Structural specifications also contain inheritance relation, the definition of groups and sub-groups, interconnected via links that encode various inter-role relationships such as acquaintance, communication, and authority, as well as compatibility relations defining which roles can be played simultaneously by the same agent, and cardinality constraints specifying the allowed number of agents assigned to roles and groups.

% \paragraph{\textbf{Functional specifications}}

% The functional specifications of the $\mathcal{M}OISE^+$ model include:
% %
% \begin{enumerate*}[label={\roman*)}, itemjoin={; \quad}]
%     \item global goals, denoted $\mathcal{G}$ (with $g \in \mathcal{G}$), decomposed into hierarchical structures through plans, where decomposition operators specify how sub-goals contribute to higher-level objectives;
%     \item missions that contain assigned goals, denoted $\mathcal{M}$ (with $m \in \mathcal{M}$)
%     \item a goal to mission mapping, denoted $mo: \mathcal{M} \rightarrow \mathbb{P}(\mathcal{G})$.
% \end{enumerate*}
% %
% Functional specifications also include mission cardinalities indicating how many agents may commit to each mission (fixed to one in our contribution), and complemented by preference orders expressing social priorities when multiple missions are simultaneously available.

% \paragraph{\textbf{Deontic specifications}}

% The deontic specifications of the $\mathcal{M}OISE^+$ model define the agents' normative constraints through:
% %
% \begin{enumerate*}[label={\roman*)}, itemjoin={; \quad}]
%     \item permissions, denoted $\mathcal{PER} = (\rho_a,m,tc)$ (aslo denoted $per(\rho_a,m,tc)$) meaning an agent playing role $\rho_a \in \mathcal{R}$ is permitted to commit on mission $m \in \mathcal{M}$ for a given time constraint $tc \in \mathcal{TC}$;
%     \item and obligations, denoted $\mathcal{OBL} = (\rho_a,m,tc)$ (aslo denoted $obl(\rho_a,m,tc)$) meaning an agent playing role $\rho_a \in \mathcal{R}$ is obliged to commit on mission $m \in \mathcal{M}$ for a given time constraint $tc \in \mathcal{TC}$;
% \end{enumerate*}
% %
% A time constraint $tc \in \mathcal{TC}$ specifies a set of periods during which a permission or obligation is valid ($Any \in \mathcal{TC}$ means everytime);

% \

% \noindent Organizational specifications applied to agents are roles and goals (as missions) through permissions or obligations. Indeed, the other structural specifications such as compatibilities or links are inherent to roles. Similarly, we consider that the goals, the missions, and their mapping ($mo$) are enough to also link all of the other functional specifications such as plans, cardinalities, or preference orders.
% Consequently, we consider it is sufficient to take into account roles, missions (goal and mapping) and permissions/obligations when linking $\mathcal{M}OISE^+$ with Dec-POMDP. 

% For this reason, we adopt a minimal but sufficient formalization focused on the following organizational specification set $\mathcal{OS} = \langle \mathcal{R}, \mathcal{M}, \mathcal{PER}, \mathcal{OBL}, mo \rangle$ to guide organizational policy learning within our $\mathcal{M}OISE^+$ MARL framework.


% \subsection{Linking $\mathcal{M}OISE^+$ with MARL}

% \begin{figure}[h!]
%     \centering
%     \input{figures/mm_synthesis_single_column.tex}
%     \caption{A minimal view of the MOISE+MARL framework:
%         Users first define $\mathcal{M}OISE^+$ specifications, which include roles ($\mathcal{R}$) and missions ($\mathcal{M}$), both associated through $rds$. They then create MOISE+MARL specifications by first defining \textbf{Constraint guides} such as $rag$ and $rrg$ to specify role logic, and $grg$ for goal logic. 
%         Next, \textbf{Linkers} are used to connect agents with roles through $ar$ and to link the logic of the constraint guides to the defined $\mathcal{M}OISE^+$ specifications. Once this is set up, roles can be assigned to agents, and the MARL framework updates accordingly during training.
%     }
%     \label{fig:mm_synthesis}
% \end{figure}

% \

% \noindent The \textbf{Constraint Guides} are three new relations introduced to describe the logics of $\mathcal{M}OISE^+$ roles and goals in the Dec-POMDP formalism:
% %
% \begin{itemize}
%     % \begin{enumerate*}[label={\roman*) },itemjoin={; \quad}]
    
%     \item \textbf{Role Action Guide} \quad $rag: H \times \Omega \rightarrow \mathcal{P}(A \times \mathbb{R})$, the relation that models a role as a set of rules which, for each pair consisting of a history $h \in H$ and an observation received by the agent $\omega \in \Omega$, associates expected actions $A \in \mathcal{P}(A)$ each associated with a constraint hardness $ch \in [0,1]$ ($ch = 1$ by default). By restricting the choice of the next action among those authorized, the agent is forced to adhere to the role's expected behavior
%     \item \textbf{Role Reward Guide} \quad $rrg: H \times \Omega \times A \to \mathbb{R} = \{r_m \text{ if } a \notin A_\omega \text{, } rag(h, \omega) \allowbreak = \allowbreak A_\omega \times \mathbb{R} \text{, } h \in H; \text{ else } 0\}$, the relation that models a role by adding a penalty $r_m$ to the global reward if the last action chosen by the agent $a \in A$ is not authorized. This is intended to encourage the agent to adhere to the expected behavior of a role
%     \item \textbf{Goal Reward Guide} \quad $grg: H \rightarrow \mathbb{R}$, the relation that models a goal as a soft constraint adding a reward bonus $r_b \in \mathbb{R}$ if the agent's history $h \in H$ contains a goal's characteristic sub-sequence $h_g \in H_g$, encouraging the agent to reach it.
%           % \end{enumerate*}
% \end{itemize}

% \

% \noindent Finally, we introduce the \textbf{Linkers} to link the $\mathcal{M}OISE^+$ organizational specifications with constraint guides and agents:
% %
% \begin{itemize}
%     % \begin{enumerate*}[label={\roman*) },itemjoin={; \quad}]
    
%     \item \textbf{Agent to Role} \quad $ar: \mathcal{A} \to \mathcal{R}$, the bijective relation linking an agent to a role;
%     \item \textbf{Role to Constraint Guide} \quad $rcg: \mathcal{R} \rightarrow rag \cup rrg$, the relation associating each $\mathcal{M}OISE^+$ role to a $rag$ or $rrg$ relation, forcing/encouraging the agent to follow the expected actions for the role $\rho \in \mathcal{R}$;
%     \item \textbf{Goal to Constraint Guide} \quad $gcg: \mathcal{G} \rightarrow grg$, the relation linking goals to $grg$ relations, representing goals as rewards in MARL.
%           % \end{enumerate*}
% \end{itemize}

% \paragraph{\textbf{Resolving the Dec-POMDP with MOISE+MARL}}

% A MOISE+MARL model is formalized as $\mathcal{MM} = \langle \mathcal{OS}, ar, rcg, gcg, rag, rrg, grg\rangle$.
% Resolving the Dec-POMDP $d \in D$ with a $mm \in \mathcal{MM}$ model involves finding a joint policy $\pi^{j} = \{\pi^j_0,\pi^j_1\dots\pi^j_n\}$ that maximizes the state-value function $V^{\pi^{j}}$ (or reaches a minimum threshold), which represents the expected cumulative reward starting from an initial state $s \in S$ and following the joint policy $\pi^{j}$, applying successive joint actions $a^{j} \in A^n$ under additional constraint guides. The state-value is described in the case where agents act sequentially and cyclically (Agent Environment Cycle - AEC mode) in \hyperref[eq:single_value_function]{Definition 1}, adapting its definition for roles (in red) and missions (in blue), impacting the action space and reward. \autoref{fig:mm_synthesis} illustrates the links between $\mathcal{M}OISE^+$ and Dec-POMDP via the MOISE+MARL framework.


% \begin{figure*}[h!]
%     \label{eq:single_value_function}
%     \raggedright
%     \textbf{\textit{Definition 1} \quad State-Value function adapted to constraint guides in AEC:}
    
%     \begin{scriptsize}
%         \vspace{-0.6cm}
%         \begin{gather*}
%             V^{\pi^j}(s_t) = \hspace{-0.75cm}
%             %
%             \sum_{\textcolor{red}{ \substack{a_{t} \in A \text{ si } rn() < ch_{t}, \\
%                         a_{t} \in A_{t} \text{ else}}
%                 }}{\hspace{-0.7cm} \pi_i(a_{t} | \omega_t)}
%             %
%             \sum_{s_{t+1} \in S}
%             %    
%             {\hspace{-0.1cm} T(s_{t+1} | s_t, a_{t})
%             \Bigl[R(s_t,a_{t},s_{t+1}) + \hspace{-0.1cm}
%             \textcolor{blue}{ \sum_{m \in \mathcal{M}_i}{ \hspace{-0.1cm} v_m(t) \frac{grg_m(h_{t+1})}{1 - p + \epsilon} } }
%             + } \\
%             {\textcolor{red}{(1-ch_t) \times rrg(\omega_t,a_{t+1})} + V^{\pi^j_{i+1 \ mod \ n}}(s_{t+1})\Bigr]}
%         \end{gather*}
%         %
%         \vspace{-0.5cm}
%         \textcolor{red}{\[\text{ \hspace{-0.1cm} With } rag(h_t, \omega_t) = A_{t} \times \mathbb{R} \text{, } \langle a_t, ch_{t} \rangle \in A_{t} \times \mathbb{R} \text{ ; } rn: \emptyset \to [0,1[ \text{, a uniform random function}\]}
%         %
%         \vspace{-0.6cm}
%         \textcolor{blue}{
%             \begin{gather*}
%                 \hspace{-0.001cm}
%                 \text{With } \omega_t = O(\omega_t | s_t, a_t) \text{ ; } h_t = \{h_0 = \langle \rangle, h_{t+1} = \langle h_t, \langle \omega_{t+1}, a_{t+1} \rangle \rangle \} \text{ ; } \epsilon \in \mathbb{R}_{>0} \text{ ; } grg_m(h) = 
%             \end{gather*}
%         }
%         \vspace{-0.95cm}
%         \textcolor{blue}{
%             \begin{gather*}
%                 \hspace{-0.5cm} \sum_{\hspace{0.3cm}(grg_i,w_i) \in mo(m)}{\hspace{-0.9cm} w_i \times grg_i(h)}
%                 \text{ ; } v_m(t) = \{ 1 \text{ if } t \in t_c \text{ ; else } 0 \} \text{ ; } \mathcal{M}_i = \{m_j | \langle ar(i),m_j,t_c,p \rangle \in \mathcal{M}\}
%             \end{gather*}
%         }
%         \vspace{-0.6cm}
%     \end{scriptsize}
    
% \end{figure*}


% At any time $t \in \mathbb{N}$ (initially $t = 0$), the agent $i = t \ mod \ n$ is constrained to a role $\rho_i = ar(i)$. For each temporally valid deontic specification $d_i = rds(\rho_i) = \langle tc_i,y_i, m_i \rangle$, the agent is permitted (if $y_i = 0$) or obligated (if $y_i = 1$) to commit in mission $m_i \in \mathcal{M}, \mathcal{G}_{m_i} = mo(m_i)$, and $n \in \mathbb{N}$ the number of agents.
% %
% First, based on the received observation $\omega_t$, the agent must choose an action either: within the expected actions of the role $A_t$ if a random value is below the role constraint hardness $ch_t$; or within the set of all actions $A$ otherwise. If $ch_t = 1$, the role is strongly constrained for the agent and weakly otherwise.
% %
% Then, the action is applied to the current state $s_t$ to transition to the next state $s_{t+1}$, generate the next observation $\omega_{t+1}$, and yield a reward. The reward is the sum of the global reward with penalties and bonuses obtained from the organizational specifications: \quad i) the sum of the bonuses for goals associated with each temporally valid mission (via Goal Reward Guides), weighted by the associated value ($\frac{1}{1-p+\epsilon}$); \quad ii) the penalty associated with the role (via \textquote{Role Reward Guides}) weighted by the role constraint hardness.
% %
% Finally, the cumulative reward calculation continues in the next state $s_{t+1} \in S$ with the next agent $(i+1) \ mod \ n$.


% \subsection{Learning World Models}

% In Reinforcement Learning (RL), particularly under partial observability, \textbf{World Models}~\cite{ha2018recurrent, hafner2020dream} aim to learn internal models approximating both the environment's transition and observation dynamics. Such models enable agents to perform planning, improve sample efficiency, and facilitate safe exploration. This modeling approach belongs to the \textit{model-based RL} (MBRL) paradigm~\cite{moerland2020model}, and is especially useful for automatically constructing high-fidelity simulation models even when explicit environment representations are unavailable.

% Formally, at each time step $t$, let $\omega_t \in \Omega$ denote the current high-dimensional observation, $a_t \in A$ the action taken, and $h_{t-1} \in \mathcal{H}$ the recurrent hidden state summarizing the interaction history up to $t-1$. Since observations are typically high-dimensional (e.g., images, complex state vectors), an encoder $Enc: \Omega \rightarrow Z$ is first applied to project observations into a compact latent space $Z$ with $z_t = Enc(\omega_t)$, where $\dim(Z) \ll \dim(\Omega)$.

% The core temporal structure is modeled using a \textbf{Recurrent Latent Dynamics Model}~\cite{hafner2020dream}, which predicts the next latent state $z_{t+1}$ by updating the recurrent hidden state and applying latent dynamics:
% \[
%     h_t = f(h_{t-1}, z_t, a_t), \quad
%     z_{t+1} = g(h_t),
% \]
% where $f(\cdot)$ typically corresponds to a recurrent neural network (e.g., LSTM~\cite{hochreiter1997long}) applied to the concatenation of $h_{t-1}$, $z_t$, and $a_t$, and $g(\cdot)$ maps the recurrent state to the next observation latent representation (often implmented as an MLP~\cite{hochreiter1997long}).

% The predicted latent state is then decoded via $Dec: Z \rightarrow \Omega$ into the predicted observation $\hat{\omega}_{t+1} = Dec(z_{t+1})$. The entire model is jointly trained to minimize both the \emph{reconstruction loss} $\|\omega_{t+1} - \hat{\omega}_{t+1}\|$ in observation space, and optionally, a \emph{latent prediction loss} to stabilize latent dynamics learning.

% The recurrent hidden state $h_t$ serves as a compact summary of the full interaction history up to time $t$, avoiding the need to explicitly store long observation-action trajectories.
% For simplicity of notation, we define the full composition that directly maps current observation, action, and recurrent state to the next predicted observation as the \textbf{Observation Prediction Model}:
% \[
%     \mathcal{T}(h_{t-1}, \omega_t, a_t) := Dec(g(f(h_{t-1}, Enc(\omega_t), a_t))) = \hat{\omega}_{t+1}.
% \]

% \subsubsection{Extension to Multi-Agent World Models}

% In multi-agent settings, joint observations and actions rapidly become high-dimensional as the number of agents increases. To address this, joint encoding functions are introduced for both observations and actions.

% Specifically, joint observations $\omega_t^{j} = (\omega_t^1, \dots, \omega_t^n) \in \Omega^{j}$ are mapped to compact latent representations using an observation encoder $Enc_{\omega_j}: \Omega^{j} \rightarrow \tilde{\Omega}$, yielding $\tilde{\omega}_t = Enc_{\omega_j}(\omega_t^{j})$. The inverse mapping $Dec_{\omega_j}: \tilde{\Omega} \rightarrow \Omega^{j}$ allows reconstruction of the joint observation if needed. Similarly, joint actions $a_t^{j} = (a_t^1, \dots, a_t^n) \in A^{j}$ are encoded via $Enc_{a_j}: A^{j} \rightarrow \tilde{\mathcal{A}}$, resulting in $\tilde{a}_t = Enc_{a_j}(a_t^{j})$, with decoder $Dec_{a_j}$ for potential reconstruction.
% %
% MLPs or attention-based architectures are typically employed for these encoders to aggregate multi-agent information into fixed-size feature vectors while capturing relevant inter-agent dependencies.

% Once joint encoding is performed, the multi-agent world model operates analogously to the single-agent case, using $\tilde{\omega}_t$ and $\tilde{a}_t$ as inputs to the recurrent latent dynamics model. This design enables scalable modeling while preserving critical interaction patterns between agents. Within the MAMAD framework, such world models instantiate the simulation core of the \hyperref[sec:modelling]{Modeling Phase}, effectively serving as high-fidelity digital twins of the target environment.


\section{The MAMAD method}\label{sec:mamad}

\subsection{General overview of the method}

The MAMAD~\footnotemark[1] method is built around four main phases: (1) modeling the environment, goal, and organizational constraints according to the proposed framework, (2) learning policies using various MARL algorithms, (3) analyzing behaviors and inferring organizational specifications with a proposed method, and (4) developing and deploying the MAS. This approach guides the agent learning process while enforcing strict organizational constraints, ensuring the efficiency of the learned policies. The lifecycle of a MAMAD-designed MAS is illustrated in \autoref{fig:cycle}.

\begin{figure}[h!]
    \centering
    \input{figures/cycle.tex}
    \caption{Lifecycle of a MAS designed with MAMAD: i) Users start by modeling the environment from a sufficient amount of real traces (obtained by initially transfered agents), global goal and design requirements as roles and goals; \quad ii) Then, they launch the training of agents with MARL techniques; \quad iii) A post-training analysis is performed to get insights into the emergent agents' roles and goals, guiding the improvement of the applied organizational specifications ; \quad v) Once validated, trained policies are launched to operate the environment's actuators, generating new traces for a better environment modeling}
    \label{fig:cycle}
\end{figure}

The MAMAD method frames MAS design as an iterative constrained optimization process. Given:
\begin{itemize}
    \item $\mathcal{E}_0$: the initial environment where agents can act;
    \item $\mathcal{G}_{\text{inf}}$: an informal description of the desired global objective;
    \item $\mathcal{C}_{\text{inf}}$: an informal specification of design constraints;
    \item $d_0 = \langle \Omega^j, A^j, \mathcal{T}_0, R_{\Omega}, \gamma \rangle$: the initial design problem, where $\Omega$, $A$, and $\gamma$ are known, $R_{\Omega}$ is to be defined, and $\mathcal{T}_0$ is to be learned;
    \item $\mathcal{MM} = \langle \mathcal{OS}, ar, rcg, gcg, rag, rrg, grg \rangle$: the MOISE+MARL specification containing roles, missions, and constraint guides.
\end{itemize}

The design problem $\langle \Omega^j, A^j, \mathcal{T}_0, R_{\Omega}^j, \gamma \rangle$ is solved similarly as a Dec-POMDP considering new the joint-observation is obtained via $\mathcal{T}$ using previous joint-observation and current joint-action $a \in A$ and reward is obtained via the \textbf{observation reward} $R^j_{\Omega}: \Omega^j \times A^j \rightarrow \mathbb{R}^{|\mathcal{A}|}$ that for any joint-observation and the joint-action that led to this joint-observation, gives the joint-reward.

\vspace{1em}

\begin{algorithm}[H]
    \caption{The MAMAD Design Loop}
    \label{alg:mamad-loop}
    \DontPrintSemicolon
    \KwIn{Initial environment $\mathcal{E}_0$, informal goal $\mathcal{G}_{\text{inf}}$, informal constraints $\mathcal{C}_{\text{inf}}$}
    \KwOut{MAS deployed on environment with aligned organizational behaviors}
    \For{$i \gets 0$ \KwTo $n$}{
    
    \tcp*[l]{1) Modeling phase}
    $(\mathcal{T}_i, R, \mathcal{MM}_i) \gets \texttt{model}(\mathcal{E}_i, \mathcal{G}_{\text{inf}}, \mathcal{C}_{\text{inf}})$
    
    $d_i \gets \langle \Omega, A, \mathcal{T}_i, R, \gamma \rangle$
    
    \tcp*[l]{2) Training phase}
    $\pi^{j}_{i} \gets \texttt{train}(d_i, \mathcal{MM}_i)$
    
    \tcp*[l]{3) Analysis phase (optional but recommended)}
    \If{$\text{analysis\_enabled}$}{
    $(\mathcal{MM}_{i,\text{implicit}}, \text{org. fit}) \gets \texttt{analyze}(d_i, \mathcal{MM}_i, \pi^{j}_{i})$
    
    \If{$\text{fit}$ is low or $\mathcal{MM}_{i,\text{implicit}}$ misaligned}{
    Refine $\mathcal{MM}_i$ using $\mathcal{MM}_{i,\text{implicit}}$ to get $\mathcal{MM}_{i+1}$
    
    \textbf{go back} to \textbf{Modeling phase (1)}
    }
    }
    
    \tcp*[l]{4) Transfer phase}
    $\mathcal{E}_{i+1} \gets \texttt{transfer}(\mathcal{E}_i, \pi^{j}_{i})$
    }
\end{algorithm}

One can point out that we leverage a modeled Digital Twin through an approximated \textbf{Observation Prediction Model} that is used as a simulator for a later training whereas MBRL both integrates environment modeling and training at the same time. Indeed, we favour decoupling environment modeling from training for : i) the reusability of the modeled environment in new agent training optionally requiring small adjustments ; \quad ii) the need for simple agents that do not embedded costly environment model for planning ; \quad iii) the need to have a high-fidelity modeled environment focusing all efforts on a single common one, benefitting to all agents in their training.

MAMAD's philosophy is to provide general workflow to follow for setting up each phase. These workflows may require choosing among different parameters such as modes, algorithms, or hyper-parameters. \autoref{tab:mamad_table_configuration} provides an overview of all of these parameters for the whole method and can be used a design canvas.

% \input{tables/mamad_table_configuration.tex}


\subsection{Modelling}\label{sec:modelling}

The \textbf{Modelling phase} aims to construct a high-fidelity simulated environment from real-world interaction traces, enabling safe and efficient offline training. Unlike many approaches in multi-agent learning that assume a preexisting simulator, MAMAD builds the environment from scratch when only a real system or black-box interface is available.

This step addresses the following formal component of the MAMAD loop:  
\[
(\mathcal{T}_i, R, \mathcal{MM}_i) \gets \texttt{model}(\mathcal{E}_i, \mathcal{G}_{\text{inf}}, \mathcal{C}_{\text{inf}})
\]

\subsubsection*{Formalization and Contribution}

Let $\Omega^j$ be the joint observation space and $A^j$ the joint action space. We approximate the observable dynamics as a deterministic function:
\[
\hat{\mathcal{T}}: \Omega^j \times A^j \rightarrow \Omega^j
\]
This function is learned in a latent space $Z$ constructed via a VAE:
\[
\phi: \Omega^j \rightarrow Z, \quad \phi^{-1}: Z \rightarrow \Omega^j
\]

We define the latent transition function:
\[
f: Z \times A^j \rightarrow Z
\]
which is approximated using a recurrent neural network (RNN), trained to minimize:
\[
\mathcal{L}_{\text{pred}} = \sum_{t} \| f(z_t, a^j_t) - z_{t+1} \|^2
\]
where $z_t = \phi(\omega^j_t)$ and $z_{t+1} = \phi(\omega^j_{t+1})$.

\textbf{Reward Function:} Since the true state is unobservable, MAMAD requires a reward function $R_\Omega$ defined on observations:
\[
R^j_\Omega: \Omega^j \times A^j \rightarrow \mathbb{R}^{|\mathcal{A}|}
\]

\textbf{Organizational Constraints:} The user provides (optionally) an initial organizational specification $\mathcal{MM}_0$ in the MOISE+ formalism, or leaves some parts unspecified to be inferred later.

\subsubsection*{Modelling Algorithm}

\begin{algorithm}[H]
    \caption{MAMAD Modelling Phase}
    \label{alg:modelling}
    \DontPrintSemicolon
    \KwIn{Real environment $\mathcal{E}_i$, goal spec $\mathcal{G}_{\text{inf}}$, constraints $\mathcal{C}_{\text{inf}}$, history buffer $\mathcal{H}$}
    \KwOut{Observation transition model $\hat{\mathcal{T}}$, observation reward $R^j_\Omega$, org. specification $\mathcal{MM}_i$}
    
    \tcp*[l]{1. Collect joint observation-action-observation triplets}
    $\mathcal{D} \gets$ extract transitions $(\omega^j_t, a^j_t, \omega^j_{t+1})$ from $\mathcal{H}$

    \tcp*[l]{2. Learn latent encoder/decoder via VAE}
    Train $\phi, \phi^{-1}$ on observations $\omega^j_t$ using HPO on latent dimension

    \tcp*[l]{3. Encode dataset in latent space}
    $Z \gets \{ z_t = \phi(\omega^j_t) \mid (\omega^j_t, a^j_t) \in \mathcal{D} \}$

    \tcp*[l]{4. Train RNN predictor}
    Train $f: Z \times A^j \rightarrow Z$ to predict $z_{t+1}$ from $(z_t, a^j_t)$

    \tcp*[l]{5. Define predictive transition function}
    $\hat{\mathcal{T}}(\omega^j_t, a^j_t) \gets \phi^{-1}(f(\phi(\omega^j_t), a^j_t))$

    \tcp*[l]{6. Define observation-level reward function}
    Define $R^j_\Omega$ manually or by reward shaping from $\mathcal{G}_{\text{inf}}$

    \tcp*[l]{7. Translate constraints into MOISE+}
    $\mathcal{MM}_i \gets \texttt{instantiate}(\mathcal{C}_{\text{inf}})$
\end{algorithm}

\subsubsection*{Discussion}

This modeling phase isolates the simulation problem from the training algorithm. Unlike end-to-end MBRL, MAMAD's modularity promotes:
\begin{itemize}
    \item reuse of the environment model across agents and tasks;
    \item interpretability and tuning of the model components;
    \item compatibility with safety-critical applications where testing on real systems is costly.
\end{itemize}


\subsection{Training}

\subsubsection*{Objective and Context}

The \textbf{Training phase} aims to learn agent policies that maximize a reward while complying with organizational constraints expressed via the MOISE+ model. This phase corresponds to the following element in the MAMAD loop:
\[
\pi^j_i \gets \texttt{train}(d_i, \mathcal{MM}_i)
\]
where \( d_i = \langle \Omega, A, \mathcal{T}_i, R^j_\Omega, \gamma \rangle \) is the modeled environment.

\subsubsection*{Organizational Model Integration}

Let us briefly recall the MOISE+ model $\mathcal{MM}$:
\[
\mathcal{MM} = \langle \mathcal{OS}, ar, rcg, gcg, rag, rrg, grg \rangle
\]
where $\mathcal{OS}$ is the organizational specification (roles, missions, goals), and the remaining components define allocation and compatibility relations.

We define:
- A \textbf{Policy Action Constraint (PAC)}: a mapping $\texttt{PAC}_r : \Omega \rightarrow 2^{A}$ for role \( r \), restricting the actions available under a given observation;
- A \textbf{Trajectory-based Reward Function (TRF)}: a function \( \texttt{TRF}_g : (\Omega \times A)^* \rightarrow \mathbb{R} \) assigning rewards based on an agent's individual trajectory toward goal \( g \).

\subsubsection*{Extended MARL Optimization Problem}

We define the constrained learning problem as follows:
\[
\begin{aligned}
\text{maximize} \quad & \mathbb{E} \left[ \sum_{t=0}^\infty \gamma^t R^j_\Omega(\omega^j_t, a^j_t) + \texttt{TRF}_g(h_t) \right] \\
\text{subject to} \quad & a^j_t \in \texttt{PAC}_r(\omega^j_t) \quad \forall r \in \texttt{Roles}, \text{ if agent in } r
\end{aligned}
\]
with \( h_t \) the local trajectory prefix of an agent up to time \( t \).

\subsubsection*{Training Algorithm}

\begin{algorithm}[H]
    \caption{MAMAD Training Phase}
    \label{alg:training}
    \DontPrintSemicolon
    \KwIn{Simulated env $d_i = (\Omega, A, \mathcal{T}_i, R_\Omega, \gamma)$, MOISE+ model $\mathcal{MM}_i$}
    \KwOut{Joint policy $\pi^j_i$}

    \tcp*[l]{1. Derive constraint mappings}
    Compute $\texttt{PAC}_r$, $\texttt{TRF}_g$ from $\mathcal{MM}_i$

    \tcp*[l]{2. Initialize MARL algorithm}
    Select algorithm (e.g., MAPPO, QMIX, MADDPG) and initialize parameters

    \For{$k \gets 1$ \KwTo $K$ training steps}{
        Sample episode $(\omega^j_0, a^j_0, \omega^j_1, ..., \omega^j_T)$ via policy $\pi^j_i$

        \tcp*[l]{3. Apply hard constraints}
        Mask actions at each step with $\texttt{PAC}_r(\omega^j_t)$

        \tcp*[l]{4. Compute rewards}
        Use $R_\Omega(\omega^j_t, a^j_t)$ and $\texttt{TRF}_g(h_t)$

        \tcp*[l]{5. Update policy}
        Use standard MARL update step (policy gradient, Q-learning, etc.)
    }
\end{algorithm}

\subsubsection*{Discussion}

This training strategy ensures organizational compliance \textit{by design}, rather than relying on emergent behavior. Compared to unconstrained MARL, our framework offers:

\begin{itemize}
    \item stronger safety guarantees;
    \item controllable and interpretable agent behaviors;
    \item improved sample efficiency through structured guidance.
\end{itemize}

\subsection{Analyzing}

\subsubsection*{Objective and Context}

The \textbf{Analyzing phase} aims to assess whether the learned joint policy aligns with the intended organizational specification, and optionally refine or infer missing structural elements. This corresponds to the third component of the MAMAD loop:
\[
(\mathcal{MM}_{i,\text{implicit}}, \text{fit}) \gets \texttt{analyze}(d_i, \mathcal{MM}_i, \pi^j_i)
\]

It supports the user in verifying organizational coherence and, if needed, refining the constraints for the next iteration.

\subsubsection*{Related Work}

Explainability in MARL has primarily focused on local policy visualization~\cite{madrona2023explain}, saliency analysis~\cite{verma2022attention}, or reward decomposition~\cite{juozapaitis2019explainable}. Few works address global coordination and its mapping to organizational semantics, which are critical for multi-agent system design.

Our contribution is the \textbf{TEMM} method: \emph{Trajectory-based Evaluation of Multi-agent behaviors w.r.t. MOISE+}, which evaluates the fit between learned behaviors and organizational expectations in terms of roles and goals.

\subsubsection*{Key Concepts}

Let $\pi^j$ be the learned joint policy and $\mathcal{MM}$ the organizational model.

TEMM decomposes analysis into three steps:

\begin{itemize}
    \item \textbf{Role Inference:} cluster agent trajectories in the action space to identify behavioral roles;
    \item \textbf{Goal Inference:} identify recurrent observations or subgoals reached by agents;
    \item \textbf{Organizational Fit:} quantify the alignment between inferred and predefined roles/goals.
\end{itemize}

We denote:
- $\hat{\mathcal{R}}$: the inferred role set from clustering;
- $\hat{\mathcal{G}}$: the set of inferred agent goals (e.g., via observation centroids);
- $\texttt{SOF}(\mathcal{MM}, \hat{\mathcal{R}})$: Structural Organizational Fit;
- $\texttt{FOF}(\mathcal{MM}, \hat{\mathcal{G}})$: Functional Organizational Fit;
- $\texttt{OF}$: global Organizational Fit as an aggregation of SOF and FOF.

\subsubsection*{Analyzing Algorithm}

\begin{algorithm}[H]
    \caption{MAMAD Analyzing Phase (TEMM)}
    \label{alg:analyzing}
    \DontPrintSemicolon
    \KwIn{Dataset $\mathcal{D}$ of trajectories under $\pi^j_i$, organizational model $\mathcal{MM}_i$}
    \KwOut{Inferred model $\mathcal{MM}_{i,\text{implicit}}$, Fit score}

    \tcp*[l]{1. Extract individual agent trajectories}
    For each agent $a$, extract $(\omega_t^a, a_t^a)$ over episodes

    \tcp*[l]{2. Role inference via clustering}
    Cluster sequences of actions to identify roles $\hat{\mathcal{R}}$

    \tcp*[l]{3. Goal inference via observation space}
    Identify stable or converged observations to form $\hat{\mathcal{G}}$

    \tcp*[l]{4. Compute organizational fit}
    Compute $\texttt{SOF}(\mathcal{MM}_i, \hat{\mathcal{R}})$ and $\texttt{FOF}(\mathcal{MM}_i, \hat{\mathcal{G}})$

    \tcp*[l]{5. Aggregate into global fit}
    $\texttt{OF} = \alpha \cdot \texttt{SOF} + (1 - \alpha) \cdot \texttt{FOF}$

    \tcp*[l]{6. Construct inferred model}
    $\mathcal{MM}_{i,\text{implicit}} \gets$ extracted roles and goals

\end{algorithm}

This phase is optional but highly recommended in MAMAD. It allows users to:

\begin{itemize}
    \item Evaluate how well the system conforms to its intended organizational structure;
    \item Automatically detect emergent structures and suggest refinement;
    \item Justify policy behaviors to stakeholders through interpretable specifications.
\end{itemize}

This creates a powerful feedback loop in MAMAD, enabling a principled transition between implicit and explicit organizational design.


\subsection{Transferring}

\subsubsection*{Objective and Context}

The \textbf{Transferring phase} closes the MAMAD design loop by operationalizing the trained policies in the real environment. It corresponds to:
\[
\mathcal{E}_{i+1} \gets \texttt{transfer}(\mathcal{E}_i, \pi^j_i)
\]
This phase serves two purposes:
\begin{enumerate}
    \item Deploy the latest trained policies in the real environment for execution;
    \item Collect new data from this deployment, used to update the environment model or trigger new MAMAD cycles.
\end{enumerate}

Unlike the previous phases, which primarily rely on simulated data, this phase ensures alignment with the operational reality.

\subsubsection*{Theoretical Framing}

We define:
\begin{itemize}
    \item The real environment $\mathcal{E}_i$ as a black-box with unknown transition dynamics;
    \item The observation-action interface $\mathcal{I}$ as an API ensuring communication with real agents;
    \item The set of deployed policies $\pi^j_i$ as either centralized (controlled externally) or decentralized (embedded into agents);
    \item A transition database $\mathcal{H}_i$ collecting $(\omega_t^j, a_t^j, \omega_{t+1}^j)$ tuples from the real environment.
\end{itemize}

This setup allows the continuous growth of a dataset $\mathcal{H}_0, \mathcal{H}_1, \ldots$, which improves the accuracy of the environment model $\mathcal{T}_i$ over time.

\subsubsection*{Modes of Execution}

\paragraph{Centralized Mode.} A remote controller computes and dispatches actions based on $\pi^j_i$. It directly receives the joint observation $\omega_t^j$ and sends back $a_t^j$. This allows precise control and trace collection but assumes synchronous interaction and full connectivity.

\paragraph{Decentralized Mode.} Agents embed their local policy and act autonomously. Periodically, they upload their local history. This enables robustness and scalability, though synchronization and observation fidelity may vary.

\subsubsection*{Transfer Algorithm}

\begin{algorithm}[H]
    \caption{MAMAD Transfer Phase}
    \label{alg:transferring}
    \DontPrintSemicolon
    \KwIn{Real environment $\mathcal{E}_i$, policy $\pi^j_i$, mode $\in$ \{centralized, decentralized\}, threshold $\theta$}
    \KwOut{Updated environment $\mathcal{E}_{i+1}$, data batch $\mathcal{H}_{i+1}$}

    Initialize connection with real environment via interface $\mathcal{I}$\;
    Deploy policy $\pi^j_i$ according to selected mode\;

    \While{process is active}{
        \eIf{mode == centralized}{
            Observe $\omega_t^j$ from $\mathcal{E}_i$\;
            Compute $a_t^j \gets \pi^j_i(\omega_t^j)$\;
            Send $a_t^j$ to actuators via $\mathcal{I}$\;
            Observe $\omega_{t+1}^j$ and store $(\omega_t^j, a_t^j, \omega_{t+1}^j)$ in $\mathcal{H}_i$\;
        }{
            Agents act autonomously and store local transitions\;
            Periodically upload new data $(\omega_t^a, a_t^a, \omega_{t+1}^a)$ via $\mathcal{I}$\;
        }

        \If{new data batch size $> \theta$}{
            Save $\mathcal{H}_i$ to persistent storage\;
            \textbf{Trigger a new MAMAD cycle}
        }
    }
\end{algorithm}

\subsubsection*{Discussion}

This phase ensures that:

\begin{itemize}
    \item The system is not just trained in silico but grounded in reality;
    \item Traces collected can be used to refine the model $\mathcal{T}_i$ or restart the loop;
    \item Deployment choices (centralized vs decentralized) impact the control, latency, and autonomy of the system.
\end{itemize}

From a design perspective, this phase closes the MAMAD loop as a continuous, self-improving organizational pipeline, akin to digital twin synchronization and lifelong learning systems.




\section{Experimental setup}
\label{sec:experimental_setup}

We developed a tool that we propose to facilitate the implementation of the MAMAD method through four environments following a proposed evaluation protocol.

% \subsection{A development environment for the method}

% To support the implementation and evaluation of the MAMAD method, we developed the \textbf{Cyber Multi-agent System Development Environment}~\footnotemark[1], a dedicated framework that facilitates modeling, training, and deploying MAS. An visual overview of CybMASDE provided in \autoref{fig:cybmasde}.

% \footnotetext[1]{Source code and details are freely available in \url{https://github.com/julien6/CybMASDE}} (\textbf{CybMASDE})

% \begin{figure}[h!]
%     \centering
%     \includegraphics[width=0.7\linewidth]{figures/CybMASDE.png}
%     \caption{A screenshot of the \href{https://github.com/julien6/CybMASDE}{CybMASDE} tool. CybMASDE integrates several state-of-the-art libraries and tools to provide a flexible, scalable, and user-friendly environment for MARL-driven MAS design.}
%     \label{fig:cybmasde}
% \end{figure}


% CybMASDE leverages the \textbf{PettingZoo}~\cite{Terry2021} library, which offers a standardized API for MARL environments, ensuring interoperability with various MARL algorithms. This allows seamless integration of different multi-agent environments without the need for extensive custom modifications.

% At the core of CybMASDE's learning capabilities lies \textbf{MARLlib}~\cite{hu2022marllib}, a comprehensive library providing access to a wide range of MARL algorithms. MARLlib ensures optimized implementations of cutting-edge MARL techniques and fine-tuned policy models, enabling efficient training across diverse environments. CybMASDE fully supports MARLlib's algorithms, offering users the flexibility to select, experiment with, and compare different approaches based on environment dynamics and learning goals.

% \paragraph{Supported MARL algorithms}
% CybMASDE supports the full range of MARL algorithms provided by MARLlib, including:
% \begin{itemize}
%     \item \textbf{Value-based methods:}
%           \begin{itemize}
%               \item Independent Q-Learning ;
%               \item VDN (Value-Decomposition Networks)~\cite{sunehag2018vdn} ;
%               \item QMIX~\cite{rashid2018qmix} ;
%               \item QTRAN~\cite{son2019qtran}.
%           \end{itemize}
%     \item \textbf{Policy-based methods:}
%           \begin{itemize}
%               \item Independent PPO ;
%               \item MAPPO (Multi-Agent Proximal Policy Optimization)~\cite{yu2021mappo} ;
%               \item MADDPG (Multi-Agent Deep Deterministic Policy Gradient)~\cite{lowe2017multi} ;
%               \item HATRPO (Heterogeneous-Agent Trust Region Policy Optimization)~\cite{kuba2022}.
%           \end{itemize}
%     \item \textbf{Actor-Critic methods:}
%           \begin{itemize}
%               \item COMA (Counterfactual Multi-Agent Policy Gradients)~\cite{foerster2018counterfactual} ;
%               \item MAVEN (Multi-Agent Variational Exploration)~\cite{mahajan2019maven} ;
%               \item ROMA (Role-Oriented Multi-Agent RL)~\cite{wang2020roma}.
%           \end{itemize}
%     \item \textbf{Model-based methods:}
%           \begin{itemize}
%               \item Dyna-Q and Dyna-Q+ (planning-based approaches) ;
%               \item MB-MARL (Model-Based MARL variants).
%           \end{itemize}
% \end{itemize}

% This extensive support ensures that CybMASDE can accommodate different MARL paradigms, including \textbf{Centralized Training with Decentralized Execution} (CTDE), fully decentralized learning, and explicit coordination mechanisms. Users can easily compare different MARL strategies to determine the most suitable algorithm for a given MAS scenario.

% \paragraph{Environment simulation and hyperparameter optimization}
% CybMASDE incorporates \textbf{TensorFlow} to enable automated environment modeling, allowing users to generate and refine environment models via deep learning-based function approximation. This is particularly useful for cases where the environment's transition dynamics are unknown or difficult to model manually. The system supports \textbf{world-model-based learning}, where agents are trained using a learned simulation of the environment, reducing dependence on real-world interaction data.

% Additionally, CybMASDE provides \textbf{Hyper-Parameter Optimization (HPO)}, allowing users to fine-tune crucial training parameters such as:
% \begin{itemize}
%     \item \textbf{Learning rate schedules:} Control how quickly the learning algorithm updates its parameters. Adaptive or decaying schedules can help improve convergence stability and performance over time.
%     \item \textbf{Discount factor} ($\gamma$): Determines the agent's preference between short-term and long-term rewards. A lower $\gamma$ encourages myopic behavior (favoring immediate rewards), whereas a higher $\gamma$ promotes planning for delayed gains.
%     \item \textbf{Exploration-exploitation balance:} Typically managed via $\epsilon$-greedy or entropy-based strategies. It regulates the trade-off between discovering new behaviors (exploration) and exploiting current knowledge (exploitation).
%     \item \textbf{Policy gradient update parameters:} Include settings like the PPO clipping factor, which prevents excessively large updates to the policy. This helps maintain training stability and avoid policy collapse.
%     \item \textbf{Reward shaping configurations:} Involves adding auxiliary rewards to guide learning more effectively. Proper shaping can accelerate convergence by making useful intermediate states more attractive.
% \end{itemize}


% This feature ensures that trained policies are not only effective but also stable across different environments and organizational constraints.

% \paragraph{User interface and deployment capabilities}
% CybMASDE provides:
% \begin{itemize}
%     \item A \textbf{full-featured API} for advanced users, enabling fine-grained control over environment configurations, learning parameters, and agent interactions ;
%     \item A \textbf{graphical user interface (GUI)} for simplified access to key functionalities, allowing non-experts to configure and launch MARL training sessions with minimal setup ;
%     \item \textbf{Support for multi-environment benchmarking}, where multiple training runs can be executed in parallel, enabling systematic comparison of different MARL methods ;
%     \item \textbf{Automated policy deployment}, where trained agents can be directly transferred to real or simulated environments for validation and real-world execution.
% \end{itemize}


\subsection{Computing resources}

All experiments were conducted on an academic high-performance computing cluster, utilizing various configurations of GPU nodes. Specifically, we employed nodes with:
\begin{itemize}
    \item \textbf{GPUs:} NVIDIA A100, AMD MI210 ;
    \item \textbf{Frameworks:} TensorFlow, PyTorch ;
    \item \textbf{Hyperparameter tuning:} \textbf{Optuna}~\cite{akiba2019optuna} for learning rate, exploration-exploitation balance, and network architecture.
\end{itemize}

Each algorithm-environment combination was executed on 5 parallel instances to ensure robust and consistent results.

\subsection{Test environments and organizational specifications}

To evaluate the MAMAD method, we employ four distinct multi-agent environments that serve as controlled testbeds. These environments span different problem domains, requiring coordination, strategic decision-making, and role-based interactions. Each environment is formally described below, including its state space, observation space, action space, reward structure, and overall goal. Additionally, we provide the corresponding organizational specifications, detailing roles, missions, and constraints used in the MAMAD framework. A summary of 

\begin{table}[h!]
    \centering
    \begin{footnotesize}
        \renewcommand{\arraystretch}{1.3}
        \begin{tabular}{p{2cm}p{2.2cm}p{2.2cm}p{2.2cm}p{2.2cm}}
            \hline
            \textbf{Key Aspect} & \textbf{CybORG}                & \textbf{Overcooked-AI} & \textbf{Predator-prey} & \textbf{Warehouse Mgmt} \\ \hline
            Realism             & Cyber-defense, dynamic threats & Human-like teamwork    & Abstract communication & Logistic workflow       \\ \hline
            Emergent Roles      & Firewall, cleaner, rescuer     & Cooker, deliverer      & Speaker, listener      & Picker, crafter, packer \\ \hline
            Goal Structure      & Multi-phase missions           & Sequential subtasks    & Shared objective       & Ordered pipeline        \\ \hline
            Observability       & Noisy, partial views           & Occlusion, congestion  & Requires messaging     & Local and shared zones  \\ \hline
            Org. Fit Evaluation & Coherence under attack         & Task delegation        & Comms-based roles      & Coordination efficiency \\ \hline
        \end{tabular}
        \caption{Key characteristics of environments used to assess MAMAD}
        \label{tab:mamad_env_characteristics}
    \end{footnotesize}
\end{table}


\paragraph{Warehouse Management (WM)}
The \textbf{Warehouse Management}~\cite{warehouse_management} environment models a grid-based logistics warehouse where multiple robots must collaborate to transport goods efficiently. The environment is inspired by industrial warehouse automation scenarios and serves as an ideal testbed for evaluating task allocation, role specialization, and real-time coordination. This environment is illustrated in \autoref{fig:warehouse}.

\begin{itemize}
    \item \textbf{State Space:} A $N \times M$ grid where each cell contains a robot, a product, a crafting machine, or a drop-off location. The system tracks agent positions, inventory levels, and machine states ;
    \item \textbf{Observation Space:} Each agent has a local $V \times V$ view, perceiving products, teammates, and nearby machines ;
    \item \textbf{Action Space:}  ;
          \begin{itemize}
              \item Move: \texttt{Up, Down, Left, Right} ;
              \item Interact: \texttt{Pick Product, Drop Product}.
          \end{itemize}
    \item \textbf{Reward Structure:} ;
          \begin{itemize}
              \item Successful product delivery: $+10$ ;
              \item Inefficient movement: $-1$ per unnecessary step ;
              \item Product mishandling: $-5$ for incorrect drop-offs.
          \end{itemize}
    \item \textbf{goal:} Transport raw materials to processing machines and deliver finished products to drop-off locations.
\end{itemize}

\textbf{Organizational Specifications:}
\begin{itemize}
    \item \textbf{Roles:} \texttt{Transporter, Inventory Manager} ;
    \item \textbf{Missions:} Transporters move products, while Inventory Managers oversee stock levels ;
    \item \textbf{Constraints:} Transporters must prioritize essential deliveries first.
\end{itemize}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.6\linewidth]{figures/wm.png}
    \caption{A screenshot of the Warehouse Management environment: agents can move up, down, left, and right, multiple agents operate within a warehouse grid, performing tasks to process and deliver products. Agents can move in four directions (up, down, left, right) and interact with pick/drop zones when adjacent. The workflow involves: (i) collecting primary products from input conveyor pick/drop areas (blue zones); (ii) transporting them to crafting machine pick/drop areas (brown zones), where the primary products are transformed into a single secondary product based on a predefined crafting schema; (iii) retrieving the resulting secondary products and delivering them to output conveyor pick/drop areas (pink zones). Successful operation requires agents to coordinate their movements and actions to optimize throughput and efficiency within the warehouse.}
    \label{fig:warehouse}
\end{figure}

\paragraph{Predator-Prey (PP)}
The \textbf{Predator-Prey} environment is a well-known MARL benchmark~\cite{lowe2017multi}, designed to evaluate coordination among cooperative pursuers (predators) attempting to capture an evasive agent (prey). This environment is illustrated in \autoref{fig:predator_prey}.

\begin{itemize}
    \item \textbf{State Space:} A continuous 2D space where agents (predators and prey) have $(x, y)$ positions and velocities ;
    \item \textbf{Observation Space:} Agents sense nearby entities within a limited radius $r$ ;
    \item \textbf{Action Space:}  ;
          \begin{itemize}
              \item Move: \texttt{Up, Down, Left, Right, Stay}.
          \end{itemize}
    \item \textbf{Reward Structure:} ;
          \begin{itemize}
              \item Predators gain $+50$ for capturing the prey ;
              \item The prey earns $+1$ per timestep survived ;.
          \end{itemize}
    \item \textbf{goal:} Predators must cooperate to trap the prey, while the prey attempts to escape as long as possible.
\end{itemize}

\textbf{Organizational Specifications:}
\begin{itemize}
    \item \textbf{Roles:} \texttt{Predator, Prey} ;
    \item \textbf{Missions:} Predators coordinate to enclose the prey; prey seeks optimal escape routes ;
    \item \textbf{Constraints:} Predators must balance aggressive pursuit with blocking strategies.
\end{itemize}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.6\linewidth]{figures/predator_prey.png}
    \caption{A screenshot of the Predator-Prey environment: \textbf{green agents} (cooperative) and \textbf{red agents} (adversarial). The green agents aim to collect food items scattered across the environment while avoiding detection by the red agents. The environment includes \textbf{forest regions} that provide concealment; when a green agent enters a forest, it becomes partially or fully hidden from the red agents' observations. One red agent acts as a \textbf{leader} with enhanced observational capabilities and can communicate with other red agents to coordinate their pursuit.}
    \label{fig:predator_prey}
\end{figure}

\paragraph{Overcooked-AI (OA)}
The \textbf{Overcooked-AI} environment~\cite{Carroll2019} simulates a cooperative cooking scenario where agents must collaborate to prepare and serve meals in a structured kitchen. This environment is illustrated in \autoref{fig:overcooked}.

\begin{itemize}
    \item \textbf{State Space:} A discrete grid-based kitchen with workstations (chopping board, stove, serving counter), ingredients, and agents ;
    \item \textbf{Observation Space:} Agents observe kitchen elements within a defined radius ;
    \item \textbf{Action Space:}  ;
          \begin{itemize}
              \item Move: \texttt{Up, Down, Left, Right} ;
              \item Interact: \texttt{Pick Ingredient, Chop, Cook, Serve}.
          \end{itemize}
    \item \textbf{Reward Structure:} ;
          \begin{itemize}
              \item Successful meal preparation: $+20$ ;
              \item Ingredient misplacement: $-5$ ;
              \item Idle behavior: $-1$ per step without meaningful action.
          \end{itemize}
    \item \textbf{goal:} Maximize completed meal orders within a fixed time limit.
\end{itemize}

\textbf{Organizational Specifications:}
\begin{itemize}
    \item \textbf{Roles:} \texttt{Chef, Assistant, Server} ;
    \item \textbf{Missions:} The Chef prepares food, the Assistant supplies ingredients, and the Server delivers meals ;
    \item \textbf{Constraints:} Task execution must be synchronized to prevent bottlenecks.
\end{itemize}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.6\linewidth]{figures/overcooked.png}
    \caption{A screenshot of the Overcooked-AI environment: Two agents (chefs) must collaborate to prepare and serve onion soups efficiently. The process involves collecting three onions (one at a time) from the dispenser, placing them into a cooking pot, waiting for the soup to cook, retrieving a clean dish, plating the soup, and delivering it to the serving counter. The kitchen layout includes obstacles and narrow pathways, requiring agents to coordinate their movements to avoid collisions and optimize task completion.}
    \label{fig:overcooked}
\end{figure}

\paragraph{Cyber-Defense Simulation (CS)}
The \textbf{Cyber-Defense Simulation} is an ad hoc drom warm network on which defender agents must defend it from malicious intrusions in various cyberattack scenarios~\cite{Standen2021}. This environment is illustrated in \autoref{fig:cyborg}.

\begin{itemize}
    \item \textbf{State Space:} A dynamic network graph where nodes represent devices and edges denote active connections ;
    \item \textbf{Observation Space:} Agents receive security alerts and network state updates ;
    \item \textbf{Action Space:}  ;
          \begin{itemize}
              \item \texttt{Monitor}: Analyze node activity ;
              \item \texttt{Block IP}: Restrict access from a suspicious source ;
              \item \texttt{Deploy Patch}: Strengthen network defenses ;.
          \end{itemize}
    \item \textbf{Reward Structure:} ;
          \begin{itemize}
              \item Preventing an attack: $+30$ ;
              \item False positive block: $-10$ ;
              \item Allowing a breach: $-50$.
          \end{itemize}
    \item \textbf{goal:} Detect and mitigate cyber threats while avoiding false positives.
\end{itemize}

\textbf{Organizational Specifications:}
\begin{itemize}
    \item \textbf{Roles:} \texttt{Threat Analyst, Firewall Manager, Security Operator} ;
    \item \textbf{Missions:} Detect threats, block unauthorized access, maintain network integrity ;
    \item \textbf{Constraints:} Minimizing false positives while ensuring security coverage.
\end{itemize}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.6\linewidth]{figures/cyborg.png}
    \caption{A screenshot of the CybORG environment: A swarm of 18 autonomous drones, initially controlled by blue (defensive) agents, forms an ad hoc network to facilitate communication between ground units. Each drone is susceptible to a hardware Trojan that can activate randomly, replacing the blue agent with a red (offensive) agent. Red agents aim to compromise the network by intercepting or blocking communications. The drones move according to a swarm algorithm, dynamically altering the network topology. Blue agents must detect and neutralize compromised drones while maintaining communication integrity.}
    \label{fig:cyborg}
\end{figure}

\bigskip

\noindent These four environments provide diverse challenges, covering cooperative, competitive, hierarchical, and adversarial scenarios, enabling a representative evaluation.


\subsection{Evaluation metrics}

To assess whether the MAMAD method effectively bridges the identified research gaps, we define a set of quantitative metrics across four evaluation criteria: \textbf{automation}, \textbf{efficiency}, \textbf{compliance with design requirements}, and \textbf{explainability}.

\subsubsection{Automation metrics}
To measure MAMAD's automatation level in generating MAS, we evaluate:
\begin{itemize}
    \item \textbf{Performance relative to time required for manual MAS design} ($T_{design}$): Measures the MAS performance to reaching its goal relative to the approximate order of magnitude of total duration (roughly expressed as days) from environment modeling to final deployment;
    \item \textbf{Injected knowledge quantity} ($K_{design}$): Measures the number of lines required to define roles and goals as a way to quantify the human involvement into MAS design;
    \item \textbf{Iterations to convergence} ($N_{iter}$): Counts the number of training cycles needed for the system to stabilize at an optimal policy (considering zero for hand-crafted MAS).
\end{itemize}

\subsubsection{Efficiency metrics}
To determine the effectiveness of the MAS solutions generated by MAMAD, we use:
\begin{itemize}
    \item \textbf{Cumulative Reward} ($R_{cum}$): The total reward achieved by agents, reflecting their overall performance in achieving the system's goals ;
    \item \textbf{Policy Stability} ($\sigma_R$): Standard deviation of cumulative rewards across episodes, assessing consistency ;
    \item \textbf{Convergence Rate} ($CR$): Measures the speed at which learning stabilizes ;
    \item \textbf{Robustness Score} ($R_{robust}$): Evaluates the ability of the MAS to maintain performance under external perturbations.
\end{itemize}

\subsubsection{Metrics for compliance with design requirements metrics and explainability}
To validate whether MAMAD produces policies that conform to predefined specifications, we measure:
\begin{itemize}
    \item \textbf{Constraint Violation Rate} ($V_c$): The percentage of policy executions where agents fail to adhere to predefined organizational constraints ;
    \item \textbf{Organizational Fit Level} ($F_{org}$): The similarity between the inferred organizational structure (post-training) and the predefined design or implicit one ;
    \item \textbf{Consistency Score} ($S_{cons}$): Quantifies how closely the assigned roles and missions match those expected by human designers. A high consistency score shows the TEMM method to be efficiently able to find back the initially given roles and goals, demonstrating its capability to organizationally explain agents behaviors.
\end{itemize}

\subsection{Evaluation protocol}

To validate the effectiveness of MAMAD, we structure the experimental protocol into the following components:

\subsubsection{Comparison with classical MAS design methods}
To benchmark MAMAD's performance, we compare it against traditional manual MAS design approaches:
\begin{itemize}
    \item \textbf{Reference Baseline (RB)}: Agents trained without organizational constraints using standard MARL techniques (e.g., MADDPG, MAPPO) ;
    \item \textbf{Organizational Baseline (OB)}: Agents trained with manually specified $\mathcal{M}OISE^+$ organizational constraints, developed by human experts ;
    \item \textbf{MAMAD-Based MAS (MB)}: Agents trained using MAMAD's automated workflow, including inferred organizational constraints.
\end{itemize}

All experiments are conducted in four test environments using the same training settings across baselines.

\subsubsection{Validation of explainability and organizational compliance}
To ensure that MAMAD produces meaningful and interpretable organizational specifications, we conduct:
\begin{itemize}
    \item \textbf{Comparative Role and Mission Analysis}: We compare predefined and inferred role structures, measuring consistency and stability ;
    \item \textbf{Similarity Analysis on Organizational Specifications}: We compute role similarity scores to assess the alignment between predefined and learned roles ;
    \item \textbf{Visualization of Goals and Transitions}: PCA of observations, actions, and transitions are used to assess the interpretability of goals and roles inferred via TEMM.
\end{itemize}

If inferred roles and missions remain stable across training runs and align with expectations, this validates MAMAD's ability to structure MAS designs.

% \subsubsection{Ablation studies and robustness evaluation}
% To evaluate the impact of MAMAD's automated components, we conduct ablation studies by selectively disabling key components~\label{sec:experimental_setup_ablations}:
% \begin{itemize}
%     \item \textbf{Without Automated Modeling (WAM)}: Manual environment models were used instead of neural world models ;
%     \item \textbf{Without Organizational Constraints (WOC)}: No MOISE+MARL specifications were provided during training ;
%     \item \textbf{Without Trajectory-Based Analysis (WTA)}: No trajectory-based organizational extraction; policies were directly deployed post-training.
% \end{itemize}

% Each ablation scenario is tested in at least two environments, with performance compared to the full MAMAD pipeline.


% \begin{table}[h!]
%     \centering
%     \renewcommand{\arraystretch}{1.3}
%     \begin{footnotesize}
%         \begin{tabular}{p{1.5cm}p{2.4cm}p{2.2cm}p{4.5cm}}
%             \hline
%             \textbf{Criterion} & \textbf{Metric}       & \textbf{Validation Method}   & \textbf{Potential Bias or Limitation}            \\
%             \hline
%             \multirow{3}{*}{Automation}
%                                & $T_{design}$ (Performance per design time)   & Experiment logs + MAS performance & May depend on subjective time estimation granularity \\
%                                & $K_{design}$ (Injected knowledge quantity)   & Code lines of role/goal specs    & Approximate proxy for knowledge complexity \\
%                                & $N_{iter}$ (Iterations to convergence)       & Training curves                  & Influenced by initial HPO space \\
%             \hline
%             \multirow{4}{*}{Efficiency}
%                                & $R_{cum}$ (Cumulative reward)   & Score tracking     & Task-specific reward shaping may bias \\
%                                & $\sigma_R$ (Policy stability)   & Reward variance     & Sensitive to stochasticity \\
%                                & $CR$ (Convergence rate)         & Convergence analysis & Hyperparameter-sensitive \\
%                                & $R_{robust}$ (Robustness score) & Perturbation tests  & Depends on perturbation type \\
%             \hline
%             \multirow{3}{*}{\shortstack{Compliance \& \\ Explainability}}
%                                & $V_c$ (Constraint violation rate) & Rule checking      & May miss implicit violations \\
%                                & $F_{org}$ (Organizational fit)    & Role/goal alignment analysis & Relies on cluster assumptions \\
%                                & $S_{cons}$ (Consistency score)    & Role recovery analysis & Sensitive to clustering granularity \\
%             \hline
%         \end{tabular}
%         \caption{Validation strategy with evaluation criteria, metrics, methods, and limitations.}
%         \label{tab:validation_strategy}
%     \end{footnotesize}
% \end{table}


\section{Results and Discussion} \label{sec:results}

This section presents the results obtained by applying MAMAD across the four test environments. The evaluation follows the defined metrics and validation protocol, structured around the targeted research gaps.

\subsection{G1 - Leveraging MARL within AOSE (Efficiency)}

We first evaluate the efficiency of learning across the three methods using:

\begin{itemize}
    \item \textbf{Cumulative reward} ($R_{cum}$) ;
    \item \textbf{Policy stability} ($\sigma_R$) ;
    \item \textbf{Convergence rate} ($CR$) ;
    \item \textbf{Robustness score} ($R_{robust}$).
\end{itemize}

\begin{table}[h!]
    \centering
    \caption{Efficiency metrics across methods and environments (G1)}
    \begin{tabular}{l|l|cccc}
        \hline
        \textbf{Env.} & \textbf{Method} & $R_{cum}$ & $\sigma_R$ & $CR$ & $R_{robust}$ \\
        \hline
        \multirow{3}{*}{Overcooked-AI}
        & RB & 85\% & 8\% & 220 & 75\% \\
        & OB & 92\% & 4\% & 160 & 85\% \\
        & MB & 95\% & 3\% & 150 & 90\% \\
        \hline
        \multirow{3}{*}{Predator-Prey}
        & RB & 80\% & 10\% & 250 & 70\% \\
        & OB & 88\% & 5\% & 180 & 80\% \\
        & MB & 90\% & 4\% & 170 & 85\% \\
        \hline
        \multirow{3}{*}{Warehouse}
        & RB & 83\% & 9\% & 230 & 72\% \\
        & OB & 91\% & 5\% & 170 & 82\% \\
        & MB & 93\% & 4\% & 160 & 87\% \\
        \hline
        \multirow{3}{*}{CyberDefense}
        & RB & 78\% & 12\% & 280 & 65\% \\
        & OB & 85\% & 7\% & 200 & 75\% \\
        & MB & 87\% & 6\% & 190 & 80\% \\
        \hline
    \end{tabular}
    \label{tab:g1_efficiency_full}
\end{table}

MAMAD (MB) consistently outperforms both baselines across environments in reward maximization and stability. The organizational guidance accelerates convergence ($CR$) and improves robustness under perturbations ($R_{robust}$). The largest gain is observed in highly cooperative tasks like Overcooked-AI.

\subsection{G2 \& G3 - Compliance and Explainability}

We now assess policy compliance and explainability using:

\begin{itemize}
    \item \textbf{Constraint violation rate} ($V_c$) ;
    \item \textbf{Organizational fit level} ($F_{org}$) ;
    \item \textbf{Consistency score} ($S_{cons}$).
\end{itemize}

\begin{table}[h!]
    \centering
    \caption{Compliance and explainability metrics across methods (G2 \& G3)}
    \begin{tabular}{l|l|ccc}
        \hline
        \textbf{Env.} & \textbf{Method} & $V_c$ & $F_{org}$ & $S_{cons}$ \\
        \hline
        \multirow{3}{*}{Overcooked-AI}
        & RB & 15\% & 70\% & 65\% \\
        & OB & 3\% & 92\% & 90\% \\
        & MB & 2\% & 95\% & 93\% \\
        \hline
        \multirow{3}{*}{Predator-Prey}
        & RB & 18\% & 65\% & 60\% \\
        & OB & 5\% & 88\% & 85\% \\
        & MB & 4\% & 90\% & 88\% \\
        \hline
        \multirow{3}{*}{Warehouse}
        & RB & 12\% & 68\% & 63\% \\
        & OB & 4\% & 91\% & 89\% \\
        & MB & 3\% & 94\% & 91\% \\
        \hline
        \multirow{3}{*}{CyberDefense}
        & RB & 20\% & 60\% & 55\% \\
        & OB & 6\% & 85\% & 82\% \\
        & MB & 5\% & 87\% & 85\% \\
        \hline
    \end{tabular}
    \label{tab:g2_g3_full}
\end{table}

MAMAD achieves high organizational fit and consistency, closely approaching OB where constraints are manually engineered. The trajectory-based analysis allows MB to recover organizational structures with minimal violations ($V_c$), even outperforming OB in certain environments due to better alignment during learning.

\subsection{G4 - Automation Capability}

Finally, we evaluate design automation using:

\begin{itemize}
    \item \textbf{Performance relative to design time} ($T_{design}$) ;
    \item \textbf{Injected knowledge quantity} ($K_{design}$) ;
    \item \textbf{Iterations to convergence} ($N_{iter}$).
\end{itemize}

\begin{table}[h!]
    \centering
    \caption{Automation metrics across methods (G4)}
    \begin{tabular}{l|l|ccc}
        \hline
        \textbf{Env.} & \textbf{Method} & $T_{design}$ (days) & $K_{design}$ (lines) & $N_{iter}$ \\
        \hline
        \multirow{3}{*}{Overcooked-AI}
        & RB & 1.0 & 20 & 1 \\
        & OB & 2.5 & 300 & 2 \\
        & MB & 1.5 & 120 & 2 \\
        \hline
        \multirow{3}{*}{Predator-Prey}
        & RB & 1.2 & 25 & 1 \\
        & OB & 3.0 & 320 & 3 \\
        & MB & 1.8 & 140 & 3 \\
        \hline
        \multirow{3}{*}{Warehouse}
        & RB & 1.5 & 30 & 1 \\
        & OB & 3.5 & 340 & 2 \\
        & MB & 2.0 & 150 & 2 \\
        \hline
        \multirow{3}{*}{CyberDefense}
        & RB & 2.0 & 40 & 1 \\
        & OB & 4.0 & 400 & 3 \\
        & MB & 2.5 & 180 & 3 \\
        \hline
    \end{tabular}
    \label{tab:g4_full}
\end{table}

While RB has minimal knowledge injection, it lacks any organizational control. OB requires large expert effort ($K_{design}$) and longer design cycles. MAMAD significantly reduces manual specification while automatically discovering relevant organizational structures in only a few iterations.


% \subsection{Ablation Studies and Component Impact}

% To assess the relative contribution of MAMAD's core components, we conducted ablation studies across the four test environments, systematically disabling key automation modules (as stated in \autoref{sec:experimental_setup_ablations}).
% The ablations were compared against the full MAMAD pipeline (denoted \textbf{FULL}).

% We report the combined compliance and efficiency impact using $F_{org}$ (organizational fit), $R_{cum}$ (cumulative reward), and $T_{design}$ (design time in days), as representative global indicators.

% \begin{table}[h!]
%     \centering
%     \caption{Ablation study results across environments}
%     \begin{tabular}{l|c|ccc}
%         \hline
%         \textbf{Environment} & \textbf{Mode} & $F_{org}$ & $R_{cum}$ & $T_{design}$ \\
%         \hline
%         \multirow{4}{*}{Overcooked-AI}
%         & FULL & 95\% & 95\% & 1.5 \\
%         & WAM & 93\% & 90\% & 3.0 \\
%         & WOC & 80\% & 85\% & 1.5 \\
%         & WTA & 92\% & 94\% & 1.5 \\
%         \hline
%         \multirow{4}{*}{Predator-Prey}
%         & FULL & 90\% & 88\% & 1.8 \\
%         & WAM & 87\% & 83\% & 3.5 \\
%         & WOC & 75\% & 80\% & 1.8 \\
%         & WTA & 85\% & 86\% & 1.8 \\
%         \hline
%         \multirow{4}{*}{Warehouse}
%         & FULL & 94\% & 92\% & 2.0 \\
%         & WAM & 91\% & 88\% & 3.8 \\
%         & WOC & 82\% & 83\% & 2.0 \\
%         & WTA & 90\% & 90\% & 2.0 \\
%         \hline
%         \multirow{4}{*}{CyberDefense}
%         & FULL & 87\% & 85\% & 2.5 \\
%         & WAM & 83\% & 80\% & 4.2 \\
%         & WOC & 70\% & 75\% & 2.5 \\
%         & WTA & 85\% & 84\% & 2.5 \\
%         \hline
%     \end{tabular}
%     \label{tab:ablation}
% \end{table}

% \textbf{Automated Modeling (WAM)}: Disabling the world model leads to longer design times ($T_{design}$ increases), as manual modeling is time-consuming. While performance ($R_{cum}$) and organizational fit ($F_{org}$) remain acceptable, degradation is observable, particularly in dynamic environments like CyberDefense.

% \textbf{Organizational Constraints (WOC)}: Removing organizational guidance has the strongest impact on $F_{org}$ and slightly reduces task performance, as agents converge toward less structured but effective policies. The drop in organizational fit confirms the importance of MOISE+MARL guidance.

% \textbf{Trajectory-Based Analysis (WTA)}: Skipping trajectory-based role/goal inference results in moderately lower $F_{org}$ but still maintains most of the learned behaviors. However, the absence of explicit post-hoc explainability limits model interpretability.


% Overall, these ablation studies confirm that all three components contribute to both performance and explainability, with organizational constraints being the most critical for role consistency.


\section{Conclusion and perspectives}\label{sec:conclusion}

This work introduced \textbf{MAMAD}, a method designed to automate the development of MAS by integrating organizational modeling with MARL. Through a structured workflow, MAMAD facilitates environment modeling, agent training, behavior analysis, and deployment, reducing reliance on expert knowledge and increasing automation across the MAS design pipeline. 
% %
% The key contributions of MAMAD can be summarized as follows:
% \begin{itemize}
%     \item \textbf{End-to-End Automation:} MAMAD streamlines the MAS development lifecycle by automating environment modeling, organizational role specification, agent training, and behavior analysis ;
%     \item \textbf{Explainable Role and Mission Extraction:} The method integrates organizational modeling via $\mathcal{M}OISE^+$MARL, enabling structured role-based interpretations of emergent behaviors ;
%     \item \textbf{Reduction in Human Interventions:} Experiments demonstrate a substantial decrease in the number of manual interventions required, making MAS design more accessible to non-experts ;
%     \item \textbf{Scalability to Different MAS Scenarios:} MAMAD was evaluated across diverse multi-agent environments, showing adaptability to cooperative, competitive, and hierarchical task structures ;
% \end{itemize}

Quantitative evaluations suggest that MAMAD significantly enhances the efficiency of MAS design by reducing design iteration time, improving compliance with design constraints, and producing explainable agent roles and missions. These results highlight the potential of combining MARL with organizational frameworks to improve MAS development.

Despite its advantages, MAMAD also presents several limitations that warrant further research:
%
\begin{itemize}
    \item \textbf{Residual Need for Expert Oversight:} While MAMAD reduces manual interventions, certain steps (e.g., defining reward structures and tuning hyperparameters) still require expert involvement ;
    \item \textbf{Scalability to High-Dimensional Problems:} The method performs well on small- to medium-scale environments but may face limitations when applied to highly complex, dynamic, or real-world MAS settings ;
    \item \textbf{Interpretability of Learned Behaviors:} Although MAMAD provides an explainable role extraction process, further improvements are needed to enhance transparency in agent decision-making, particularly in adversarial settings ;
    \item \textbf{Computational Overhead:} The integration of automated modeling and learning algorithms increases computational demand, which may limit real-time applications.
\end{itemize}


To further develop MAMAD and enhance its applicability, several research directions can be explored:
%
\begin{itemize}
    \item \textbf{Improving Interpretability Tools:} Future work could focus on integrating more advanced interpretability techniques, such as causal reasoning and attention-based visualizations, to better explain learned behaviors ;
    \item \textbf{Scalability to Large-Scale MAS:} Enhancing MAMAD's efficiency for large-scale agent populations and high-dimensional state-action spaces will be key for broader applicability ;
    \item \textbf{Hybrid Human-in-the-Loop Approaches:} Combining automated learning with interactive user feedback could balance automation with domain expertise, improving both performance and interpretability ;
    \item \textbf{Real-World Deployments:} Extending the evaluation of MAMAD beyond simulated environments to real-world MAS applications (e.g., robotics, cybersecurity, logistics) would provide further validation of its effectiveness.
\end{itemize}



% \section*{Declarations}

% Some journals require declarations to be submitted in a standardised format. Please check the Instructions for Authors of the journal to which you are submitting to see if you need to complete this section. If yes, your manuscript must contain the following sections under the heading `Declarations':

% \begin{itemize}
% \item Funding ;
% \item Conflict of interest/Competing interests (check journal-specific guidelines for which heading to use) ;
% \item Ethics approval and consent to participate ;
% \item Consent for publication ;
% \item Data availability  ;
% \item Materials availability ;
% \item Code availability  ;
% \item Author contribution ;
% \end{itemize}

% \noindent
% If any of the sections are not relevant to your manuscript, please include the heading and write `Not applicable' for that section. 

% %%===================================================%%
% %% For presentation purpose, we have included        %%
% %% \bigskip command. Please ignore this.             %%
% %%===================================================%%
% \bigskip
% \begin{flushleft}%
% Editorial Policies for:

% \bigskip\noindent
% Springer journals and proceedings: \url{https://www.springer.com/gp/editorial-policies}

% \bigskip\noindent
% Nature Portfolio journals: \url{https://www.nature.com/nature-research/editorial-policies}

% \bigskip\noindent
% \textit{Scientific Reports}: \url{https://www.nature.com/srep/journal-policies/editorial-policies}

% \bigskip\noindent
% BMC journals: \url{https://www.biomedcentral.com/getpublished/editorial-policies}
% \end{flushleft}



% \begin{appendices}

%     \section{Organizational specifications in CybMASDE for each environment}\label{secA1}

%     TODO

%     \section{Detailed results for each baseline}\label{secA2}

%     TODO

% \end{appendices}

%%===========================================================================================%%
%% If you are submitting to one of the Nature Portfolio journals, using the eJP submission   %%
%% system, please include the references within the manuscript file itself. You may do this  %%
%% by copying the reference list from your .bbl file, paste it into the main manuscript .tex %%
%% file, and delete the associated \verb+\bibliography+ commands.                            %%
%%===========================================================================================%%

\bibliography{references}

\end{document}
