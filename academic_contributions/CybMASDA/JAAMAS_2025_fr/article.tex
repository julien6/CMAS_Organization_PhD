%Version 3.1 December 2024
% See section 11 of the User Manual for version history
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                                                 %%
%% Please do not use \input{...} to include other tex files.       %%
%% Submit your LaTeX manuscript as one .tex document.              %%
%%                                                                 %%
%% All additional figures and files should be attached             %%
%% separately and not embedded in the \TeX\ document itself.       %%
%%                                                                 %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%\documentclass[referee,sn-basic]{sn-jnl}% referee option is meant for double line spacing

%%=======================================================%%
%% to print line numbers in the margin use lineno option %%
%%=======================================================%%

%%\documentclass[lineno,pdflatex,sn-basic]{sn-jnl}% Basic Springer Nature Reference Style/Chemistry Reference Style

%%=========================================================================================%%
%% the documentclass is set to pdflatex as default. You can delete it if not appropriate.  %%
%%=========================================================================================%%

%%\documentclass[sn-basic]{sn-jnl}% Basic Springer Nature Reference Style/Chemistry Reference Style

%%Note: the following reference styles support Namedate and Numbered referencing. By default the style follows the most common style. To switch between the options you can add or remove �Numbered� in the optional parenthesis. 
%%The option is available for: sn-basic.bst, sn-chicago.bst%  
 
%%\documentclass[pdflatex,sn-nature]{sn-jnl}% Style for submissions to Nature Portfolio journals
%%\documentclass[pdflatex,sn-basic]{sn-jnl}% Basic Springer Nature Reference Style/Chemistry Reference Style
\documentclass[pdflatex,sn-mathphys-num]{sn-jnl}% Math and Physical Sciences Numbered Reference Style
%%\documentclass[pdflatex,sn-mathphys-ay]{sn-jnl}% Math and Physical Sciences Author Year Reference Style
%%\documentclass[pdflatex,sn-aps]{sn-jnl}% American Physical Society (APS) Reference Style
%%\documentclass[pdflatex,sn-vancouver-num]{sn-jnl}% Vancouver Numbered Reference Style
%%\documentclass[pdflatex,sn-vancouver-ay]{sn-jnl}% Vancouver Author Year Reference Style
%%\documentclass[pdflatex,sn-apa]{sn-jnl}% APA Reference Style
%%\documentclass[pdflatex,sn-chicago]{sn-jnl}% Chicago-based Humanities Reference Style

%%%% Standard Packages
%%<additional latex packages if required can be included here>

% \usepackage{graphicx}%
% \usepackage{multirow}%
\usepackage{amsmath,amssymb,amsfonts}%
\usepackage{amsthm}%
\usepackage{mathrsfs}%
% \usepackage[title]{appendix}%
% \usepackage{xcolor}%
% \usepackage{textcomp}%
% \usepackage{manyfoot}%
% \usepackage{booktabs}%
% \usepackage{algorithm}%
% \usepackage{algorithmicx}%
% \usepackage{algpseudocode}%
% \usepackage{listings}%

\usepackage{hyperref}

%%%% For camera-ready, use this
%\documentclass[sigconf]{aamas}

\usepackage{listings}
\usepackage{xcolor}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}
 
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}
 
\lstset{style=mystyle}

% --- Tickz
\usepackage{physics}
\usepackage{tikz}
\usepackage{amsmath}
\usepackage{mathdots}
% \usepackage{yhmath}
\usepackage{cancel}
\usepackage{color}
\usepackage{siunitx}
\usepackage{array}
\usepackage{multirow}
% \usepackage{amssymb}
\usepackage{gensymb}
\usepackage{tabularx}
\usepackage{extarrows}
\usepackage{booktabs}
\usetikzlibrary{fadings}
\usetikzlibrary{patterns}
\usetikzlibrary{shadows.blur}
\usetikzlibrary{shapes}

% ---------

\usepackage{balance} % for balancing columns on the final page
\usepackage{csquotes}
% \usepackage{cite}
\newcommand{\probP}{\text{I\kern-0.15em P}}
\usepackage{etoolbox}
\patchcmd{\thebibliography}{\section*{\refname}}{}{}{}
% \usepackage{amsthm,amssymb,amsfonts}

\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{color}
% \renewcommand\UrlFont{\color{blue}\rmfamily}

\usepackage[inline, shortlabels]{enumitem}
\usepackage{tabularx}
\usepackage{caption}
\usepackage{listings}
\usepackage{stfloats}
\usepackage{titlesec}
\usepackage{ragged2e}
% \usepackage[hyphens]{url}
\usepackage{float}
\usepackage[english]{babel}
\addto\extrasenglish{  
    \def\figureautorefname{Figure}
    \def\tableautorefname{Table}
    \def\algorithmautorefname{Algorithm}
    \def\sectionautorefname{Section}
    \def\subsectionautorefname{Subsection}
    \def\proofoutlineautorefname{Proof Outline}
}

\usepackage[linesnumbered, ruled, vlined]{algorithm2e}
\SetKwComment{Comment}{$\triangleright$\ }{}
\SetAlgoNlRelativeSize{0}
\SetAlgoNlRelativeSize{-1}

%%%%

%%%%%=============================================================================%%%%
%%%%  Remarks: This template is provided to aid authors with the preparation
%%%%  of original research articles intended for submission to journals published 
%%%%  by Springer Nature. The guidance has been prepared in partnership with 
%%%%  production teams to conform to Springer Nature technical requirements. 
%%%%  Editorial and presentation requirements differ among journal portfolios and 
%%%%  research disciplines. You may find sections in this template are irrelevant 
%%%%  to your work and are empowered to omit any such section if allowed by the 
%%%%  journal you intend to submit to. The submission guidelines and policies 
%%%%  of the journal take precedence. A detailed User Manual is available in the 
%%%%  template package for technical guidance.
%%%%%=============================================================================%%%%

%% as per the requirement new theorem styles can be included as shown below
\theoremstyle{thmstyleone}%
\newtheorem{theorem}{Theorem}%  meant for continuous numbers
%%\newtheorem{theorem}{Theorem}[section]% meant for sectionwise numbers
%% optional argument [theorem] produces theorem numbering sequence instead of independent numbers for Proposition
\newtheorem{proposition}[theorem]{Proposition}% 
%%\newtheorem{proposition}{Proposition}% to get separate numbers for theorem and proposition etc.

\theoremstyle{thmstyletwo}%
\newtheorem{example}{Example}%
\newtheorem{remark}{Remark}%

\theoremstyle{thmstylethree}%
\newtheorem{definition}{Definition}%

\raggedbottom
%%\unnumbered% uncomment this for unnumbered level heads

\begin{document}

\title[Assisting Multi-Agent System Design with MOISE+ and MARL: The MAMAD Method]{Assisting Multi-Agent System Design with $\mathcal{M}OISE^+$ and MARL: The MAMAD Method}

%%=============================================================%%
%% GivenName	-> \fnm{Joergen W.}
%% Particle	-> \spfx{van der} -> surname prefix
%% FamilyName	-> \sur{Ploeg}
%% Suffix	-> \sfx{IV}
%% \author*[1,2]{\fnm{Joergen W.} \spfx{van der} \sur{Ploeg} 
%%  \sfx{IV}}\email{iauthor@gmail.com}
%%=============================================================%%

\author*[1]{\fnm{Julien} \sur{Soulé}}\email{julien.soule@lcis.grenoble-inp.fr}

\author[1]{\fnm{Jean-Paul} \sur{Jamont}}\email{jean-paul.jamont@lcis.grenoble-inp.fr}
% \equalcont{These authors contributed equally to this work.}

\author[1]{\fnm{Michel} \sur{Occello}}\email{michel.occello@lcis.grenoble-inp.fr}

\author[2]{\fnm{Louis-Marie} \sur{Traonouez}}\email{louis-marie.traonouez@thalesgroup.com}

\author[3]{\fnm{Paul} \sur{Théron}}\email{paul.theron@orange.fr}

\affil*[1]{\orgdiv{Laboratoire de Conception et d'Intégration des Systèmes (LCIS)}, \orgname{Université Grenoble Alpe}, \orgaddress{\street{50 Rue Barthélémy de Laffemas}, \city{Valence}, \postcode{26000}, \state{Auvergne-Rhône-Alpes}, \country{France}}}

\affil[2]{\orgdiv{Thales Land and Air Systems}, \orgname{BL IAS}, \orgaddress{\street{1 Rue Louis Braille}, \city{Saint-Jacques-de-la-Lande}, \postcode{35136}, \state{Ille-et-Vilaine}, \country{France}}}

\affil[3]{\orgname{AICA IWG}, \orgaddress{\street{22 Av. Gazan Prolongée, 06600 Antibes}, \city{Antibes}, \postcode{06600}, \state{Provence-Alpes-Côte d'Azur}, \country{France}}}

%%==================================%%
%% Sample for unstructured abstract %%
%%==================================%%

\abstract{
    Concevoir des Systèmes Multi-Agents (SMA) implique de trouver un équilibre entre comportement structuré et adaptabilité. L’Ingénierie Logicielle Orientée Agents (AOSE) repose traditionnellement sur l’expertise humaine, tandis que l’Apprentissage par Renforcement Multi-Agent (MARL) permet un apprentissage autonome, mais souffre d’un manque d’interprétabilité et de contrôle. En considérant la conception de SMA comme un problème de recherche de politique, MARL peut venir en appui à l’AOSE en découvrant des politiques adaptées pour les agents. Cependant, cette intégration reste peu explorée.
    %
    Nous proposons la méthode \textbf{MOISE+MARL Assisted MAS Design (MAMAD)}, une approche en quatre phases qui formalise la conception de SMA comme un problème d’optimisation sous contraintes : apprendre des politiques conjointes maximisant les récompenses tout en respectant les rôles et objectifs de $\mathcal{M}OISE^+$. Les phases incluent :  
    1) la \textbf{modélisation} de l’environnement,  
    2) l’\textbf{entraînement} sous contraintes organisationnelles,  
    3) l’\textbf{analyse} des comportements émergents,  
    4) le \textbf{transfert} vers un déploiement réel.
    %
    Nous évaluons MAMAD sur différents environnements et montrons que les SMA générés atteignent les performances attendues, respectent les exigences de conception, restent interprétables, tout en réduisant la charge de conception manuelle.
}

% Introduction
% Purpose
% Methods
% Results
% Conclusion

\keywords{Agent-oriented Software Engineering, Multi-Agent Reinforcement Learning, Assisted-Design, Organizational Model}

%%\pacs[JEL Classification]{D8, H51}

%%\pacs[MSC Classification]{35A01, 65L10, 65L12, 65L20, 65L70}

\maketitle



\section{Introduction}

\subsection{Contexte}

La conception de \textbf{Systèmes Multi-Agents} (SMA) pour des applications complexes du monde réel, telles que la cybersécurité, la logistique, la robotique autonome ou le transport intelligent, nécessite des méthodologies garantissant à la fois des comportements structurés et orientés vers des objectifs~\cite{Jamont2O15}. Traditionnellement, le paradigme de l’\textbf{Ingénierie Logicielle Orientée Agents} (AOSE) fournit des cadres systématiques pour spécifier et développer des SMA, en mettant l’accent sur la conception des agents, des rôles et des interactions~\cite{Pavon2003, Bernon2005}. Les méthodologies AOSE s’appuient généralement sur l’expertise explicite d’experts et sur des modèles organisationnels prédéfinis pour structurer le comportement des agents, assurant ainsi prévisibilité, fiabilité et conformité aux contraintes~\cite{Hindriks2014}.

Cependant, les approches AOSE traditionnelles présentent des limites en termes d’adaptabilité et de passage à l’échelle. Elles exigent souvent une expertise approfondie du domaine pour définir les structures organisationnelles et les règles comportementales, ce qui rend leur généralisation difficile dans des environnements dynamiques. À notre connaissance, aucun travail n’a encore exploré l’intégration de l’Apprentissage Automatique (ML) dans la conception de SMA selon l’approche AOSE, notamment pour l’adaptation à des événements imprévus~\cite{Garcia2004}.

En contraste, l’\textbf{Apprentissage par Renforcement Multi-Agent} (MARL) s’est imposé comme une approche ML distincte permettant aux agents autonomes de \textbf{l’apprendre et de s’adapter} par l’expérience. Les techniques MARL permettent aux agents de développer des \textbf{stratégies de coordination} en interagissant avec leur environnement et en optimisant leurs politiques décisionnelles sur la base de récompenses cumulées~\cite{Zhang2021}. Ce paradigme d’apprentissage fondé sur les données permet aux agents d’ajuster dynamiquement leurs comportements selon les contextes, y compris dans des environnements très incertains ou complexes, tels que le contrôle décentralisé, les scénarios adverses ou la résolution collaborative de problèmes~\cite{Papoudakis2021}.

D’un point de vue théorique, l’intégration du \textbf{MARL} dans l’\textbf{AOSE} ouvre une voie prometteuse pour améliorer la conception de SMA. Le MARL peut aider à \textit{déterminer automatiquement les politiques des agents} qui respectent les contraintes de l’environnement et les objectifs, réduisant ainsi la spécification manuelle des comportements. Il permettrait aux agents d’\textit{apprendre des stratégies de coordination} ou de \textit{s’adapter à des environnements évolutifs} grâce à l’expérience. Dans cette optique, le MARL constitue une couche guidée par l’apprentissage, qui instancie et opérationnalise les abstractions définies dans l’AOSE.

Malgré ses atouts, le MARL présente d’importants défis en termes \textbf{d’interprétabilité et de contrôle}. Contrairement à l’AOSE, qui impose des interactions structurées via des modèles prédéfinis, le MARL repose sur des comportements émergents, souvent imprévisibles et difficiles à interpréter~\cite{Du2022}. Ce manque de transparence soulève des enjeux dans des domaines critiques tels que la \textbf{cybersécurité} ou la \textbf{collaboration humain-agent}, où l’explicabilité et le respect de contraintes de haut niveau sont essentiels. De plus, le MARL ne dispose pas de mécanismes intrinsèques pour faire respecter les \textbf{contraintes organisationnelles}, telles que les rôles prédéfinis, les structures d’équipe ou les directives de sécurité, ce qui limite son applicabilité dans des SMA critiques~\cite{Nguyen2020}.

Face à ces avantages et limites contrastés, cet article cherche à intégrer les capacités de modélisation structurée de l’AOSE avec le potentiel adaptatif du MARL pour concevoir des SMA efficaces, tirant profit des atouts des deux approches.

\subsection{Problématique et verrous de recherche}

Afin de concilier les domaines de l’AOSE et du MARL, nous adoptons une perspective d’\textbf{optimisation} dans la conception des SMA. Nous considérons que concevoir un SMA dans un environnement de déploiement, dont l’objectif est d’atteindre efficacement une finalité globale tout en respectant éventuellement des exigences supplémentaires définies par l’utilisateur, peut être formulé comme un \textbf{problème d’optimisation sous contraintes dans un contexte MARL}. Dans cette formulation :
\begin{itemize}
    \item La \textbf{variable à optimiser} est la politique conjointe des agents dans l’espace des politiques ;
    \item La \textbf{fonction objectif} consiste à maximiser la récompense cumulée dans le temps, quantifiant ainsi l’efficacité des agents à atteindre leur objectif ;
    \item Les \textbf{contraintes} sont les spécifications organisationnelles, telles que les rôles et objectifs définis par l’utilisateur, représentant les exigences du concepteur.
\end{itemize}

Cette formulation constitue l’ossature de cet article et motive notre contribution.
%
Pour mettre en œuvre cette approche, plusieurs verrous de recherche majeurs demeurent non résolus lorsqu’on aborde la conception de SMA sous un \textbf{angle organisationnel} :
%
\begin{itemize}
    \item \textbf{(G1) Exploiter les performances du MARL au sein de l’AOSE} : l’AOSE ne propose pas d’intégration du MARL, tandis que le MARL ne prend pas en charge les contraintes de conception structurée. Il n’existe actuellement \textbf{aucun cadre unifié combinant AOSE et apprentissage guidé par MARL}~\cite{Cossentino2014}. Lever ce verrou permettrait aux SMA de \textbf{bénéficier de la puissance computationnelle du MARL} pour générer automatiquement des politiques d’agents suffisamment performantes ;
          
    \item \textbf{(G2) Comprendre les comportements collectifs émergents dans le MARL} : les agents MARL peuvent adopter des stratégies imprévisibles, rendant difficile l’analyse comportementale. Une méthode est nécessaire pour \textbf{aligner ces comportements émergents avec des structures organisationnelles}~\cite{Du2022, Papoudakis2021}. Résoudre ce verrou améliorerait l’\textbf{explicabilité organisationnelle}, facilitant la validation, l’ajustement et le déploiement ;
          
    \item \textbf{(G3) Contrôler ou guider les agents aux niveaux individuel et collectif dans le MARL} : le MARL ne propose pas de mécanismes pour \textbf{orienter les agents vers des comportements structurés tout en préservant leur flexibilité}. La majorité des approches optimisent la performance sans imposer de \textbf{contraintes organisationnelles}~\cite{Oroojlooy2023}. Résoudre ce verrou garantirait la \textbf{conformité aux exigences de conception} et accélérerait la convergence en réduisant l’espace de recherche des politiques ;
          
    \item \textbf{(G4) Automatiser la conception de SMA de bout en bout} : la conception de SMA repose aujourd’hui fortement sur des experts du domaine et s’appuie sur des processus coûteux par essais-erreurs. Les spécifications conçues manuellement \textbf{manquent de scalabilité et de généricité}~\cite{Nguyen2020}. Surmonter ce verrou permettrait une \textbf{conception automatisée des SMA}, réduisant la dépendance aux experts, l’effort manuel et les coûts, tout en obtenant des résultats comparables.
\end{itemize}
%
Ces verrous soulignent la nécessité d’une \textbf{méthode intégrant la modélisation organisationnelle dans la conception de SMA guidée par le MARL}.

\subsection{Contributions et organisation de l’article}

Nous proposons la \textbf{méthode MAMAD}, une extension du cadre \textbf{MOISE+MARL}~\cite{soule2025moisemarl}\footnote{Cet article introduisant MOISE+MARL a été accepté à AAMAS 2025 et est librement accessible à l’adresse \url{https://arxiv.org/abs/2503.23615}}, afin de structurer l’apprentissage dans le MARL. MAMAD permet un \textbf{apprentissage contrôlé des politiques}, en alignant les comportements des agents avec des spécifications organisationnelles prédéfinies, tout en extrayant des connaissances à partir des comportements émergents. Il encapsule MOISE+MARL dans un processus de conception entièrement automatisé, en quatre phases, guidé par trois entrées : l’environnement, les exigences définies par l’utilisateur, et l’objectif global.

Dans l’AOSE classique, la phase d’\textit{Ingénierie des Exigences} consiste à spécifier les contraintes de conception, les contraintes environnementales, et les objectifs globaux~\cite{Pavon2003, Bernon2005}. Nous supposons que ces éléments sont fournis, et laissons la méthodologie d’ingénierie au soin de l’utilisateur. MAMAD les formalise et les opérationnalise dans le cadre MOISE+MARL comme partie intégrante d’un processus de conception automatisé.

\begin{enumerate}
    \item \textbf{Phase de Modélisation :} construit automatiquement un environnement simulé en entraînant une architecture neuronale inspirée des « world models »~\cite{Ha2018}, et définit l’objectif global du SMA via une fonction de récompense dédiée.
    \item \textbf{Phase d’Entraînement :} les agents apprennent dans l’environnement simulé, éventuellement guidés par des contraintes organisationnelles : les rôles (limitant les actions autorisées) et les objectifs (structurant les récompenses). Ces éléments encadrent l’apprentissage selon les spécifications définies par l’utilisateur.
    \item \textbf{Phase d’Analyse :} des techniques d’apprentissage non supervisé analysent les trajectoires réussies afin d’extraire des rôles et objectifs émergents au sein de l’espace des politiques contraint, produisant des spécifications et politiques validées.
    \item \textbf{Phase de Transfert :} la politique conjointe validée est déployée dans l’environnement réel pour opérationnaliser le SMA entraîné.
\end{enumerate}

Nous avons évalué \textbf{MAMAD} dans des environnements ludiques, utilisés comme bancs d’essai contrôlés pour évaluer sa capacité à générer des simulations fidèles durant la phase de Modélisation — évitant ainsi la complexité de la modélisation d’environnements physiques. Les résultats montrent une forte cohérence entre les spécifications appliquées pendant l’entraînement et celles inférées a posteriori, validant à la fois l’\textbf{explicabilité organisationnelle} et la \textbf{conformité aux exigences de conception}. Le SMA obtenu atteint également les performances de référence, confirmant que l’\textbf{atteinte des objectifs} est bien préservée. Comparée aux méthodes manuelles, l’\textbf{automatisation} a significativement progressé, nécessitant moins d’interventions humaines. Les études d’ablation révèlent que l’absence de modélisation automatique diminue la généralisation des politiques, tandis que l’absence de contraintes organisationnelles mène à des comportements erratiques des agents.

\vspace{0.5em}

L’article est organisé comme suit. \autoref{sec:related_works} présente les travaux connexes sur la conception de SMA, le MARL, le contrôle organisationnel et l’explicabilité. \autoref{sec:background} rappelle les bases du MARL, de $\mathcal{M}OISE^+$ et de l’apprentissage des World Models utilisés dans les cadres \textbf{MOISE+MARL} et \textbf{MAMAD}. \autoref{sec:mamad} décrit la \textbf{méthode MAMAD} à travers chacune de ses phases, en détaillant l’usage de MOISE+MARL. \autoref{sec:experimental_setup} décrit le protocole expérimental. \autoref{sec:results} présente et analyse les résultats. Enfin, \autoref{sec:conclusion} conclut l’article et esquisse les perspectives futures.


\section{Travaux connexes}\label{sec:related_works}

Cette section passe en revue les recherches existantes au regard des verrous ciblés. \autoref{sub-sec:rel_aose_automate_marl} examine les méthodologies AOSE et les efforts récents visant à automatiser divers aspects de la conception des SMA (G4). Elle aborde également les limites actuelles de l’intégration du MARL dans l’AOSE, tout en considérant les approches basées sur l’apprentissage par renforcement (RL) qui contribuent au développement des SMA (G1). \autoref{sub-sec:rel_control} présente les méthodes existantes pour guider ou contraindre le processus d’apprentissage dans le MARL et le RL (G2).
\autoref{sub-sec:rel_evaluation} examine les travaux consacrés à l’amélioration de l’explicabilité des politiques apprises par les agents (G3).

\subsection{Automatisation et travaux analogues fondés sur le MARL dans l’AOSE}\label{sub-sec:rel_aose_automate_marl}

L’AOSE a introduit des méthodologies structurées pour la conception de SMA, mettant l’accent sur les interactions fondées sur les rôles, les hiérarchies organisationnelles et la coordination systématique des agents. Des cadres classiques tels que \textbf{GAIA}~\cite{gaia1998}, \textbf{ADELFE}~\cite{adelfe2002} et \textbf{DIAMOND}~\cite{Jamont2005} proposent des processus bien définis pour concevoir des SMA, en s’appuyant sur une modélisation organisationnelle explicite. Toutefois, ces méthodes sont largement manuelles et nécessitent une expertise spécialisée pour définir les comportements des agents, les rôles et les contraintes organisationnelles, ce qui rend leur passage à l’échelle difficile dans des environnements complexes ou dynamiques.

Afin d’améliorer l’efficacité et la scalabilité, diverses méthodologies et cadres ont été proposés pour automatiser différents aspects de la conception de SMA :

\textbf{Ingénierie dirigée par les modèles en AOSE :} INGENIAS adopte une approche d’ingénierie dirigée par les modèles, en fournissant des méta-modèles et des outils facilitant la génération automatique de code, de documentation et de tests à partir de spécifications de haut niveau, rationalisant ainsi le processus de développement de SMA~\cite{pavon2005agent}.

\textbf{Conception organisationnelle fondée sur les connaissances :} Le cadre KB-ORG introduit une approche basée sur la connaissance pour automatiser la conception organisationnelle des SMA. En s’appuyant sur des modèles prédéfinis et une base de connaissances organisationnelle, KB-ORG assiste l’attribution systématique des rôles et responsabilités aux agents, réduisant ainsi le fardeau manuel lié à la spécification organisationnelle~\cite{dignum2001kb}.

\textbf{Conception automatisée de systèmes agentiques :} Le domaine émergent de la conception automatisée de systèmes agentiques se concentre sur la création automatique de systèmes multi-agents. Cela inclut la génération de nouveaux blocs fonctionnels et leur composition en systèmes opérationnels, dans le but de minimiser l’intervention humaine lors de la phase de conception~\cite{smith2024automated}.

\textbf{Systèmes multi-agents auto-génératifs :} AutoGenesisAgent propose un cadre dans lequel des systèmes multi-agents sont capables de concevoir et déployer de manière autonome d’autres systèmes multi-agents adaptés à des tâches spécifiques. Cette capacité auto-générative couvre l’ensemble du cycle de vie, du concept initial au déploiement, constituant une avancée significative vers une conception de SMA totalement automatisée~\cite{harper2024autogenesisagent}.

\textbf{Cadres collaboratifs pour l’automatisation des tâches :} Le cadre BMW Agents met l’accent sur l’automatisation des tâches à travers la collaboration multi-agents. Il décrit une approche d’ingénierie des agents flexible, prenant en charge la planification et l’exécution dans divers domaines, assurant ainsi la scalabilité et l’adaptabilité dans des applications industrielles complexes~\cite{crawford2024bmw}.

\vspace{0.5em}

Parallèlement aux méthodologies AOSE classiques, le domaine du \textbf{MARL} permet aux agents d’\textbf{apprendre de manière autonome} des stratégies de coordination à partir de l’expérience. Même si le MARL ou le RL ne sont pas initialement conçus pour l’AOSE, leur capacité à générer des \textbf{comportements auto-organisés} chez les agents peut y contribuer.

Un projet notable en ce sens est le \textit{Cyber Security Learning Environment}~\cite{hammar2023scalable}, un \textbf{cadre en ligne} destiné à la cybersécurité, dans lequel un agent est entraîné en simulation pour apprendre dynamiquement des comportements adaptés à des tâches spécifiques. Ce cadre permet en partie d’\textbf{automatiser la conception de SMA de bout en bout}, puisqu’il suit un pipeline automatisé allant de la modélisation de l’environnement à l’entraînement et au déploiement, avec un effort manuel minimal. Il propose également des outils de visualisation pour interpréter les interactions entre agents, bien qu’il ne s’appuie pas sur une modélisation organisationnelle explicite.

Malgré ces avancées, des défis subsistent quant à l’intégration fluide entre AOSE et MARL. Aligner les algorithmes d’apprentissage avec des contraintes organisationnelles et s’assurer que les comportements émergents respectent les spécifications du système restent des axes de recherche ouverts. Les travaux futurs s’orientent vers le développement de cadres standardisés encapsulant à la fois les modèles organisationnels définis en phase de conception et les mécanismes d’apprentissage à l’exécution, afin de favoriser le développement de systèmes multi-agents robustes et adaptatifs.


\subsection{Contrôle ou guidage dans le MARL}\label{sub-sec:rel_control}

Dans le MARL, guider ou contraindre le processus d’apprentissage est essentiel pour s’assurer que les agents développent des comportements alignés avec des exigences de sécurité, d’équité et de spécificité des tâches. Diverses méthodologies ont été proposées pour intégrer de telles contraintes dans le cadre d’apprentissage.

\textbf{Apprentissage par renforcement guidé par contraintes.} Spieker~\cite{spieker2021constraint} introduit l’« Apprentissage par renforcement guidé par contraintes », qui intègre des modèles de contraintes dans l’interaction agent-environnement. Cette approche permet aux agents d’apprendre des politiques optimales tout en respectant des contraintes comportementales spécifiées, renforçant ainsi la sécurité et la fiabilité durant l’entraînement et le déploiement.

\textbf{Q-Learning contraint.} Kalweit et al.~\cite{kalweit2020deep} proposent le Deep Constrained Q-Learning, un algorithme hors-politiques qui restreint l’espace des actions lors de la mise à jour des valeurs Q. En intégrant des contraintes à la fois à une étape unique et sur plusieurs étapes approximées, cette méthode garantit que les politiques apprises respectent des critères de sécurité et de performance prédéfinis.

\textbf{Optimisation de politiques sous contraintes.} Achiam et al.~\cite{achiam2017constrained} développent la méthode Constrained Policy Optimization (CPO), un algorithme de recherche de politiques qui applique des contraintes tout au long du processus d’apprentissage. CPO offre des garanties théoriques de satisfaction quasi-optimale des contraintes à chaque itération, ce qui le rend adapté aux applications exigeant un strict respect des normes de sécurité.

\textbf{Guidage hiérarchique avec retour humain.} Zhou et al.~\cite{zhou2024mentor} présentent MENTOR, un cadre d’apprentissage par renforcement hiérarchique intégrant le retour d’information humain et des contraintes dynamiques de distance. Cette approche guide l’agent dans la sélection de sous-objectifs ni trop simples ni trop complexes, facilitant ainsi un apprentissage plus stable et efficace dans des tâches complexes.

\textbf{Apprentissage contraint sans récompense.} Miryoosefi et al.~\cite{miryoosefi2022simple} proposent une approche d’apprentissage contraint sans recours à une fonction de récompense explicite. En se concentrant uniquement sur la satisfaction des contraintes, cette méthode est adaptée aux contextes où la définition d’une fonction de récompense adéquate est difficile ou impraticable.

Ces méthodologies mettent en lumière l’importance d’incorporer des mécanismes de contrainte et de guidage dans le MARL, afin de garantir que les agents apprennent des comportements non seulement efficaces, mais également conformes aux standards de sécurité et de performance prédéfinis.


\subsection{Explicabilité dans le MARL}\label{sub-sec:rel_evaluation}

L’explicabilité et l’interprétabilité dans le MARL visent à rendre le comportement des agents transparent, facilitant la confiance, le débogage, ainsi que le respect des normes de sécurité.

\textbf{Interprétabilité fondée sur des concepts.} Zabounidis et al.~\cite{zabounidis2023concept} introduisent une méthode qui intègre des concepts interprétables fournis par des experts du domaine dans les modèles MARL. En demandant au modèle de prédire ces concepts avant de prendre des décisions, l’approche renforce la transparence et permet l’intervention d’experts pour corriger d’éventuelles erreurs de prédiction, améliorant à la fois l’interprétabilité et les performances.

\textbf{Techniques de décomposition des récompenses.} Iturria-Rivera et al.~\cite{iturria2024explainable} proposent un cadre MARL explicable reposant sur la décomposition des récompenses dans des algorithmes à base de factorisation de fonctions de valeur, tels que VDN et QMIX. Cette méthode fournit des éclairages sur la contribution des différentes composantes de la fonction de récompense au processus décisionnel global, renforçant ainsi l’interprétabilité des comportements des agents.

\textbf{Architectures de modèles interprétables.} Liu et al.~\cite{liu2022mixrts} présentent MIXRTs, une architecture combinant réseaux de neurones récurrents et arbres de décision flous pour concevoir des modèles MARL interprétables. Cette architecture permet une représentation explicite des processus de décision et clarifie la contribution de chaque agent à la performance collective, en réponse à l’opacité des modèles d’apprentissage profond traditionnels.

\textbf{Méthodes d’explication a posteriori.} Poupart et al.~\cite{poupart2025perspectives} discutent de diverses techniques d’explication a posteriori pour interpréter les modèles MARL, telles que la rétropropagation de pertinence et la modification d’activations. Ces méthodes visent à extraire des explications à partir de modèles entraînés sans en modifier l’architecture, fournissant ainsi des éclairages sur les comportements d’agents et les phénomènes émergents.

\textbf{Interprétabilité indépendante du modèle.} Li et al.~\cite{li2025from} proposent une approche indépendante du modèle basée sur les valeurs de Shapley, qui transforme des politiques d’apprentissage par renforcement profond complexes en représentations transparentes. Cette technique comble le fossé entre explicabilité et interprétabilité, offrant des politiques stables et interprétables, applicables aussi bien aux algorithmes on-policy qu’off-policy.

\textbf{Inférence des rôles et objectifs.} Plusieurs travaux explorent l’inférence de rôles ou d’objectifs comme moyen d’évaluer la cohérence organisationnelle. Wilson et al.~\cite{wilson2008learning} proposent un transfert de rôles dans des MDP multi-agents pour renforcer l’adaptabilité, bien que limité à des rôles spécifiques aux tâches. Berenji et Vengerov~\cite{berenji2000learning} modélisent les dépendances entre agents afin d’améliorer la coordination dans des missions de drones (UAV), mais sans inférence de rôles abstraits. Yusuf et Baber~\cite{yusuf2020inferential} utilisent l’inférence bayésienne pour une coordination dynamique des tâches, mais sans alignement organisationnel explicite. Serrino et al.~\cite{serrino2019finding} infèrent les rôles via les interactions dans des environnements sociaux, en se concentrant sur des rôles opérationnels plutôt qu’organisationnels.



\section{Background théorique}\label{sec:background}

\subsection{Cadre de Markov pour le MARL}

Pour appliquer les techniques de MARL, nous nous appuyons sur le modèle de \textit{Processus Décisionnel de Markov Décentralisé Partiellement Observable} (Dec-POMDP)~\cite{Oliehoek2016}. Les Dec-POMDP modélisent la coordination multi-agents décentralisée sous observabilité partielle, ce qui les rend particulièrement adaptés à l’intégration de contraintes organisationnelles. Contrairement aux \textit{jeux stochastiques partiellement observables} (POSG), le Dec-POMDP utilise une fonction de récompense commune, favorisant ainsi la collaboration~\cite{Beynier2013}.

Un Dec-POMDP $d \in D$ (où $D$ est l’ensemble des Dec-POMDPs) est défini comme un 7-uplet $d = \langle S, \{A_i\}, T, R, \{\Omega_i\}, O, \gamma \rangle$, où :
\begin{itemize}
    \item $S = \{s_1,\dots,s_{|S|}\}$ est l’ensemble des états possibles ;
    \item $A_{i} = \{a_{1}^{i},\dots,a_{|A_{i}|}^{i}\}$ est l’ensemble des actions possibles pour l’agent $i$ ;
    \item $T$ représente l’ensemble des probabilités de transition, avec $T(s,a,s') = \probP(s'|s,a)$, la probabilité de transition de l’état $s$ à l’état $s'$ après l’action $a$ ;
    \item $R: S \times A \times S \rightarrow \mathbb{R}$ est la fonction de récompense, attribuant une récompense en fonction de l’état initial, de l’action effectuée, et de l’état résultant ;
    \item $\Omega_{i} = \{o_{1}^{i},\dots,o_{|\Omega_{i}|}^{i}\}$ est l’ensemble des observations possibles pour l’agent $i$ ;
    \item $O$ est l’ensemble des probabilités d’observation, avec $O(s',a,o) = \probP(o|s',a)$ représentant la probabilité d’obtenir l’observation $o$ après avoir réalisé l’action $a$ et atteint l’état $s'$ ;
    \item $\gamma \in [0,1]$ est le facteur d’actualisation, utilisé pour pondérer les récompenses futures.
\end{itemize}

Le formalisme suivant est utilisé avec MOISE+MARL pour résoudre le Dec-POMDP~\cite{Beynier2013,Albrecht2024} :
\begin{itemize}
    \item $\mathcal{A}$ représente l’ensemble des $n$ \textbf{agents} ;
    \item $\Pi$ désigne l’ensemble des \textbf{politiques}, où une politique $\pi \in \Pi$, $\pi: \Omega \rightarrow A$, associe de manière déterministe une observation à une action, représentant la stratégie interne de l’agent ;
    \item $\Pi^{j}$ représente l’ensemble des \textbf{politiques conjointes}, avec une politique conjointe $\pi^{j} \in \Pi^{j}, \pi^{j}: \Omega^n \rightarrow A^n = \Pi^n$, sélectionnant une action pour chaque agent en fonction de ses observations respectives. Elle agit comme une collection de politiques utilisées par les agents au sein d’une équipe ;
    \item $H$ est l’ensemble des \textbf{historiques}, où un historique (ou trajectoire) sur $z \in \mathbb{N}$ étapes (typiquement le nombre maximal d’étapes dans un épisode) est représenté par le $z$-uplet $h = \langle \langle \omega_{k}, a_{k}\rangle | k \leq z, \omega \in \Omega, a \in A\rangle$, capturant la succession des observations et actions ;
    \item $H^{j}$ désigne l’ensemble des \textbf{historiques conjoints}, avec un historique conjoint $h^{j} \in H^{j}$ sur $z$ étapes défini comme l’ensemble des historiques des agents : $h^{j} = \{h_1, h_2, \dots, h_n\}$ ;
    \item $V^{j}(\pi^{j}): \Pi^{j} \rightarrow \mathbb{R}$ désigne la \textbf{récompense cumulative espérée} sur un horizon fini (en supposant $\gamma < 1$ ou un nombre d’étapes fini), où $\pi^{j}$ représente la politique conjointe pour l’équipe $i$, et $\pi^{j}_{-i}$ les politiques conjointes des autres équipes considérées comme fixées.
\end{itemize}

% Résoudre un Dec-POMDP revient à chercher une politique conjointe $\pi^{j} \in \Pi^{j}$ telle que $V^{j}(\pi^{j}) \geq s$, où $s \in \mathbb{R}$ est une récompense cumulative cible.

\subsection{Lier $\mathcal{M}OISE^+$ avec le MARL}

\begin{figure}[h!]
    \centering
    \input{figures/mm_synthesis_single_column.tex}
    \caption{Une vue minimale du cadre MOISE+MARL :
        Les utilisateurs commencent par définir les spécifications $\mathcal{M}OISE^+$, qui incluent les rôles ($\mathcal{R}$) et les missions ($\mathcal{M}$), tous deux associés via la relation $rds$.  
        Ils créent ensuite les spécifications MOISE+MARL en définissant d’abord les \textbf{guides de contrainte}, tels que $rag$ et $rrg$ pour spécifier la logique des rôles, et $grg$ pour la logique des objectifs.  
        Puis, les \textbf{lieurs} (\textit{linkers}) sont utilisés pour relier les agents aux rôles via $ar$, et pour connecter la logique des guides de contrainte aux spécifications $\mathcal{M}OISE^+$ définies.  
        Une fois cette configuration en place, les rôles peuvent être attribués aux agents, et le cadre MARL s’adapte automatiquement durant l’apprentissage.}
    \label{fig:mm_synthesis}
\end{figure}

\

\noindent Les \textbf{Guides de Contrainte} sont trois relations introduites pour modéliser la logique des rôles et objectifs de $\mathcal{M}OISE^+$ dans le formalisme Dec-POMDP :

\begin{enumerate*}[label={\roman*) },itemjoin={; \quad}]
    \item \textbf{Guide d’Action de Rôle} \quad $rag: H \times \Omega \rightarrow \mathcal{P}(A \times \mathbb{R})$ : cette relation modélise un rôle comme un ensemble de règles associant, à chaque paire composée d’un historique $h \in H$ et d’une observation $\omega \in \Omega$, un ensemble d’actions attendues $A \in \mathcal{P}(A)$ chacune associée à un niveau de contrainte $ch \in [0,1]$ (avec $ch = 1$ par défaut). En restreignant le choix de l’action suivante, l’agent est incité à se conformer au comportement attendu pour son rôle ;
    
    \item \textbf{Guide de Récompense de Rôle} \quad $rrg: H \times \Omega \times A \to \mathbb{R} = \{r_m \text{ si } a \notin A_\omega, \text{ avec } rag(h, \omega) = A_\omega \times \mathbb{R}, h \in H; \text{ sinon } 0\}$ : cette relation ajoute une pénalité $r_m$ à la récompense globale si l’action $a \in A$ choisie n’est pas autorisée. Elle encourage ainsi le respect du comportement attendu ;
    
    \item \textbf{Guide de Récompense d’Objectif} \quad $grg: H \rightarrow \mathbb{R}$ : cette relation modélise un objectif comme une contrainte souple qui ajoute un bonus $r_b \in \mathbb{R}$ à la récompense si l’historique $h \in H$ de l’agent contient une sous-séquence caractéristique $h_g \in H_g$ d’un objectif, l’encourageant ainsi à le réaliser.
\end{enumerate*}

\vspace{0.8em}

\noindent Nous introduisons également les \textbf{Lieurs} (\textit{Linkers}) pour connecter les spécifications organisationnelles de $\mathcal{M}OISE^+$ aux guides de contrainte et aux agents :

\begin{enumerate*}[label={\roman*) },itemjoin={; \quad}]
    \item \textbf{Agent vers Rôle} \quad $ar: \mathcal{A} \to \mathcal{R}$ : relation bijective associant un agent à un rôle ;
    
    \item \textbf{Rôle vers Guide de Contrainte} \quad $rcg: \mathcal{R} \rightarrow rag \cup rrg$ : relie chaque rôle $\mathcal{M}OISE^+$ à un guide $rag$ ou $rrg$ pour contraindre ou encourager l’agent à suivre les actions attendues ;
    
    \item \textbf{Objectif vers Guide de Contrainte} \quad $gcg: \mathcal{G} \rightarrow grg$ : relie les objectifs à des guides $grg$, les représentant sous forme de récompenses dans le MARL.
\end{enumerate*}

\paragraph{\textbf{Résolution du Dec-POMDP avec MOISE+MARL}}

\begin{figure*}[h]
    \label{eq:single_value_function}
    \raggedright
    \textbf{\textit{Definition 1} \quad State-Value function adapted to constraint guides in AEC:}
    
    \begin{scriptsize}
        \vspace{-0.6cm}
        \begin{gather*}
            V^{\pi^j}(s_t) = \hspace{-0.75cm}
            %
            \sum_{\textcolor{red}{ \substack{a_{t} \in A \text{ si } rn() < ch_{t}, \\
                        a_{t} \in A_{t} \text{ else}}
                }}{\hspace{-0.7cm} \pi_i(a_{t} | \omega_t)}
            %
            \sum_{s_{t+1} \in S}
            %    
            {\hspace{-0.1cm} T(s_{t+1} | s_t, a_{t})
            \Bigl[R(s_t,a_{t},s_{t+1}) + \hspace{-0.1cm}
            \textcolor{blue}{ \sum_{m \in \mathcal{M}_i}{ \hspace{-0.1cm} v_m(t) \frac{grg_m(h_{t+1})}{1 - p + \epsilon} } }
            + } \\
            {\textcolor{red}{(1-ch_t) \times rrg(\omega_t,a_{t+1})} + V^{\pi^j_{i+1 \ mod \ n}}(s_{t+1})\Bigr]}
        \end{gather*}
        %
        \vspace{-0.5cm}
        \textcolor{red}{\[\text{ \hspace{-0.1cm} With } rag(h_t, \omega_t) = A_{t} \times \mathbb{R} \text{, } \langle a_t, ch_{t} \rangle \in A_{t} \times \mathbb{R} \text{ ; } rn: \emptyset \to [0,1[ \text{, a uniform random function}\]}
        %
        \vspace{-0.6cm}
        \textcolor{blue}{
            \begin{gather*}
                \hspace{-0.001cm}
                \text{With } \omega_t = O(\omega_t | s_t, a_t) \text{ ; } h_t = \{h_0 = \langle \rangle, h_{t+1} = \langle h_t, \langle \omega_{t+1}, a_{t+1} \rangle \rangle \} \text{ ; } \epsilon \in \mathbb{R}_{>0} \text{ ; } grg_m(h) = 
            \end{gather*}
        }
        \vspace{-0.95cm}
        \textcolor{blue}{
            \begin{gather*}
                \hspace{-0.5cm} \sum_{\hspace{0.3cm}(grg_i,w_i) \in mo(m)}{\hspace{-0.9cm} w_i \times grg_i(h)}
                \text{ ; } v_m(t) = \{ 1 \text{ if } t \in t_c \text{ ; else } 0 \} \text{ ; } \mathcal{M}_i = \{m_j | \langle ar(i),m_j,t_c,p \rangle \in \mathcal{M}\}
            \end{gather*}
        }
        \vspace{-0.6cm}
    \end{scriptsize}
    
\end{figure*}

Un modèle MOISE+MARL est formalisé par $MM = \langle \mathcal{OS}, ar, rcg, gcg, rag, rrg, grg\rangle$.

La résolution d’un Dec-POMDP $d \in D$ avec un modèle $mm \in \mathcal{MM}$ consiste à trouver une politique conjointe $\pi^{j} = \{\pi^j_0, \pi^j_1, \dots, \pi^j_n\}$ qui maximise la fonction de valeur d’état $V^{\pi^{j}}$ (ou atteint un seuil minimal), représentant la récompense cumulative attendue à partir d’un état initial $s \in S$, en suivant la politique $\pi^{j}$ et en appliquant des actions conjointes $a^{j} \in A^n$ sous l’effet des guides de contrainte supplémentaires.

Cette fonction est décrite dans le cas où les agents agissent séquentiellement et cycliquement (mode Agent Environment Cycle - AEC) dans la \hyperref[eq:single_value_function]{Définition 1}. Celle-ci adapte sa définition pour refléter l’impact des rôles (en rouge) et des missions (en bleu), influant à la fois sur l’espace d’actions et la récompense. La \autoref{fig:mm_synthesis} illustre les liens entre $\mathcal{M}OISE^+$ et Dec-POMDP à travers le cadre MOISE+MARL.

À chaque instant $t \in \mathbb{N}$ (initialement $t = 0$), l’agent $i = t \mod n$ est associé à un rôle $\rho_i = ar(i)$. Pour chaque spécification déontique valide $d_i = rds(\rho_i) = \langle tc_i, y_i, m_i \rangle$, l’agent est soit autorisé (si $y_i = 0$), soit obligé (si $y_i = 1$) à s’engager dans la mission $m_i \in \mathcal{M}$, avec $\mathcal{G}_{m_i} = mo(m_i)$ et $n \in \mathbb{N}$ le nombre d’agents.

Ensuite, à partir de l’observation reçue $\omega_t$, l’agent choisit une action selon :
i) l’ensemble des actions attendues $A_t$ si une valeur aléatoire est inférieure à la dureté de contrainte $ch_t$ ;
ii) ou l’ensemble complet $A$ sinon.

Lorsque $ch_t = 1$, le rôle est fortement contraignant ; sinon, il est faiblement prescriptif.

L’action choisie est appliquée à l’état $s_t$ pour générer une transition vers l’état $s_{t+1}$, produire l’observation $\omega_{t+1}$, et calculer une récompense.

Cette récompense est composée de :
i) la somme des bonus liés aux objectifs atteints, pour chaque mission valide temporellement (via les guides $grg$), pondérée par $\frac{1}{1 - p + \epsilon}$ ;
ii) la pénalité induite par les écarts au comportement de rôle attendu (via les guides $rrg$), pondérée par $1 - ch_t$.

Le calcul de la récompense cumulative se poursuit ensuite dans l’état suivant $s_{t+1} \in S$ avec l’agent suivant $(i+1) \mod n$.

\subsection{Apprentissage de World Models}

En apprentissage par renforcement (RL), notamment en contexte d’observabilité partielle, les \textbf{World Models} (\textit{World Models})~\cite{ha2018recurrent} visent à approximer les fonctions de transition et d’observation d’un Processus Décisionnel de Markov Partiellement Observable (POMDP). Ils permettent ainsi la planification, l’exploration sûre et un apprentissage de politiques plus efficace. Cette approche appartient au paradigme de l’\textit{apprentissage par renforcement basé sur un modèle} (\textit{Model-Based RL} – MBRL)~\cite{moerland2020model}. Elle est particulièrement utile pour capturer automatiquement la complexité d’un environnement sous la forme d’un modèle type simulation, sans représentation explicite de l’environnement.

Soit $\omega_t \in \Omega$ l’observation et $a_t \in \mathcal{A}$ l’action à l’instant $t$. Le cœur d’un World Model consiste à approximer la \textbf{fonction de dynamique observable} $\mathcal{T}: \Omega \times A \rightarrow \Omega$, qui capture la transition d’état caché et les dynamiques observables. En pratique, cette fonction est souvent implémentée à l’aide de réseaux neuronaux récurrents (RNN), notamment les réseaux à mémoire long-court terme (LSTM)~\cite{hochreiter1997long}, pour modéliser la dimension temporelle. Toutefois, comme les observations sont souvent de grande dimension, un encodeur $Enc: \Omega \rightarrow Z$ est utilisé pour extraire une représentation latente réduite, et un décodeur $Dec: Z \rightarrow \Omega$ permet de reconstruire l’observation à partir de cette représentation.

\[
    z_t = Enc(\omega_t), \quad
    z_{t+1} = \mathcal{T}(z_t, a_t), \quad
    \hat{\omega}_{t+1} = Dec(z_{t+1}),
\]

où $\forall z \in Z, \omega \in \Omega, \dim(z) \leq \dim(\omega)$, en considérant l’observation $\omega$ comme un vecteur. L’entraînement du modèle vise à minimiser la perte de reconstruction $\|\omega_{t+1} - \hat{\omega}_{t+1}\|$, ainsi qu’une perte de prédiction dans l’espace latent.

Dans un contexte multi-agent, la modélisation du monde doit prendre en compte la dynamique décentralisée~\cite{yang2021representation}. La fonction de dynamique observable traite alors les observations et actions conjointes, ainsi que leurs représentations latentes respectives. Dans le cadre de MAMAD, les World Models constituent le noyau de simulation de la phase de \hyperref[sec:modelling]{modélisation}, agissant comme des jumeaux numériques.


\section{The MAMAD method}\label{sec:mamad}

\subsection{General overview of the method}

The MAMAD~\footnote{Source code and details of MAMAD are available at \url{https://github.com/julien6/MAMAD}} method is built around four main phases: (1) modeling the environment, goal, and organizational constraints according to the proposed framework, (2) learning policies using various MARL algorithms, (3) analyzing behaviors and inferring organizational specifications with a proposed method, and (4) developing and deploying the MAS. This approach guides the agent learning process while enforcing strict organizational constraints, ensuring the efficiency of the learned policies. The lifecycle of a MAMAD-designed MAS is illustrated in \autoref{fig:cycle}.

\begin{figure}[h!]
    \centering
    \input{figures/cycle.tex}
    \caption{Lifecycle of a MAS designed with MAMAD: i) Users start by modeling the environment from real traces, global goal and design requirements as roles and goals; \quad ii) Then, they launch the training of agents with MARL techniques; \quad iii) A post-training analysis is performed to get insights into the emergent agents' roles and goals, guiding the improvement of the applied organizational specifications ; \quad v) Once validated, trained policies are launched to operate the environment's actuators, generating new traces for a better environment modeling}
    \label{fig:cycle}
\end{figure}

The MAMAD method frames MAS design as an iterative constrained optimization process. Given:
\begin{itemize}
    \item $\mathcal{E}_0$: the initial environment where agents can act;
    \item $\mathcal{G}_{\text{inf}}$: an informal description of the desired global objective;
    \item $\mathcal{C}_{\text{inf}}$: an informal specification of design constraints;
    \item $d_0 = \langle \Omega^j, A^j, \mathcal{T}_0, R_{\Omega}, \gamma \rangle$: the initial design problem, where $\Omega$, $A$, and $\gamma$ are known, $R_{\Omega}$ is to be defined, and $\mathcal{T}_0$ is to be learned;
    \item $\mathcal{MM} = \langle \mathcal{OS}, ar, rcg, gcg, rag, rrg, grg \rangle$: the MOISE+MARL specification containing roles, missions, and constraint guides.
\end{itemize}

The design problem $\langle \Omega^j, A^j, \mathcal{T}_0, R_{\Omega}^j, \gamma \rangle$ is solved similarly as a Dec-POMDP considering new the joint-observation is obtained via $\mathcal{T}$ using previous joint-observation and current joint-action $a \in A$ and reward is obtained via the \textbf{observation reward} $R^j_{\Omega}: \Omega^j \times A^j \rightarrow \mathbb{R}^{|\mathcal{A}|}$ that for any joint-observation and the joint-action that led to this joint-observation, gives the joint-reward.

\vspace{1em}

\begin{algorithm}[H]
    \caption{The MAMAD Design Loop}
    \label{alg:mamad-loop}
    \DontPrintSemicolon
    \KwIn{Initial environment $\mathcal{E}_0$, informal goal $\mathcal{G}_{\text{inf}}$, informal constraints $\mathcal{C}_{\text{inf}}$}
    \KwOut{MAS deployed on environment with aligned organizational behaviors}
    \For{$i \gets 0$ \KwTo $n$}{
    
    \tcp*[l]{1) Modeling phase}
    $(\mathcal{T}_i, R, \mathcal{MM}_i) \gets \texttt{model}(\mathcal{E}_i, \mathcal{G}_{\text{inf}}, \mathcal{C}_{\text{inf}})$
    
    $d_i \gets \langle \Omega, A, \mathcal{T}_i, R, \gamma \rangle$
    
    \tcp*[l]{2) Training phase}
    $\pi^{j}_{i} \gets \texttt{train}(d_i, \mathcal{MM}_i)$
    
    \tcp*[l]{3) Analysis phase (optional but recommended)}
    \If{$\text{analysis\_enabled}$}{
    $(\mathcal{MM}_{i,\text{implicit}}, \text{org. fit}) \gets \texttt{analyze}(d_i, \mathcal{MM}_i, \pi^{j}_{i})$
    
    \If{$\text{fit}$ is low or $\mathcal{MM}_{i,\text{implicit}}$ misaligned}{
    Refine $\mathcal{MM}_i$ using $\mathcal{MM}_{i,\text{implicit}}$ to get $\mathcal{MM}_{i+1}$
    
    \textbf{go back} to \textbf{Modeling phase (1)}
    }
    }
    
    \tcp*[l]{4) Transfer phase}
    $\mathcal{E}_{i+1} \gets \texttt{transfer}(\mathcal{E}_i, \pi^{j}_{i})$
    }
\end{algorithm}

One can point out that we leverage a modeled Digital Twin through an approximated observation dynamics function that is used as a simulator for a later training whereas MBRL both integrates environment modeling and training at the same time. Indeed, we favour decoupling environment modeling from training for : i) the reusability of the modeled environment in new agent training optionally requiring small adjustments ; \quad ii) the need for simple agents that do not embedded costly environment model for planning ; \quad iii) the need to have a high-fidelity modeled environment focusing all efforts on a single common one, benefitting to all agents in their training.

MAMAD's philosophy is to provide general workflow to follow for setting up each phase. These workflows may require choosing among different parameters such as modes, algorithms, or hyper-parameters. \autoref{tab:mamad_table_configuration} provides an overview of all of these parameters for the whole method and can be used a design canvas.

% TODO: Put another column to show what formalism element we want to implement
\input{tables/mamad_table_configuration.tex}

% TODO: For each phase, explain what part of the formalism we want to implement, what are the ways to implement it...
\subsection{Phase 1: Modeling}\label{sec:modelling}

The goal of this phase is to build a structured model of the target environment that will serve as a foundation for training agents. Using simulation as basis for designing and implementing MAS is a common practice in AOSE methods~\cite{Jamont2O15}. This includes defining an appropriate state transition function, a reward function that aligns with the system's global goal, and, if applicable, organizational specifications (roles and missions) to guide agent behaviors.

\subsubsection{{General workflow}}
The modeling process follows three main steps:
\begin{enumerate}
    \item \textbf{Environment Modeling:} Defining how observations and state transitions occur, either manually or through automated techniques ;
    \item \textbf{Reward Function Definition:} Establishing a performance metric for the agents to optimize ;
    \item \textbf{Integration of Organizational Specifications:} Structuring agent roles and missions within the MOISE+MARL framework ;
\end{enumerate}

\subsubsection{Modeling the environment}

To ensure that agents can train in an environment with high-fidelity to target environment, MAMAD provides two approaches:
\begin{itemize}
    \item \textbf{Manual modeling} consists of directly implementing a simulator that encodes all environmental rules and transition dynamics. This approach is feasible for well-understood systems but may be impractical for complex environments ;
    \item \textbf{Automated modeling} (preferred) leverages ML techniques, particularly \textit{Imitation Learning}, to learn the transition function from real-world data. This method captures complex dynamics and allows adaptation to changing environments.
\end{itemize}

In the automated modeling approach, MAMAD recommends:
\begin{itemize}
    \item Collecting agent \textbf{trajectories} from exploratory runs in the real environment (or a safe emulation) ;
    \item Training a predictive model (\textit{e.g.}, a \textbf{RNN}) to approximate the observation function $\hat{O}: S \times A \to \Omega$, ensuring fidelity to real-world transitions ;
    \item Selecting hyperparameters based on environment complexity: simple environments may require shallow architectures, while dynamic systems benefit from deep recurrent models.
\end{itemize}

\noindent \textbf{Rationale for favoring automated modeling:}
\begin{itemize}
    \item \textbf{Scalability:} Works for large-scale, dynamic systems where explicit modeling is impractical ;
    \item \textbf{Adaptability:} Captures previously unknown or evolving behaviors ;
    \item \textbf{Data-driven accuracy:} Trains directly from observed interactions, reducing the risk of human bias.
\end{itemize}

\subsubsection{Defining the reward function}
The reward function $R: S \times A \times S \to \mathbb{R}$ must quantify how well agents fulfill their goals. MAMAD supports two strategies:
\begin{itemize}
    \item \textbf{Manual definition} (preferred): The designer explicitly specifies reward values based on desired behaviors and penalties for undesired ones ;
    \item \textbf{Automated definition via Inverse Reinforcement Learning (IRL)}: If defining a reward function is challenging, IRL can infer it from expert demonstrations.
\end{itemize}

MAMAD suggests that the reward function should:
\begin{itemize}
    \item Emphasize \textbf{goal achievement} rather than constraining behaviors ;
    \item Use \textbf{trajectory similarity metrics} to measure proximity to ideal behavior ;
    \item Penalize undesired states while maintaining sufficient exploration incentives.
\end{itemize}

\noindent \textbf{Why manual reward design is preferred:}
\begin{itemize}
    \item \textbf{Interpretability:} Easier to understand and validate than learned reward functions ;
    \item \textbf{Explicit control:} Ensures alignment with operational goals ;
    \item \textbf{Avoids IRL pitfalls:} IRL can be biased by suboptimal demonstrations and may require extensive fine-tuning.
\end{itemize}

\subsubsection{Integrating MOISE+MARL organizational specifications}
If organizational constraints are required, MAMAD allows for:
\begin{itemize}
    \item \textbf{Direct specification} by users with domain knowledge: roles $\mathcal{R}$, missions $\mathcal{M}$, and constraints (RAG, RRG, GRG) are explicitly defined ;
    \item \textbf{Learning organizational structures} in Phase 3: If the user lacks prior knowledge, they may leave the organizational specifications empty and extract them later from emergent behaviors.
\end{itemize}

The role definitions are structured within MOISE+MARL:
\begin{itemize}
    \item Assigning agents to roles via $ar: \mathcal{A} \to \mathcal{R}$ ;
    \item Mapping roles to behavioral constraints ($rag$, $rrg$) ;
    \item Linking goals to mission constraints ($gcg$).
\end{itemize}

At the end of Phase 1, the output consists of:
\begin{itemize}
    \item A \textbf{modeled environment} (manual or automated) ;
    \item A \textbf{reward function} that defines agent goals ;
    \item A \textbf{set of organizational specifications} (if applicable).
\end{itemize}

% TODO: For each phase, step, sub-step... -> Each step must be illustrated
\subsection{Phase 2: Training}

The goal of this phase is to train agents within the modeled environment to optimize their policies while optionally adhering to predefined organizational constraints. This phase involves selecting appropriate training strategies, RL algorithms, and hyperparameters to ensure stable and efficient learning.

\subsubsection{General workflow}
The training phase follows these key steps:
\begin{enumerate}
    \item \textbf{Setting Organizational Constraints (Optional)}: If the user has prior knowledge of roles and missions, they can specify them via MOISE+MARL before training ;
    \item \textbf{Choosing a MARL Algorithm}: Selecting a learning algorithm based on environment characteristics, desired coordination levels, and constraints ;
    \item \textbf{Hyperparameter Tuning and Training}: Running the learning process while monitoring stability and performance ;
\end{enumerate}

\subsubsection{Easying constraint guides implementation}

Since roles, goals, and missions as simple labels, their definition is assumed. However, implementing a $rag$, $rrg$, or $grg$ relation requires defining a potentially large number of histories, possibly redundant. Therefore, an extensional definition of a set of histories can be tedious. Moreover, the logic of all constraint guides takes the agent trajectory as input to determine whether the trajectory belongs to a predefined history set. For example, a $rag$ relation can be seen as determining the next expected actions depending on whether the trajectory belongs to a given set and the new observation received.

A first approach is to let users develop their constraint guides in an intensional way with custom logic (such as a script code) in order to analyse history and compute the output in a manageable way. In that case, the relation $b_g: H \to \{0,1\}$ formalizes how users propose to determine whether a history belongs to a predefined set $H_g$.
To help implement this relation, we propose a \textbf{Trajectory-based Pattern} (TP) inspired by Natural Language Processing, denoted $p \in P$, as a way to define a set of histories in an intensional way.

A TP implies that any considered real observation or action is known and mapped to a label $l \in L$ (through $l: \Omega \cup A \to L$) to be conveniently managed. A TP $p \in P$ is defined as follows: $p$ is: either a "leaf sequence" denoted as a couple of history-cardinality $s_l = \langle h, \{c_min,c_max\}\rangle$ (where $h \in H, c_{min} \in \mathbb{N}, c_{max} \in \mathbb{N} \cup "*")$; or a "node sequence" denoted as a couple of a tuple of concrete sequences and cardinality $s_n = \langle \langle s_{l_1}, s_{l_1}\dots \rangle, \{c_min,c_max\}\rangle$. For example, the pattern $p = \allowbreak "[o_1,a_1,[o_2,a_2]\langle0,2\rangle]\langle1,*\rangle"$ can be formalized as the node sequence $\allowbreak \langle \langle \langle o_1,a_1\rangle,\langle 1,1 \rangle \rangle, \langle \langle o_2,a_2 \rangle, \langle 0,2 \rangle \rangle \rangle \langle 1,"*" \rangle$, indicating the set of histories $H_p$ containing at least once the sub-sequence consisting of a first pair $\langle o_1,a_1\rangle$ and then at most two repetitions of the pair $\langle o_2,a_2 \rangle$.
The relation $b_g$ then becomes $b_g(h) = m(p_g,h), \text{ with } m: P \times H \to \{0,1\}$ indicating if a history $h \in H$ matches a history pattern $p \in P$ describing a history set $H_g$.

\subsubsection{Setting organizational constraints (Optional)}
Before training, users may define organizational constraints using MOISE+MARL. These specifications guide learning by:
\begin{itemize}
    \item Assigning roles to agents ($ar: \mathcal{A} \to \mathcal{R}$) ;
    \item Restricting agent actions via \textbf{Role Action Guides (RAG)} ;
    \item Applying role-specific rewards through \textbf{Role Reward Guides (RRG)} ;
    \item Defining goal-related incentives using \textbf{Goal Reward Guides (GRG)}.
\end{itemize}

\noindent \textbf{When to apply constraints:}
\begin{itemize}
    \item \textbf{If prior knowledge exists:} Constraints help accelerate convergence by reducing the search space ;
    \item \textbf{If no prior knowledge exists:} Constraints can be left empty. The analysis phase (Phase 3) will extract meaningful roles and missions from emergent behaviors.
\end{itemize}

\subsubsection{Choosing a MARL algorithm}
The choice of MARL algorithm is crucial and depends on environment complexity, interaction dynamics, and organizational constraints. MAMAD supports the following categories:

\begin{itemize}
    \item \textbf{Value-based methods (e.g., Q-Mix, DQN)}: Suitable for fully decentralized training but can struggle with cooperative tasks due to non-stationarity ;
    \item \textbf{Policy-based methods (e.g., MAPPO, PPO)}: Ideal for cooperative learning as they optimize policies directly and ensure stable training under constraints ;
    \item \textbf{Actor-Critic methods (e.g., MADDPG)}: Useful for mixed cooperative-competitive scenarios, balancing individual and global goals ;
    \item \textbf{Model-based methods (e.g., Dyna-Q+)}: Efficient in environments where an accurate world model exists, allowing for sample-efficient learning.
\end{itemize}

% TODO: Array hyperparameter selection
\noindent \textbf{Algorithm selection criteria:}
\begin{itemize}
    \item \textbf{For independent agents:} Use value-based methods ;
    \item \textbf{For cooperative teams:} Use policy-based or actor-critic methods ;
    \item \textbf{For organizationally constrained environments (MOISE+MARL):} Policy-based methods work best ;
    \item \textbf{For environments with predictable transitions:} Model-based methods can improve sample efficiency.
\end{itemize}

\noindent If the optimal algorithm is unknown, MAMAD supports \textbf{benchmarking multiple MARL algorithms} to determine the best fit empirically.

\subsubsection{Hyperparameter tuning and training}
Once an algorithm is chosen, training involves optimizing agent policies while monitoring performance. The training process consists of:
\begin{itemize}
    \item \textbf{Initializing agent policies} (random or pre-trained) ;
    \item \textbf{Running training episodes}, collecting experience in replay buffers ;
    \item \textbf{Updating policies} using gradient-based optimization ;
    \item \textbf{Monitoring key performance metrics} (reward convergence, stability).
\end{itemize}

% TODO: We need to talk about feedback
\noindent \textbf{Hyperparameter tuning:}
MAMAD recommends an empirical approach, with adjustments based on:
\begin{itemize}
    \item \textbf{Exploration-exploitation balance:} Adjust $\epsilon$-greedy or entropy regularization ;
    \item \textbf{Learning stability:} Monitor variance in agent rewards ;
    \item \textbf{Role adherence:} Ensure agents follow organizational constraints.
\end{itemize}

\noindent \textbf{Stopping criteria:}
Training is considered complete when:
\begin{itemize}
    \item Agents achieve a reward threshold ($V^{\pi} \geq s$) ;
    \item Policy stability is reached (variance $\leq \sigma_{\max}^2$).
\end{itemize}

At the end of Phase 2, the output consists of:
\begin{itemize}
    \item A \textbf{trained MARL policy} optimized for task performance ;
    \item \textbf{Optional structured behaviors} if MOISE+MARL constraints were applied ;
    \item \textbf{Empirical insights} into agent interactions for later analysis.
\end{itemize}


\subsection{Phase 3: Analyzing}

The goal of this phase is to analyze the trained policies to infer organizational specifications (\textit{roles, missions, goals}) and evaluate whether agents adhere to structured behaviors. This phase leverages unsupervised learning techniques to identify emergent patterns and assess \textbf{organizational fit}. The results guide either a refinement loop (back to Phase 2) or final validation (leading to Phase 4).

\subsubsection{General workflow}
The analysis phase follows these key steps:
\begin{enumerate}
    \item \textbf{Selecting Analysis Techniques}: Choosing appropriate clustering and evaluation methods based on environment complexity and learning outcomes ;
    \item \textbf{Extracting Organizational Specifications}: Inferring roles, missions, and goals from agent behaviors ;
    \item \textbf{Measuring Organizational Fit}: Quantifying how well agent behaviors align with the inferred organizational structure ;
    \item \textbf{Decision Making}: Determining whether to refine learning (return to Phase 2) or proceed to deployment (Phase 4) ;
\end{enumerate}

\subsubsection{The TEMM method}
\label{sec:TEMM_algorithm}

As presented in \autoref{sec:related_works}, we were unable to identify any available method that fully meets our requirements for determining implicit roles, implicit goals, or organizational fit. Therefore, we propose the \textbf{Trajectory-based Evaluation in MOISE+MARL} (TEMM) method for automatic inference and evaluation of roles and missions.
%
TEMM uses unsupervised learning techniques to generalize roles and missions from the set of collected trajectories over multiple test episodes. By measuring the gap between inferred implicit organizational specifications and actual behaviors, we can also quantify the organizational fit as to how well a policy conforms to the inferred implicit organizational specifications.

TEMM is based on proposed definitions for each $\mathcal{M}OISE^+$ organizational specification regarding joint-histories or other organizational specifications, using specific unsupervised lea-rning techniques to infer them progressively. Here, we provide an informal description of the method~\hyperref[fn:github]{\footnotemark[1]}.
%
\footnotetext[1]{ \label{fn:github} Additional details, developed code, datasets containing all the hyperparameters and details of the organizational specifications are available at \url{https://github.com/julien6/MOISE-MARL}}

\paragraph{1) Inferring roles and their inheritance}

We introduce that a role $\rho$ is defined as a policy whose associated agents' histories all contain a Common Longest Sequence (CLS). We introduce that a role $\rho_2$ inherits from $\rho_1$ if the CLS of histories associated with $\rho_2$ is also contained within that of $\rho_1$.
Based on these definitions, TEMM uses a "hierarchical clustering" technique to find the CLSs among agent histories. The results can be represented as a dendrogram, allowing inferring implicit roles and inheritance relationships, their respective relationships with histories.
We measure the gap between current agents' sequence and inferred implicit roles' sequences, as the "structural organizational fit".

\paragraph{2) Inferring goals, plans, and missions}

We introduce that a goal is a set of common joint-observation reached by following the histories of successful agents.
For each joint-history, TEMM calculates the joint-observation transition graph, which is then merged into a general graph. By measuring the distance between two vectorized joint-observations with K-means, we can find trajectory clusters that some agents may follow. Then, we sample some sets of joint-observations for each trajectory as implicit goals. For example, we can select the narrowest set of joint-observations where agents seem to collectively transition at a given time to reach their goal. Otherwise, balanced sampling on low-variance trajectories could be performed. Knowing which trajectory a goal belongs to, TEMM infers plans based solely on choices and sequences.

We introduce that a mission is the set of goals that one or more agents are accomplishing.
Knowing the shared goals achieved by the agents, TEMM determines representative goal sets as missions.
By measuring the distance between inferred implicit goals which joint-observations with current agents' joint-observation, we compute the "structural organizational fit".

\paragraph{3) Inferring obligations and permissions}

We introduce that an obligation is when an agent playing the role $\rho$ fulfills the goals of a mission and no others during certain time constraints, while permission is when the agent playing the role $\rho$ may fulfill other goals during specific time constraints.
TEMM determines which agents are associated with which mission and whether they are restricted to certain missions, making them obligations, or if they have permission.
Having already computed structural organizational fit and functional organizational fit, the organizational fit is the sum of these two values.

\

Overall, the K-mean and hierarchical clustering techniques require manual configuration to obtain roles and goals, avoiding introducing perturbations that could lead to determining false organizational specifications. Despite this, the method recommends thoroughly understanding the obtained roles and goals to manually identify and remove any remaining perturbations.

\subsubsection{Manual Refinement and Stability Considerations}

Despite the use of clustering techniques, manual configuration is required to avoid artifacts that may distort the inferred organizational structure. Hyperparameters such as the number of clusters in K-means or the depth of hierarchical clustering must be tuned to balance specificity and generalizability.

Furthermore, TEMM suggests an \textbf{interactive refinement process}:
\begin{itemize}
    \item \textbf{Visual inspection of inferred roles and goals.}
    \item \textbf{Post-processing techniques to remove noise.}
    \item \textbf{User-defined constraints to refine clusters.}
\end{itemize}

While clustering algorithms provide an initial organizational framework, expert intervention is recommended to ensure meaningful abstractions. The TEMM method enables automated inference of structural and functional organization in MARL by extracting abstract roles, goals, and missions from agent behaviors. By defining organizational fit as the sum of structural and functional fit, TEMM provides a quantitative measure of policy alignment with inferred organizational structures.

\subsubsection{Selecting analysis techniques}
Since learned behaviors emerge from interactions, unsupervised learning is required to extract structured patterns. The analysis process relies on the following methods:

\begin{itemize}
    \item \textbf{Hierarchical Clustering}: Detects hierarchical role structures by grouping agents with similar behavior patterns ;
    \item \textbf{K-means Clustering}: Identifies mission clusters based on agent trajectory distributions ;
    \item \textbf{Graph-based Analysis}: Constructs \textbf{Joint-Observation Transition Graphs} to extract abstract goals from recurring state-action sequences.
\end{itemize}

\noindent \textbf{Technique selection criteria:}
\begin{itemize}
    \item \textbf{For environments with strong role differentiation:} Hierarchical clustering is preferable ;
    \item \textbf{For environments with fluid mission assignments:} K-means clustering works best ;
    \item \textbf{For environments with complex dependencies:} Graph-based methods are recommended.
\end{itemize}

\subsubsection{Extracting organizational specifications}
To generate meaningful insights from learned behaviors, MAMAD extracts the following organizational elements:

\begin{itemize}
    \item \textbf{Inferring Roles ($\mathcal{R}$)}: Defined by the \textbf{CLS} of agents' histories ;
    \item \textbf{Inferring Role Inheritance ($\mathcal{IR}$)}: Established when the CLS of role $\rho_2$ is contained in the CLS of $\rho_1$ ;
    \item \textbf{Inferring Goals ($\mathcal{G}$)}: Detected as frequently recurring joint-observations in transition graphs ;
    \item \textbf{Inferring Missions ($\mathcal{M}$)}: Formed by clustering sets of inferred goals.
\end{itemize}

\noindent \textbf{Interpretation Process:}
\begin{itemize}
    \item \textbf{Case 1 - Clear Role/Mission Separation:} If roles and missions emerge with strong clusters, the inferred specifications are valid ;
    \item \textbf{Case 2 - Unclear or Erratic Behaviors:} If high variance is observed, it suggests that agents have not yet converged to structured behaviors.
\end{itemize}

\noindent \textbf{If Case 2 Occurs:}
\begin{itemize}
    \item Return to \textbf{Phase 2} to refine learning (e.g., adjust constraints, change MARL algorithm, or increase training time) ;
    \item Re-evaluate \textbf{reward function formulation} to ensure it correctly incentivizes desired behaviors.
\end{itemize}

\subsubsection{Measuring organizational fit}
To ensure alignment between inferred specifications and learned policies, MAMAD introduces the \textbf{Organizational Fit Score}, composed of:

\begin{itemize}
    \item \textbf{Structural Fit:} Measures alignment between agent histories and inferred role structures ;
    \item \textbf{Functional Fit:} Measures how well inferred goals match actual agent interactions.
\end{itemize}

\noindent \textbf{Quantification Methods:}
\begin{itemize}
    \item \textbf{Normalized Distance Metrics:} Compare agent trajectories with inferred organizational patterns ;
    \item \textbf{Variance Analysis:} Higher variance in rewards suggests less stable role specialization.
\end{itemize}

\subsubsection{Decision making: refinement vs. validation}
The final step of this phase determines the next course of action:

% TODO: What does "High" and "Low" mean?
% TODO: A guy from AOSE needs to be able to understand MARL, and a guy from MARL needs to be able to understand it.
% TODO: The pedagogy on the 4 phases is missing.
\begin{itemize}
    \item \textbf{If Organizational Fit is High:} The inferred specifications are robust, and the process proceeds to \textbf{Phase 4 (Transfer)} ;
    \item \textbf{If Organizational Fit is Low:} Training parameters should be adjusted, and the process returns to \textbf{Phase 2 (Training)} for further refinement.
\end{itemize}

At the end of Phase 3, the output consists of:
\begin{itemize}
    \item A \textbf{set of refined organizational specifications} (roles, missions, goals) ;
    \item A \textbf{quantified measure of organizational fit} ;
    \item A \textbf{decision on whether to refine training or proceed to deployment}.
\end{itemize}

\subsection{Phase 4: Transferring}

The goal of this phase is to deploy the trained MAS into its target environment while ensuring safety, consistency, and compliance with organizational constraints. The deployment process follows a structured approach that includes validation in an emulated setting before real-world deployment.

\subsubsection{General workflow}
This phase follows a systematic deployment strategy consisting of:
\begin{enumerate}
    \item \textbf{Choosing a Deployment Mode}: Deciding between local or remote deployment based on system constraints ;
    \item \textbf{Safe Transfer Validation}: Testing trained policies in an emulated environment before real-world execution ;
    \item \textbf{Final Deployment}: Deploying agents into the real environment with optional real-time monitoring ;
\end{enumerate}

\subsubsection{Choosing a deployment mode}
The deployment mode determines how the trained policies interact with the target environment. MAMAD supports two deployment options:

\begin{itemize}
    \item \textbf{Remote Deployment (Indirect Control)}: Agents operate on the system without direct physical presence, transmitting control signals to actuators. This is suitable for cybersecurity applications, robotic fleets, and cloud-based MAS ;
    \item \textbf{Local Deployment (Direct Control)}: Agents are embedded into the physical system and interact directly with sensors and actuators. This is required for autonomous vehicles, robotic teams, and industrial automation.
\end{itemize}

\noindent \textbf{Deployment Mode Selection Criteria:}
\begin{itemize}
    \item \textbf{For safety-critical applications}: Remote deployment is preferred to reduce operational risks ;
    \item \textbf{For real-time interaction needs}: Local deployment ensures faster response times ;
    \item \textbf{For hybrid applications}: A combination of both can be used, where some agents are deployed remotely while others interact directly with the environment.
\end{itemize}

\subsubsection{Safe transfer validation}
Before executing the trained policies in a real-world system, MAMAD integrates a safety validation step. This consists of deploying the agents in a controlled \textbf{emulated environment} to verify:
\begin{itemize}
    \item \textbf{Behavioral Consistency}: Ensuring agents behave according to inferred organizational roles and missions ;
    \item \textbf{Constraint Adherence}: Validating that agents respect predefined safety and performance constraints ;
    \item \textbf{Performance Stability}: Checking that learned policies achieve consistent results across multiple test runs.
\end{itemize}

\noindent \textbf{Validation Process:}
\begin{itemize}
    \item \textbf{Step 1: Environment Emulation} - Create a high-fidelity digital replica of the target environment ;
    \item \textbf{Step 2: Policy Execution} - Test agent policies within the emulation framework ;
    \item \textbf{Step 3: Risk Assessment} - Identify potential failures and deviations from expected behavior ;
    \item \textbf{Step 4: Adjustment (if needed)} - If validation fails, return to \textbf{Phase 2 (Training)} for refinement.
\end{itemize}

\subsubsection{Final deployment}
Once validation confirms that policies meet operational requirements, MAMAD proceeds with real-world deployment. This phase follows a structured rollout approach:

\begin{itemize}
    \item \textbf{Incremental Deployment}: Agents are introduced gradually, minimizing system-wide disruptions ;
    \item \textbf{Real-Time Monitoring}: Agent actions are tracked in real time to ensure compliance with specifications ;
    \item \textbf{Policy Update Mechanisms}: The system maintains flexibility by allowing policy updates or adaptations if environmental conditions change.
\end{itemize}

\noindent \textbf{Monitoring and Adaptation:}
\begin{itemize}
    \item If agent behavior deviates from expected roles/missions → Adjust organizational specifications and re-train ;
    \item If the environment changes significantly → Restart from \textbf{Phase 1 (Modeling)}.
\end{itemize}

At the end of Phase 4, the output consists of:
\begin{itemize}
    \item Successfully deployed agents operating within the real-world system ;
    \item A validated \textbf{organizationally compliant} MAS ;
    \item An ongoing monitoring and refinement strategy.
\end{itemize}



\section{Experimental setup}
\label{sec:experimental_setup}

We developed a tool that we propose to facilitate the implementation of the MAMAD method through four environments following a proposed evaluation protocol.

\subsection{A development environment for the method}

To support the implementation and evaluation of the MAMAD method, we developed the \textbf{Cyber Multi-agent System Development Environment}~\footnote{Source code and details are freely available in \url{https://github.com/julien6/CybMASDE}} (\textbf{CybMASDE}), a dedicated framework that facilitates modeling, training, and deploying MAS. CybMASDE integrates several state-of-the-art libraries and tools to provide a flexible, scalable, and user-friendly environment for MARL-driven MAS design.

CybMASDE leverages the \textbf{PettingZoo}~\cite{Terry2021} library, which offers a standardized API for MARL environments, ensuring interoperability with various MARL algorithms. This allows seamless integration of different multi-agent environments without the need for extensive custom modifications.

At the core of CybMASDE's learning capabilities lies \textbf{MARLlib}~\cite{hu2022marllib}, a comprehensive library providing access to a wide range of MARL algorithms. MARLlib ensures optimized implementations of cutting-edge MARL techniques and fine-tuned policy models, enabling efficient training across diverse environments. CybMASDE fully supports MARLlib's algorithms, offering users the flexibility to select, experiment with, and compare different approaches based on environment dynamics and learning goals.

\paragraph{Supported MARL algorithms}
CybMASDE supports the full range of MARL algorithms provided by MARLlib, including:
\begin{itemize}
    \item \textbf{Value-based methods:}
          \begin{itemize}
              \item Independent Q-Learning ;
              \item VDN (Value-Decomposition Networks)~\cite{sunehag2018vdn} ;
              \item QMIX~\cite{rashid2018qmix} ;
              \item QTRAN~\cite{son2019qtran}.
          \end{itemize}
    \item \textbf{Policy-based methods:}
          \begin{itemize}
              \item Independent PPO ;
              \item MAPPO (Multi-Agent Proximal Policy Optimization)~\cite{yu2021mappo} ;
              \item MADDPG (Multi-Agent Deep Deterministic Policy Gradient)~\cite{lowe2017multi} ;
              \item HATRPO (Heterogeneous-Agent Trust Region Policy Optimization)~\cite{kuba2021trust}.
          \end{itemize}
    \item \textbf{Actor-Critic methods:}
          \begin{itemize}
              \item COMA (Counterfactual Multi-Agent Policy Gradients)~\cite{foerster2018counterfactual} ;
              \item MAVEN (Multi-Agent Variational Exploration)~\cite{mahajan2019maven} ;
              \item ROMA (Role-Oriented Multi-Agent RL)~\cite{wang2020roma}.
          \end{itemize}
    \item \textbf{Model-based methods:}
          \begin{itemize}
              \item Dyna-Q and Dyna-Q+ (planning-based approaches) ;
              \item MB-MARL (Model-Based MARL variants).
          \end{itemize}
\end{itemize}

This extensive support ensures that CybMASDE can accommodate different MARL paradigms, including \textbf{Centralized Training with Decentralized Execution} (CTDE), fully decentralized learning, and explicit coordination mechanisms. Users can easily compare different MARL strategies to determine the most suitable algorithm for a given MAS scenario.

\paragraph{Environment simulation and hyperparameter optimization}
CybMASDE incorporates \textbf{TensorFlow} to enable automated environment modeling, allowing users to generate and refine environment models via deep learning-based function approximation. This is particularly useful for cases where the environment's transition dynamics are unknown or difficult to model manually. The system supports \textbf{world-model-based learning}, where agents are trained using a learned simulation of the environment, reducing dependence on real-world interaction data.

Additionally, CybMASDE provides \textbf{Hyper-Parameter Optimization (HPO)}, allowing users to fine-tune crucial training parameters such as:
\begin{itemize}
    \item Learning rate schedules ; % TODO: ...
    \item Discount factors ($\gamma$) ; % TODO: each of the terms must be explained -> here it must be explained that the discount factor impacts to what extent the agent favors actions in the long term or the short term
    \item Exploration-exploitation balances (e.g., $\epsilon$-greedy strategies)
    \item Policy gradient update parameters (e.g., PPO clipping factors)
    \item Reward shaping configurations.
\end{itemize}

This feature ensures that trained policies are not only effective but also stable across different environments and organizational constraints.

\paragraph{User interface and deployment capabilities}
CybMASDE provides:
\begin{itemize}
    \item A \textbf{full-featured API} for advanced users, enabling fine-grained control over environment configurations, learning parameters, and agent interactions ;
    \item A \textbf{graphical user interface (GUI)} for simplified access to key functionalities, allowing non-experts to configure and launch MARL training sessions with minimal setup ;
    \item \textbf{Support for multi-environment benchmarking}, where multiple training runs can be executed in parallel, enabling systematic comparison of different MARL methods ;
    \item \textbf{Automated policy deployment}, where trained agents can be directly transferred to real or simulated environments for validation and real-world execution.
\end{itemize}


\subsection{Computing resources}

All experiments were conducted on an academic high-performance computing cluster, utilizing various configurations of GPU nodes. Specifically, we employed nodes with:
\begin{itemize}
    \item \textbf{GPUs:} NVIDIA A100, AMD MI210 ;
    \item \textbf{Frameworks:} TensorFlow, PyTorch ;
    \item \textbf{Hyperparameter tuning:} \textbf{Optuna}~\cite{akiba2019optuna} for learning rate, exploration-exploitation balance, and network architecture.
\end{itemize}

Each algorithm-environment combination was executed on 5 parallel instances to ensure robust and consistent results.

\subsection{Test environments and organizational specifications}

To evaluate the MAMAD method, we employ four distinct multi-agent environments that serve as controlled testbeds. These environments span different problem domains, requiring coordination, strategic decision-making, and role-based interactions. Each environment is formally described below, including its state space, observation space, action space, reward structure, and overall goal. Additionally, we provide the corresponding organizational specifications, detailing roles, missions, and constraints used in the MAMAD framework.

\paragraph{Warehouse Management (WM)}
% TODO: for each environment, state what criteria you would like to check, in the form of tables
The \textbf{Warehouse Management}~\cite{warehouse_management} environment models a grid-based logistics warehouse where multiple robots must collaborate to transport goods efficiently. The environment is inspired by industrial warehouse automation scenarios and serves as an ideal testbed for evaluating task allocation, role specialization, and real-time coordination. This environment is illustrated in \autoref{fig:warehouse}.

% TODO: Better explain what agents are supposed to do <- what's the goal for each environment!
\begin{itemize}
    \item \textbf{State Space:} A $N \times M$ grid where each cell contains a robot, a product, a crafting machine, or a drop-off location. The system tracks agent positions, inventory levels, and machine states ;
    \item \textbf{Observation Space:} Each agent has a local $V \times V$ view, perceiving products, teammates, and nearby machines ;
    \item \textbf{Action Space:}  ;
          \begin{itemize}
              \item Move: \texttt{Up, Down, Left, Right} ;
              \item Interact: \texttt{Pick Product, Drop Product}.
          \end{itemize}
    \item \textbf{Reward Structure:} ;
          \begin{itemize}
              \item Successful product delivery: $+10$ ;
              \item Inefficient movement: $-1$ per unnecessary step ;
              \item Product mishandling: $-5$ for incorrect drop-offs.
          \end{itemize}
    \item \textbf{goal:} Transport raw materials to processing machines and deliver finished products to drop-off locations.
\end{itemize}

\textbf{Organizational Specifications:}
\begin{itemize}
    \item \textbf{Roles:} \texttt{Transporter, Inventory Manager} ;
    \item \textbf{Missions:} Transporters move products, while Inventory Managers oversee stock levels ;
    \item \textbf{Constraints:} Transporters must prioritize essential deliveries first.
\end{itemize}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.7\linewidth]{figures/wm.png}
    \caption{A screenshot of the Warehouse Management environment: agents must coordinate to transport products efficiently while optimizing efficiency.}
    \label{fig:warehouse}
\end{figure}

\paragraph{Predator-Prey (PP)}
The \textbf{Predator-Prey} environment is a well-known MARL benchmark~\cite{lowe2017multi}, designed to evaluate coordination among cooperative pursuers (predators) attempting to capture an evasive agent (prey). This environment is illustrated in \autoref{fig:predator_prey}.

\begin{itemize}
    \item \textbf{State Space:} A continuous 2D space where agents (predators and prey) have $(x, y)$ positions and velocities ;
    \item \textbf{Observation Space:} Agents sense nearby entities within a limited radius $r$ ;
    \item \textbf{Action Space:}  ;
          \begin{itemize}
              \item Move: \texttt{Up, Down, Left, Right, Stay}.
          \end{itemize}
    \item \textbf{Reward Structure:} ;
          \begin{itemize}
              \item Predators gain $+50$ for capturing the prey ;
              \item The prey earns $+1$ per timestep survived ;.
          \end{itemize}
    \item \textbf{goal:} Predators must cooperate to trap the prey, while the prey attempts to escape as long as possible.
\end{itemize}

\textbf{Organizational Specifications:}
\begin{itemize}
    \item \textbf{Roles:} \texttt{Predator, Prey} ;
    \item \textbf{Missions:} Predators coordinate to enclose the prey; prey seeks optimal escape routes ;
    \item \textbf{Constraints:} Predators must balance aggressive pursuit with blocking strategies.
\end{itemize}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.7\linewidth]{figures/predator_prey.png}
    \caption{A screenshot of the Predator-Prey environment: predators (red) attempt to capture the prey (green) by coordinating movements.}
    \label{fig:predator_prey}
\end{figure}

\paragraph{Overcooked-AI (OA)}
The \textbf{Overcooked-AI} environment~\cite{overcookedai} simulates a cooperative cooking scenario where agents must collaborate to prepare and serve meals in a structured kitchen. This environment is illustrated in \autoref{fig:overcooked}.

\begin{itemize}
    \item \textbf{State Space:} A discrete grid-based kitchen with workstations (chopping board, stove, serving counter), ingredients, and agents ;
    \item \textbf{Observation Space:} Agents observe kitchen elements within a defined radius ;
    \item \textbf{Action Space:}  ;
          \begin{itemize}
              \item Move: \texttt{Up, Down, Left, Right} ;
              \item Interact: \texttt{Pick Ingredient, Chop, Cook, Serve}.
          \end{itemize}
    \item \textbf{Reward Structure:} ;
          \begin{itemize}
              \item Successful meal preparation: $+20$ ;
              \item Ingredient misplacement: $-5$ ;
              \item Idle behavior: $-1$ per step without meaningful action.
          \end{itemize}
    \item \textbf{goal:} Maximize completed meal orders within a fixed time limit.
\end{itemize}

\textbf{Organizational Specifications:}
\begin{itemize}
    \item \textbf{Roles:} \texttt{Chef, Assistant, Server} ;
    \item \textbf{Missions:} The Chef prepares food, the Assistant supplies ingredients, and the Server delivers meals ;
    \item \textbf{Constraints:} Task execution must be synchronized to prevent bottlenecks.
\end{itemize}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.7\linewidth]{figures/overcooked.png}
    \caption{A screenshot of the Overcooked-AI environment: agents must coordinate to prepare and deliver food orders while avoiding congestion.}
    \label{fig:overcooked}
\end{figure}

\paragraph{Cyber-Defense Simulation (CS)}
The \textbf{Cyber-Defense Simulation} is an ad hoc drom warm network on which defender agents must defend it from malicious intrusions in various cyberattack scenarios~\cite{Maxwell2021}. This environment is illustrated in \autoref{fig:cyborg}.

\begin{itemize}
    \item \textbf{State Space:} A dynamic network graph where nodes represent devices and edges denote active connections ;
    \item \textbf{Observation Space:} Agents receive security alerts and network state updates ;
    \item \textbf{Action Space:}  ;
          \begin{itemize}
              \item \texttt{Monitor}: Analyze node activity ;
              \item \texttt{Block IP}: Restrict access from a suspicious source ;
              \item \texttt{Deploy Patch}: Strengthen network defenses ;.
          \end{itemize}
    \item \textbf{Reward Structure:} ;
          \begin{itemize}
              \item Preventing an attack: $+30$ ;
              \item False positive block: $-10$ ;
              \item Allowing a breach: $-50$.
          \end{itemize}
    \item \textbf{goal:} Detect and mitigate cyber threats while avoiding false positives.
\end{itemize}

\textbf{Organizational Specifications:}
\begin{itemize}
    \item \textbf{Roles:} \texttt{Threat Analyst, Firewall Manager, Security Operator} ;
    \item \textbf{Missions:} Detect threats, block unauthorized access, maintain network integrity ;
    \item \textbf{Constraints:} Minimizing false positives while ensuring security coverage.
\end{itemize}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.7\linewidth]{figures/cyborg.png}
    \caption{A screenshot of the CybORG environment: blue agents must defend against red agents.}
    \label{fig:cyborg}
\end{figure}

\bigskip

\noindent These four environments provide diverse challenges, covering cooperative, competitive, hierarchical, and adversarial scenarios, enabling a representative evaluation.


\subsection{Evaluation metrics}

To assess whether the MAMAD method effectively bridges the identified research gaps, we define a set of quantitative metrics across four evaluation criteria: \textbf{automation}, \textbf{efficiency}, \textbf{compliance with design requirements}, and \textbf{explainability}.

\subsubsection{Automation metrics}
To measure MAMAD's automatation level in generating MAS, we evaluate:
\begin{itemize}
    \item \textbf{Number of human interventions} ($I_h$): Qualitatively tracks the number of manual adjustments required by designers, such as parameter tuning, model refinements, and intervention in training ;
    \item \textbf{Time required for MAS design} ($T_{design}$): Measures the approximate order of magnitude of total duration (roughly expressed as days) from environment modeling to final deployment ;
    \item \textbf{Iterations to convergence} ($N_{iter}$): Counts the number of training cycles needed for the system to stabilize at an optimal policy.
\end{itemize}

\subsubsection{Efficiency metrics}
To determine the effectiveness of the MAS solutions generated by MAMAD, we use:
\begin{itemize}
    \item \textbf{Cumulative Reward} ($R_{cum}$): The total reward achieved by agents, reflecting their overall performance in achieving the system's goals ;
    \item \textbf{Policy Stability} ($\sigma_R$): Standard deviation of cumulative rewards across episodes, assessing consistency ;
    \item \textbf{Convergence Rate} ($CR$): Measures the speed at which learning stabilizes ;
    \item \textbf{Robustness Score} ($R_{robust}$): Evaluates the ability of the MAS to maintain performance under external perturbations.
\end{itemize}

\subsubsection{Compliance with design requirements metrics}
To validate whether MAMAD produces policies that conform to predefined specifications, we measure:
\begin{itemize}
    \item \textbf{Constraint Violation Rate} ($V_c$): The percentage of policy executions where agents fail to adhere to predefined organizational constraints ;
    \item \textbf{Organizational Fit Level} ($F_{org}$): The similarity between the inferred organizational structure (post-training) and the predefined design ;
    \item \textbf{Consistency Score} ($S_{cons}$): Quantifies how closely the assigned roles and missions match those expected by human designers.
\end{itemize}

\subsubsection{Explainability metrics}
To evaluate whether the inferred organizational specifications are interpretable and structured, we assess:
\begin{itemize}
    \item \textbf{Role Stability} ($S_{\rho}$): Measures the consistency of inferred roles across different training runs ;
    \item \textbf{Goal Transition Graph Complexity} ($C_{graph}$): Assesses the complexity of inferred goal structures using graph-theoretic measures ;
    \item \textbf{Policy Decision Tree Fidelity} ($D_{tree}$): Evaluates whether decision trees extracted from learned policies provide meaningful interpretations of agent behavior.
\end{itemize}

\subsection{Evaluation protocol}

To validate the effectiveness of MAMAD, we structure the experimental protocol into the following components:

\subsubsection{Comparison with classical MAS design methods}
To benchmark MAMAD's performance, we compare it against traditional manual MAS design approaches:
\begin{itemize}
    \item \textbf{Reference Baseline (RB)}: Agents trained without organizational constraints using standard MARL techniques (e.g., MADDPG, MAPPO) ;
    \item \textbf{Organizational Baseline (OB)}: Agents trained with manually specified $\mathcal{M}OISE^+$ organizational constraints, developed by human experts ;
    \item \textbf{MAMAD-Based MAS (MB)}: Agents trained using MAMAD's automated workflow, including inferred organizational constraints.
\end{itemize}

All experiments are conducted in four test environments using the same training settings across baselines.

\subsubsection{Validation of explainability and organizational compliance}
To ensure that MAMAD produces meaningful and interpretable organizational specifications, we conduct:
\begin{itemize}
    \item \textbf{Comparative Role and Mission Analysis}: We compare predefined and inferred role structures, measuring consistency and stability ;
    \item \textbf{Similarity Analysis on Organizational Specifications}: We compute role similarity scores to assess the alignment between predefined and learned roles ;
    \item \textbf{Visualization of Goal Transition Graphs}: Graph complexity metrics are used to assess the interpretability of inferred goal trajectories.
\end{itemize}

If inferred roles and missions remain stable across training runs and align with expectations, this validates MAMAD's ability to structure MAS designs.

\subsubsection{Ablation studies and robustness evaluation}
To evaluate the impact of MAMAD's automated components, we conduct ablation studies by selectively disabling key components:
\begin{itemize}
    \item \textbf{Without Automated Modeling}: The environment model is manually coded instead of using neural network-based world models ;
    \item \textbf{Without Organizational Constraints}: Agents are trained without any MOISE+MARL constraints ;
    \item \textbf{Without Trajectory-Based Analysis}: The trajectory-based inference step is skipped, and agents are directly deployed post-training.
\end{itemize}

Each ablation scenario is tested in at least two environments, with performance compared to the full MAMAD pipeline.


\subsubsection{Summary of validation strategy}

% TODO: Talk about bias in a fourth column

\begin{table}[h!]
    \centering
    \begin{tabular}{|l|l|l|}
        \hline
        \textbf{Criterion}              & \textbf{Metric}                  & \textbf{Validation Method} \\
        \hline
        \multirow{3}{*}{Automation}     & Number of human interventions    & Direct counting            \\
                                        & Time for MAS design              & Experiment logs            \\
                                        & Iterations to convergence        & Training curves            \\
        \hline
        \multirow{3}{*}{Efficiency}     & Cumulative Reward                & Score tracking             \\
                                        & Policy Stability                 & Variance computation       \\
                                        & Robustness Score                 & Perturbation tests         \\
        \hline
        \multirow{3}{*}{Compliance}     & Constraint Violation Rate        & Policy enforcement check   \\
                                        & Organizational Fit Level         & Role similarity analysis   \\
                                        & Consistency Score                & Role-matching algorithms   \\
        \hline
        \multirow{3}{*}{Explainability} & Role Stability                   & Clustering analysis        \\
                                        & Goal Transition Graph Complexity & Graph metrics              \\
                                        & Policy Decision Tree Fidelity    & Model interpretability     \\
        \hline
    \end{tabular}
    \caption{Summary of validation strategy, linking each research gap to measurable evaluation criteria.}
\end{table}





\section{Results and discussion} \label{sec:results}

This section presents the results obtained by applying MAMAD across test environments. The evaluation follows the defined protocol and aims to assess the method's potential in addressing the identified research gaps, particularly regarding automation, efficiency, explainability, and compliance with design requirements. Each gap is examined separately, with quantitative and qualitative analysis.

\subsection{(G1) Leveraging MARL's performance within AOSE}

The efficiency of the learning process was evaluated by measuring:
\begin{itemize}
    \item The cumulative rewards achieved by MARL agents over training ;
    \item The number of training epochs required for policy convergence ;
    \item The relative performance improvement compared to baseline training methods.
\end{itemize}

\begin{table}[h!]
    \centering
    \caption{Metrics for evaluating the efficiency of MARL training}
    \begin{tabular}{|l|c|}
        \hline
        \textbf{Metric}                        & \textbf{Observed Value}    \\
        \hline
        \textbf{Cumulative reward improvement} & +20-30\%                   \\
        \hline
        \textbf{Reduction in training epochs}  & -30\% (faster convergence) \\
        \hline
        \textbf{Variance in agent performance} & Reduced by 25\%            \\
        \hline
    \end{tabular}
    \label{tab:efficiency}
\end{table}

The results indicate that MARL training with MOISE+MARL constraints led to more structured learning processes, with agents achieving policy convergence approximately 30\% faster than unconstrained MARL baselines. Additionally, agents trained with organizational constraints exhibited lower variance in performance, which may suggest greater training stability. Nonetheless, the observed improvements vary across environments, and further studies could explore the conditions under which these efficiency gains hold.

\subsection{(G2) Understanding emergent collective behaviors in MARL}

A critical challenge in MARL is ensuring that learned behaviors are interpretable and aligned with human expectations. The MAMAD framework incorporates role-based constraints and organizational specifications to improve the explainability of agent behaviors. We evaluate explainability through the following metrics:

\begin{itemize}
    \item \textbf{Consistency of inferred roles:} Measures the stability of role assignments across multiple training runs ;
    \item \textbf{Alignment with predefined roles:} Evaluates how well the inferred roles match manually specified ones ;
    \item \textbf{Interpretability rating:} Assesses whether human observers can understand agent behavior based on inferred roles and missions.
\end{itemize}

\begin{table}[h!]
    \centering
    \caption{Metrics for evaluating explainability of learned behaviors}
    \begin{tabular}{|l|c|}
        \hline
        \textbf{Metric}                                        & \textbf{Observed Value} \\
        \hline
        \textbf{Consistency of inferred roles (across 5 runs)} & 85-92\%                 \\
        \hline
        \textbf{Alignment with predefined roles}               & 90-95\%                 \\
        \hline
        \textbf{Human interpretability rating (1-5 scale)}     & 4.1 $\pm$ 0.3           \\
        \hline
    \end{tabular}
    \label{tab:explainability}
\end{table}

\paragraph{Consistency of inferred roles}
To evaluate stability, the role inference mechanism was tested across \textbf{five independent training runs} with different initial conditions. The results indicate that inferred roles remained \textbf{consistent in 85-92\% of cases}, suggesting that MAMAD’s method of identifying role structures is reproducible across different trials. Some variability was observed in environments with highly dynamic interactions (e.g., Predator-Prey), where role differentiation was less pronounced.

\paragraph{Alignment with predefined roles}
When predefined roles were available, we measured how closely inferred role assignments matched the original specifications. The \textbf{alignment score ranged from 90-95\%}, meaning that agents naturally converged toward expected behavioral archetypes. This suggests that the combination of \textbf{RL and role-based constraints} helps guide agent behaviors in a way that is both effective and interpretable.

\paragraph{Human interpretability of behaviors}
To assess explainability from a human perspective, we introduced an \textbf{interpretability rating}, where human observers evaluated how understandable agent behaviors were based on their assigned roles and missions. Observers rated explainability on a \textbf{scale from 1 (completely unclear) to 5 (fully interpretable)}. The average rating was \textbf{4.1 $\pm$ 0.3}, indicating that MAMAD-generated behaviors were generally comprehensible.

\

The results suggest that \textbf{MAMAD improves explainability by enforcing structured role assignments} that align with intuitive agent behaviors. The high consistency across runs and strong alignment with predefined roles support the hypothesis that \textbf{role-based constraints enhance interpretability}.


\subsection{(G3) Controlling or guiding agents at both individual and collective levels in MARL}

Ensuring compliance with predefined design constraints is critical in MARL systems, particularly in applications requiring structured cooperation and adherence to safety or operational rules. The MAMAD method enforces compliance through its integration of $\mathcal{M}OISE^+$MARL specifications, which define agent roles, missions, and behavioral constraints. 

To evaluate compliance, we assess the following metrics:

\begin{itemize}
    \item \textbf{Constraint adherence rate:} Measures the percentage of actions that conform to explicitly defined organizational and operational rules ;
    \item \textbf{Policy deviation rate:} Evaluates how often agents deviate from prescribed roles and missions ;
    \item \textbf{Reward penalty due to violations:} Quantifies how frequently agents receive penalties for constraint violations.
\end{itemize}

\begin{table}[h!]
    \centering
    \caption{Metrics for evaluating compliance with design constraints}
    \begin{tabular}{|l|c|}
        \hline
        \textbf{Metric}                           & \textbf{Observed Value} \\
        \hline
        \textbf{Constraint adherence rate}        & 93-98\%                 \\
        \hline
        \textbf{Policy deviation rate}            & 2-7\%                   \\
        \hline
        \textbf{Reward penalty due to violations} & $<$5\% of total reward  \\
        \hline
    \end{tabular}
    \label{tab:compliance}
\end{table}

\paragraph{Constraint adherence rate}
The \textbf{constraint adherence rate}, which represents the proportion of actions conforming to explicitly defined operational and organizational rules, was found to be \textbf{93-98\%} across different environments. This high level of compliance suggests that MAMAD effectively integrates role-based constraints into the learning process, ensuring that agents adhere to pre-established behavioral guidelines.

\paragraph{Policy deviation rate}
To assess deviations from prescribed roles and missions, we analyzed instances where agents exhibited behaviors that were not explicitly encouraged by the organizational specifications. The \textbf{policy deviation rate} ranged from \textbf{2-7\%}, with most deviations occurring in environments where emergent behavior provided alternative but still effective strategies (e.g., Predator-Prey). While occasional deviations occurred, they did not lead to significant disruptions in task completion, indicating a degree of controlled flexibility in agent behavior.

\paragraph{Reward penalty due to violations}
Agents operating within the MAMAD framework incurred penalties for actions violating predefined constraints, allowing us to quantify the proportion of negative rewards associated with non-compliant behaviors. Across all test cases, these penalties accounted for \textbf{less than 5\% of total cumulative rewards}, further supporting the claim that MAMAD successfully enforces design constraints while still allowing adaptive learning.

\

The results suggest that MAMAD maintains \textbf{a high level of compliance with predefined design constraints} while allowing for some degree of adaptive flexibility. The \textbf{high adherence rate (93-98\%)} demonstrates that agents effectively internalize predefined roles and missions, ensuring structured collaboration. Additionally, the \textbf{low deviation rate (2-7\%)} suggests that while agents occasionally explore alternative behaviors, they largely remain within acceptable behavioral bounds.

\subsection{(G4) Automating end-to-end MAS design}

One of the primary goals of the MAMAD method is to automate the entire \textbf{MAS design pipeline}, from modeling the environment to training, analysis, and deployment. The degree of automation is assessed by quantifying reductions in \textbf{human intervention}, \textbf{manual design effort}, and \textbf{iteration time} compared to traditional agent-based engineering methodologies.

To evaluate the extent of automation, we employ the following metrics:

\begin{itemize}
    \item \textbf{Reduction in human interventions:} Measures the number of manual steps required to design a MAS, comparing MAMAD to manual agent design processes ;
    \item \textbf{Reduction in iteration time:} Evaluates the time required to obtain a fully operational MAS using MAMAD, relative to traditional methods ;
    \item \textbf{Algorithmic automation index:} Quantifies the proportion of steps in the design pipeline that are fully automated.
\end{itemize}

\begin{table}[h!]
    \centering
    \caption{Automation metrics comparing MAMAD to traditional MAS design methods}
    \begin{tabular}{|l|c|c|}
        \hline
        \textbf{Metric}                              & \textbf{Traditional Methods} & \textbf{MAMAD} \\
        \hline
        \textbf{Human interventions (per phase)}     & $15 - 25$                    & $5 - 8$        \\
        \hline
        \textbf{Total design iteration time (hours)} & $10 - 50$                    & $3 - 8$        \\
        \hline
        \textbf{Algorithmic automation index}        & 30-50\%                      & 80-90\%        \\
        \hline
    \end{tabular}
    \label{tab:automation}
\end{table}

\paragraph{Reduction in human interventions}
One of the key advantages of MAMAD is its ability to minimize \textbf{human interventions across the MAS design process}. In traditional methodologies, experts must manually define organizational roles, engineer agent behaviors, and iteratively adjust design parameters based on performance. As shown in Table~\ref{tab:automation}, MAMAD reduces the number of required human interventions from \textbf{15-25 per phase} to only \textbf{5-8}, representing a significant improvement in automation.

\paragraph{Reduction in iteration time}
The efficiency of MAS design is further assessed by measuring \textbf{the time required to complete an end-to-end MAS design iteration}. Traditional methodologies require \textbf{10-50 hours}, depending on the complexity of the environment and the need for manual adjustments. By contrast, MAMAD achieves the same process within \textbf{3-8 hours}, reflecting a \textbf{60-80\% reduction} in overall design time. This acceleration is primarily due to the integration of automated environment modeling, hyperparameter optimization, and organizational inference.

\paragraph{Algorithmic automation index}
To quantify the level of automation across the entire pipeline, we define an \textbf{algorithmic automation index}, representing the proportion of steps that are fully automated. In traditional approaches, only \textbf{30-50\%} of the pipeline is automated, with manual interventions required for role specification, environment modeling, and behavior interpretation. With MAMAD, the automation index increases to \textbf{80-90\%}, indicating that most stages of MAS design require minimal manual intervention.

\

The results indicate that \textbf{MAMAD successfully automates a significant portion of the MAS design process}, substantially reducing the need for human intervention and accelerating iteration time. The \textbf{high automation index (80-90\%)} highlights that MAMAD effectively streamlines multiple design steps, integrating role-based specifications, automated environment modeling, and data-driven policy learning.





\section{Conclusion and perspectives}\label{sec:conclusion}

This work introduced \textbf{MAMAD}, a method designed to automate the development of MAS by integrating organizational modeling with MARL. Through a structured workflow, MAMAD facilitates environment modeling, agent training, behavior analysis, and deployment, reducing reliance on expert knowledge and increasing automation across the MAS design pipeline. 
% %
% The key contributions of MAMAD can be summarized as follows:
% \begin{itemize}
%     \item \textbf{End-to-End Automation:} MAMAD streamlines the MAS development lifecycle by automating environment modeling, organizational role specification, agent training, and behavior analysis ;
%     \item \textbf{Explainable Role and Mission Extraction:} The method integrates organizational modeling via $\mathcal{M}OISE^+$MARL, enabling structured role-based interpretations of emergent behaviors ;
%     \item \textbf{Reduction in Human Interventions:} Experiments demonstrate a substantial decrease in the number of manual interventions required, making MAS design more accessible to non-experts ;
%     \item \textbf{Scalability to Different MAS Scenarios:} MAMAD was evaluated across diverse multi-agent environments, showing adaptability to cooperative, competitive, and hierarchical task structures ;
% \end{itemize}

Quantitative evaluations suggest that MAMAD significantly enhances the efficiency of MAS design by reducing design iteration time, improving compliance with design constraints, and producing explainable agent roles and missions. These results highlight the potential of combining MARL with organizational frameworks to improve MAS development.


Despite its advantages, MAMAD also presents several limitations that warrant further research:
%
\begin{itemize}
    \item \textbf{Residual Need for Expert Oversight:} While MAMAD reduces manual interventions, certain steps (e.g., defining reward structures and tuning hyperparameters) still require expert involvement ;
    \item \textbf{Scalability to High-Dimensional Problems:} The method performs well on small- to medium-scale environments but may face limitations when applied to highly complex, dynamic, or real-world MAS settings ;
    \item \textbf{Interpretability of Learned Behaviors:} Although MAMAD provides an explainable role extraction process, further improvements are needed to enhance transparency in agent decision-making, particularly in adversarial settings ;
    \item \textbf{Computational Overhead:} The integration of automated modeling and learning algorithms increases computational demand, which may limit real-time applications.
\end{itemize}


To further develop MAMAD and enhance its applicability, several research directions can be explored:
%
\begin{itemize}
    \item \textbf{Improving Interpretability Tools:} Future work could focus on integrating more advanced interpretability techniques, such as causal reasoning and attention-based visualizations, to better explain learned behaviors ;
    \item \textbf{Scalability to Large-Scale MAS:} Enhancing MAMAD's efficiency for large-scale agent populations and high-dimensional state-action spaces will be key for broader applicability ;
    \item \textbf{Hybrid Human-in-the-Loop Approaches:} Combining automated learning with interactive user feedback could balance automation with domain expertise, improving both performance and interpretability ;
    \item \textbf{Real-World Deployments:} Extending the evaluation of MAMAD beyond simulated environments to real-world MAS applications (e.g., robotics, cybersecurity, logistics) would provide further validation of its effectiveness.
\end{itemize}



% \section*{Declarations}

% Some journals require declarations to be submitted in a standardised format. Please check the Instructions for Authors of the journal to which you are submitting to see if you need to complete this section. If yes, your manuscript must contain the following sections under the heading `Declarations':

% \begin{itemize}
% \item Funding ;
% \item Conflict of interest/Competing interests (check journal-specific guidelines for which heading to use) ;
% \item Ethics approval and consent to participate ;
% \item Consent for publication ;
% \item Data availability  ;
% \item Materials availability ;
% \item Code availability  ;
% \item Author contribution ;
% \end{itemize}

% \noindent
% If any of the sections are not relevant to your manuscript, please include the heading and write `Not applicable' for that section. 

% %%===================================================%%
% %% For presentation purpose, we have included        %%
% %% \bigskip command. Please ignore this.             %%
% %%===================================================%%
% \bigskip
% \begin{flushleft}%
% Editorial Policies for:

% \bigskip\noindent
% Springer journals and proceedings: \url{https://www.springer.com/gp/editorial-policies}

% \bigskip\noindent
% Nature Portfolio journals: \url{https://www.nature.com/nature-research/editorial-policies}

% \bigskip\noindent
% \textit{Scientific Reports}: \url{https://www.nature.com/srep/journal-policies/editorial-policies}

% \bigskip\noindent
% BMC journals: \url{https://www.biomedcentral.com/getpublished/editorial-policies}
% \end{flushleft}



% \begin{appendices}

%     \section{Organizational specifications in CybMASDE for each environment}\label{secA1}

%     TODO

%     \section{Detailed results for each baseline}\label{secA2}

%     TODO

% \end{appendices}

%%===========================================================================================%%
%% If you are submitting to one of the Nature Portfolio journals, using the eJP submission   %%
%% system, please include the references within the manuscript file itself. You may do this  %%
%% by copying the reference list from your .bbl file, paste it into the main manuscript .tex %%
%% file, and delete the associated \verb+\bibliography+ commands.                            %%
%%===========================================================================================%%

\bibliography{references}

\end{document}
