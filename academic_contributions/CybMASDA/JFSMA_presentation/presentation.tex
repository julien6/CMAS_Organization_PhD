\documentclass{beamer}

\usetheme{metropolis}

\usepackage{fontspec}

\title{Une Approche basée sur l’Apprentissage par Renforcement pour l’Ingénierie Organisationnelle d’un SMA}
\author{Julien Soulé, Jean-Paul Jamont, Michel Occello, Louis-Marie Traonouez, Paul Théron}
\date{\today}

\begin{document}

\begin{frame}
  \titlepage
\end{frame}

\begin{frame}{Introduction}
    \begin{itemize}
        \item Les Systèmes Multi-Agents (SMA) de Cyberdéfense doivent assurer la protection de systèmes en réseau hétérogènes et distribués.
        \item L'organisation des agents est cruciale pour atteindre les objectifs de cyberdéfense tout en respectant les contraintes environnementales.
        \item Problème : La recherche empirique d'une organisation efficace est complexe et risquée.
        \item Contribution : Proposition d'une approche générique combinant l'apprentissage par renforcement et un modèle organisationnel pour assister la conception de l'organisation d'un SMA.
    \end{itemize}
\end{frame}

\begin{frame}{Cadre théorique}
    \begin{itemize}
        \item Modèle organisationnel MOISE+ : Permet de lier les politiques des agents à des spécifications organisationnelles formelles (structurelles, fonctionnelles, déontiques).
        \item Apprentissage par renforcement multi-agent (MARL) : Processus où les agents apprennent à collaborer pour maximiser une récompense commune.
        \item Défi : Combiner ces deux cadres pour fournir des organisations expliquant les comportements émergents des agents.
    \end{itemize}
\end{frame}

\begin{frame}{Phases de l'approche AOMEA}
    \begin{itemize}
        \item L'approche AOMEA comprend quatre phases :
        \begin{enumerate}
            \item Modélisation de l'environnement
            \item Résolution via MARL
            \item Analyse des politiques apprises
            \item Transfert vers l'environnement réel
        \end{enumerate}
    \end{itemize}
\end{frame}

\begin{frame}{Phase 1 : Modélisation}
    \begin{itemize}
        \item Définir un modèle simulé reflétant les dynamiques et contraintes de l'environnement cible.
        \item Inputs : Environnement émulé, description du problème, contraintes organisationnelles et de sûreté.
        \item Utilisation de techniques d'apprentissage par imitation pour modéliser l'environnement à partir de traces.
        \item Exemple : Utilisation de RNN pour prédire les prochaines observations.
    \end{itemize}
\end{frame}

\begin{frame}{Phase 1 : Modélisation (suite)}
    \begin{itemize}
        \item Élaboration d'une fonction de récompense en mesurant la distance entre l'état actuel et les trajectoires désirées.
        \item Formulation des spécifications organisationnelles (MOISE+) avec les rôles, missions et objectifs assignés aux agents.
        \item Exemple d'output : Un fichier JSON décrivant les rôles, objectifs et guides de contraintes (RAG, RRG, GRG).
    \end{itemize}
\end{frame}

\begin{frame}{Phase 2 : Résolution}
    \begin{itemize}
        \item Objectif : Trouver une politique jointe pour maximiser la récompense cumulative tout en respectant les contraintes organisationnelles.
        \item Utilisation d'algorithmes MARL comme MAPPO, MADDPG ou DynaQ+.
        \item L'apprentissage se fait en parallèle pour accélérer la convergence dans des environnements complexes.
        \item Utilisation de l'optimisation des hyper-paramètres pour ajuster automatiquement les algorithmes à l'environnement.
    \end{itemize}
\end{frame}

\begin{frame}{Phase 2 : Résolution (suite)}
    \begin{itemize}
        \item MAPPO et MADDPG sont souvent recommandés pour les environnements où une coordination centralisée est nécessaire.
        \item DynaQ+ peut être utilisé pour des environnements à faible complexité dynamique.
        \item Output : Modèle de politique jointe entraînée avec un checkpoint des poids des agents.
    \end{itemize}
\end{frame}

\begin{frame}{Phase 3 : Analyse}
    \begin{itemize}
        \item Analyse des politiques apprises via la méthode HEMM (History-based Evaluation in MOISE+MARL).
        \item HEMM infère les rôles et missions en analysant les comportements des agents sur plusieurs épisodes.
        \item Utilisation de techniques d'apprentissage non supervisé (clustering hiérarchique, K-means) pour identifier des rôles et plans abstraits.
    \end{itemize}
\end{frame}

\begin{frame}{Phase 3 : Analyse (suite)}
    \begin{itemize}
        \item Inférence des rôles : Analyse des séquences d'actions communes (Common Longest Sequence) pour chaque rôle.
        \item Inférence des objectifs et plans : Identification des objectifs communs à travers les trajectoires des agents.
        \item Output : "Blueprint" organisationnel comprenant les rôles et missions inférés, et les arbres de décision des agents pour leurs politiques.
    \end{itemize}
\end{frame}

\begin{frame}{Phase 4 : Transfert}
    \begin{itemize}
        \item Transfert des "blueprints" vers l'environnement réel.
        \item Test dans un environnement émulé pour vérifier la conformité aux contraintes de sûreté et de performance.
        \item Déploiement progressif des politiques dans l'environnement réel.
        \item Output : Agents déployés dans l'environnement cible avec des rôles et politiques optimisés.
    \end{itemize}
\end{frame}

\begin{frame}{Conclusion}
    \begin{itemize}
        \item AOMEA propose une approche semi-automatisée pour concevoir des SMA en combinant MARL et MOISE+.
        \item Cette méthode permet de réduire la dépendance à l'expertise humaine et d'accélérer la convergence des politiques multi-agents.
        \item Les rôles et missions émergents sont automatiquement inférés, facilitant l'analyse et le transfert des agents dans des environnements réels.
    \end{itemize}
\end{frame}

\end{document}
