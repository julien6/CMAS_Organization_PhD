\documentclass{beamer}

\usetheme{metropolis}

\usepackage{fontspec}
\usepackage{media9}
\usepackage[T1]{fontenc}
\usepackage[french=quotes]{csquotes} \MakeOuterQuote{"}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{tikz}
\usepackage{multicol}
\usepackage{pgfplots}
\usepackage[inkscapeformat=png]{svg}
\pgfplotsset{compat=1.14}
\usepackage{xcolor}
\usepackage{adjustbox}
\usepackage{tabularx}
\usepackage{caption}
\usepackage[french]{babel}
\captionsetup{font=it}

\DeclareMathAlphabet\mathbfcal{OMS}{cmsy}{b}{n}

\title{Une Approche basée sur l'Apprentissage par Renforcement pour l'Ingénierie Organisationnelle d'un SMA}
\author{Julien Soulé, Jean-Paul Jamont, Michel Occello, Louis-Marie Traonouez, Paul Théron}
\date{\today}

\begin{document}

\begin{frame}
  \titlepage
\end{frame}

\begin{frame}{Introduction}
    \begin{itemize}
        \item SMA de Cyberdéfense : protection réseaux hétérogènes et distribués;
        \item Organisation : clé pour atteinte des objectifs.
        \item Problème : Recherche empirique complexe et risquée.
        \item Contribution : Approche MARL + modèle organisationnel $\rightarrow$ assister conception SMA \& organisation.
    \end{itemize}

    \begin{figure}
        \centering
        % \includegraphics[width=0.9\columnwidth]{images/MASCARA_Organization.pdf}
        \includesvg[width=0.8\columnwidth]{figures/MAS_definition_illustration.svg}
        \caption{Vue schématique d'un SMA de Cyberdéfense en action}
        \label{fig:my_label}
    \end{figure}

\end{frame}

\begin{frame}{Cadre théorique}
    \begin{itemize}
        \item Modèle organisationnel $\mathcal{M}OISE^+$ : Permet de lier les politiques des agents à des spécifications organisationnelles formelles (structurelles, fonctionnelles, déontiques).
        \item Apprentissage par renforcement multi-agent (MARL) : Processus où les agents apprennent à collaborer pour maximiser une récompense commune.
        \item Défi : Combiner ces deux cadres pour fournir des organisations expliquant les comportements émergents des agents.
    \end{itemize}
\end{frame}


\begin{frame}{Introduction}
    \begin{itemize}
        \item Le MARL permet aux agents de découvrir une politique conjointe pour atteindre un objectif global dans un environnement multi-agent.
        \item Problème : Les agents apprennent des comportements qui peuvent difficilement être interprétés ou contrôlés de manière organisationnelle.
        \item Contribution : Le cadre MOISE+MARL intègre des rôles et des missions organisationnels pour guider l'apprentissage, améliorer l'explicabilité et le contrôle des agents.
    \end{itemize}
\end{frame}

\begin{frame}{Cadre théorique}
    \begin{itemize}
        \item \textbf{$\mathcal{M}OISE^+$} : Modèle organisationnel structurant les agents en rôles, missions, et obligations.
        \item \textbf{MARL} : Les agents apprennent collectivement à optimiser leurs politiques dans un environnement incertain.
        \item \textbf{MOISE+MARL} : Fusion de ces deux cadres pour contraindre les politiques des agents en fonction de spécifications organisationnelles.
    \end{itemize}
\end{frame}

\begin{frame}{$\mathcal{M}OISE^+$ Organizational Model}
    \begin{itemize}
        \item \textbf{Spécifications structurelles} : Rôles, groupes, et relations entre agents.
        \item \textbf{Spécifications fonctionnelles} : Objectifs globaux, missions et plans.
        \item \textbf{Spécifications déontiques} : Obligations et permissions associées à chaque rôle.
    \end{itemize}
\end{frame}

\begin{frame}{MOISE+MARL Framework}
    \begin{itemize}
        \item MOISE+MARL intègre le modèle \textbf{Dec-POMDP} pour modéliser les agents dans un environnement partiellement observable.
        \item Les relations de \textbf{Role Action Guide (RAG)} et \textbf{Role Reward Guide (RRG)} imposent des contraintes sur les actions et les récompenses des agents.
        \item \textbf{Goal Reward Guide (GRG)} encourage les agents à accomplir des sous-objectifs en ajustant leurs récompenses.
    \end{itemize}
\end{frame}

\begin{frame}{Méthode TEMM}
    \begin{itemize}
        \item \textbf{TEMM} : Méthode d'inférence basée sur les trajectoires pour évaluer les rôles et missions des agents.
        \item Infère les rôles en utilisant des techniques de clustering non supervisé, telles que le \textbf{Common Longest Sequence (CLS)}.
        \item Mesure la \textbf{correspondance organisationnelle} entre les spécifications préalables et les comportements appris des agents.
    \end{itemize}
\end{frame}

\begin{frame}{Évaluation Expérimentale}
    \begin{itemize}
        \item Expérimentations sur quatre environnements : Predator-Prey, Overcooked-AI, Gestion d'entrepôt, Simulation de Cybersécurité.
        \item Mesures : Récompense cumulative, taux de convergence, violation des contraintes, score de robustesse, niveau de correspondance organisationnelle.
    \end{itemize}
\end{frame}

\begin{frame}{Résultats}
    \begin{itemize}
        \item Les agents guidés par MOISE+MARL montrent une meilleure stabilité et un alignement plus fort avec les rôles définis.
        \item Par exemple, dans l'environnement \textbf{Predator-Prey}, le score de correspondance organisationnelle a augmenté de 44% avec MOISE+MARL.
        \item Des algorithmes comme \textbf{MADDPG} et \textbf{MAPPO} montrent des améliorations notables en termes de robustesse et de performance.
    \end{itemize}
\end{frame}

\begin{frame}{Comparaison avec AGR+MARL}
    \begin{itemize}
        \item Comparaison avec le cadre AGR+MARL (basé uniquement sur les rôles, sans missions).
        \item \textbf{MOISE+MARL} montre une amélioration substantielle des performances, avec des gains jusqu'à 33 dans le niveau de correspondance organisationnelle.
        \item Les agents formés avec des objectifs intermédiaires atteignent des comportements plus stables et orientés vers les objectifs.
    \end{itemize}
\end{frame}

\begin{frame}{Phases de l'approche AOMEA}
    \begin{itemize}
        \item L'approche AOMEA comprend quatre phases :
        \begin{enumerate}
            \item Modélisation de l'environnement
            \item Résolution via MARL
            \item Analyse des politiques apprises
            \item Transfert vers l'environnement réel
        \end{enumerate}
    \end{itemize}
\end{frame}

\begin{frame}{Phase 1 : Modélisation}
    \begin{itemize}
        \item Définir un modèle simulé reflétant les dynamiques et contraintes de l'environnement cible.
        \item Inputs : Environnement émulé, description du problème, contraintes organisationnelles et de sûreté.
        \item Utilisation de techniques d'apprentissage par imitation pour modéliser l'environnement à partir de traces.
        \item Exemple : Utilisation de RNN pour prédire les prochaines observations.
    \end{itemize}
\end{frame}

\begin{frame}{Phase 1 : Modélisation (suite)}
    \begin{itemize}
        \item Élaboration d'une fonction de récompense en mesurant la distance entre l'état actuel et les trajectoires désirées.
        \item Formulation des spécifications organisationnelles ($\mathcal{M}OISE^+$) avec les rôles, missions et objectifs assignés aux agents.
        \item Exemple d'output : Un fichier JSON décrivant les rôles, objectifs et guides de contraintes (RAG, RRG, GRG).
    \end{itemize}
\end{frame}

\begin{frame}{Phase 2 : Résolution}
    \begin{itemize}
        \item Objectif : Trouver une politique jointe pour maximiser la récompense cumulative tout en respectant les contraintes organisationnelles.
        \item Utilisation d'algorithmes MARL comme MAPPO, MADDPG ou DynaQ+.
        \item L'apprentissage se fait en parallèle pour accélérer la convergence dans des environnements complexes.
        \item Utilisation de l'optimisation des hyper-paramètres pour ajuster automatiquement les algorithmes à l'environnement.
    \end{itemize}
\end{frame}

\begin{frame}{Phase 2 : Résolution (suite)}
    \begin{itemize}
        \item MAPPO et MADDPG sont souvent recommandés pour les environnements où une coordination centralisée est nécessaire.
        \item DynaQ+ peut être utilisé pour des environnements à faible complexité dynamique.
        \item Output : Modèle de politique jointe entraînée avec un checkpoint des poids des agents.
    \end{itemize}
\end{frame}

\begin{frame}{Phase 3 : Analyse}
    \begin{itemize}
        \item Analyse des politiques apprises via la méthode HEMM (History-based Evaluation in MOISE+MARL).
        \item HEMM infère les rôles et missions en analysant les comportements des agents sur plusieurs épisodes.
        \item Utilisation de techniques d'apprentissage non supervisé (clustering hiérarchique, K-means) pour identifier des rôles et plans abstraits.
    \end{itemize}
\end{frame}

\begin{frame}{Phase 3 : Analyse (suite)}
    \begin{itemize}
        \item Inférence des rôles : Analyse des séquences d'actions communes (Common Longest Sequence) pour chaque rôle.
        \item Inférence des objectifs et plans : Identification des objectifs communs à travers les trajectoires des agents.
        \item Output : "Blueprint" organisationnel comprenant les rôles et missions inférés, et les arbres de décision des agents pour leurs politiques.
    \end{itemize}
\end{frame}

\begin{frame}{Phase 4 : Transfert}
    \begin{itemize}
        \item Transfert des "blueprints" vers l'environnement réel.
        \item Test dans un environnement émulé pour vérifier la conformité aux contraintes de sûreté et de performance.
        \item Déploiement progressif des politiques dans l'environnement réel.
        \item Output : Agents déployés dans l'environnement cible avec des rôles et politiques optimisés.
    \end{itemize}
\end{frame}

\begin{frame}{Conclusion}
    \begin{itemize}
        \item AOMEA propose une approche semi-automatisée pour concevoir des SMA en combinant MARL et $\mathcal{M}OISE^+$.
        \item Cette méthode permet de réduire la dépendance à l'expertise humaine et d'accélérer la convergence des politiques multi-agents.
        \item Les rôles et missions émergents sont automatiquement inférés, facilitant l'analyse et le transfert des agents dans des environnements réels.
    \end{itemize}
\end{frame}

\end{document}
