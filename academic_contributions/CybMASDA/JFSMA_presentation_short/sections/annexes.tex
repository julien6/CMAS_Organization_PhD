\begin{frame}{Annexes}
    {Context}

    \begin{block}{Multi-Agent Systems (MAS) paradigm for complex \& distributed problems}
        \begin{itemize}
            \item \textbf{task decomposition}: missions delegated to agents achieved through cooperation~\parencite{Raileanu2023};
            \item \textbf{benefits}: handle conflicting goals, parallel computation, system robustness, scalability\dots
        \end{itemize}
    \end{block}

    \begin{block}{\textbf{Organization}: key for MAS designing}
        \begin{itemize}
            \item \textbf{coordination}: how to collaboratively achieve a common goal~\parencite{Hubner2007};
            \item \textbf{dynamic \& uncertain environments}: flexible runtime behavior to adapt~\parencite{Kathleen2020};
        \end{itemize}
    \end{block}

    \begin{block}{Methods and practice for MAS design}
        \begin{itemize}
            \item \textbf{approach + organizational model}: methods rely on designers' experience to hand-craft agents' \textbf{policies} so resulting MAS achieve goals;
                  %   \begin{itemize}
                  %       \item Examples: \emph{GAIA}~\parencite{Wooldridge2000,Cernuzzi2014}, \emph{ADELFE}~\parencite{Mefteh2015}, or \emph{DIAMOND}~\parencite{Jamont2015}, \emph{KB-ORG}~\parencite{Sims2008}
                  %   \end{itemize}
            \item \textbf{simulation to reality}: 1) safe \& efficient MAS design in high fidelity simulated environment; \quad 2) transfer to real environment to perform adequately~\parencite{Schon2021}.
        \end{itemize}
        \quad $\Longrightarrow$ \textbf{Iterative process proceeding by trial and error}

    \end{block}

\end{frame}

\begin{frame}{Annexes}{CAGE Challenge 3}
    \begin{figure}
        \includegraphics[width=0.9\linewidth]{figures/cage_challenge_3.png}
        \caption*{Florin forces patrolling the Guilder border with troops that communicate via an ad-hoc network provided by aerial drones. Here, Guilder has successfully corrupted two drones within this network and can thus intercept, or alter, some of the messages being communicated.}
    \end{figure}
\end{frame}

\begin{frame}{Annexes}{Pr√©dateur-proie}
    \begin{figure}
        \includegraphics[width=0.5\linewidth]{figures/mpe_simple_world_comm.png}
    \end{figure}
\end{frame}

\begin{frame}{Annexes}{Knight, archers, zombies}
    \begin{figure}
        \includegraphics[width=0.8\linewidth]{figures/butterfly_knights_archers_zombies.png}
    \end{figure}
\end{frame}

\begin{frame}{Annexes}
    {MAS basics}

    \begin{block}{Keywords}
        \begin{itemize}
            \item \textbf{Agent}: entity immersed in an environment perceiving observation and making decision autonomously to achieve some goals;
            \item \textbf{MAS}: a set of agents collaborating with self/re-organizing mechanisms to achieve their goal;
            \item \textbf{Organization}: the agents' interactions even though it may be implicit;
            \item \textbf{Organizational Model (OM)}: medium to formally describe an explicit/implicit organization;
            \item \textbf{Organizational Specifications (OS)}: components of an OM to characterize an organization
        \end{itemize}
    \end{block}

    \begin{block}{Organizational model: $\mathcal{M}OISE^+$}
        \begin{itemize}
            \item more complex than \emph{Agent Group Roles} (integration of standards);
            \item takes into account the social aspects between agents explicitly;
            \item possible to link agents' policies to organizational specifications.
        \end{itemize}
    \end{block}

\end{frame}

\begin{frame}{Annexes}
    {MARL basics}

    \begin{block}{Keywords}
        \begin{itemize}
            \item \textbf{Policy}: the \textquote{logic} to choose next action according to observation for an agent;
            \item \textbf{History/trajectory}: the tuple of (observation, action) couples over an episode;
            \item \textbf{Joint-policy / Joint-history}: all of the agents' policies / histories as tuples;
            \item \textbf{Reinforcement learning}: an agent updates its policy to maximize a cumulative reward;
            \item \textbf{Multi-Agent Reinforcement Learning (MARL)}: extends to multiple agents that learn while considering the actions of other agents;
        \end{itemize}
    \end{block}

\end{frame}

\begin{frame}{Annexes}{AOMEA approach: Overview}

    \begin{columns}

        \begin{column}{0.6\textwidth}

            \textbf{Phase 1: Modeling}

            \begin{itemize}
                \item Manually develop a simulated environment ($1.1$) where agents must cooperate to achieve goal ($1.2$);
                \item Can define expected roles behavior as histories;
                \item May constraint agents to roles ($1.3$).
            \end{itemize}

        \end{column}

        \begin{column}{0.4\textwidth}
            \centering
            \adjustbox{trim={0.\width} {0.82\height} {0.\width} {0.\height}, clip}{%
                \includegraphics[width=1.2\linewidth]{figures/AOMEA_illustrative_view}
            }
        \end{column}

    \end{columns}

\end{frame}


\begin{frame}{Annexes}{AOMEA approach: Overview}

    \begin{columns}

        \begin{column}{0.6\textwidth}

            \textbf{Phase 2: Solving}

            \begin{itemize}
                \item Organization-oriented MARL (OMARL) algorithm: MARL process augmented with Organizational model;
                \item Solve satisfying constrained roles' histories ($2.1$);
                \item Gets associated OS ($2.2$)
            \end{itemize}

        \end{column}

        \begin{column}{0.4\textwidth}
            \centering
            \adjustbox{trim={0.\width} {0.56\height} {0.\width} {0.\height}, clip}{%
                \includegraphics[width=1.2\linewidth]{figures/AOMEA_illustrative_view}
            }
        \end{column}

    \end{columns}


\end{frame}

\begin{frame}{Annexes}{AOMEA approach: Overview}

    \begin{columns}

        \begin{column}{0.6\textwidth}

            \textbf{Phase 3: Analyzing}

            \begin{itemize}
                \item Designers observe the trained agents' policies ($3.2$);
                \item Designers observe the computed OS ($3.1$): understand how they reach the goal;
                \item Designers get some design indications for a MAS to achieve the goal: curated OS ($3.3$).
            \end{itemize}


        \end{column}

        \begin{column}{0.4\textwidth}
            \centering
            \adjustbox{trim={0.\width} {0.35\height} {0.\width} {0.188\height}, clip}{%
                \includegraphics[width=1.2\linewidth]{figures/AOMEA_illustrative_view}
            }
        \end{column}

    \end{columns}

\end{frame}

\begin{frame}{Annexes}{AOMEA approach: Overview}

    \begin{columns}

        \begin{column}{0.6\textwidth}

            \textbf{Phase 4: Developing}

            \begin{itemize}
                \item Designers observe the curated OS for implementing a MAS;
                \item Regular MAS development hence addressing safety issues;
                \item Implemented MAS assessed in simulations.
            \end{itemize}

        \end{column}

        \begin{column}{0.4\textwidth}
            \centering
            \adjustbox{trim={0.\width} {0.15\height} {0.\width} {0.57\height}, clip}{%
                \includegraphics[width=1.2\linewidth]{figures/AOMEA_illustrative_view}
            }
        \end{column}

    \end{columns}

\end{frame}



\begin{frame}{Annexes}{AOMEA approach: Theoretical core}

    \begin{block}{Organization-oriented MARL (OMARL)}
        An MARL process augmented with an OM for:
        \begin{itemize}
            \item \textbf{Constraining Policies Space}: gets the joint-policies satisfying the given design specifications;
            \item \textbf{Inferring Organizational Specifications}: gets the specifications from the agents' policies.
        \end{itemize}

    \end{block}

    \begin{block}{\emph{Partial Relations with Agent History and Organization Model} algorithm (PRAHOM)}
        Implementing an OMARL process\dots
        \begin{enumerate}
            \item \textbf{Constraining Policies Space}
                  \begin{itemize}
                      \item Cannot use policies directly $\rightarrow$ \textbf{histories} characterizing \textbf{policies};
                      \item Relations between \textbf{OS} to expected \textbf{histories};
                      \item Agents constrained to OS $\rightarrow$ at each step: available actions updated regarding \textbf{OS} histories.
                  \end{itemize}

            \item \textbf{Inferring Organizational Specifications}
                  \begin{itemize}
                      \item Analyze histories $\rightarrow$ characterize collective behaviors as OS;
                      \item Using known relations between OS and histories;
                      \item Using general OS definition regarding histories.
                  \end{itemize}
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}{Annexes}{AOMEA approach: Theoretical core}

    \textbf{Constraining Policies Space} during training

    \begin{columns}

        \begin{column}{0.3\textwidth}

            \begin{itemize}
                \item At each step, available actions set is changed to match policy constraints defined by users;
                \item Constraints integrated through: external correction, learning, internal policy change.
            \end{itemize}

        \end{column}

        \begin{column}{0.8\textwidth}
            \begin{figure}
                \centering
                \includegraphics[width=0.7\linewidth]{figures/prahom_training_constrain.png}
                \caption*{A summary view of the PRAHOM constraining}
                \label{fig:prahom_process}
            \end{figure}
        \end{column}

    \end{columns}

\end{frame}


\begin{frame}{Annexes}{AOMEA approach: Theoretical core}

    \textbf{Inferring Organizational Specifications}

    \begin{columns}

        \begin{column}{0.3\textwidth}

            \begin{itemize}
                \item \textbf{Knowledge-based Organizational Specifications Identification (KOSIA)}
                \item \textbf{General Organizational Specifications Infererence (GOSIA)}
            \end{itemize}

        \end{column}

        \begin{column}{0.8\textwidth}
            \begin{figure}
                \centering
                \includegraphics[width=0.95\linewidth]{figures/GOSIA_view.png}
                \caption*{A summary view of the GOSIA process}
                \label{fig:gosia_process}
            \end{figure}
        \end{column}

    \end{columns}

\end{frame}


\begin{frame}[allowframebreaks]{Annexes} {Contexte}

    \begin{block}{Paradigme des Syst√®mes Multi-Agents (SMA) pour des probl√®mes complexes et distribu√©s}
        \begin{itemize}
            \item \textbf{d√©composition des t√¢ches} : missions d√©l√©gu√©es aux agents r√©alis√©es par coop√©ration~\parencite{Raileanu2023} ;
            \item \textbf{avantages} : g√©rer des objectifs contradictoires, calcul parall√®le, robustesse du syst√®me, √©volutivit√©\dots
        \end{itemize}
    \end{block}
    
    \begin{block}{\textbf{Organisation} : cl√© pour la conception des SMA}
        \begin{itemize}
            \item \textbf{coordination} : comment atteindre un objectif commun de mani√®re collaborative~\parencite{Hubner2007} ;
            \item \textbf{environnements dynamiques et incertains} : comportement flexible √† l'ex√©cution pour s'adapter~\parencite{Kathleen2020} ;
        \end{itemize}
    \end{block}
    
    \begin{block}{M√©thodes et pratiques pour la conception des SMA}
        \begin{itemize}
            \item \textbf{approche + mod√®le organisationnel} : les m√©thodes s'appuient sur l'exp√©rience des concepteurs pour concevoir manuellement les \textbf{politiques} des agents afin que le SMA atteigne ses objectifs ;
                  %   \begin{itemize}
                  %       \item Exemples : \emph{GAIA}~\parencite{Wooldridge2000,Cernuzzi2014}, \emph{ADELFE}~\parencite{Mefteh2015}, ou \emph{DIAMOND}~\parencite{Jamont2015}, \emph{KB-ORG}~\parencite{Sims2008}
                  %   \end{itemize}
            \item \textbf{simulation vers la r√©alit√©} : 1) conception s√ªre et efficace des SMA dans un environnement simul√© √† haute fid√©lit√© ; \quad 2) transfert √† un environnement r√©el pour des performances ad√©quates~\parencite{Schon2021}.
        \end{itemize}
        \quad $\Longrightarrow$ \textbf{Processus it√©ratif par essais et erreurs}
    \end{block}

\end{frame}

\begin{frame}[allowframebreaks]{Annexes} {Fondamentaux des SMA}

    \begin{block}{Mots-cl√©s}
        \begin{itemize}
            \item \textbf{Agent} : entit√© immerg√©e dans un environnement, percevant des observations et prenant des d√©cisions de mani√®re autonome pour atteindre des objectifs ;
            \item \textbf{SMA} : ensemble d'agents collaborant avec des m√©canismes d'auto/r√©organisation pour atteindre leurs objectifs ;
            \item \textbf{Organisation} : interactions des agents m√™me si elles peuvent √™tre implicites ;
            \item \textbf{Mod√®le organisationnel (OM)} : moyen de d√©crire formellement une organisation explicite/implicite ;
            \item \textbf{Sp√©cifications organisationnelles (OS)} : composants d'un OM pour caract√©riser une organisation.
        \end{itemize}
    \end{block}
    
    \begin{block}{Mod√®le organisationnel : $\mathcal{M}OISE^+$}
        \begin{itemize}
            \item plus complexe que \emph{Agent Group Roles} (int√©gration des normes) ;
            \item prend explicitement en compte les aspects sociaux entre les agents ;
            \item permet de lier les politiques des agents aux sp√©cifications organisationnelles.
        \end{itemize}
    \end{block}

\end{frame}

\begin{frame}[allowframebreaks]{Annexes} {Fondamentaux du MARL}

    \begin{block}{Mots-cl√©s}
        \begin{itemize}
            \item \textbf{Politique} : la \textquote{logique} pour choisir la prochaine action en fonction de l'observation pour un agent ;
            \item \textbf{Historique/trajectoire} : le couple (observation, action) sur un √©pisode ;
            \item \textbf{Politique/historique conjoints} : l'ensemble des politiques/historiques de tous les agents sous forme de tuples ;
            \item \textbf{Apprentissage par renforcement} : un agent met √† jour sa politique pour maximiser une r√©compense cumulative ;
            \item \textbf{Apprentissage par renforcement multi-agent (MARL)} : extension √† plusieurs agents qui apprennent en prenant en compte les actions des autres agents ;
        \end{itemize}
    \end{block}
    
\end{frame}

\begin{frame}[allowframebreaks]{Annexes}{Approche AOMEA : Fondement th√©orique}
    \begin{block}{MARL orient√© organisation (OMARL)}
        Un processus de MARL augment√© avec un OM pour :
        \begin{itemize}
            \item \textbf{Contraindre l'espace des politiques} : obtenir les politiques conjointes satisfaisant les sp√©cifications de conception donn√©es ;
            \item \textbf{Inf√©rer des sp√©cifications organisationnelles} : obtenir des sp√©cifications √† partir des politiques des agents.
        \end{itemize}
    \end{block}
    
    \begin{block}{Algorithme \emph{Partial Relations with Agent History and Organization Model} (PRAHOM)}
        Impl√©mentation d'un processus OMARL\dots
        \begin{enumerate}
            \item \textbf{Contraindre l'espace des politiques}
                  \begin{itemize}
                      \item Impossible d'utiliser directement les politiques $\rightarrow$ \textbf{historiques} caract√©risant les \textbf{politiques} ;
                      \item Relations entre \textbf{OS} et historiques attendus ;
                      \item Les agents contraints aux OS $\rightarrow$ √† chaque √©tape : actions disponibles mises √† jour en fonction des historiques \textbf{OS}.
                  \end{itemize}
    
            \item \textbf{Inf√©rer des sp√©cifications organisationnelles}
                  \begin{itemize}
                      \item Analyser les historiques $\rightarrow$ caract√©riser les comportements collectifs comme OS ;
                      \item Utilisation des relations connues entre OS et historiques ;
                      \item Utilisation de la d√©finition g√©n√©rale des OS par rapport aux historiques.
                  \end{itemize}
        \end{enumerate}
    \end{block}
    
\end{frame}

\begin{frame}{Annexes}{Aper√ßu de \textit{PRAHOM}}
    \begin{figure}
        \includegraphics[width=0.6\linewidth]{figures/mm_simple_representation.png}
    \end{figure}
\end{frame}
    
\begin{frame}{Annexes}{Aper√ßu de PRAHOM}
    \begin{figure}
        \includegraphics[width=\linewidth]{figures/modified_state_value_function.png}
    \end{figure}
\end{frame}
    
\begin{frame}[allowframebreaks]{Annexes}{Approche AOMEA : Fondement th√©orique}
    \textbf{Contraindre l'espace des politiques} pendant l'entra√Ænement

    \begin{columns}
    
        \begin{column}{0.3\textwidth}
    
            \begin{itemize}
                \item √Ä chaque √©tape, l'ensemble des actions disponibles est modifi√© pour correspondre aux contraintes de politiques d√©finies par les utilisateurs ;
                \item Contraintes int√©gr√©es via : correction externe, apprentissage, modification interne des politiques.
            \end{itemize}
    
        \end{column}
    
        \begin{column}{0.8\textwidth}
            \begin{figure}
                \centering
                \includegraphics[width=0.7\linewidth]{figures/prahom_training_constrain.png}
                \caption*{Vue r√©sum√©e de la contrainte PRAHOM}
                \label{fig:prahom_process}
            \end{figure}
        \end{column}
    
    \end{columns}
\end{frame}

\begin{frame}{Annexes}{Constrained Reinforcement Learning (Constrained-RL)}
    
    \begin{itemize}
        \item Apprendre une politique optimisant la r√©compense tout en respectant des \textbf{contraintes de s√©curit√©} ou de \textbf{performance}.
        
        \item \textbf{Contraintes dures} : doivent toujours √™tre respect√©es (Shielding).
        \item \textbf{Contraintes douces} : respect√©es en moyenne ou sous forme de p√©nalit√©s.
        
        \item \textbf{M√©thodes :}
            \begin{itemize}
                \item \textbf{Reward Shaping} : ajout de p√©nalit√©s pour violation de contraintes.
                \item \textbf{Policy Projection} : ajustement des actions pour rester dans les limites.
                \item \textbf{Dual Variables} : int√©gration de multiplicateurs de Lagrange pour g√©rer les contraintes.
            \end{itemize}
            
    \end{itemize}    
\end{frame}

\begin{frame}{Annexes}{Safe Exploration et Shielding en Reinforcement Learning}
    
    \begin{itemize}
        \item \textbf{Safe Exploration} $\rightarrow$ garantir la s√©curit√© lors de la phase d'exploration en limitant les risques de comportements dangereux.
        \item Principalement modifier la fonction de r√©compense (Langragien) pour integrer contraintes mais aussi\dots
        \item \textbf{Shielding} intervenir en temps r√©el pour bloquer les actions susceptibles de violer ces contraintes, permettant une exploration s√©curis√©e.
    \end{itemize}
    
    \textbf{R√©f√©rence :} \\
    \textit{Akifumi Wachi, Wataru Hashimoto, Xun Shen, \& Kazumune Hashimoto (2023). Safe Exploration in Reinforcement Learning: A Generalized Formulation and Algorithms. In Thirty-seventh Conference on Neural Information Processing Systems.}

\end{frame}

\begin{frame}[allowframebreaks]{Annexes}{Approche AOMEA: Fondement th√©orique}

    \textbf{Inferrer des Sp√©cifications Organisationnelles}

    \begin{columns}

        \begin{column}{0.3\textwidth}

            \begin{itemize}
                \item \textbf{Knowledge-based Organizational Specifications Identification (KOSIA)}
                \item \textbf{General Organizational Specifications Infererence (GOSIA)}
            \end{itemize}

        \end{column}

        \begin{column}{0.8\textwidth}
            \begin{figure}
                \centering
                \includegraphics[width=0.95\linewidth]{figures/GOSIA_view.png}
                \caption*{A summary view of the GOSIA process}
                \label{fig:gosia_process}
            \end{figure}
        \end{column}

    \end{columns}

\end{frame}


%%%%%%%%%%%%%%%%

% Slide 2: Exemple d'utilisation
\begin{frame}[fragile]{Annexes}{Exemple d'utilisation d'Optuna}
    \begin{itemize}
        \item \textbf{Optuna} est une biblioth√®que open-source pour l'optimisation des hyperparam√®tres (HPO), utile en apprentissage automatique.
        \item Exemples d'hyper-param√®tre : taux d'apprentissage, fonction activation, nb couche, taille couches, seuil de distance pour Hierarchical Clustering\dots
        \item \textbf{√âtapes pour utiliser Optuna :}
        \begin{itemize}
            \item \texttt{1.} D√©finir une fonction d'objectif.
            \item \texttt{2.} Lancer une √©tude avec Optuna.
            \item \texttt{3.} Utiliser le meilleur r√©sultat pour entra√Æner le mod√®le.
        \end{itemize}
    \end{itemize}

    \begin{lstlisting}[language=Python, basicstyle=\small\ttfamily, frame=single, caption=Exemple d'Optuna en Python]
import optuna

def objective(trial):
    x = trial.suggest_float("x", -10, 10)
    return (x - 2) ** 2 # Mock : fonction "etat-valeur"

study = optuna.create_study(direction="minimize")
study.optimize(objective, n_trials=100)

print(study.best_params)  # Affiche les meilleurs parametres
    \end{lstlisting}
\end{frame}


\begin{frame}{Annexes}{Aper√ßu PettingZoo}
    \begin{itemize}
        \item Biblioth√®que Python pour environnements multi-agents.
        \item Simplifier l'entra√Ænement et l'√©valuation des agents dans divers environnements.
        \item \textbf{Caract√©ristiques principales} :
        \begin{itemize}
            \item Supporte plusieurs types d'environnements multi-agents (tour par tour, simultan√©, etc.).
            \item Int√©gration facile avec des frameworks de reinforcement learning comme RLlib.
            \item Compatible avec les API de Gym, permettant une utilisation intuitive.
        \end{itemize}
        \item \textbf{Exemples d'environnements inclus} :
        \begin{itemize}
            \item Jeux : \textit{TicTacToe}, \textit{ConnectFour}.
            \item Sc√©narios de collaboration et de comp√©tition : \textit{Pistonball}, \textit{Prisoner's Dilemma}.
            \item Int√©gration avec la suite d'environnements Atari pour le multi-agent.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]{Annexes}{Exemple utilisation de PettingZoo}
    \begin{itemize}
        \item Exemple : Cr√©ation et interaction avec un environnement.
        \item Chargement de l'environnement, r√©initialisation et √©tapes d'interaction pour un agent.
    \end{itemize}
    \vspace{0.3cm}
    \begin{lstlisting}[language=Python, basicstyle=\ttfamily\small]
from pettingzoo.butterfly import pistonball_v6

# Creer et reinitialiser l'environnement
env = pistonball_v6.env()
env.reset()

# Boucle principale d'interaction
for agent in env.agent_iter():
    obs, reward, done, info = env.last()
    action = env.action_space(agent).sample()  # Action aleatoire
    env.step(action)
    if done:
        env.reset()  # Reinitialiser si l'episode est termine
\end{lstlisting}
\end{frame}


\begin{frame}{Annexes}{KB-Org}
    \frametitle{Organization-based multi-agent systems: From modeling to implementation}
    
    \begin{itemize}
        \item Mod√©lisation et mise en ≈ìuvre des SMA bas√©s sur organisation ;
        \item Int√®gre les concepts d'organisation pour structurer les interactions et le comportement des agents ;
        \item Banque d'organisations disponibles pr√™tes √† √™tre utilis√© ;
        \item Explicabilit√© et √† la coordination.
    \end{itemize}
    
    \

    Sims, V. (2008). Automated organization design for multi-agent systems. Autonomous Agents and Multi-Agent Systems, 16(2), 151-185.

\end{frame}

\begin{frame}{Annexes}{Pr√©sentation de la biblioth√®que MARLlib}

    \begin{itemize}
        \item Biblioth√®que Python pour MARL
        \item Supporte plusieurs environnements MARL comme PettingZoo, StarCraft II, MPE (Multi-Agent Particle Environment), etc.
        \item Impl√©mente divers algorithmes MARL, incluant MADDPG, MAPPO, etc.
        \item Fournit une interface pour comparaison d‚Äôalgorithmes, l‚Äôentra√Ænement et l‚Äô√©valuation.
        \item Offre des configurations \textit{fine-tun√©s} pour de nombreux environnements
    \end{itemize}

\end{frame}

\begin{frame}[allowframebreaks]{Annexes}{Pr√©sentation de la biblioth√®que MARLlib}

    \begin{itemize}
        \item \textbf{Algorithmes Bas√©s sur les Valeurs}  
        \begin{itemize}
            \item \textbf{Multi-Agent Q-Learning} : Une extension multi-agent fondamentale de Q-learning.  
            \textit{Description} : Simple √† impl√©menter, mais avec des difficult√©s de scalabilit√© et de non-stationnarit√©.
            \item \textbf{MADDPG} : Une adaptation de DDPG pour les environnements multi-agents.  
            \textit{Description} : G√®re bien les espaces d'actions continues, mais requiert beaucoup de donn√©es et est complexe.
        \end{itemize}
    
        \

        \item \textbf{Algorithmes Bas√©s sur les Politiques}  
        \begin{itemize}
            \item \textbf{REINFORCE} : Une m√©thode de gradient de politique basique pour l'apprentissage direct de la politique.  
            \textit{Description} : Adaptable aux environnements stochastiques mais souffre de variances √©lev√©es des gradients.
            \item \textbf{Multi-Agent PPO (MAPPO)} : Une extension de PPO con√ßue pour les configurations multi-agents.  
            \textit{Description} : Stabilise les mises √† jour, mais n√©cessite un ajustement minutieux et un co√ªt de calcul √©lev√©.
        \end{itemize}
    
        \item \textbf{Algorithmes Hybrides}  
        \begin{itemize}
            \item \textbf{A3C (Asynchronous Advantage Actor-Critic)} : Combine l'apprentissage des politiques et des valeurs pour un √©quilibre exploration/exploitation.  
            \textit{Description} : Acc√©l√®re l'entra√Ænement mais n√©cessite une synchronisation complexe.
            \item \textbf{MAPPO} : Un hybride int√©grant PPO avec un entra√Ænement centralis√©.  
            \textit{Description} : Efficace pour les t√¢ches coop√©ratives, mais difficile dans les environnements comp√©titifs et exigeant en ressources.
        \end{itemize}
    
        \item \textbf{Algorithmes Th√©oriques et Coop√©ratifs Bas√©s sur le Jeu}  
        \begin{itemize}
            \item \textbf{Independent Q-Learning (IQL)} : Une version ind√©pendante de Q-learning pour chaque agent.  
            \textit{Description} : Simple √† impl√©menter mais avec de s√©rieux probl√®mes de non-stationnarit√© en multi-agent.
            \item \textbf{COMA (Counterfactual Multi-Agent Policy Gradients)} : Utilise des baselines contrefactuelles pour √©valuer les contributions des agents.  
            \textit{Description} : R√©duit la variance et am√©liore la coop√©ration, mais demande des calculs lourds.
        \end{itemize}
    
        \item \textbf{Entra√Ænement Centralis√© avec Ex√©cution D√©centralis√©e}  
        \begin{itemize}
            \item \textbf{QMIX} : D√©compose les valeurs Q pour am√©liorer la coordination multi-agent.  
            \textit{Description} : √âquilibre l'entra√Ænement centralis√© et l'action d√©centralis√©e, mais moins efficace en environnements tr√®s comp√©titifs.
            \item \textbf{VDN (Value Decomposition Networks)} : Simplifie la coordination multi-agent avec la d√©composition des valeurs.  
            \textit{Description} : Efficace mais limit√© dans la gestion d'interactions complexes.
        \end{itemize}
    \end{itemize}
    

\end{frame}