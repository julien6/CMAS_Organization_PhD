@misc{kwiatkowski2024,
  title         = {Gymnasium: A Standard Interface for Reinforcement Learning Environments},
  author        = {Ariel Kwiatkowski and Mark Towers and Jordan Terry and John U. Balis and Gianluca De Cola and Tristan Deleu and Manuel Goulão and Andreas Kallinteris and Markus Krimmel and Arjun KG and Rodrigo Perez-Vicente and Andrea Pierré and Sander Schulhoff and Jun Jet Tai and Hannah Tan and Omar G. Younis},
  year          = {2024},
  eprint        = {2407.17032},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG},
  url           = {https://arxiv.org/abs/2407.17032}
}

@article{yang2021role,
  title   = {Role-based multi-agent reinforcement learning},
  author  = {Yang, Feng and Liu, Min and Shen, Zhan and Wang, Zhe and Yu, Yi and Wang, Yanan and Wang, Xiao and Chen, Ping},
  journal = {arXiv preprint arXiv:2105.14292},
  year    = {2021}
}

@inproceedings{grover2018role,
  title     = {Learning policy representations in multiagent systems},
  author    = {Grover, Aditya and Al-Shedivat, Maruan and Gupta, Jayesh K and Burda, Yuri and Edwards, Harrison and Fan, Luyu and Krama, Uros and Achiam, Joshua and Rao, Siddhartha S and Schulman, John and others},
  booktitle = {Proceedings of the 35th International Conference on Machine Learning},
  pages     = {1764--1773},
  year      = {2018}
}

@inproceedings{borsa2019constrained,
  title     = {Constrained Policy Optimization for Reinforcement Learning},
  author    = {Borsa, Diana and Graesser, Lauren and Xiao, Tiago Ramalho and Munos, Remi and Barreto, Andre and Silver, David},
  booktitle = {Advances in Neural Information Processing Systems},
  pages     = {3557--3567},
  year      = {2019}
}

@inproceedings{ferber2003,
  title       = {{Agent/Group/Roles: Simulating with Organizations}},
  author      = {Ferber, Jacques and Gutknecht, Olivier and Michel, Fabien},
  url         = {https://hal-lirmm.ccsd.cnrs.fr/lirmm-00269714},
  booktitle   = {{ABS 2003 - 4th International Workshop on Agent-Based Simulation}},
  address     = {Montpellier, France},
  editor      = {Muller, J.P.},
  year        = {2003},
  month       = Apr,
  keywords    = {Multi-agent systems ; organizations ; organizational structures ; multi-agent methodology ; multi-agent design ; agent based simulation},
  pdf         = {https://hal-lirmm.ccsd.cnrs.fr/lirmm-00269714v1/file/ArticleABS03.AGR-Simulating_with_org.pdf},
  hal_id      = {lirmm-00269714},
  hal_version = {v1}
}

@article{Hubner2007,
  title     = {Developing organised multiagent systems using the MOISE+ model: programming issues at the system and agent levels},
  issn      = {1746-1383},
  url       = {http://dx.doi.org/10.1504/IJAOSE.2007.016266},
  doi       = {10.1504/ijaose.2007.016266},
  journal   = {Int. Journal of Agent-Oriented Software Engineering},
  publisher = {Inderscience Publishers},
  author    = {{Hubner,  Jomi F et. al.}},
  year      = {2007},
  pages     = {370}
}

﻿@article{ghavamzadeh2006cooperative,
  author   = {Ghavamzadeh, Mohammad
              and Mahadevan, Sridhar
              and Makar, Rajbala},
  title    = {Hierarchical multi-agent reinforcement learning},
  journal  = {Autonomous Agents and Multi-Agent Systems},
  year     = {2006},
  month    = {Sep},
  day      = {01},
  volume   = {13},
  number   = {2},
  pages    = {197-229},
  abstract = {In this paper, we investigate the use of hierarchical reinforcement learning (HRL) to speed up the acquisition of cooperative multi-agent tasks. We introduce a hierarchical multi-agent reinforcement learning (RL) framework, and propose a hierarchical multi-agent RL algorithm called Cooperative HRL. In this framework, agents are cooperative and homogeneous (use the same task decomposition). Learning is decentralized, with each agent learning three interrelated skills: how to perform each individual subtask, the order in which to carry them out, and how to coordinate with other agents. We define cooperative subtasks to be those subtasks in which coordination among agents significantly improves the performance of the overall task. Those levels of the hierarchy which include cooperative subtasks are called cooperation levels. A fundamental property of the proposed approach is that it allows agents to learn coordination faster by sharing information at the level of cooperative subtasks, rather than attempting to learn coordination at the level of primitive actions. We study the empirical performance of the Cooperative HRL algorithm using two testbeds: a simulated two-robot trash collection task, and a larger four-agent automated guided vehicle (AGV) scheduling problem. We compare the performance and speed of Cooperative HRL with other learning algorithms, as well as several well-known industrial AGV heuristics. We also address the issue of rational communication behavior among autonomous agents in this paper. The goal is for agents to learn both action and communication policies that together optimize the task given a communication cost. We extend the multi-agent HRL framework to include communication decisions and propose a cooperative multi-agent HRL algorithm called COM-Cooperative HRL. In this algorithm, we add a communication level to the hierarchical decomposition of the problem below each cooperation level. Before an agent makes a decision at a cooperative subtask, it decides if it is worthwhile to perform a communication action. A communication action has a certain cost and provides the agent with the actions selected by the other agents at a cooperation level. We demonstrate the efficiency of the COM-Cooperative HRL algorithm as well as the relation between the communication cost and the learned communication policy using a multi-agent taxi problem.},
  issn     = {1573-7454},
  doi      = {10.1007/s10458-006-7035-4},
  url      = {https://doi.org/10.1007/s10458-006-7035-4}
}

@article{liu2021efficient,
  title     = {Efficient policy gradient methods for reinforcement learning in multi-agent systems},
  author    = {Liu, Xuan and Mao, Jiliang and Liu, Jing and Pan, Zhijun and Huang, Fei and Wu, Mingxuan},
  journal   = {IEEE Transactions on Neural Networks and Learning Systems},
  volume    = {32},
  number    = {2},
  pages     = {641--654},
  year      = {2021},
  publisher = {IEEE}
}

@article{van2018explainable,
  title   = {Explainable reinforcement learning: A survey},
  author  = {van Harmelen, Frank and Ten Teije, Annette},
  journal = {arXiv preprint arXiv:1809.10356},
  year    = {2018}
}

@inproceedings{van2020explainability,
  title     = {Explainability for intelligent systems: Why, when, and how},
  author    = {Van Harmelen, Frank and Teije, Annette and Ziafati, Payam},
  booktitle = {Proceedings of the 29th International Joint Conference on Artificial Intelligence},
  year      = {2020}
}

@inproceedings{achiam2017cpo,
  title     = {Constrained Policy Optimization},
  author    = {Achiam, Joshua and Held, David and Tamar, Aviv and Abbeel, Pieter},
  booktitle = {Proceedings of the 34th International Conference on Machine Learning},
  pages     = {22--31},
  year      = {2017}
}

@inproceedings{ray2019benchmarking,
  title     = {Benchmarking Safe Exploration in Deep Reinforcement Learning},
  author    = {Ray, Alex and Achiam, Joshua and Amodei, Dario},
  booktitle = {arXiv preprint arXiv:1910.01708},
  year      = {2019}
}

@article{garcia2015comprehensive,
  title   = {A comprehensive survey on safe reinforcement learning},
  author  = {Garcia, Javier and Fernandez, Fernando},
  journal = {Journal of Machine Learning Research},
  volume  = {16},
  number  = {1},
  pages   = {1437--1480},
  year    = {2015}
}

@article{alshiekh2018safe,
  title   = {Safe reinforcement learning via shielding},
  author  = {Alshiekh, Mohammed and Bloem, Roderick and Johnson, Matthew and Kapinski, James and Julian, Keith and Kochenderfer, Mykel J},
  journal = {Proceedings of the 32nd AAAI Conference on Artificial Intelligence},
  year    = {2018}
}

@inproceedings{ghavamzadeh2006hrl,
  title     = {Hierarchical reinforcement learning with cooperative agents},
  author    = {Ghavamzadeh, Mohammad and Mahadevan, Sridhar},
  booktitle = {Proceedings of the 23rd International Conference on Machine Learning},
  pages     = {119--126},
  year      = {2006}
}

@article{foerster2018communication,
  title   = {Learning to Communicate with Deep Multi-Agent Reinforcement Learning},
  author  = {Foerster, Jakob and Assael, Yannis and de Freitas, Nando and Whiteson, Shimon},
  journal = {Advances in Neural Information Processing Systems},
  volume  = {31},
  pages   = {2137--2145},
  year    = {2018}
}

@inproceedings{zhang2020safemarl,
  title     = {Safe multi-agent reinforcement learning with shielding via model predictive control},
  author    = {Zhang, Xianyuan and Su, Hao and Zhang, Yuhang and Tamar, Aviv},
  booktitle = {Proceedings of the 38th International Conference on Machine Learning},
  pages     = {11608--11618},
  year      = {2020}
}

@article{overcookedai,
  title   = {Overcooked-AI: A Benchmark for Multi-Agent Learning under Partial Observability},
  author  = {Carroll, Micah and Shah, Rohin and Ho, Mark and Griffiths, Tom and Abbeel, Pieter and Dragan, Anca},
  journal = {Proceedings of the 2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  year    = {2020},
  pages   = {2374--2380}
}

@misc{Maxwell2021,
      title={CybORG: A Gym for the Development of Autonomous Cyber Agents}, 
      author={Maxwell Standen and Martin Lucas and David Bowman and Toby J. Richer and Junae Kim and Damian Marriott},
      year={2021},
      eprint={2108.09118},
      archivePrefix={arXiv},
      primaryClass={cs.CR}
}

@article{lowe2017multi,
  title   = {Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments},
  author  = {Lowe, Ryan and Wu, Yi I and Tamar, Aviv and Harb, Jean and Abbeel, Pieter and Mordatch, Igor},
  journal = {Advances in Neural Information Processing Systems},
  year    = {2017},
  volume  = {30}
}

@article{yu2021mappo,
  title   = {The Surprising Effectiveness of PPO in Cooperative Multi-Agent Games},
  author  = {Yu, Chengjie and Dong, Hao and Zhao, Yiqun and Zheng, Shuxin},
  journal = {Advances in Neural Information Processing Systems},
  volume  = {34},
  pages   = {1091--1104},
  year    = {2021}
}

@article{rashid2018qmix,
  title   = {QMIX: Monotonic value function factorisation for deep multi-agent reinforcement learning},
  author  = {Rashid, Tabish and Samvelyan, Mikayel and Schroeder, Christian and Farquhar, Gregory and Foerster, Jakob and Whiteson, Shimon},
  journal = {Proceedings of the 35th International Conference on Machine Learning},
  pages   = {4295--4304},
  year    = {2018}
}

@article{mnih2015dqn,
  title     = {Human-level control through deep reinforcement learning},
  author    = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A and Veness, Joel and Bellemare, Marc G and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K and Ostrovski, Georg and others},
  journal   = {Nature},
  volume    = {518},
  number    = {7540},
  pages     = {529--533},
  year      = {2015},
  publisher = {Nature Publishing Group}
}

@book{oliehoek2016pomdp,
  title     = {A concise introduction to decentralized POMDPs},
  author    = {Oliehoek, Frans and Amato, Christopher},
  publisher = {Springer},
  year      = {2016}
}

@article{beynier2013joint,
  title   = {A Joint Approach for Decentralized POMDP Resolution},
  author  = {Beynier, Aur{\'e}lie and Mouaddib, Abdel-Illah and Zilberstein, Shlomo},
  journal = {Proceedings of the 2013 International Conference on Autonomous Agents and Multiagent Systems},
  pages   = {249--256},
  year    = {2013}
}

@article{terry2020pettingzoo,
  title   = {PettingZoo: Gym for multi-agent reinforcement learning},
  author  = {Terry, Justin K and Black, Benjamin and Grammel, Nathaniel and Jayakumar, Mario and Hari, Ananth and Sulivan, Eugene and Perez, Ruben Glatt and Santos, Lukas and Horsch, Cameron and Dieffendahl, Christian and others},
  journal = {Proceedings of the NeurIPS 2020 Track on Datasets and Benchmarks},
  pages   = {21--23},
  year    = {2020}
}

@article{hu2021marlib,
  title   = {MarlLib: A comprehensive library for multi-agent reinforcement learning},
  author  = {Hu, Qi and Chen, Jun and Zhao, Jiajun and Xu, Zhenyu and Liu, Xiaolin and others},
  journal = {arXiv preprint arXiv:2106.05912},
  year    = {2021}
}



% ============
@book{Oliehoek2016,
  title     = {A Concise Introduction to Decentralized POMDPs},
  author    = {Frans A. Oliehoek and Christopher Amato},
  year      = {2016},
  publisher = {Springer},
  isbn      = {978-3-319-28929-8},
  url       = {https://link.springer.com/book/10.1007/978-3-319-28929-8}
}

@inproceedings{Beynier2013,
  title     = {A Decentralized Approach for Reinforcement Learning in Cooperative Multi-agent Systems},
  author    = {Aurélie Beynier and Alain Mouaddib},
  booktitle = {Proceedings of the 23rd International Joint Conference on Artificial Intelligence (IJCAI)},
  pages     = {163-168},
  year      = {2013}
}

@article{Albrecht2024,
  title   = {Survey on Recent Advances in Cooperative Multi-Agent Reinforcement Learning},
  author  = {Stefano V. Albrecht and Jacob Y. Foerster},
  journal = {Journal of Artificial Intelligence Research},
  year    = {2024},
  note    = {to appear}
}

% --- SoA ----

@book{altman1999cmdp,
  title     = {Constrained Markov Decision Processes},
  author    = {Altman, Eitan},
  year      = {1999},
  publisher = {CRC Press}
}

@article{hubner2007moise,
  title     = {MOISE+: Towards a Structural, Functional, and Deontic Model for MAS Organization},
  author    = {H{\"u}bner, Jomi Fred and Sichman, Jaime Sim{\~a}o and Boissier, Olivier},
  journal   = {Autonomous Agents and Multi-Agent Systems},
  volume    = {12},
  number    = {3},
  pages     = {207--238},
  year      = {2007},
  publisher = {Springer}
}

@article{krouka2021federated,
  title   = {Communication-Efficient and Federated Multi-Agent Reinforcement Learning},
  author  = {Krouka, Mounssif and Elgabli, Anis and Issaid, Chaouki Ben and Bennis, Mehdi},
  journal = {arXiv preprint arXiv:2109.07599},
  year    = {2021}
}
% --------

@article{tan1993multi,
  title   = {Multi-agent reinforcement learning: Independent vs. cooperative agents},
  author  = {Tan, Ming},
  journal = {Proceedings of the Tenth International Conference on Machine Learning},
  year    = {1993},
  pages   = {330--337}
}

@article{foerster2016learning,
  title   = {Learning to communicate with deep multi-agent reinforcement learning},
  author  = {Foerster, Jakob N and Assael, Yannis M and de Freitas, Nando and Whiteson, Shimon},
  journal = {Advances in Neural Information Processing Systems},
  volume  = {29},
  year    = {2016}
}

@article{campos2021survey,
  title   = {A survey of deep reinforcement learning for safe autonomous driving},
  author  = {Campos, Gustavo R and others},
  journal = {arXiv preprint arXiv:2104.14221},
  year    = {2021}
}

@article{wei2019safe,
  title   = {Safe reinforcement learning: Using formal methods to improve stability},
  author  = {Wei, Chen-Yu and others},
  journal = {Advances in Neural Information Processing Systems},
  year    = {2019}
}

@article{de2020survey,
  title   = {A survey of learning in multiagent environments: Dealing with non-stationarity},
  author  = {De, Abhishek and others},
  journal = {arXiv preprint arXiv:2006.05076},
  year    = {2020}
}

@article{ghosal2021explainable,
  title   = {Explainable reinforcement learning: A survey},
  author  = {Ghosal, Soumyajit and others},
  journal = {IEEE Transactions on Neural Networks and Learning Systems},
  year    = {2021}
}

@article{bajcsy2019efficient,
  title   = {Efficient risk-averse exploration in constrained environments},
  author  = {Bajcsy, Andrea and others},
  journal = {IEEE International Conference on Robotics and Automation (ICRA)},
  year    = {2019}
}

@article{bastani2018verifiable,
  title   = {Verifiable reinforcement learning via policy extraction},
  author  = {Bastani, Osbert and others},
  journal = {Advances in Neural Information Processing Systems},
  year    = {2018}
}

@article{chu2020multi,
  title   = {Multi-agent deep reinforcement learning: A survey},
  author  = {Chu, Tianhong and others},
  journal = {IEEE Transactions on Neural Networks and Learning Systems},
  year    = {2020}
}

@article{hubner2007using,
  title   = {Using the MOISE+ model for designing an organization in multi-agent systems},
  author  = {Hubner, Jomi Fred and others},
  journal = {Proceedings of the 6th International Joint Conference on Autonomous Agents and Multiagent Systems},
  year    = {2007}
}

@article{mataric1997using,
  title   = {Using role models for designing and evaluating multi-robot systems},
  author  = {Mataric, Maja J and others},
  journal = {IEEE International Conference on Robotics and Automation (ICRA)},
  year    = {1997}
}

@article{yang2018mean,
  title   = {Mean field multi-agent reinforcement learning},
  author  = {Yang, Yaodong and others},
  journal = {International Conference on Machine Learning (ICML)},
  year    = {2018}
}

@article{hubner2010moise,
  title   = {MOISE+: Towards a structural, functional, and deontic model for MAS organizational modeling},
  author  = {Hubner, Jomi Fred and others},
  journal = {International Journal of Agent-Oriented Software Engineering},
  year    = {2010}
}

@article{foerster2018counterfactual,
  title   = {Counterfactual multi-agent policy gradients},
  author  = {Foerster, Jakob and others},
  journal = {International Conference on Machine Learning (ICML)},
  year    = {2018}
}

@article{su2021toward,
  title   = {Toward explainable artificial intelligence (XAI) for multi-agent systems},
  author  = {Su, Guangyao and others},
  journal = {Proceedings of the 34th AAAI Conference on Artificial Intelligence},
  year    = {2021}
}

@article{castaneda2019policy,
  title   = {Policy shaping in multi-agent systems through targeted communications},
  author  = {Castaneda, Daniel and others},
  journal = {Proceedings of the 18th International Conference on Autonomous Agents and MultiAgent Systems},
  year    = {2019}
}

@article{oliehoek2016concise,
  title   = {A concise introduction to decentralized POMDPs},
  author  = {Oliehoek, Frans and Amato, Christopher},
  journal = {Springer},
  year    = {2016}
}

@article{camara2020symbolic,
  title   = {Symbolic knowledge acquisition for safe reinforcement learning},
  author  = {Camara, Florian and others},
  journal = {International Conference on Autonomous Agents and MultiAgent Systems (AAMAS)},
  year    = {2020}
}

@article{role_based_rl_reference,
  title   = {Role-based reinforcement learning for multi-agent systems},
  author  = {Xiao, Jing and others},
  journal = {Proceedings of the 16th International Conference on Autonomous Agents and Multiagent Systems (AAMAS)},
  year    = {2017}
}

@article{options_hrl_reference,
  title   = {Between MDPs and semi-MDPs: A framework for temporal abstraction in reinforcement learning},
  author  = {Sutton, Richard S and others},
  journal = {Artificial Intelligence},
  volume  = {112},
  year    = {1999}
}

@article{feudal_rl_reference,
  title   = {Feudal networks for hierarchical reinforcement learning},
  author  = {Vezhnevets, Alexander S and others},
  journal = {Proceedings of the 34th International Conference on Machine Learning (ICML)},
  year    = {2017}
}

@article{hi_marl_reference,
  title   = {Hierarchical multi-agent reinforcement learning with dynamic goal assignment},
  author  = {Wang, Jun and others},
  journal = {Proceedings of the 36th AAAI Conference on Artificial Intelligence},
  year    = {2022}
}
