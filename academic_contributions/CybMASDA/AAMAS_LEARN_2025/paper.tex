%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% LaTeX Template for AAMAS-2025 (based on sample-sigconf.tex)
%%% Prepared by the AAMAS-2025 Program Chairs based on the version from AAMAS-2025. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% Start your document with the \documentclass command.

\RequirePackage[2020-02-02]{latexrelease}

%%%% For anonymized submission, use this
\documentclass[sigconf,anonymous]{aamas} 

%%%% For camera-ready, use this
%\documentclass[sigconf]{aamas}

\usepackage{listings}
% \usepackage{xcolor}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}
 
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}
 
\lstset{style=mystyle}

% --- Tickz
\usepackage{physics}
\usepackage{tikz}
\usepackage{amsmath}
\usepackage{mathdots}
% \usepackage{yhmath}
\usepackage{cancel}
\usepackage{color}
\usepackage{siunitx}
\usepackage{array}
\usepackage{multirow}
% \usepackage{amssymb}
\usepackage{gensymb}
\usepackage{tabularx}
\usepackage{extarrows}
\usepackage{booktabs}
\usetikzlibrary{fadings}
\usetikzlibrary{patterns}
\usetikzlibrary{shadows.blur}
\usetikzlibrary{shapes}

% ---------

\usepackage{balance} % for balancing columns on the final page
\usepackage{csquotes}
% \usepackage{cite}
\newcommand{\probP}{\text{I\kern-0.15em P}}
\usepackage{etoolbox}
\patchcmd{\thebibliography}{\section*{\refname}}{}{}{}
% \usepackage{amsthm,amssymb,amsfonts}

\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{color}
% \renewcommand\UrlFont{\color{blue}\rmfamily}

\usepackage[inline, shortlabels]{enumitem}
\usepackage{tabularx}
\usepackage{caption}
\usepackage{listings}
\usepackage{stfloats}
\usepackage{titlesec}
\usepackage{ragged2e}
% \usepackage[hyphens]{url}
\usepackage[linesnumbered,ruled,vlined]{algorithm2e}
\usepackage{float}
\usepackage[english]{babel}
\addto\extrasenglish{  
    \def\figureautorefname{Figure}
    \def\tableautorefname{Table}
    \def\algorithmautorefname{Algorithm}
    \def\sectionautorefname{Section}
    \def\subsectionautorefname{Subsection}
    \def\proofoutlineautorefname{Proof Outline}
}

\newcommand{\supertiny}{\fontsize{1}{2}\selectfont}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% AAMAS-2025 copyright block (do not change!)

\setcopyright{ifaamas}
\acmConference[AAMAS '25]{Proc.\@ of the 24th International Conference
on Autonomous Agents and Multiagent Systems (AAMAS 2025)}{May 19 -- 23, 2025}
{Detroit, Michigan, USA}{A.~El~Fallah~Seghrouchni, Y.~Vorobeychik, S.~Das, A.~Nowe (eds.)}
\copyrightyear{2025}
\acmYear{2025}
\acmDOI{}
\acmPrice{}
\acmISBN{}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% == IMPORTANT ==
%%% Use this command to specify your EasyChair submission number.
%%% In anonymous mode, it will be printed on the first page.

\acmSubmissionID{<<EasyChair submission id>>}

%%% Use this command to specify the title of your paper.

\title[AAMAS-2025 CybMASDE]{An Organizationally-Oriented Approach to Enhancing Explainability and Control in Multi-Agent Reinforcement Learning}

% Evaluating and Controlling Organization in Multi-Agent Learning

% Organizationally Constrained Multi-Agent Learning: A Framework for Safe and Interpretable Policies

%%% Provide names, affiliations, and email addresses for all authors.

\author{Julien Soulé}
\affiliation{
  \institution{Univ. Grenoble Alpes}
  \city{Valence}
  \country{France}}
\email{julien.soule@lcis.grenoble-inp.fr}

\author{Jean-Paul Jamont}
\affiliation{
  \institution{Univ. Grenoble Alpes}
  \city{Valence}
  \country{France}}
\email{jean-paul.jamont@lcis.grenoble-inp.fr}

\author{Michel Occello}
\affiliation{
  \institution{Univ. Grenoble Alpes}
  \city{Valence}
  \country{France}}
\email{michel.occello@lcis.grenoble-inp.fr}

\author{Louis-Marie Traonouez}
\affiliation{
  \institution{Thales Land and Air Systems, BU IAS}
  \city{Rennes}
  \country{France}}
\email{louis-marie.traonouez@thalesgroup.com}

\author{Paul Théron}
\affiliation{
  \institution{AICA IWG}
  \city{La Guillermie}
  \country{France}}
\email{paul.theron@orange.fr}


\begin{abstract}
    Multi-Agent Reinforcement Learning (MARL) can lead to the development of collaborative agent behaviors that show similarities with organizational concepts. Pushing forward this perspective, we introduce a novel framework that explicitly incorporates organizational roles and goals from the $\mathcal{M}OISE^+$ model into the MARL process, guiding agents to satisfy corresponding organizational constraints. By structuring training with roles and goals, we aim to enhance both the explainability and control of agent behaviors at the organizational level, whereas much of the literature primarily focuses on individual agents. Additionally, our framework includes a post-training analysis method to infer implicit roles and goals, offering insights into emergent agent behaviors. This framework has been applied across various MARL environments and algorithms, demonstrating coherence between predefined organizational specifications and those inferred from trained agents.
\end{abstract}


% TODO: Mention the specific contributions related to the LEARN topics such as robustness, adaptability, and learning in constrained environments.

%%% The code below was generated by the tool at http://dl.acm.org/ccs.cfm.
%%% Please replace this example with code appropriate for your own paper.

%%% Use this command to specify a few keywords describing your work.
%%% Keywords should be separated by commas.

\keywords{  }

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% Include any author-defined commands here.
         
% \newcommand{\BibTeX}{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em\TeX}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

%%% The following commands remove the headers in your paper. For final 
%%% papers, these will be inserted during the pagination process.

\pagestyle{fancy}
\fancyhead{}

%%% The next command prints the information defined in the preamble.

\maketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}

% Context
Multi-Agent Reinforcement Learning (MARL) enables the discovery of a joint policy that controls agents' behaviors so they can achieve a global goal within a specific environment. 
This joint policy not only dictates the individual actions of agents but also manages their interactions with one another, and potentially with all other agents, without any preconceived notion of a predefined organizational order or structure. 

In environments that require social interaction among agents to optimally achieve the global goal, agents may converge in such a way that they exhibit recurring sets of similar behaviors across different testing episodes. 
These distinct sets of behaviors can demonstrate properties of specialization, complementarity, and stability, making them akin to implicit roles. Moreover, the trajectories of agents assuming these abstract roles may display similarities, such as recurrent observations at the end of each episode. These recurring parts of trajectories can be interpreted as "abstract" goals, suggesting that agents may aim to pursue these as intermediate objectives before reaching the global goal. These "abstract" roles and "abstract" goals form the foundation of an "abstract" structural and functional organization.

However, it would be misleading to assume that all trained agents in any environment can be faithfully compared to a structured and functional organization. Indeed, we can interpret the behaviors of trained agents concerning their similarity to the potential vision of an "abstract" structured and functional organization, which we define as \textbf{organizational fit}.
While evaluating organizational fit would be useful to assess to what extent trained agents can naturally be explained as roles and goals, one could also consider the reverse approach. By guiding or encouraging agents to converge towards structured and functional organizations with higher organizational fit, we can enhance explainability and control in MARL.

\

% Problem
Building on these assumptions, this paper aims to further explore two key aspects:
\begin{enumerate*}[label={\roman*)}]
    \item The \textbf{evaluation of organizational fit}, which seeks to measure how closely a joint policy aligns with a structured and functional organization. A significant challenge here is to understand under what conditions agents can be considered to form a structured and functional organization, given constraints imposed by the environment, objectives, and other optional factors.
    Existing literature often addresses policy evaluation in terms of roles or goals, but these works generally lack a systematic and comprehensive approach. Current methods offer few clear tools for quantitatively and qualitatively measuring this organizational fit.
    \item The \textbf{control of organizational fit}, which aims to guide agents towards policies that conform to a structured and functional organization through user-defined constraints or incentives that implement roles and goals.
    The primary challenges include reducing the policy search space, improving convergence, and ensuring compliance with safety constraints.
    Existing approaches in this field often fall short in terms of enabling users to easily define and manage the application of organizational specifications in a practical and flexible manner within a standard MARL framework, without relying on alternative paradigms such as Hierarchical Reinforcement Learning (HRL).
\end{enumerate*}

\

% Contribution
\noindent We introduce the \textbf{MOISE+MARL} framework, which integrates the Dec-POMDP (Decentralized Partially Observable Markov Decision Process) MARL framework with the $\mathcal{M}OISE^+$~\cite{Hubner2007} organizational model through proposed relationships. This framework allows users to manually define the logic of a role or a goal by relying on trajectory-based patterns to describe the expected behavior of an agent that has adopted a goal or mission. Once configured, they allow users to apply a role to an agent, adding constraints that automatically influence agents' policies by dynamically updating both the action space and reshaping the reward function. This framework also includes a method called \textbf{Trajectory-based Evaluation in MOISE+MARL} (TEMM), which uses unsupervised learning techniques to generalize "abstract" roles and "abstract" missions from observed trajectories across multiple test episodes. By measuring the gap between inferred abstract organizational specifications and actual behaviors, this method allows for a quantitative assessment of organizational fit.

% Evaluation & Findings
We evaluated the MOISE+MARL framework in the following scenarios:
\begin{enumerate*}[label={\roman*) },itemjoin={; \quad}]
  \item Four distinct environments, each expected to result in the training of joint policies with different "abstract" organizations, to assess the generalizability of MOISE+MARL's applicability
  \item Four MARL algorithms from the several families to assess their suitability with MOISE+MARL during training and post-analysis
  \item Four sets of organizational specifications as PBDTs, one for each environment, to constrain agents in a manner that either enforces conformity to predefined behaviors or allows for more freedom, intended for both manual and quantitative evaluation.
\end{enumerate*}

In all environments, we observed that applying organizational specifications through PBDT significantly increased the organizational fit as measured by TEMM. The roles and missions inferred by TEMM closely align with the predefined specifications, demonstrating the internal consistency of MOISE+MARL, as the policy modifications introduced by organizational specifications are effectively captured by TEMM.
The results also indicate that Actor-Critic algorithms are particularly well-suited for guiding agents towards stable policies. This stability allows agents to maintain consistent and coherent behaviors across episodes, which is essential for TEMM's generation of a stable "abstract" organization. In contrast, value-based algorithms showed greater variability in agent behaviors, though overall performance remained high.

\

% Structure of the paper
\noindent The rest of the paper is organized as follows: \autoref{sec:related_works} presents works relative to evaluating and controlling organizational fit. \autoref{sec:moise_marl_framework} introduces the MOISE+MARL framework. \autoref{sec:TEMM_algorithm} describes the TEMM method. \autoref{sec:experimental_setup} describes the experimental protocol, particularly the environments and MARL algorithms. \autoref{sec:results} presents the experimental results. Finally, \autoref{sec:discussion_conclusion} discusses and concludes on the evaluation and control of organizational fit.


\section{Related Works}
\label{sec:related_works}

This section explores works related to organizational fit, as framed by the two core issues introduced.

\subsection{Evaluating Organizational Fit}
This section discusses evaluating how well agent behaviors align with organizational structures, whether predefined or emergent.

\

Yang et al. \cite{yang2021role} show that roles can emerge in decentralized multi-agent systems, with agents displaying consistent patterns over time. Similarly, Grover et al. \cite{grover2018role} detect roles based on agent interactions, though they do not address organizational constraints or predefined structures.
Borsa et al. \cite{borsa2019constrained} stabilize agent behaviors through constrained reinforcement learning, indirectly enhancing organizational fit. Liu et al. \cite{liu2021efficient} evaluate role consistency across episodes but do not explore role emergence within broader organizational contexts.
Van der Waa et al. \cite{van2020explainability} link actions to global goals, providing system insights but lacking granularity in evaluating specific roles and missions within organizational structures.

\subsection{Controlling Organizational Fit}
Controlling organizational fit involves guiding agents to align their policies with predefined organizational structures, often using constraints or incentives.

\paragraph{Constrained Policy Optimization (CPO)}
Achiam et al. \cite{achiam2017cpo} introduce CPO, adjusting policies with safety constraints while maximizing rewards. MOISE+MARL, however, introduces constraints beyond safety to shape behavior toward organizational expectations by externally guiding agent learning.
Ray et al. \cite{ray2019benchmarking} use Lagrange multipliers to integrate constraints into the reward function, balancing reward and constraint adherence. MOISE+MARL extends this by dynamically modifying the action space to enforce constraint adherence at various levels, offering flexible control over agent behaviors.

\paragraph{Safe Exploration}
Safe exploration ensures agents learn while adhering to safety constraints. Garcia et al. \cite{garcia2015comprehensive} overview methods maintaining safe exploration, and Alshiekh et al. \cite{alshiekh2018safe} propose shielding to block unsafe actions. MOISE+MARL goes further by using constraints to guide agents towards behaviors that align with organizational roles.

\paragraph{Hierarchical Reinforcement Learning}
Hierarchical reinforcement learning (HRL) breaks tasks into subtasks, aligning with organizational hierarchies. Ghavamzadeh et al. \cite{ghavamzadeh2006hrl} illustrate that HRL can improve coordination. MOISE+MARL constrains MARL externally, offering refined behaviors under organizational constraints without introducing new hierarchical concepts.

\paragraph{Controlling Communication and Coordination}
is essential for ensuring organizational fit, especially in large-scale systems. Foerster et al. \cite{foerster2018communication} propose decentralized coordination through shared knowledge, allowing agents to operate without centralized control.

% \autoref{tab:related_work} compares key properties of related works and highlights the unique aspects of MOISE+MARL.

% \begin{table}[ht]
%     \centering
%     \small
%     \renewcommand{\arraystretch}{1.3}
%     \caption{Comparison of Related Works on Organizational Fit in MARL}
%     \label{tab:related_work}
%     \begin{tabular}{m{2.1cm} >{\centering\arraybackslash}m{0.8cm} >{\centering\arraybackslash}m{0.5cm} >{\centering\arraybackslash}m{0.8cm} >{\centering\arraybackslash}m{0.5cm} >{\centering\arraybackslash}m{0.5cm} >{\centering\arraybackslash}m{0.8cm}}
%         \textbf{Criterion} & \textbf{\cite{yang2021role}} & \textbf{\cite{grover2018role}} & \textbf{\cite{borsa2019constrained}} & \textbf{\cite{achiam2017cpo}} & \textbf{\cite{ray2019benchmarking}} & \textbf{\cite{ghavamzadeh2006hrl}} \\
%         \hline \vspace{0.2cm}
%         \textbf{Role Emergence} & Yes & Yes & Partial & No & No & No \\
%         \textbf{External Org. \newline Constraints} & No & No & Partial & Yes & Yes & No \\
%         \textbf{Roles \newline Adaptation} & Partial & No & No & Yes & Yes & No \\
%         \textbf{Action \newline Modification} & No & No & No & Yes & Yes & No \\
%         \textbf{Hierarchical \newline Control} & No & No & No & No & No & Yes \\
%         \textbf{Org. XMARL} & Limited & No & No & Partial & No & Yes \\
%         \textbf{Modular \newline granularity} & No & No & Partial & No & Yes & No \\
%         \textbf{Scalability} & No & No & Yes & Partial & Yes & Yes
%     \end{tabular}
% \end{table}
%
% \noindent From \autoref{tab:related_work}, u

\

Unlike HRL, the MOISE+MARL framework stands out for incorporating external organizational constraints that influence agents within a standard MARL framework, enabling modular granularity and dynamic adaptation. Unlike Shielding or CPO, which typically focus on safety constraints, MOISE+MARL goes further by relying on space modification to align with roles. MOISE+MARL handles scalability and adaptability simplifying users interactions to defining and applying a smaller amount of organizational specifications. Finally, more than controling agents, MOISE+MARL also integrates a coherent organisation .


\section{The MOISE+MARL Framework}
\label{sec:moise_marl_framework}

This section introduces the formalism used to describe the functioning framework of the MOISE+MARL framework.

\subsection{Markov Framework for MARL}

To apply MARL techniques, we rely on the \textit{Decentralized Partially Observable Markov Decision Process} (Dec-POMDP)~\citep{Oliehoek2016}. This model is well-suited for MAS, as it models multiple agents in an uncertain environment. Unlike \textit{Partially Observable Stochastic Games} (POSG), the Dec-POMDP allows for a common reward function for agents, which promotes learning of collaborative actions~\citep{Beynier2013}.

A Dec-POMDP $d \in D$ (where $D$ is the set of Dec-POMDPs) is defined as a 7-tuple $d = (S, \{A_i\}, T, R, \{\Omega_i\}, O, \gamma)$, where $S = \{s_1,\dots,s_{|S|}\}$ is the set of possible states; $A_{i} = \{a_{1}^{i},\dots,a_{|A_{i}|}^{i}\}$ is the set of possible actions for agent $i$; $T$ represents the set of transition probabilities, with $T(s,a,s') = \probP(s'|s,a)$ as the probability of transitioning from state $s$ to state $s'$ following action $a$; $R: S \times A \times S \rightarrow \mathbb{R}$ is the reward function, assigning a reward based on the initial state, the action taken, and the resulting state; $\Omega_{i} = \{o_{1}^{i},\dots,o_{|\Omega_{i}|}^{i}\}$ is the set of possible observations for agent $i$; $O$ represents the set of observation probabilities, where $O(s',a,o) = \probP(o|s',a)$ is the probability of obtaining observation $o$ after performing action $a$ and reaching state $s'$; and $\gamma \in [0,1]$ is the discount factor, used to weight future rewards.


The following formalism is used with MOISE+MARL to solve the Dec-POMDP~\citep{Beynier2013,Albrecht2024}: $\mathcal{A}$ represents the set of $n$ \textbf{agents}; $\Pi$ denotes the set of \textbf{policies}, where a policy $\pi \in \Pi, \pi: \Omega \rightarrow A$ deterministically maps an observation to an action, representing the agent's internal strategy; $\Pi_{joint}$ represents the set of \textbf{joint policies}, with a joint policy $\pi_{joint} \in \Pi_{joint}, \pi_{joint}: \Omega^n \rightarrow A^n = \Pi^n$, which selects an action for each agent based on their respective observations, acting as a collection of policies used by agents within a team; $H$ is the set of \textbf{histories}, where a history (or trajectory) over $z \in \mathbb{N}$ steps (typically the maximum number of steps in an episode) is represented as the $z$-tuple $h = ((\omega_{k}, a_{k}) | k \leq z, \omega \in \Omega, a \in A)$, capturing successive observations and actions; $H_{joint}$ stands for the set of \textbf{joint histories}, with a joint history $h_{joint} \in H_{joint}$ over $z$ steps defined as the set of agent histories: $h_{joint} = \{h_1, h_2, \dots, h_n\}$; and finally, $V_{joint}(\pi_{joint}): \Pi_{joint} \rightarrow \mathbb{R}$ denotes the \textbf{expected cumulative reward} over a finite horizon (assuming $\gamma < 1$ or if the number of steps in an episode is finite), where $\pi_{joint}$ represents the joint policy for team $i$, with $\pi_{joint,-i}$ being the joint policies of other teams, considered as fixed.


% We refer to \textbf{solving the Dec-POMDP} as the search for a joint policy $\pi_{joint} \in \Pi_{joint}$ such that $\pi_{joint}s)$, achieving at least an expected cumulative reward of $s$, where $s \in \mathbb{R}$.

\subsection{The $\mathcal{M}OISE^+$ Organizational model}

\begin{figure}[h!]
    \input{figures/moise_model.tex}
    \caption{A synthetic view of the $\mathcal{M}OISE^+$ model}
    \label{fig:moise_model}
\end{figure}

As illustrated in \autoref{fig:moise_model}, $\mathcal{M}OISE^+$ comprises three types of organizational specifications:

\noindent \paragraph{\textbf{Structural Specifications (SS)}} define how agents are structured to achieve goals, expressed as $\mathcal{SS} = \langle \mathcal{R}, \mathcal{IR}, \mathcal{G} \rangle$. $\mathcal{R}_{ss}$ is the set of roles ($\rho \in \mathcal{R}$) with an inheritance relation $\mathcal{IR}$ where $\rho_1 \sqsubset \rho_2$ if $\rho_1$ inherits from $\rho_2$. $\mathcal{GR}$ includes groups $\langle \mathcal{R}, \mathcal{SG}, \mathcal{L}^{intra}, \mathcal{L}^{inter}, \allowbreak \mathcal{C}^{intra}, \mathcal{C}^{inter}, np, ng \rangle$. Links ($\mathcal{L}$) define connections between roles: acquaintance, communication, or authority. Compatibilities $\mathcal{C}$ denote roles that agents can play together. Intra- and inter-group links and compatibilities are shown by $\mathcal{L}^{intra}$, $\mathcal{L}^{inter}$, $\mathcal{C}^{intra}$, and $\mathcal{C}^{inter}$, with $np$ and $ng$ defining role and subgroup counts.

\noindent \paragraph{\textbf{Functional Specifications (FS)}} describe the agents' goals, represented as $\mathcal{FS} = \langle \mathcal{SCH}, \mathcal{PO} \rangle$. The social scheme $\mathcal{SCH}$ includes global goals $\mathcal{G}$, missions $\mathcal{M}$, and plans $\mathcal{P}$ that organize goals in a tree structure. Plans link goals with an operator ($op$) indicating sequence, choice, or parallel completion. Missions map to goal sets ($mo$), and agent counts per mission are specified by $nm$. Preferences $\mathcal{PO}$ indicate which missions agents prefer, denoted as $m_1 \prec m_2$.

\noindent \paragraph{\textbf{Deontic Specifications (DS)}} indicate how structural specifications achieve functional goals, given by $\mathcal{DS} = \langle \mathcal{OBL}, \mathcal{PER} \rangle$. Time constraints $\mathcal{TC}$ set periods for permissions or obligations ($Any$ for any time). Obligations ($\mathcal{OBL}$) require agents in role $\rho_a$ to undertake mission $m$ at times $tc$, while permissions ($\mathcal{PER}$) allow it. The $rds$ function maps roles to their deontic specifications as $(tc, y, m)$ where $y$ distinguishes permission (0) from obligation (1).

\

\noindent Organizational specifications applied to agents are roles and goals (as missions) through permissions or obligations. Indeed, the other structural specifications such as compatibilities or links are inherent to roles. Similarly, we consider that the goals, the missions and their mapping ($mo$) are enough to also link all of the other fonctional specifications such as plans, cardinalities or preference orders.
Consequently, we consider it is sufficient to take into account roles, missions (goal and mapping) and permissions/obligations when linking $\mathcal{M}OISE^+$ with Dec-POMDP. 

\begin{figure*}[t]
    \label{eq:single_value_function}
    \raggedright
    \textbf{\textit{Definition 1} \quad Sate-Value function adapted to constraint guides in AEC mode:}
    \begin{gather*}
      \text{\quad \quad} V^{\pi^j}(s_t) = \hspace{-0.75cm} \sum_{\textcolor{red}{ \substack{a_{t} \in A \text{ if } rn() < ch_{t}, \\ 
      a_{t} \in A_{t} \text{ else}}
      }}{\hspace{-0.7cm} \pi_i(a_{t} | \omega_t)} \sum_{s_{t+1} \in S}{\hspace{-0.1cm} T(s_{t+1} | s_t, a_{t})[R(s_t,a_{t},s_{t+1}) + \hspace{-0.1cm} \textcolor{blue}{ \sum_{m \in \mathcal{M}_i}{ \hspace{-0.1cm} v_m(t) \frac{grg_m(h_{t+1})}{1 - p + \epsilon} } } + \textcolor{red}{(1-ch_t) \times rrg(\omega_t,a_{t+1})} + V^{\pi^j_{i+1 \ mod \ n}}(s_{t+1})]}
    \end{gather*}  
    %
    \textcolor{red}{\[\text{With } rag(h_t, \omega_t) = A_{t} \times \mathbb{R} \text{, } (a_t, ch_{t}) \in A_{t} \times \mathbb{R} \text{ ; } \text{ and } rn: \emptyset \to [0,1[ \text{, a uniform random function}\]}
    %
    \vspace{-0.5cm}
    \textcolor{blue}{
    \begin{gather*}
    \text{With } \omega_t = O(\omega_t | s_t, a_t) \text{ ; } h_t = \{h_0 = \langle \rangle, h_{t+1} = \langle h_t, (\omega_{t+1}, a_{t+1}) \rangle \} \text{ ; } grg_m(h) = \hspace{-0.8cm} \sum_{(grg_i,w_i) \in mo(m)}{\hspace{-0.8cm} w_i \times grg_i(h)} \text{ ; } \epsilon \in \mathbb{R}_{>0} \text{ ; }
    \end{gather*}
    }
    \vspace{-0.75cm}
    \textcolor{blue}{
    \begin{gather*}
    v_m(t) = \{ 1 \text{ if } t \in t_c \text{ ; else } 0 \} \text{ ; } \text{ and } \mathcal{M}_i = \{m_j | (ar(i),m_j,t_c,p) \in \mathcal{M}\}
    \end{gather*}
    }
    \vspace{-0.6cm}
\end{figure*}

\subsection{Linking $\mathcal{M}OISE^+$ with MARL}

\begin{figure}[h!]
    \centering
    \input{figures/mm_synthesis_single_column.tex}
    \caption{A minimal view of the MOISE+MARL framework: 
    Users first define $\mathcal{M}OISE^+$ specifications, which include roles ($\mathcal{R}$) and associated with missions ($\mathcal{M}$) through $rds$. They then create MOISE+MARL specifications by first defining \textbf{Constraint guides} such as $rag$ and $rrg$ to specify role logic, and $grg$ for goal logic. 
    Next, \textbf{Linkers} are used to connect agents with roles through $ar$ and to link the logic of the constraint guides to the defined $\mathcal{M}OISE^+$ specifications. Once this is set up, roles can be assigned to agents, and the MARL framework updates accordingly during training.
    }
    \label{fig:mm_synthesis}
\end{figure}

We identified the \textit{AGR}~\cite{ferber2003} (Agent Group Role) and the $\mathcal{M}OISE^+$~\citep{Hubner2007} organizational models. Unlike AGR which is an informal framework introducing roles according to groups, $\mathcal{M}OISE^+$ provides a more detailed and flexible description of the structures and fonctions of a MAS, easing a formal description of agents' policies in MARL.

\noindent The \textbf{Constraint Guides} are three new relations introduced to describe the logic of the roles and objectives of $\mathcal{M}OISE^+$ in the Dec-POMDP formalism:
%
\begin{itemize}
    \item \textbf{Role Action Guide} \quad $rag: H \times \Omega \rightarrow \mathcal{P}(A \times \mathbb{R})$, the relation that models a role as a set of rules which, for each pair consisting of a history $h \in H$ and an observation received by the agent $\omega \in \Omega$, associates expected actions $A \in \mathcal{P}(A)$ each associated with a constraint hardness $ch \in [0,1]$ ($ch = 1$ by default). By restricting the choice of the next action among those authorized, the agent is forced to adhere to the expected behavior of the role
    \item \textbf{Role Reward Guide} \quad $rrg: H \times \Omega \times A \to \mathbb{R} = \{r_m \text{ if } a \notin A_\omega \text{, } rag(h, \omega) = A_\omega \times \mathbb{R} \text{, } h \in H; \text{ else } 0\}$, the relation that models a role by adding a penalty $r_m$ to the global reward if the last action chosen by the agent $a \in A$ is not authorized. This is intended to encourage the agent to adhere to the expected behavior of a role
    \item \textbf{Goal Reward Guide} \quad $grg: H \rightarrow \mathbb{R}$, the relation that models a goal as a soft constraint by adding a bonus $r_b \in \mathbb{R}$ to the global reward if the agent's history $h \in H$ contains a characteristic sub-sequence $h_g \in H_g$ of the goal, encouraging the agent to find a way to reach it.
\end{itemize}

\

\noindent Finally, we introduce the \textbf{Linkers} to link the $\mathcal{M}OISE^+$ organizational specifications with constraint guides and agents:
%
\begin{itemize}
    \item \textbf{Agent to Role} \quad $ar: \mathcal{A} \to \mathcal{R}$, the bijective relation linking an agent to a role;
    \item \textbf{Role to Constraint Guide} \quad $rcg: \mathcal{R} \rightarrow rag \cup rrg$, the relation associating each $\mathcal{M}OISE^+$ role to a $rag$ or $rrg$ relation, forcing/encouraging the agent to follow the expected actions for the role $\rho \in \mathcal{R}$;
    \item \textbf{Goal to Constraint Guide} \quad $gcg: \mathcal{G} \rightarrow grg$, the relation linking goals to trf relations, representing goals as rewards in MARL.
\end{itemize}

\paragraph{\textbf{Resolving the MOISE+MARL problem}}
% formalized as $MM = \langle D, \mathcal{OS}\allowbreak, ar, rcg, \allowbreak gcg, rag, rrg, grg\rangle$
involves finding a joint policy $\pi^{j} = \{\pi^j_0,\pi^j_1\dots\pi^j_n\}$ that maximizes the state-value function $V^{\pi^{j}}$ (or reaches a minimum threshold), which represents the expected cumulative reward starting from an initial state $s \in S$ and following the joint policy $\pi^{j}$, applying successive joint actions $a^{j} \in A^n$ under additional constraint guides. The state-value is described in the case where agents act sequentially and cyclically (Agent Environment Cycle - AEC mode) in \hyperref[eq:single_value_function]{Definition 1}, adapting its definition for roles (in red) and missions (in blue), impacting the action space and reward. \autoref{fig:mm_synthesis} illustrates the links between $\mathcal{M}OISE^+$ and Dec-POMDP via the MOISE+MARL framework.

At any time $t \in \mathbb{N}$ (initially $t = 0$), the agent $i = t \ mod \ n$ is constrained to a role $\rho_i = ar(i)$. For each temporally valid deontic specification $d_i = rds(\rho_i) = (tc_i,y_i, m_i)$, the agent is permitted (if $y_i = 0$) or obligated (if $y_i = 1$) to commit in mission $m_i \in \mathcal{M}, \mathcal{G}_{m_i} = mo(m_i)$, and $n \in \mathbb{N}$ the number of agents.
%
First, based on the received observation $\omega_t$, the agent must choose an action either: within the expected actions of the role $A_t$ if a random value is below the role constraint hardness $ch_t$; or within the set of all actions $A$ otherwise. If $ch_t = 1$, the role is strongly constrained for the agent and weakly otherwise.
%
Then, the action is applied to the current state $s_t$ to transition to the next state $s_{t+1}$, generate the next observation $\omega_{t+1}$, and yield a reward. The reward is the sum of the global reward with penalties and bonuses obtained from the organizational specifications: \quad i) the sum of the bonuses for objectives associated with each temporally valid mission (via Goal Reward Guides), weighted by the associated value ($\frac{1}{1-p+\epsilon}$); \quad ii) the penalty associated with the role (via "Role Reward Guides") weighted by the role constraint hardness.
%
Finally, the cumulative reward calculation continues in the next state $s_{t+1} \in S$ with the next agent $(i+1) \ mod \ n$.

\subsection{Easying constraint guides implementation}

In practice, since roles, objectives, and missions as simple labels, their definition is assumed. However, implementing a $rag$, $rrg$, or $grg$ relation requires defining a potentially large number of histories, possibly partially redundant. Therefore, an extensional definition of a set of histories can be tedious. Moreover, the logic of all constraint guides takes the agent trajectory as input to determine whether the trajectory belongs to a predefined history set. For example, a $rag$ relation can be seen as determining the next expected actions depending on whether the trajectory belongs to a given set and the new observation received.

A first approach, is to let users develop their constraint guides in an intensional way with a custom logic (such as a script code) in order to analyse history and compute the output in a manageable way. In that case, the relation $b_g: H \to \{0,1\}$ formalizes how users propose to determine whether a history belongs to a predefined set $H_g$.
To help implementing this relation, we propose a \textbf{Trajectory-based Pattern} (TP) inspired by Natural Language Processing, denoted $p \in P$, as a way to define a set of histories in an intensional way.

A TP implies that any considered real observation or action is known and mapped to a label $l \in L$ (through $l: \Omega \cup A \to L$) to be conveniently managed. A TP $p \in P$ is defined as follows: $p$ is: either a "leaf sequence" denoted as a couple of history-cardinality $s_l = (h, \{c_min,c_max\})$ (where $h \in H, c_{min} \in \mathbb{N}, c_{max} \in \mathbb{N} \cup "*")$; or a "node sequence" denoted as a couple of a tuple of concrete sequences and cardinality $s_n = (\langle s_{l_1}, s_{l_1}\dots \rangle, \{c_min,c_max\})$. For example, the pattern $p = \allowbreak "[o_1,a_1,[o_2,a_2](0,2)](1,*)"$ can be formalized as the node sequence $\allowbreak \langle ((o_1,a_1),(1,1)), ((o_2,a_2),(0,2))\rangle(1,"*")$, indicating the set of histories $H_p$ containing at least once the sub-sequence consisting of a first pair $(o_1,a_1)$ and then at most two repetitions of the pair $(o_2,a_2)$.
The relation $b_g$ then becomes $b_g(h) = m(p_g,h), \text{ with } m: P \times H \to \{0,1\}$ indicating if a history $h \in H$ matches a history pattern $p \in P$ describing a history set $H_g$.

\section{The TEMM Method}
\label{sec:TEMM_algorithm}
The TEMM method allows for automatic inference and evaluation of roles and missions based on observed behaviors over multiple simulation episodes. TEMM generates abstract roles and missions from agents' action histories and observations. The deviation of agents' behavior from these roles and missions serves as the basis for evaluating organizational fit.

\subsection{TEMM Method Steps}

We propose an evaluation method called \textbf{History-based Evaluation in MOISE+MARL} (TEMM). This method uses unsupervised learning techniques to generalize roles and missions from the set of observed behaviors over multiple test episodes. By measuring the gap between inferred abstract organizational specifications and actual behaviors, we can also quantify the organizational fit as how well a policy conforms to the inferred organizational specifications.

TEMM is based on proposed definitions for each $\mathcal{M}OISE^+$ organizational specification regarding joint-histories or other organizational specifications, using specific unsupervised learning techniques to infer them progressively. Here, we provide an informal description of the method\footnotemark[1].
%
\footnotetext[1]{additional details and developped code can be found in \url{https://github.com/REDACTED FOR BLIND REVIEW}}

\paragraph{\textbf{1) Inferring roles and their inheritance}}

We introduce that a role $\rho$ is defined as a policy whose associated agents' histories all contain a Longest Common Sequence (CLS). We introduce that a role $\rho_2$ inherits from $\rho_1$ if the CLS of histories associated with $\rho_2$ is also contained within that of $\rho_1$.
Based on these definitions, TEMM uses a "hierarchical clustering" technique to find the CLSs among agent histories. The results can be represented as a dendrogram. This allows inferring "abstract" roles and inheritance relationships, their respective relationships with histories, as well as current agents.
We measure the gap between current agents sequence and inferred "abstract" roles' sequences, as the "structural organizational fit".

\paragraph{\textbf{2) Inferring goals, plans, and missions}}

We introduce that a goal is a set of common joint-observation reached by following the histories of successful agents.
For each joint-history, TEMM calculates the joint-observation transition graph, which is then merged into a general graph. By measuring the distance between two vectorized joint-observations with K-means, we can find trajectory clusters that some agents may follow. Then, we sample some sets of joint-observations for each trajectory as "abstract" goals. For example, we can select the narrowest set of joint-observations where agents seem to collectively transition at a given time to reach their goal. Otherwise, balanced sampling on low variance trajectories could be performed. Knowing which trajectory an goal belongs to, TEMM infers plans based solely on choices and sequences.

We introduce that a mission is the set of goals that one or more agents are accomplishing.
Knowing the shared goals achieved by the agents, TEMM determines representative goal sets as missions.
By measuring the distance between infered "abstract" goals which joint-observations with current agents' joint-observation, we compute the "structural organizational fit".

\paragraph{\textbf{3) Inferring obligations and permissions}}

We introduce that an obligation is when an agent playing the role $\rho$ fulfills the goals of a mission and no others during certain time constraints, while permission is when the agent playing the role $\rho$ may fulfill other goals during specific time constraints.
TEMM determines which agents are associated with which mission and whether they are restricted to certain missions, making them obligations, or if they have permission.
Having already computed structural organizational fit and fonctional organizational fit, the organizational fit is the sum of these two values.

\

Overall, the K-mean and hierarchical clustering techniques require manual configuration to obtain roles and goals, avoiding introducing perturbations that could lead to determining false organizational specifications. Despite this, the method recommends thoroughly understanding the obtained roles and objectives to manually identify and remove any remaining perturbations.

\section{Experimental Framework}
\label{sec:experimental_setup}

This section details the experimental framework used to evaluate the MOISE+MARL framework. We adapted existing tools to implement our approach. We then present the environments used, the MARL algorithms, the organizational specifications, and the evaluation metrics.

\subsection{Implementing MOISE+MARL}

We have developped an implementation of the MOISE+MARL framework called \textquote{MMA} (MOISE+MARL API) as a Python API integrating all of its theoretical sets and relations trying to simplify user interactions to minimum without restricting the framework capabilities.

MMA implements the $\mathcal{M}OISE^+$ model following an Object-oriented view as nested data classes (with the "Moise" class a the root class) ready to be instantiated by users to define organizational specifications such as roles, goals to define missions, and then permissions or obligations.
%
To implement the Dec-POMDP, we favoured the \textit{PettingZoo} libray which is a standard API for multi-agent environments developed to facilitate interoperability between various environments. It is similar to Gymnasium~\cite{kwiatkowski2024} but specifically designed for multi-agent systems, with a focus on standardizing interfaces and environments \cite{terry2020pettingzoo}.

To implement the MOISE+MARL specifications, we have setted up the observation/action label mapping ($l$) as a dictionnary to be enriched by users. We also implemented the TP within the framework, converting collected histories into labeled sequences, allowing for the definition of patterns or applying simple pattern-matching techniques.

Then, we have setted up an abstract class for each relations in the constraint guides. For instance, users can instantiate a $rag$ with a a custom function or a list of JSON rules associating a (TP,last observation) couple to a list of expected actions. Similarly, $grg$ can be instantiated with a custom function or providing JSON rules associating a TP with a bonus reward value.
%
Once organizational specifications and constraint guides instantiated, a "MMA" global class enables integrating all of those by implementing the linkers relations. Users can indicate an agent name to a role ($ar$), a role to a $rrg$ or a $rag$, and a goal to a $grg$, and the previously instantied $\mathcal{M}OISE^+$ object.

Then, the "MMA" object is passed as an argument when encapsulating the environment with a "PettingZoo Wrapper". In this wrapper, actions can be restricted by masking and reward can be modified at each iteration. Thus, each agent can be constrained or guided by the corresponding organizational specifications during training.
%
MMA also integrates the \textit{MARLlib} library, a collection of MARL algorithms based on state-of-the-art approaches like \textit{MADDPG}, \textit{MAPPO}, and \textit{Q-Mix} \cite{hu2021marlib}.


\subsection{Environments Used}

We tested our MOISE+MARL framework in four distinct multi-agent environments. These environments were selected for their diversity in terms of collaboration, competition, and resource management. Here is a description of each:

\begin{itemize}
    \item \textbf{Predator-Prey}: A classic environment where several predators must cooperate to capture prey. This environment tests the agents' ability to coordinate their actions to achieve a collective goal\cite{lowe2017multi}

    \item \textbf{Overcooked-AI}: A team cooking game where several agents must collaborate to prepare and serve dishes in increasingly complex kitchens\cite{overcookedai}. Agents must manage tasks such as chopping, cooking, assembling, and serving ingredients while optimizing their movements and avoiding obstacles. This environment is ideal for testing coordination and task allocation in dynamic, highly interdependent scenarios, where clear roles (such as "chef," "assistant," "server") can be defined via organizational specifications
    
    \item \textbf{Warehouse Management}: A proposed environment, where agents must manage a warehouse by coordinating resource deliveries to demand points. Roles and missions here influence agent specialization in specific tasks (transportation of products, inventory management)
    
    \item \textbf{Cyber-Defense Simulation}: A complex environment simulating network defense against cyberattacks. Agents must identify and counter threats while adhering to strict security rules, thus testing the robustness and safety of trained agents\cite{Maxwell2021}.
\end{itemize}

These environments are encapsulable in the PettingZoo API, enabling seamless integration with our MOISE+MARL implementation and facilitating the application of organizational specifications.

\subsection{MARL Algorithms Used}

We evaluated our framework with several MARL algorithms :
%
\begin{enumerate*}[label={\roman*)}, itemjoin={; \quad}]
    \item \textbf{MADDPG (Multi-Agent Deep Deterministic Policy Gradient)} \cite{lowe2017multi}: A centralized learning, decentralized execution algorithm, allowing each agent to have a deterministic policy while using global information during training
    
    \item \textbf{MAPPO (Multi-Agent Proximal Policy Optimization)} \cite{yu2021mappo}: An adapted version of PPO for multi-agent systems, optimized for stable joint policy convergence in complex scenarios
    
    \item \textbf{Q-Mix} \cite{rashid2018qmix}: A Q-value-based algorithm that learns to combine individual agents' Q-values into a joint value to optimize cooperation
    
    \item \textbf{COMA) (Counterfactual Multi-Agent) } \cite{foerster2018counterfactual} An Actor-Critic algorithm able to estimate the impact of an individual agent's actions on the team's overall reward.

\end{enumerate*}

\subsection{Organizational Specifications}

For each environment, we defined a set of organizational specifications. These specifications include roles, missions, as well as permissions and obligations for each agent. Here, we give an informal description of these\footnotemark[1]:
%
\begin{enumerate}
    \item \textbf{Predator-Prey} (PP): Predator and prey roles are defined, with each predator having specific objectives such as "capture the prey" or "block escape routes."

    \item \textbf{Overcooked-AI} (OA): Agents adopt three main roles: chef, assistant, and server. The Chef is responsible for cooking and assembling dishes, the Assistant handles ingredient chopping and supply, while the Server is in charge of delivering dishes to customers. Missions primarily involve preparing and serving a certain number of dishes within a given time.
    
    \item \textbf{Warehouse Management} (WM): Agents adopt roles such as "transporter" and "inventory manager," with missions related to managing logistics flows and optimized delivery goals.
    
    \item \textbf{Cyber-Defense Simulation} (CD): Agents have network defender roles, each with obligations such as intrusion detection or protecting specific drone swarm ad hoc network segments.
\end{enumerate}

These specifications are used both to guide agent policies during training and to evaluate their organizational fit via the TEMM method after training.

\subsection{Computing Resources and Hyperparameters}

All experiments were executed on a high-performance computing cluster equipped with NVIDIA A100 GPUs and AMD EPYC 7742 CPUs. Each algorithm-environment combination was run on 5 parallel instances to ensure robust results. Hyperparameters for each algorithm, such as learning rates, discount factors, and exploration rates, were retrieved from MARLlib data banks or optimized for each environment via a grid search using the \textit{Optuna} tool.

We retrieved datasets containing all the hyperparameters used and the details of the organizational specifications (roles, missions, permissions, obligations). % These datasets are available in Appendix \ref{appendix:hyperparameters}.

\subsection{Evaluation Metrics}

To measure the effectiveness of the learned policies and the impact of organizational specifications, we defined the following metrics:
%
\begin{enumerate*}[label={\roman*)}, itemjoin={; \quad}]
    \item \textbf{Cumulative Reward}: The total rewards obtained by agents over each episode. It measures the overall effectiveness of policies in achieving environment goals.
    \item \textbf{Reward Standard Deviation}: The variability of rewards from one episode to another, indicating the stability of learned policies.
    \item \textbf{Convergence Rate}: The number of episodes required for an agent's policy to achieve stable performance, measured by the reduction of fluctuations in cumulative reward.
    \item \textbf{Constraint Violation Rate}: The number of agent actions that violate the permissions or obligations defined in the organizational specifications. This rate is crucial for assessing policy safety in critical environments like cyber defense simulation.
    \item \textbf{Consistency Score}: This score measures how well the organizational specifications applied to agents during training can be re-identified using the TEMM method. The higher this score, the more aligned agent behaviors are with specified roles and missions.
    \item \textbf{Robustness Score}: This score is calculated by combining the mean rewards and standard deviation over a series of heavily modified test episodes (modified initial states, introduction of perturbations). A high robustness score indicates that agents can maintain high performance in challenging conditions
    \item \textbf{Organizational Fit}: This is the quantitative value of the organizational fit previously defined. This value would increase with more organizational constraints.
\end{enumerate*}

These metrics provide a comprehensive overview of learned policies' performance, ranging from robustness to compliance with organizational specifications.

\section{Results}
\label{sec:results}

This section presents and analyzes the experimental results obtained from the configurations of environments.

\subsection{Performance: Cumulative Reward, Standard Deviation, and Organizational Fit}

We first examine the cumulative reward, reward standard deviation, and organizational fit across all environments and organizational conditions. Results for each algorithm are summarized in \autoref{tab:performance_results}.

\begin{table}[h!]
    \centering
    \caption{Performance Metrics by Algorithm and Organizational Condition (500 Episodes).}
    \label{tab:performance_results}
    \small
    \renewcommand{\arraystretch}{1.1}
    \begin{tabular}{p{1.4cm}p{0.5cm}p{1.cm}p{1.3cm}p{1.cm}p{1.cm}}
        \hline
        \textbf{Algorithm} & \textbf{Env.} & \textbf{Org. Spec.} & \textbf{Cumulative Reward} & \textbf{STD} & \textbf{Org. Fit} \\ \hline
        MADDPG & PP & Yes & 252.3 & 14.8 & 0.83 \\
        MADDPG & PP & No & 220.1 & 19.5 & 0.46 \\
        MAPPO & OA & Yes & 394.5 & 9.6 & 0.89 \\
        MAPPO & OA & No & 358.4 & 13.8 & 0.51 \\
        Q-Mix & WM & Yes & 302.7 & 16.3 & 0.84 \\
        Q-Mix & WM & No & 269.5 & 20.4 & 0.49 \\
        COMA & CD & Yes & 187.4 & 12.1 & 0.81 \\
        COMA & CD & No & 170.8 & 15.9 & 0.47 \\ \hline
    \end{tabular}
\end{table}

In the \textbf{Predator-Prey} environment, \textbf{MADDPG} with organizational specifications (Yes) achieved a cumulative reward of 252.3, significantly higher than the 220.1 achieved without organizational specifications (No). The organizational fit was also considerably higher with Yes (0.83) compared to No (0.46), indicating that predefined roles and missions enhance agent coordination and alignment with organizational roles.

In the collaborative \textbf{Overcooked-AI} environment, \textbf{MAPPO} demonstrated robust performance with a cumulative reward of 394.5 and an organizational fit of 0.89 under organizational specifications (Yes). The high organizational fit reflects strong alignment with roles, and the reduced standard deviation (9.6 vs. 13.8) highlights increased stability when agents follow structured missions.

For the \textbf{Warehouse Management} environment, \textbf{Q-Mix} yielded a cumulative reward of 302.7 with Yes, compared to 269.5 with No. The organizational fit score of 0.84 reinforces the value of mission-aligned agent behaviors in achieving optimal performance on resource management tasks.

Finally, in the \textbf{Cyber-Defense} environment, \textbf{COMA} showed improvement with organizational specifications (Yes), obtaining a cumulative reward of 187.4 and an organizational fit of 0.81. This confirms that explicit role guidance improves agent performance and alignment with predefined roles even in highly dynamic and complex environments.

\subsection{Analysis of Robustness, Convergence Rate, and Safety Metrics}

To further evaluate the impact of organizational specifications, we examined robustness, convergence rate, and constraint violation rate for each configuration. The results are summarized in \autoref{tab:robustness_results}.

\begin{table}[h!]
    \centering
    \caption{Analysis of Robustness, Convergence Rate, and Safety Metrics by Organizational Condition.}
    \label{tab:robustness_results}
    \small
    \renewcommand{\arraystretch}{1.1}
    \begin{tabular}{p{1.2cm}p{0.5cm}p{1.4cm}p{0.5cm}p{1.2cm}p{1.2cm}}
        \hline
        \textbf{Algorithm} & \textbf{Env.} & \textbf{Org. Spec.} & \textbf{Conv. Rate} & \textbf{Violation Rate} & \textbf{Robustness Score} \\ \hline
        MADDPG & PP & Yes & 0.88 & 5.2\% & 0.81 \\
        MADDPG & PP & No & 0.74 & 7.5\% & 0.72 \\
        MAPPO & OA & Yes & 0.93 & 4.0\% & 0.88 \\
        MAPPO & OA & No & 0.79 & 6.8\% & 0.69 \\
        Q-Mix & WM & Yes & 0.89 & 5.4\% & 0.85 \\
        Q-Mix & WM & No & 0.77 & 7.0\% & 0.66 \\
        COMA & CD & Yes & 0.87 & 5.7\% & 0.79 \\
        COMA & CD & No & 0.73 & 8.1\% & 0.68 \\ \hline
    \end{tabular}
\end{table}

Across all environments, agents with organizational specifications (Yes) consistently displayed higher \textbf{convergence rates} and \textbf{robustness scores}, especially in the \textbf{Overcooked-AI} and \textbf{Warehouse Management} environments. For instance, MAPPO in Overcooked-AI showed a convergence rate of 0.93 with Yes, compared to 0.79 with No. This faster convergence underscores the benefits of role and mission guidance, which enable agents to stabilize on effective policies more quickly.

The \textbf{robustness score} was also markedly higher under Yes conditions. In Overcooked-AI, MAPPO with organizational specifications (Yes) achieved a robustness score of 0.88 versus 0.69 with No. This trend held for Warehouse Management as well, where Q-Mix with Yes scored 0.85, compared to 0.66 with No, supporting the hypothesis that mission alignment enhances resilience against disturbances.

Regarding \textbf{constraint violations}, the use of organizational specifications (Yes) reduced violation rates in every tested environment, especially in the \textbf{Cyber-Defense} environment. For instance, COMA with Yes recorded a violation rate of 5.7\%, while the No configuration resulted in an 8.1\% violation rate. These results affirm that organizational guidance improves not only the agents' performance but also their adherence to safety constraints.

The \textbf{organizational fit} consistently yielded higher scores with organizational specifications (Yes), highlighting the benefits of structured guidance in aligning agents' behaviors with intended roles. Across all environments, particularly in Overcooked-AI and Warehouse Management, agents trained with Yes demonstrated superior coherence and alignment with predefined roles and missions. These findings illustrate the \textbf{MOISE+MARL} framework's effectiveness in not only enhancing performance but also promoting robustness, safety, and explainability, ultimately fostering more coordinated and resilient multi-agent systems in complex environments.


\subsection{Ablation Study: Comparison Between MOISE+MARL and AGR+MARL}

To understand the specific impact of intermediary goals on agent performance, we conducted an ablation study comparing the full \textbf{MOISE+MARL} framework (which includes both roles and goals) with a reduced framework \textbf{AGR+MARL}, where only roles are considered, and goals are not included. By excluding goals, the agents rely solely on role specifications to reach the global objective, thus lacking the guidance provided by intermediary goals.
The results are summarized in \autoref{tab:ablation_study}, showing key metrics across both frameworks.

\begin{table}[h!]
    \centering
    \caption{Ablation Study: Performance Comparison Between MOISE+MARL and AGR+MARL.}
    \label{tab:ablation_study}
    \small
    \renewcommand{\arraystretch}{1.1}
    \begin{tabular}{p{1.6cm}p{0.5cm}p{0.6cm}p{1.4cm}p{0.8cm}p{1.3cm}}
        \hline
        \textbf{Framework} & \textbf{Env.} & \textbf{Conv. Rate} & \textbf{Robustness Score} & \textbf{Org. Fit} & \textbf{Cumulative Reward} \\ \hline
        MOISE+MARL & PP & 0.88 & 0.81 & 0.83 & 252.3 \\
        AGR+MARL & PP & 0.76 & 0.68 & 0.52 & 220.6 \\
        MOISE+MARL & OA & 0.93 & 0.88 & 0.89 & 394.5 \\
        AGR+MARL & OA & 0.81 & 0.74 & 0.54 & 357.8 \\
        MOISE+MARL & WM & 0.89 & 0.85 & 0.84 & 302.7 \\
        AGR+MARL & WM & 0.78 & 0.70 & 0.55 & 271.4 \\ \hline
    \end{tabular}
\end{table}

The ablation study underscores the critical role of intermediary goals in enhancing multi-agent performance across various metrics, as seen in environments like Overcooked-AI and Warehouse Management, where the full MOISE+MARL framework consistently outperformed AGR+MARL. The presence of goals led to higher cumulative rewards, faster convergence rates, and improved robustness, with MAPPO achieving 394.5 versus 357.8 in Overcooked-AI and Q-Mix showing a robustness score of 0.85 compared to 0.70 in Warehouse Management. Notably, organizational fit was higher with goals, reflecting better alignment with the overall structure as agents followed clearer paths towards the global objective. Goals provided structured guidance, reducing trial and error, and delivering incremental feedback that stabilized learning, enabling agents to handle environmental disturbances more effectively. This study highlights the significant advantages of the MOISE+MARL framework, demonstrating that intermediary goals are essential for guiding agents toward efficient, coordinated behaviors.


\section{Conclusion and Future Work}
\label{sec:discussion_conclusion_future_work}

The MOISE+MARL framework introduced in this paper aims to enhance MARL by incorporating organizational models that define explicit roles and missions for agents. Experimental results across several structured environments indicate that this approach can improve agent coordination, safety, and alignment with intended organizational structures. In controlled scenarios like the Overcooked-AI and Predator-Prey environments, the framework facilitated better policy convergence and reduced constraint violations, suggesting that role and mission specifications can streamline the learning process by limiting the policy search space.

However, the framework's reliance on predefined organizational specifications means that it may struggle to adapt in highly dynamic or unstructured environments where agent roles and missions are less clear or evolve over time. This limitation could constrain the flexibility of agents to develop emergent behaviors or adapt to unforeseen changes in their environments. Moreover, the computational overhead associated with enforcing organizational constraints and dynamically modifying rewards raises questions about the framework's scalability, particularly in large-scale systems with many agents.
The scalability and explainability of the MOISE+MARL framework also warrant further exploration. While TEMM provides a method for evaluating organizational fit, it can be computationally intensive, which may hinder real-time applications or scenarios with a large number of agents.

Several directions for future research can be pursued to address these limitations:
%
\begin{enumerate*}[label={\roman*)}, itemjoin={; \quad}]
% \begin{enumerate}
    \item Developing adaptive mechanisms that allow roles and missions to evolve dynamically during training could enhance the framework's flexibility, enabling agents to respond to changes in real-time. This would be particularly valuable in scenarios where environmental conditions or agent objectives are subject to frequent change
    \item Exploring automated methods for generating organizational specifications based on observed agent behaviors could reduce the burden on users to define these specifications manually. Such techniques may include Large Language Model to propose roles and missions more fluidly, facilitating a more emergent and data-driven approach to organization
    \item Improving the computational efficiency of TEMM or exploring alternative evaluation methods could make the framework more practical for real-world applications with larger agent populations.
\end{enumerate*}


\section*{References}

\bibliographystyle{ACM-Reference-Format}
\renewcommand
\refname{}
\bibliography{references}

\end{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
