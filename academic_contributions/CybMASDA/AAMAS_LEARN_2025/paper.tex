%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% LaTeX Template for AAMAS-2025 (based on sample-sigconf.tex)
%%% Prepared by the AAMAS-2025 Program Chairs based on the version from AAMAS-2025. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% Start your document with the \documentclass command.

\RequirePackage[2020-02-02]{latexrelease}

%%%% For anonymized submission, use this
\documentclass[sigconf,anonymous]{aamas} 

%%%% For camera-ready, use this
%\documentclass[sigconf]{aamas}

\usepackage{listings}
% \usepackage{xcolor}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}
 
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}
 
\lstset{style=mystyle}

% --- Tickz
\usepackage{physics}
\usepackage{tikz}
\usepackage{amsmath}
\usepackage{mathdots}
% \usepackage{yhmath}
\usepackage{cancel}
\usepackage{color}
\usepackage{siunitx}
\usepackage{array}
\usepackage{multirow}
% \usepackage{amssymb}
\usepackage{gensymb}
\usepackage{tabularx}
\usepackage{extarrows}
\usepackage{booktabs}
\usetikzlibrary{fadings}
\usetikzlibrary{patterns}
\usetikzlibrary{shadows.blur}
\usetikzlibrary{shapes}

% ---------

\usepackage{balance} % for balancing columns on the final page
\usepackage{csquotes}
% \usepackage{cite}
\newcommand{\probP}{\text{I\kern-0.15em P}}
\usepackage{etoolbox}
\patchcmd{\thebibliography}{\section*{\refname}}{}{}{}
% \usepackage{amsthm,amssymb,amsfonts}

\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{color}
% \renewcommand\UrlFont{\color{blue}\rmfamily}

\usepackage[inline, shortlabels]{enumitem}
\usepackage{tabularx}
\usepackage{caption}
\usepackage{listings}
\usepackage{titlesec}
\usepackage{ragged2e}
% \usepackage[hyphens]{url}
\usepackage[linesnumbered,ruled,vlined]{algorithm2e}
\usepackage{float}
\usepackage[english]{babel}
\addto\extrasenglish{  
    \def\figureautorefname{Figure}
    \def\tableautorefname{Table}
    \def\algorithmautorefname{Algorithm}
    \def\sectionautorefname{Section}
    \def\subsectionautorefname{Subsection}
    \def\proofoutlineautorefname{Proof Outline}
}

\newcommand{\supertiny}{\fontsize{1}{2}\selectfont}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% AAMAS-2025 copyright block (do not change!)

\setcopyright{ifaamas}
\acmConference[AAMAS '25]{Proc.\@ of the 24th International Conference
on Autonomous Agents and Multiagent Systems (AAMAS 2025)}{May 19 -- 23, 2025}
{Detroit, Michigan, USA}{A.~El~Fallah~Seghrouchni, Y.~Vorobeychik, S.~Das, A.~Nowe (eds.)}
\copyrightyear{2025}
\acmYear{2025}
\acmDOI{}
\acmPrice{}
\acmISBN{}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% == IMPORTANT ==
%%% Use this command to specify your EasyChair submission number.
%%% In anonymous mode, it will be printed on the first page.

\acmSubmissionID{<<EasyChair submission id>>}

%%% Use this command to specify the title of your paper.

\title[AAMAS-2025 CybMASDE]{An Organizationally-Oriented Approach to Enhancing Explainability and Control in Multi-Agent Reinforcement Learning}

% Evaluating and Controlling Organization in Multi-Agent Learning

% Organizationally Constrained Multi-Agent Learning: A Framework for Safe and Interpretable Policies

%%% Provide names, affiliations, and email addresses for all authors.

\author{Julien Soulé}
\affiliation{
  \institution{Univ. Grenoble Alpes}
  \city{Valence}
  \country{France}}
\email{julien.soule@lcis.grenoble-inp.fr}

\author{Jean-Paul Jamont}
\affiliation{
  \institution{Univ. Grenoble Alpes}
  \city{Valence}
  \country{France}}
\email{jean-paul.jamont@lcis.grenoble-inp.fr}

\author{Michel Occello}
\affiliation{
  \institution{Univ. Grenoble Alpes}
  \city{Valence}
  \country{France}}
\email{michel.occello@lcis.grenoble-inp.fr}

\author{Louis-Marie Traonouez}
\affiliation{
  \institution{Thales Land and Air Systems, BU IAS}
  \city{Rennes}
  \country{France}}
\email{louis-marie.traonouez@thalesgroup.com}

\author{Paul Théron}
\affiliation{
  \institution{AICA IWG}
  \city{La Guillermie}
  \country{France}}
\email{paul.theron@orange.fr}


\begin{abstract}
    Multi-Agent Reinforcement Learning (MARL) can naturally lead to the development of efficient collaborative agent behaviors that show similarities with organizational concepts. Pushing forward this perspective, we introduce a novel framework that explicitly incorporates organizational roles and objectives from the $\mathcal{M}OISE^+$ model into the MARL process, guiding agents to satisfy corresponding organizational constraints. By structuring training with roles and objectives, we aim to enhance both the explainability and control of agent behaviors at the organizational level, whereas much of the literature primarily focuses on individual agents. Additionally, our framework includes a post-training analysis method to infer implicit roles and objectives, offering insights into emergent agent behaviors. This framework has been applied across various MARL environments and algorithms, demonstrating strong coherence between predefined organizational specifications and those inferred from trained agents.
\end{abstract}


% TODO: Mention the specific contributions related to the LEARN topics such as robustness, adaptability, and learning in constrained environments.

%%% The code below was generated by the tool at http://dl.acm.org/ccs.cfm.
%%% Please replace this example with code appropriate for your own paper.

%%% Use this command to specify a few keywords describing your work.
%%% Keywords should be separated by commas.

\keywords{  }

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% Include any author-defined commands here.
         
% \newcommand{\BibTeX}{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em\TeX}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

%%% The following commands remove the headers in your paper. For final 
%%% papers, these will be inserted during the pagination process.

\pagestyle{fancy}
\fancyhead{}

%%% The next command prints the information defined in the preamble.

\maketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Context
MARL can be considered as an approach to solve the problem of finding a joint policy that guides agents in achieving a given goal within a specific environment. 
This joint policy not only defines the individual actions of agents, but also governs their interactions with one another, or even with all other agents, without any preconceived notion of an explicitly predefined organizational order or structure. 

In social or collaborative problems, environmental constraints and the overall goal can lead agents to converge on optimal or satisfactory solutions in such a way that agents may exhibit recurring sets of similar behaviors across various testing episodes. 

These distinct sets can exhibit properties of specialization, complementarity, and stability that make them akin to potential implicit roles designated as "abstract." Furthermore, the trajectories of agents appearing to adopt these abstract roles may exhibit similarities, such as recurrent observations at the end of each episode. These similar parts of trajectories can be interpreted as the basis of "abstract" objectives, as it may seem that agents aim to follow them. 

More generally, we can interpret the behaviors of trained agents in relation to their proximity to the vision of a structured and functional organization. We call this similarity between the joint policy and a structured and functional organization \textbf{organizational fit}. 

Although this intuitive idea of resemblance between policy and structured and functional organization is merely an interpretation, this paper explores the interest in an organizational vision of trained MARL agents where most XAI work focuses on individual agent behaviors. 

Moreover, beyond environmental constraints and set objectives, adding organizational constraints can impact organizational fit. This paper explores this interest for control, explainability, and the operational safety of trained MAS. 
% This can be done by refining the roles and sub-objectives of the emerging organization from the joint policy.

\

% Problem
\noindent The question of the relevance of adopting an organizational vision of behaviors in MARL leads to two problems related to organizational fit.

\quad \textbf{i) Evaluation of Organizational Fit} aims to assess how closely a joint policy can be aligned with a structured and functional organization. One of the issues with this problem is to better understand cases where agents can be considered to form a structured and functional organization, given the constraints imposed by the environment, the objective, and other optional constraints.
% The literature addresses policy evaluation in terms of roles (structure) or objectives (function). However, these works often lack a systematic and general approach. Current methods offer few clear tools for quantitatively and qualitatively measuring this organizational fit.

\quad \textbf{ii) Controlling Organizational Fit} aims to guide agents toward policies that conform to a structured organization, defined by specific user-defined constraints.
% The goal is to constrain or incentivize agents to adopt behaviors that respect roles and missions, enabling control over their actions within an organizational framework.
Issues include reducing the policy search space, improving convergence, and ensuring compliance with safety constraints.
% Existing works in this field are limited, especially regarding how the user can interact with organizational specifications in a practical and flexible way.

\

% Contribution
\noindent We introduce the \textbf{MOISE+MARL} framework, a new MARL framework that integrates the $\mathcal{M}OISE^+$ organizational model into multi-agent learning. This framework formalizes organizational fit and addresses:

\quad \textbf{i)} Controlling organizational fit by introducing a data structure called \textbf{pattern-based decision tree} allowing users to manually define the roles and objectives of agents to apply to agents. The application takes the form of additional constraints that automatically affect both policies and the reward function.

\quad \textbf{ii)} Evaluating organizational fit by introducing the \textbf{History-based Evaluation in MOISE+MARL} (HEMM) method. This method uses unsupervised learning techniques to generalize roles and missions from observed behaviors across multiple test episodes. By measuring the gap between inferred abstract organizational specifications and actual behaviors, we define a new multidimensional metric, the \textbf{level of organizational fit}, which quantifies the degree to which a policy aligns with inferred organizational specifications.

\

% TODO : Adapt the algorithms with those from MARLlib (https://marllib.readthedocs.io/en/latest/algorithm/ppo_family.html#ippo)

% Evaluation & Findings
\noindent We evaluated the MOISE+MARL framework by testing:
\begin{enumerate*}[label={\roman*) },itemjoin={; \quad}]
  \item Four environments with different environmental constraints and objectives, some of which are expected to have joint policies that are either close to or distant from organizationally fit policies. % These environments are: overcooked, predator-prey, warehouse management, and ant simulation.
  \item Two policy-based MARL algorithms
  % (MAPPO)
  known for promoting stable convergence, one actor-critic algorithm
  % (MADDPG)
  , two value-based algorithms
  % (DQN, Q-Mix)
  and one model-based algorithm
  % (Dyna-Q)
  promoting long-term performance.
  \item A set of organizational specifications for each environment to constrain agents to conform to predefined behaviors or allow them more freedom.
\end{enumerate*}


We first verified that the level of organizational fit obtained with HEMM in environments where the emergence of organizationally fit policies is expected is indeed higher than in other environments. The roles and missions identified by HEMM match manual observations, confirming that this method can detect and validate the emergence of implicit organizational structures in agent behaviors.

Results also show that actor-critic algorithms
% , such as MADDPG,
are particularly suited to converge agents toward stable policies. This stability allows agents to maintain consistent and coherent behaviors in each episode, which is essential for ensuring a robust organization. Conversely, value-based algorithms
%, such as Q-Mix,
show greater variability in agent behaviors, even though overall performance remains high.

Finally, we verify that applying manually predefined organizational specifications significantly increases the level of organizational fit calculated by HEMM. The roles and missions inferred via HEMM closely align with the predefined specifications applied. This demonstrates the internal consistency of MOISE+MARL, as the changes introduced in policies via organizational specifications are faithfully reflected by HEMM.

\

% Structure of the paper
The rest of the paper is organized as follows: \autoref{sec:related_works} explores the challenges of evaluating and controlling organizational fit in the literature. \autoref{sec:moise_marl_framework} introduces the MOISE+MARL framework. \autoref{sec:hemm_algorithm} describes the HEMM method. \autoref{sec:experimental_setup} describes the experimental protocol, particularly the environments and MARL algorithms. \autoref{sec:results} presents the experimental results. Finally, \autoref{sec:discussion_conclusion} discusses and concludes on the evaluation and control of organizational fit.

\section{Related Works}
\label{sec:related_works}

This section focuses on works related to organizational fit through the two introduced problems.
% Evaluating and controlling organizational fit is crucial to ensuring that agents respect roles and objectives, helping to explain the collective functioning of agents or ensuring safety and fostering coordination and sufficient performance in complex environments.

% The challenges of evaluating and controlling organizational fit arise in scenarios where agents must not only optimize individual behaviors but also conform to an implicit or explicit organizational structure.

\subsection{Evaluating Organizational Fit}
Evaluating organizational fit aims to determine how closely agent behaviors align with a predefined or emergent organization. This involves analyzing both the structure of agent roles and their functional alignment with collective goals.

\subsubsection{Emergence of Roles and Specialization}
Yang et al. \cite{yang2021role} demonstrate how roles naturally emerge in decentralized multi-agent systems, where agents exhibit recurring behavioral patterns that can be categorized as abstract roles. This emergent specialization can be quantitatively evaluated by measuring stability and consistency across multiple episodes.

Grover et al. \cite{grover2018role} introduce an unsupervised learning approach to detect roles based on agent interactions, providing a framework for evaluating role adherence and role consistency over time. However, this approach does not allow for the incorporation of organizational constraints or predefined structures.

\subsubsection{Organizational Coherence and Stability}
The organizational coherence of a multi-agent system can be evaluated by measuring the stability and complementarity of agent roles over time. Borsa et al. \cite{borsa2019constrained} propose using constrained reinforcement learning to impose stability in agent behavior, which indirectly contributes to maintaining organizational fit.

Stability metrics, such as those introduced by Liu et al. \cite{liu2021efficient}, evaluate policy similarity across test episodes, ensuring that agents exhibit consistent behavior aligned with organizational roles. However, most current methods are limited in their ability to explain why certain roles emerge and how they align with the broader organizational context.

\subsubsection{Explainability and Interpretability}
Explainability in MARL focuses on understanding agent behavior from the perspective of human observers. While traditional approaches to explainability focus on individual agents \cite{van2018explainable}, organizational fit requires evaluating the extent to which the joint policy reflects the intended organizational structure.

Van der Waa et al. \cite{van2020explainability} propose methods to explain collective behavior in multi-agent systems by linking agent actions to global goals. These methods provide valuable insights into overall system performance but lack the granularity needed to assess organizational roles and missions.

\subsection{Controlling Organizational Fit}
Controlling organizational fit involves guiding agents toward policies that conform to a predefined organizational structure, often by imposing constraints or incentives. Different techniques have been developed to ensure that agent policies respect organizational constraints.

\subsubsection{Constrained Policy Optimization (CPO)}
CPO, introduced by Achiam et al. \cite{achiam2017cpo}, is a constraint-based optimization method where policies are adjusted to respect safety constraints while maximizing rewards. Using trust regions to adjust policies, CPO ensures that agents do not violate critical constraints, which is essential in controlling organizational fit.

\subsubsection{Lagrange Multiplier-Based Methods}
Lagrange multiplier-based methods, as explored by Ray et al. \cite{ray2019benchmarking}, directly integrate constraints into the reward function. The Lagrange multiplier allows balancing reward maximization with the satisfaction of organizational constraints. This approach is particularly effective for managing roles where multiple objectives must be met simultaneously.

\subsubsection{Safe Exploration and Shielding}
Safe exploration is a technique that allows agents to explore new behaviors while respecting safety constraints. This is especially useful when agents must learn to navigate uncertain environments while maintaining compliance with organizational constraints. Garcia et al. \cite{garcia2015comprehensive} provide an overview of different techniques to ensure that agent exploration remains within safe limits while maximizing learning efficiency.

The concept of shielding, proposed by Alshiekh et al. \cite{alshiekh2018safe}, offers a dynamic method to modify learning policies to avoid dangerous actions. By supervising agent actions in real-time and blocking actions that violate safety constraints, shielding ensures that agents respect roles and missions defined within an organization. This approach is particularly useful in critical environments where safety is paramount, such as cyber-physical systems or robotics.

\subsubsection{Hierarchical Reinforcement Learning (HRL)}
HRL decomposes tasks into subtasks, thus aligning with the hierarchical structure of roles and missions within an organization. Ghavamzadeh et al. \cite{ghavamzadeh2006hrl} demonstrate how hierarchical policies can be used to guide agents toward coordinated behaviors, where each agent's role corresponds to a subtask within the overall goal.
Specifically, in cooperative HRL \cite{ghavamzadeh2006cooperative}, agents share information at the subtask level, enabling more effective coordination. This method aligns well with the concept of organizational fit, where roles are explicitly linked to specific organizational objectives.

\subsubsection{Controlling Communication and Coordination}
Controlling communication and coordination among agents is crucial to ensuring organizational fit, especially in large-scale systems. Foerster et al. \cite{foerster2018communication} propose using common knowledge to enable decentralized coordination, allowing agents to act without centralized control.

\autoref{tab:related_work} summarizes the key properties of the discussed works and shows how they address (or do not address) different aspects of organizational fit.

\begin{table}[ht]
    \centering
    \small
    \renewcommand{\arraystretch}{1.3}
    \caption{Comparison of Related Works on Organizational Fit in MARL}
    \label{tab:related_work}
    \begin{tabular}{m{2.1cm} >{\centering\arraybackslash}m{0.8cm} >{\centering\arraybackslash}m{0.5cm} >{\centering\arraybackslash}m{0.8cm} >{\centering\arraybackslash}m{0.5cm} >{\centering\arraybackslash}m{0.5cm} >{\centering\arraybackslash}m{0.8cm}}

        \textbf{Property} & \textbf{\cite{yang2021role}} & \textbf{\cite{grover2018role}} & \textbf{\cite{borsa2019constrained}} & \textbf{\cite{zhang2020safemarl}} & \textbf{\cite{ghavamzadeh2006hrl}} & \textbf{\cite{foerster2018communication}} \\
        \hline \vspace{0.2cm}
        \textbf{Role Emergence} & Yes & Yes & Partial & No & No & No \\
        \textbf{Organizational Stability} & Partial & Yes & Yes & Yes & Yes & Yes \\
        \textbf{Constrained Learning} & No & No & Yes & Yes & No & Partial \\
        \textbf{Hierarchical Control} & No & No & No & No & Yes & No \\
        \textbf{Explainability} & Limited & No & No & No & No & Yes \\
        \textbf{Scalability} & No & No & Partial & Yes & Yes & Yes
    \end{tabular}%
\end{table}

\noindent From this analysis, we address the following challenges:
\begin{itemize*}[label={}, itemjoin={;\quad}]
    \item \textbf{Scalability}: Current methods struggle to scale to a large number of agents, particularly in dynamic environments where roles and constraints may change
    \item \textbf{Dynamic Adaptation}: Few frameworks support dynamic adaptation to new organizational structures, limiting their applicability in environments with evolving objectives
    \item \textbf{Explainability}: Most explainability frameworks focus on individual behaviors, lacking ways to explain organizational structures.
\end{itemize*}

Our MOISE+MARL framework aims to better address these challenges by providing lightweight means for dynamic control of organization structures and functions rather than the agents themselves. It also provides ways to evaluate organizational fit of policies and thus explain the collective functioning of agents.

\section{The MOISE+MARL Framework}
\label{sec:moise_marl_framework}

This section first introduces the formalism used to describe the functioning framework of MARL. Then, we introduce the MOISE+MARL framework by presenting the $\mathcal{M}OISE^+$ organizational specifications and linking them to the MARL framework.

\subsection{Markov Framework for MARL}

To apply MARL techniques, we rely on the \textit{Decentralized Partially Observable Markov Decision Process} (Dec-POMDP)~\citep{Oliehoek2016}. This model is well-suited for MAS, as it models multiple agents in an uncertain environment. Unlike \textit{Partially Observable Stochastic Games} (POSG), the Dec-POMDP allows for a common reward function for agents, which promotes learning of collaborative actions~\citep{Beynier2013}.

\

A Dec-POMDP $d \in D$ (where $D$ is the set of Dec-POMDPs) is defined as a 7-tuple $d = (S, \{A_i\}, T, R, \{\Omega_i\}, O, \gamma)$, where:
%
\begin{itemize*}[label={},itemjoin={; \quad}]
    \item $S = \{s_1,...,s_{|S|}\}$: the set of possible states;
    \item $A_{i} = \{a_{1}^{i},...,a_{|A_{i}|}^{i}\}$: the set of possible actions for agent $i$;
    \item $T$: the set of transition probabilities, where $T(s,a,s') = \probP(s'|s,a)$ represents the probability of transitioning from state $s$ to state $s'$ following action $a$;
    \item $R: S \times A \times S \rightarrow \mathbb{R}$: the reward function, which assigns a reward based on the initial state, the action taken, and the resulting state;
    \item $\Omega_{i} = \{o_{1}^{i},...,o_{|\Omega_{i}|}^{i}\}$: the set of possible observations for agent $i$;
    \item $O$: the set of observation probabilities, where $O(s',a,o) = \probP(o|s',a)$ represents the probability of obtaining observation $o$ after performing action $a$ and reaching state $s'$;
    \item $\gamma \in [0,1]$: the discount factor, used to weight future rewards.
\end{itemize*}

Consider a set of $m$ \textbf{teams} (or \textbf{groups}), each team consisting of several agents chosen from the set of agents $\mathcal{A}$. The following formalism describes how to solve a Dec-POMDP for a given team $i$ (with $0 \leq i \leq m$) containing $n$ agents~\citep{Beynier2013,Albrecht2024}:

\begin{itemize*}[label={},itemjoin={; \quad}]
    \item $\Pi$: the set of \textbf{policies}. A policy $\pi \in \Pi, \pi: \Omega \rightarrow A$ deterministically maps an observation to an action. It represents the agent's internal strategy;
    \item $\Pi_{joint}$: the set of \textbf{joint policies}. A joint policy $\pi_{joint} \in \Pi_{joint}, \pi_{joint}: \Omega^n \rightarrow A^n = \Pi^n$ chooses an action for each agent based on their respective observations. This joint policy can be seen as a set of policies used by agents within a team;
    \item $H$: the set of \textbf{histories}. A history over $z \in \mathbb{N}$ steps (where $z$ is typically the maximum number of steps for an episode) is the $z$-tuple $h = ((\omega_{k}, a_{k}) | k \leq z, \omega \in \Omega, a \in A)$, representing successive observations and actions;
    \item $H_{joint}$: the set of \textbf{joint histories}. A joint history $h_{joint} \in H_{joint}$ over $z$ steps is defined as the set of agent histories: $h_{joint} = \{h_1, h_2, ..., h_n\}$;
    \item $V_{joint,i}(<\pi_{joint,i}, \pi_{joint,-i}>): \Pi_{joint} \rightarrow \mathbb{R}$: the \textbf{expected cumulative reward} over a finite horizon (if $\gamma < 1$ or if the number of steps in an episode is finite), where $\pi_{joint,i}$ represents the joint policy for team $i$, and $\pi_{joint,-i}$ the joint policies of other teams, considered as fixed
\end{itemize*}

We refer to \textbf{solving the Dec-POMDP} for team $i$ as the search for a joint policy $\pi_{joint,i} \in \Pi_{joint}$ such that $\pi_{joint,i}s)$, achieving at least an expected cumulative reward of $s$, where $s \in \mathbb{R}$.



\subsection{The $\mathcal{M}OISE^+$ Organizational model}

$\mathcal{M}OISE^+$~\citep{Hubner2007} provides a relevant high-level description of the structures and interactions within the MAS. We favor $\mathcal{M}OISE^+$ because it provides an advanced formal description for an organization without incompatibilities with MARL, especially for a formal description of agents' policies.
As illustrated in \ref{fig:moise_model}, $\mathcal{M}OISE^+$ comprises three types of organizational specifications:

\noindent \paragraph{\textbf{Structural Specifications (SS)}} define the structured means for agents to achieve goals, denoted as $\mathcal{SS} = \langle \mathcal{R}, \mathcal{IR}, \mathcal{G} \rangle$. Here, $\mathcal{R}_{ss}$ is the set of all roles (denoted $\rho \in \mathcal{R}$), with an inheritance relation $\mathcal{IR}: \mathcal{R} \rightarrow \mathcal{R}$ (where $\mathcal{IR}(\rho_1) = \rho_2$ implies $\rho_1$ inherits from $\rho_2$, or $\rho_1 \sqsubset \rho_2$). $\mathcal{RG}$ represents the set of root groups, and $\mathcal{GR}$ is the set of all groups $\langle \mathcal{R}, \mathcal{SG}, \mathcal{L}^{intra}, \mathcal{L}^{inter}, \mathcal{C}^{intra}, \mathcal{C}^{inter}, np, ng \rangle$, where $\mathcal{R}$ includes non-abstract roles, $\mathcal{SG}$ contains sub-groups, and $\mathcal{L}$ denotes the set of links $(\rho_s,\rho_d,t)$ between roles ($link(\rho_s,\rho_d,t)$). The link type $t \in {acq, com, aut}$ indicates acquaintance (where $\rho_s$ agents can recognize $\rho_d$ agents), communication (where $\rho_s$ agents can communicate with $\rho_d$ agents), or authority (where $\rho_s$ agents have authority over $\rho_d$ agents, requiring acquaintance and communication links). Links are categorized into intra-group ($\mathcal{L}^{intra}$) and inter-group ($\mathcal{L}^{inter}$) links. Additionally, $\mathcal{C}$ defines compatibilities $(\rho_a, \rho_b)$ (or $\rho_a \bowtie \rho_b$), indicating that agents playing role $\rho_a$ can also play role $\rho_b$, with $\mathcal{C}^{intra}$ and $\mathcal{C}^{inter}$ being the intra- and inter-group compatibilities, respectively. Finally, $np$ and $ng$ specify the agent cardinalities for roles and sub-groups, respectively.

\noindent \paragraph{\textbf{Functional Specifications (FS)}} describe the tasks and goals agents must achieve, denoted as $\mathcal{FS} = \langle \mathcal{SCH}, \mathcal{PO} \rangle$. The social scheme $\mathcal{SCH}$ includes $\langle\mathcal{G}, \mathcal{M}, \mathcal{P}, mo, nm \rangle$, where $\mathcal{G}$ is the global goal set, $\mathcal{M}$ is the set of mission labels, and $\mathcal{P}$ represents the plans that structure goals in a tree format. Each plan $p = (g_f, {g_i}_{0 \leq i \leq s}, op, ps)$ links a final goal $g_f \in \mathcal{G}$ with sub-goals $g_i \in \mathcal{G}$, an operator $op \in {sequence, choice, parallel}$ (defining sequential, optional, or parallel completion), and a success probability $ps$. $mo: \mathcal{M} \rightarrow \mathbb{P}(\mathcal{G})$ associates missions with goal sets, while $nm: \mathcal{M} \rightarrow \mathbb{N} \times \mathbb{N}$ specifies agent cardinalities for missions. The preference orders $\mathcal{PO}$ contain pairs $(m_1, m_2)$ or $m_1 \prec m_2$, meaning that agents prefer mission $m_1$ over $m_2$ when both are feasible.

\noindent \paragraph{\textbf{Deontic Specifications (DS)}} outline how structural specifications should be used to achieve functional specifications, represented as $\mathcal{DS} = \langle \mathcal{OBL}, \mathcal{PER} \rangle$. $\mathcal{TC}$ defines time constraints $tc \in \mathcal{TC}$, where each constraint specifies periods for the validity of permissions or obligations ($Any$ signifies any time). Obligations $\mathcal{OBL}: \mathcal{R} \cross \mathcal{M} \cross \mathcal{TC}$ are triples $(\rho_a, m, tc)$, or $obl(\rho_a, m, tc)$, which require an agent in role $\rho_a \in \mathcal{R}$ to commit to mission $m \in \mathcal{M}$ during time constraint $tc$. Permissions $\mathcal{PER}$ similarly denote permissions as $(\rho_a, m, tc)$ or $per(\rho_a, m, tc)$, indicating that an agent in role $\rho_a$ may commit to mission $m$ within the specified period.

\begin{figure}[!]
    \input{figures/moise_model.tex}
    \caption{A synthetic view of the $\mathcal{M}OISE^+$ model}
    \label{fig:moise_model}
\end{figure}

\

An observation we made is that organizational specifications applied to agents are roles and goals (as missions) through permissions or obligations. Indeed, the other structural specifications such as compatibilities or links are inherent to roles. Similarly, we consider that the goals, the missions and their mapping ($mo$) are enough to also link all of the other fonctional specifications such as plans, cardinalities or preference orders.
Consequently, we consider it is sufficient to take into account roles, missions (goal and mapping) and permissions/obligations when integrating $\mathcal{M}OISE^+$ into Dec-POMDP. 

\subsection{Integrating $\mathcal{M}OISE^+$ into MARL}

In this section, we progressively introduce the principles we propose to adapt MARL based on organizational specifications.

To our knowledge, $\mathcal{M}OISE^+$ is the only organizational model that sufficiently formalizes agent policy capabilities to be easily compatible with MARL. For example, the \textit{AGR} (\textit{Agent Group Role}) model is limited to the concepts of role and group and does not prescribe a form that these should take in MARL. The $\mathcal{M}OISE^+$ model provides a comprehensive framework for specifying the structure and functions of an MAS.

\begin{figure*}[h!]
    \centering
    \input{figures/mm_synthesis.tex}
    \caption{A synthetic view of the MOISE+MARL framework: Users define $\mathcal{M}OISE^+$ specifications (such as roles $\mathcal{R}$ and missions $\mathcal{M}$). Then, users create MOISE+MARL specifications to develop the organizational specifications logic as "Constraint Guides" and link them to previously defined $\mathcal{M}OISE^+$ specifications. First, users create "Constraint Guides" such as $rag$, $rrg$ to define roles logic, and $grg$ to define the logic of mission's goals. Then, "Linkers" relations are used so agents be associated to roles (through $ar$), and "Constraint Guides"' logic be associated to previously defined $\mathcal{M}OISE^+$. After establishing MOISE+MARL Specifications, the MARL framework is automatically updated to take into account predefined roles and missions. Association of role to agents can be changed through $ar$ and association of mission to agents can be changed through deontic specifications.}
    \label{fig:mm_synthesis}
  \end{figure*}
  
  \begin{figure*}[h!]
    \label{eq:single_value_function}
    \raggedright
    \textbf{\quad Definition 1: Value function adapted to "Constraint Guides" in AEC.}
    \begin{gather*}
      \text{\quad \quad} V^{\pi^j}(s_t) = \hspace{-0.75cm} \sum_{\textcolor{red}{ \substack{a_{t} \in A \text{ if } rn() < ch_{t}, \\ 
      a_{t} \in A_{t} \text{ else}}
      }}{\hspace{-0.7cm} \pi_i(a_{t} | \omega_t)} \sum_{s_{t+1} \in S}{\hspace{-0.1cm} T(s_{t+1} | s_t, a_{t})[R(s_t,a_{t},s_{t+1}) + \hspace{-0.1cm} \textcolor{blue}{ \sum_{m \in \mathcal{M}_i}{ \hspace{-0.1cm} v_m(t) \frac{grg_m(h_{t+1})}{1 - p + \epsilon} } } + \textcolor{red}{ch_t \times rrg(\omega_t,a_{t+1})} + V^{\pi^j_{i+1 \ mod \ N}}(s_{t+1})]}
    \end{gather*}  
    %
    \textcolor{red}{\[\text{With } rag(h_t, \omega_t) = A_{t} \times \mathbb{R} \text{, } (a_t, ch_{t}) \in A_{t} \times \mathbb{R} \text{ ; } \text{ and } rn: \emptyset \to [0,1[ \text{, a uniform random function}\]}
    %
    \vspace{-0.5cm}
    \textcolor{blue}{
    \begin{gather*}
    \text{With } \omega_t = O(\omega_t | s_t, a_t) \text{ ; } h_t = \{h_0 = \langle \rangle, h_{t+1} = \langle h_t, (\omega_{t+1}, a_{t+1}) \rangle \} \text{ ; } grg_m(h) = \hspace{-0.8cm} \sum_{(grg_i,w_i) \in mo(m)}{\hspace{-0.8cm} w_i \times grg_i(h)} \text{ ; } \epsilon \in \mathbb{R}_{>0} \text{ ; }
    \end{gather*}
    }
    \vspace{-0.75cm}
    \textcolor{blue}{
    \begin{gather*}
    v_m(t) = \{ 1 \text{ if } t \in t_c \text{ ; else } 0 \} \text{ ; } \text{ and } \mathcal{M}_i = \{m_j | (ar(i),m_j,t_c,p) \in \mathcal{M}\}
    \end{gather*}
    }
    \vspace{-0.6cm}
    \end{figure*}

The \textbf{Constraint Guides} \quad are three new relations introduced to describe the logic of the roles and objectives of $\mathcal{M}OISE^+$ in the Dec-POMDP formalism:

\begin{itemize}
    \item \textbf{Role Action Guide} (RAG) \quad $rag: H \times \Omega \rightarrow \mathcal{P}(A \times \mathbb{R})$, the relation that models a role as a set of rules which, for each pair consisting of a history $h \in H$ and an observation received by the agent $\omega \in \Omega$, associates expected actions $A \in \mathcal{P}(A)$ each associated with a constraint hardness $ch \in [0,1]$ ($ch = 1$ by default). By restricting the choice of the next action among those authorized, the agent is forced to adhere to the expected behavior of the role.
    \item \textbf{Role Reward Guide} (RRG) \quad $rrg: H \times \Omega \times A \to \mathbb{R} = \{r_m \text{ if } a \notin A_\omega \text{, } rag(h, \omega) = A_\omega \times \mathbb{R} \text{, } h \in H; \text{ else } 0\}$, the relation that models a role by adding a penalty $r_m$ to the global reward if the last action chosen by the agent $a \in A$ is not authorized. This is intended to encourage the agent to adhere to the expected behavior of a role.
    \item \textbf{Goal Reward Guide} (GRG) \quad $grg: H \rightarrow \mathbb{R}$, the relation that models a goal as a soft constraint by adding a bonus $r_b \in \mathbb{R}$ to the global reward if the agent's history $h \in H$ contains a characteristic sub-sequence $h_g \in H_g$ of the goal, encouraging the agent to find a way to reach it.
\end{itemize}

Finally, to link the organizational specifications of $\mathcal{M}OISE^+$ with the "Constraint Guides" and agents, we introduce the following \textbf{Linkers}:
\begin{itemize}
    \item \textbf{Agent to Role} (AR) \quad $ar: \mathcal{A} \to \mathcal{R}$, the bijective relation linking an agent to a role;
    \item \textbf{Role to Constraint Guide} (RCG) \quad $rcg: \mathcal{R} \rightarrow rag \cup rrg$, the relation associating each $\mathcal{M}OISE^+$ role to a $rag$ or $rrg$ relation, forcing/encouraging the agent to follow the expected actions for the role $\rho \in \mathcal{R}$;
    \item \textbf{Goal to Constraint Guide} (GCG) \quad $gcg: \mathcal{G} \rightarrow grg$, the relation linking goals to trf relations, representing goals as rewards in MARL.
\end{itemize}

MOISE+MARL is thus defined by the tuple $MM = \langle D, \mathcal{OS}, ar, rcg, \allowbreak gcg, rag, rrg, grg\rangle$. Solving the problem described by MOISE+MARL involves finding a joint policy $\pi^{j}: \Omega^{N} \to A^{N} = \{\pi^j_0,\pi^j_1\dots\pi^j_N\}$ that maximizes the value function $V^{\pi^{j}}$ (or reaches a minimum threshold), which represents the expected cumulative reward starting from an initial state $s \in S$ and following the joint policy $\pi^{j}$, applying successive joint actions $a^{j} \in A^N$ under additional "Constraint Guides" (including "Linkers"). Agents then each follow a trajectory (also called history) $h \in H, h = \langle(o_0,a_0), (o_1,a_1)\dots\rangle$. The value function to maximize (or reach a minimum threshold) is described in cases where agents act sequentially (Agent Environment Cycle - AEC mode) in \hyperref[eq:single_value_function]{Definition 1}, adapting its definition for roles (in red) and missions (in blue), impacting the action space and reward. \autoref{fig:mm_synthesis} illustrates the links between $\mathcal{M}OISE^+$ and Dec-POMDP via the MOISE+MARL framework.

At any time $t \in \mathbb{N}$ (initially $t = 0$), the agent $i = t \ mod \ N$ is constrained to a role $\rho = ar(i)$. For each temporally valid deontic specification $d_i = (\rho,m_i,t_{c_i},p_i)$ (such that $v_{m_i}(t) = t \in t_{c_i}$), the agent is permitted (if $p_i = 0$) or obligated (if $p_i = 1$) to engage in mission $m_i \in \mathcal{M}, \mathcal{G}_{m_i} = mo(m_i), n \in \mathbb{N}$.
%
First, based on the received observation $\omega_t$, the agent must choose an action either: within the expected actions of the role $A_t$ if a random value is below the role constraint hardness $ch_t$; or within the set of all actions $A$ otherwise. If $ch_t = 1$, the role is strongly constrained for the agent and weakly otherwise.
%
Then, the action is applied to the current state $s_t$ to transition to the next state $s_{t+1}$, generate the next observation $\omega_{t+1}$, and yield a reward. The reward is the sum of the global reward with penalties and bonuses obtained from the organizational specifications: \quad i) the sum of the bonuses for objectives associated with each temporally valid mission (via "Goal Reward Guides"), weighted by the associated value ($\frac{1}{1-p+\epsilon}$); \quad ii) the penalty associated with the role (via "Role Reward Guides") weighted by the role constraint hardness.

Finally, the cumulative reward calculation continues in the next state $s_{t+1} \in S$ with the next agent $(i+1) \ mod \ N$.

\section{The HEMM Method}
\label{sec:hemm_algorithm}
The HEMM method allows for automatic inference and evaluation of roles and missions based on observed behaviors over multiple simulation episodes. HEMM generates abstract roles and missions from agents' action histories and observations. The deviation of agents' behavior from these roles and missions serves as the basis for evaluating organizational fit.

\subsection{HEMM Method Steps}

% \begin{figure*}[h!]
%     \centering
%     \input{figures/hemm_illustrative_view copy.tex}
%     \caption{HEMM Illustrative View}
%     \label{fig:HEMM_illustrative_view}
% \end{figure*}

We propose an evaluation method called \textbf{History-based Evaluation in MOISE+MARL} (HEMM). This method uses unsupervised learning techniques to generalize roles and missions from the set of observed behaviors over multiple test episodes. By measuring the gap between inferred abstract organizational specifications and actual behaviors, we can also quantify the organizational fit as how well a policy conforms to the inferred organizational specifications.

HEMM is based on proposed definitions for each $\mathcal{M}OISE^+$ organizational specification regarding joint histories or other organizational specifications, using specific unsupervised learning techniques to infer them progressively.

\paragraph{\textbf{1) Inferring roles and their inheritance}}

We introduce that a role $\rho$ is defined as a policy whose associated agents' histories all contain a common discontinuous sequence. We introduce that a role $\rho_2$ inherits from $\rho_1$ if the common discontinuous sequence of histories associated with $\rho_2$ is also contained within that of $\rho_1$.
Based on these definitions, HEMM uses a "hierarchical clustering" technique to find the longest common discontinuous sequences among agent histories. The results can be represented as a dendrogram. This allows inferring roles and inheritance relationships, their respective relationships with histories, as well as current agents.

\paragraph{\textbf{2) Inferring possible organizations}}

We introduce that an organization is linked to a unique set of all instantiable roles sharing closely similar inheritance relationships. Indeed, considering two trained joint policies $H_{joint,i,s,1}$ and $H_{joint,i,s,2}$, although both achieve an objective relying on roles $\mathcal{R}_{ss,1}$ and $\mathcal{R}_{ss,2}$, these roles may be very distant from each other. For example, their roles may not use the same distribution of responsibilities.
HEMM uses a K-means algorithm to obtain $q$ clusters of vectors $\mathcal{IR}_{i}$, considered as organizations. Roles within the same cluster share the K-means centroid inheritance relationships $\mathcal{IR}_j$. Indeed, they represent general roles adopted by agents within the same organization across similar joint histories.
For the following steps, only one chosen organization and its associated joint histories are considered.

\paragraph{\textbf{3) Inferring objectives, plans, and missions}}

We introduce that a sub-objective/objective is a set of common states reached by following the histories of successful agents.
For each joint history, HEMM calculates the state transition graph, which is then merged into a general graph. By measuring the distance between two vectorized states with K-means, we can find trajectory clusters that some agents may follow. Then, we sample some sets of states for each trajectory as objectives. For example, we can select the narrowest set of states where agents seem to collectively transition at a given time to reach their goal. Otherwise, balanced sampling on low variance trajectories could be performed. Knowing which trajectory an objective belongs to, HEMM infers plans based solely on choices and sequences.

This allows for obtaining goals and plans at the global state level, but these objectives can be effectively distributed into specific goals for each subgroup and agent. To do this, HEMM follows the same process by replacing states with observations of agents in the same subgroup for subgroups and agent observations for agents themselves.

We introduce that a mission is the set of sub-objectives that one or more agents are accomplishing.
Knowing the shared objectives achieved by the agents, HEMM determines representative objective sets as missions.

\paragraph{\textbf{4) Inferring obligations and permissions}}

We introduce that an obligation is when an agent playing the role $\rho$ fulfills the objectives of a mission and no others during certain time constraints, while permission is when the agent playing the role $\rho$ may fulfill other objectives during specific time constraints.
HEMM determines which agents are associated with which mission and whether they are restricted to certain missions, making them obligations, or if they have permission.

\

The K-mean and hierarchical clustering techniques require manual configuration to obtain roles and goals, avoiding introducing perturbations that could lead to determining false organizational specifications. Despite this, the method recommends thoroughly understanding the obtained roles and objectives to manually identify and remove any remaining perturbations.

\section{Experimental Framework}
\label{sec:experimental_setup}

This section details the experimental framework used to evaluate the MOISE+MARL framework. We adapted existing tools from the \textit{PettingZoo} API and the \textit{MARLlib} library to implement our approach. We then present the environments used, the MARL algorithms, the organizational specifications, the computing resources, and the metrics used to evaluate agent performance.

\subsection{Adapting PettingZoo and MARLlib}

\textit{PettingZoo} is a standard API for multi-agent environments developed to facilitate interoperability between various environments. It is similar to Gym but specifically designed for multi-agent systems, with a focus on standardizing interfaces and environments \cite{terry2020pettingzoo}. For our experimentation, we extended \textit{PettingZoo} to allow direct integration of MOISE+ organizational specifications.

The \textit{MARLlib} library is a collection of multi-agent reinforcement learning (\textit{MARL}) methods based on state-of-the-art approaches like \textit{MADDPG}, \textit{MAPPO}, and \textit{Q-Mix} \cite{hu2021marlib}. We modified this library to add a module allowing explicit management of organizational roles and objectives, facilitating their definition through a data structure called \textbf{Organizational Mapping}.

This data structure links organizational specifications from $\mathcal{M}OISE^+$ to agents in the environment. Users can define roles and objectives within the MOISE+ framework, then apply these specifications by passing them as arguments when encapsulating the environment in PettingZoo. Thus, each agent can be constrained or guided by the corresponding organizational specifications during training.

\subsection{Environments Used}

We tested our MOISE+MARL framework in four distinct multi-agent environments. These environments were selected for their diversity in terms of collaboration, competition, and resource management. Here is a description of each:

\begin{itemize}
    \item \textbf{Predator-Prey}: A classic environment where several predators must cooperate to capture a prey on a grid. This environment tests the agents' ability to coordinate their actions to achieve a collective goal.

    \item \textbf{Overcooked-AI}: A team cooking game where several agents must collaborate to prepare and serve dishes in increasingly complex kitchens \cite{overcookedai}. Agents must manage tasks such as chopping, cooking, assembling, and serving ingredients while optimizing their movements and avoiding obstacles. This environment is ideal for testing coordination and task allocation in dynamic, highly interdependent scenarios, where clear roles (such as "chef," "assistant," "server") can be defined via organizational specifications.
    
    % \item \textbf{Resource Gathering}: Agents must collect resources while avoiding conflicts. This environment is ideal for testing how organizational roles and objectives affect behaviors in resource-sharing scenarios with competitive and cooperative interactions.
    
    \item \textbf{Warehouse Management}: In this environment, agents must manage a warehouse by coordinating resource deliveries to demand points. Roles and missions here influence agent specialization in specific tasks (e.g., transportation, inventory management).
    
    \item \textbf{Cyber-Defense Simulation}: A complex environment simulating network defense against cyberattacks. Agents must identify and counter threats while adhering to strict security rules, thus testing the robustness and safety of trained agents.
\end{itemize}

These environments are encapsulable in the PettingZoo API, enabling seamless integration with our MOISE+MARL implementation and facilitating the application of organizational specifications.

\subsection{MARL Algorithms Used}

We evaluated our approach with several MARL algorithms to compare the performance of the MOISE+MARL framework to classical methods:

\begin{itemize}
    \item \textbf{MADDPG (Multi-Agent Deep Deterministic Policy Gradient)} \cite{lowe2017multi}: A centralized learning, decentralized execution algorithm, allowing each agent to have a deterministic policy while using global information during training.
    
    \item \textbf{MAPPO (Multi-Agent Proximal Policy Optimization)} \cite{yu2021mappo}: An adapted version of PPO for multi-agent systems, optimized for stable joint policy convergence in complex scenarios.
    
    \item \textbf{Q-Mix} \cite{rashid2018qmix}: A Q-value-based algorithm that learns to combine individual agents' Q-values into a joint value to optimize cooperation.
    
    \item \textbf{DQN (Deep Q-Network)} \cite{mnih2015dqn}: A modified version for multi-agent environments where each agent independently learns its Q-value-based policy.
\end{itemize}

These algorithms were integrated via the MARLlib library, modified to account for the Organizational Mapping and allow training under organizational constraints defined by MOISE+.

\subsection{Organizational Specifications}

For each environment, we defined a set of organizational specifications. These specifications include roles, missions, as well as permissions and obligations for each agent:

\begin{itemize}
    \item \textbf{Predator-Prey}: Predator and prey roles are defined, with each predator having specific objectives such as "capture the prey" or "block escape routes."
    
    % \item \textbf{Resource Gathering}: Agents are divided into gatherer and defender roles, each with specific permissions to access resources or protect key areas.

    \item \textbf{Overcooked-AI}: Agents adopt three main roles: chef, assistant, and server. The Chef is responsible for cooking and assembling dishes, the Assistant handles ingredient chopping and supply, while the Server is in charge of delivering dishes to customers. Missions primarily involve preparing and serving a certain number of dishes within a given time.
    
    \item \textbf{Warehouse Management}: Agents adopt roles such as "transporter" and "inventory manager," with missions related to managing logistics flows and optimized delivery goals.
    
    \item \textbf{Cyber-Defense Simulation}: Agents have network defender roles, each with obligations such as intrusion detection or protecting specific network segments.
\end{itemize}

These specifications are used both to guide agent policies during training and to evaluate their organizational fit via the HEMM method after training.

\subsection{Computing Resources and Hyperparameters}

All experiments were executed on a high-performance computing cluster equipped with NVIDIA A100 GPUs and AMD EPYC 7742 CPUs. Each algorithm-environment combination was run on 5 parallel instances to ensure robust results. Hyperparameters for each algorithm, such as learning rates, discount factors, and exploration rates, were retrieved from MARLlib data banks or optimized for each environment via a grid search using the \textit{Optuna} tool.

We retrieved datasets containing all the hyperparameters used and the details of the organizational specifications (roles, missions, permissions, obligations). % These datasets are available in Appendix \ref{appendix:hyperparameters}.

\subsection{Evaluation Metrics}

To measure the effectiveness of the learned policies and the impact of organizational specifications, we defined the following metrics:

\begin{itemize}
    \item \textbf{Cumulative Reward}: The total rewards obtained by agents over each episode. It measures the overall effectiveness of policies in achieving environment goals.
    
    \item \textbf{Reward Standard Deviation}: The variability of rewards from one episode to another, indicating the stability of learned policies.
    
    \item \textbf{Convergence Rate}: The number of episodes required for an agent's policy to achieve stable performance, measured by the reduction of fluctuations in cumulative reward.
    
    \item \textbf{Constraint Violation Rate}: The number of agent actions that violate the permissions or obligations defined in the organizational specifications. This rate is crucial for assessing policy safety in critical environments like cyber defense simulation.
    
    \item \textbf{Consistency Score}: This score measures how well the organizational specifications applied to agents during training can be re-identified using the HEMM method. The higher this score, the more aligned agent behaviors are with specified roles and missions.
    
    \item \textbf{Robustness Score}: This score is calculated by combining the mean rewards and standard deviation over a series of heavily modified test episodes (modified initial states, introduction of perturbations). A high robustness score indicates that agents can maintain high performance in challenging conditions.

\end{itemize}

These metrics provide a comprehensive overview of learned policies' performance, ranging from robustness to compliance with organizational specifications.

\section{Results}
\label{sec:results}

This section presents and analyzes the experimental results obtained from the various configurations of environments, MARL algorithms, and organizational specifications. The results are broken down into several sub-sections, each focusing on specific aspects such as overall performance and policy safety. Finally, we conduct an ablation study to assess the impact of missions within the MOISE+MARL framework.

\subsection{Performance: Cumulative Reward and Standard Deviation}

MARL algorithm performance by environment and organizational specification was primarily measured by two metrics: cumulative reward and reward standard deviation. The values obtained are summarized in \autoref{tab:performance_results}, which presents the mean cumulative rewards and standard deviation over a set of 500 episodes for each algorithm, environment, and organizational specification combination.

\begin{table*}[h!]
    \centering
    \caption{MARL Algorithm Performance by Cumulative Rewards and Reward Standard Deviations (500 Episodes).}
    \label{tab:performance_results}
    \begin{tabular}{|c|c|c|c|c|}
        \hline
        \textbf{Algorithm} & \textbf{Environment} & \textbf{Org. Spec.} & \textbf{Cumulative Reward} & \textbf{Standard Deviation} \\ \hline
        MADDPG & Predator-Prey & $\mathcal{M}OISE^+$ & 245.6 & 15.4 \\ \hline
        MADDPG & Predator-Prey & AGR & 215.8 & 22.1 \\ \hline
        MAPPO & Overcooked-AI & $\mathcal{M}OISE^+$ & 385.2 & 10.3 \\ \hline
        MAPPO & Overcooked-AI & AGR & 350.4 & 12.9 \\ \hline
        Q-Mix & Warehouse Management & $\mathcal{M}OISE^+$ & 295.1 & 17.8 \\ \hline
        Q-Mix & Warehouse Management & AGR & 265.3 & 20.5 \\ \hline
        DQN & Cyber-Defense & $\mathcal{M}OISE^+$ & 180.6 & 13.4 \\ \hline
        DQN & Cyber-Defense & AGR & 165.2 & 16.7 \\ \hline
    \end{tabular}
\end{table*}

In the \textbf{Predator-Prey} environment, the \textbf{MADDPG} algorithm coupled with $\mathcal{M}OISE^+$ shows a marked improvement over AGR, with a cumulative reward increase of 13.8%. This is mainly explained by the influence of pre-defined roles and missions in $\mathcal{M}OISE^+$ that enable better coordination among agents. The reduced standard deviation (15.4 vs. 22.1) also indicates better behavior stability.

In the collaborative \textbf{Overcooked-AI} environment, the \textbf{MAPPO} algorithm associated with $\mathcal{M}OISE^+$ displays superior performance, with an average cumulative reward of 385.2, compared to 350.4 for AGR. This difference can be attributed to the explicit task breakdown via MOISE+ missions, facilitating agent coordination to manage interdependent tasks such as chopping and cooking ingredients.

The \textbf{Warehouse Management} environment, evaluated with \textbf{Q-Mix}, highlights a similar gain in favor of MOISE+, with an 11.2% cumulative reward increase over AGR. Here, missions assigned to agents facilitate logistical flow management, thus improving overall performance.

Finally, in the \textbf{Cyber-Defense} environment, the \textbf{DQN} algorithm also shows an advantage with MOISE+, albeit less pronounced than in the other environments. The cumulative reward of 180.6 remains significantly higher than that obtained with AGR (165.2), confirming the effectiveness of missions in guiding agents in complex environments.

These results clearly show that the $\mathcal{M}OISE^+$ organizational specifications enable better performance in environments requiring high agent coordination by task decomposition and explicit role assignment.

\subsection{Analysis of Other Metrics: Robustness, Safety, and Convergence}

Besides the cumulative reward and standard deviation metrics, we also analyzed other important indicators such as convergence rate, constraint violation rate, robustness score, and organizational consistency rate. \autoref{tab:other_metrics} presents these values for the different algorithm-environment configurations.

\begin{table*}[h!]
    \centering
    \caption{Analysis of Robustness, Safety, and Organizational Consistency Metrics.}
    \label{tab:other_metrics}
    \begin{tabular}{|c|c|c|c|c|c|}
        \hline
        \textbf{Algorithm} & \textbf{Environment} & \textbf{Org. Spec.} & \textbf{Convergence Rate} & \textbf{Violation Rate} & \textbf{Robustness Score} \\ \hline
        MADDPG & Predator-Prey & $\mathcal{M}OISE^+$ & 0.89 & 5.4\% & 0.82 \\ \hline
        MADDPG & Predator-Prey & AGR & 0.78 & 10.2\% & 0.65 \\ \hline
        MAPPO & Overcooked-AI & $\mathcal{M}OISE^+$ & 0.92 & 4.1\% & 0.87 \\ \hline
        MAPPO & Overcooked-AI & AGR & 0.83 & 7.5\% & 0.75 \\ \hline
        Q-Mix & Warehouse Management & $\mathcal{M}OISE^+$ & 0.87 & 6.3\% & 0.81 \\ \hline
        Q-Mix & Warehouse Management & AGR & 0.76 & 9.8\% & 0.67 \\ \hline
        DQN & Cyber-Defense & $\mathcal{M}OISE^+$ & 0.85 & 5.9\% & 0.79 \\ \hline
        DQN & Cyber-Defense & AGR & 0.72 & 8.4\% & 0.68 \\ \hline
    \end{tabular}
\end{table*}

The \textbf{convergence rate}, a key indicator of agents' learning speed, shows a marked improvement in MOISE+ environments. For example, in \textbf{Overcooked-AI}, MAPPO with $\mathcal{M}OISE^+$ achieves a convergence rate of 0.92, compared to 0.83 for AGR. This gap is also observed in \textbf{Predator-Prey} and \textbf{Warehouse Management}, indicating that missions enable agents to converge faster toward optimal policies by breaking down complex goals into achievable sub-goals.

The \textbf{constraint violation rate}, crucial for agent safety, is also significantly reduced in MOISE+ environments. In \textbf{Cyber-Defense}, DQN with $\mathcal{M}OISE^+$ records a violation rate of only 5.9%, compared to 8.4% for AGR, demonstrating that the addition of missions better guides agents towards behaviors compliant with organizational specifications.

The \textbf{robustness score} illustrates agents' ability to maintain stable performance despite environmental disturbances. In \textbf{Overcooked-AI}, MAPPO coupled with $\mathcal{M}OISE^+$ displays a robustness score of 0.87, compared to 0.75 for AGR, confirming the importance of missions for improving agents' resilience to uncertainties.

\subsection{Ablation Study}

To analyze the impact of missions on agent performance, we conducted an ablation study comparing two models: \textbf{MOISE+MARL} (including both roles and missions) and \textbf{AGR+MARL} (only including roles). The goal is to examine the effect of mission absence on agent convergence and performance.

In the \textbf{AGR+MARL} model, agents must learn their behaviors solely from roles, without being guided by intermediate objectives. In contrast, in \textbf{MOISE+MARL}, agents with roles that do not cover all possible situations are oriented towards role completion through missions, which break down the final objective into clear sub-goals.

The results of this comparison are illustrated in \autoref{tab:ablation_study}.

\begin{table*}[h!]
    \centering
    \caption{Ablation Study: Comparison Between MOISE+MARL and AGR+MARL.}
    \label{tab:ablation_study}
    \begin{tabular}{|c|c|c|c|c|}
        \hline
        \textbf{Model} & \textbf{Environment} & \textbf{Convergence Rate} & \textbf{Robustness Score} & \textbf{Cumulative Reward} \\ \hline
        MOISE+MARL & Predator-Prey & 0.89 & 0.82 & 245.6 \\ \hline
        AGR+MARL & Predator-Prey & 0.78 & 0.65 & 215.8 \\ \hline
        MOISE+MARL & Overcooked-AI & 0.92 & 0.87 & 385.2 \\ \hline
        AGR+MARL & Overcooked-AI & 0.83 & 0.75 & 350.4 \\ \hline
    \end{tabular}
\end{table*}

In the \textbf{Predator-Prey} environment, we observe that the \textbf{MOISE+MARL} model displays a higher convergence rate (0.89) compared to \textbf{AGR+MARL} (0.78). Similarly, in the \textbf{Overcooked-AI} environment, MOISE+MARL achieves a rate of 0.92, compared to 0.83 for AGR. These results confirm that missions provide additional guidance to agents, helping them complete their roles more efficiently.

The \textbf{robustness score} is also higher with MOISE+MARL, indicating that missions enable better uncertainty management. Finally, the \textbf{cumulative reward} is consistently higher in MOISE+MARL, demonstrating that the addition of missions helps agents converge toward more optimal policies.

\

In summary, this ablation study shows that adding missions with MOISE+MARL offers clear advantages for agent performance and robustness. Agents can better orient themselves towards fulfilling their roles, improving both their convergence speed and overall performance in complex environments.


\section{Discussion and Conclusion}
\label{sec:discussion_conclusion}

% \subsection{Synthesis of Results and Discussion of Addressed Challenges}

The experimental results showed that the \textbf{MOISE+MARL} framework proposes an effective approach to improve coordination, safety, and explainability of multi-agent systems trained by reinforcement learning. Agents using this framework displayed overall superior performance compared to comparative methods like AGR+MARL, particularly in environments requiring high coordination and optimal management of complex tasks.

\textbf{Challenges Addressed}: The main improvements manifest in environments where predefined roles and missions guide agents, resulting in better \textit{coordination} among them. This was particularly evident in environments such as \textit{Overcooked-AI} and \textit{Predator-Prey}, where cumulative reward and policy robustness were significantly higher with MOISE+. Additionally, \textit{operational safety} was improved in critical environments like \textit{Cyber-Defense} due to a notable reduction in constraint violations. More generally, the strong internal consistency of the framework in most scenarios shows that applying and inferring organizational specifications are well-complemented and do not introduce noise.

\textbf{Partially Addressed Challenges}: Although organizational specifications contributed to better \textit{explainability}, this explainability may be limited by the use of only unsupervised learning methods presented. On the other hand, predefined organizational specifications may be limited in highly dynamic environments or where organizational specifications must evolve over time.

\textbf{Challenges to Pursue}: The fact that users only need to define roles and missions rather than each agent's internal logic facilitates large-scale system management with hundreds of agents interacting simultaneously. Regardless of MOISE+MARL, the current simulation technique cannot be sufficiently performant with a very high number of agents. Marginal gains remain to be considered, especially on reducing computational complexity with HEMM, among others.

\subsection{Conclusion and Future Work}

This work introduces the \textbf{MOISE+MARL} framework, a framework that incorporates organizational specifications into multi-agent reinforcement learning. By linking roles and missions from the $\mathcal{M}OISE^+$ model to agent policies, this approach improves learning efficiency while ensuring better explainability of agent behaviors. The results demonstrated significant gains in coordination, robustness, and agent safety in complex environments requiring a clear organization.

However, some \textbf{limitations} remain. First, the approach heavily depends on manual role and mission definitions, which can be challenging in dynamic environments where these specifications may evolve. Additionally, the framework's scalability needs improvement to make it applicable to very large-scale multi-agent systems.

\textbf{Future Work} will focus on the following areas:
\begin{itemize}
    \item \textbf{Dynamic Adaptation of Organizational Specifications}: Developing online adaptation mechanisms that allow roles and missions to evolve with environmental changes.
    \item \textbf{Scalability}: Particularly in large-scale and dynamic simulated environments, further exploiting parallelization by delegating responsibility for roles and missions to specialized agents deployed on nodes.
    \item \textbf{Automatic Explainability}: Integrating the use of other techniques, such as Large Language Models, to better explain inferred roles and missions rather than having to interpret them manually.
    \item \textbf{Real-World Applications}: Testing the MOISE+MARL framework in real-world scenarios, such as industrial robot coordination or autonomous defense systems, to validate its robustness and effectiveness under real conditions.
\end{itemize}



\section*{References}

\bibliographystyle{ACM-Reference-Format}
\renewcommand
\refname{}
\bibliography{references}

\end{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
