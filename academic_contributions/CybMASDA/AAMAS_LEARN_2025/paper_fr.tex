%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% LaTeX Template for AAMAS-2025 (based on sample-sigconf.tex)
%%% Prepared by the AAMAS-2025 Program Chairs based on the version from AAMAS-2025. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% Start your document with the \documentclass command.

\RequirePackage[2020-02-02]{latexrelease}

%%%% For anonymized submission, use this
\documentclass[sigconf,anonymous]{aamas} 

%%%% For camera-ready, use this
%\documentclass[sigconf]{aamas}

\usepackage{listings}
% \usepackage{xcolor}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}
 
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}
 
\lstset{style=mystyle}

% --- Tickz
\usepackage{physics}
\usepackage{tikz}
\usepackage{amsmath}
\usepackage{mathdots}
% \usepackage{yhmath}
\usepackage{cancel}
\usepackage{color}
\usepackage{siunitx}
\usepackage{array}
\usepackage{multirow}
% \usepackage{amssymb}
\usepackage{gensymb}
\usepackage{tabularx}
\usepackage{extarrows}
\usepackage{booktabs}
\usetikzlibrary{fadings}
\usetikzlibrary{patterns}
\usetikzlibrary{shadows.blur}
\usetikzlibrary{shapes}

% ---------

\usepackage{balance} % for balancing columns on the final page
\usepackage{csquotes}
% \usepackage{cite}
\newcommand{\probP}{\text{I\kern-0.15em P}}
\usepackage{etoolbox}
\patchcmd{\thebibliography}{\section*{\refname}}{}{}{}
% \usepackage{amsthm,amssymb,amsfonts}

\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{color}
% \renewcommand\UrlFont{\color{blue}\rmfamily}

\usepackage[inline, shortlabels]{enumitem}
\usepackage{tabularx}
\usepackage{caption}
\usepackage{listings}
\usepackage{titlesec}
\usepackage{ragged2e}
% \usepackage[hyphens]{url}
\usepackage[linesnumbered,ruled,vlined]{algorithm2e}
\usepackage{float}
\usepackage[english]{babel}
\addto\extrasenglish{  
    \def\figureautorefname{Figure}
    \def\tableautorefname{Table}
    \def\algorithmautorefname{Algorithm}
    \def\sectionautorefname{Section}
    \def\subsectionautorefname{Subsection}
    \def\proofoutlineautorefname{Proof Outline}
}

\newcommand{\supertiny}{\fontsize{1}{2}\selectfont}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% AAMAS-2025 copyright block (do not change!)

\setcopyright{ifaamas}
\acmConference[AAMAS '25]{Proc.\@ of the 24th International Conference
on Autonomous Agents and Multiagent Systems (AAMAS 2025)}{May 19 -- 23, 2025}
{Detroit, Michigan, USA}{A.~El~Fallah~Seghrouchni, Y.~Vorobeychik, S.~Das, A.~Nowe (eds.)}
\copyrightyear{2025}
\acmYear{2025}
\acmDOI{}
\acmPrice{}
\acmISBN{}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% == IMPORTANT ==
%%% Use this command to specify your EasyChair submission number.
%%% In anonymous mode, it will be printed on the first page.

\acmSubmissionID{<<EasyChair submission id>>}

%%% Use this command to specify the title of your paper.

\title[AAMAS-2025 CybMASDE]{Une Approche Intégrant l'Organisation dans l'Apprentissage par Renforcement Multi-Agents}

% Evaluating and Controlling Organization in Multi-Agent Learning

% Organizationally Constrained Multi-Agent Learning: A Framework for Safe and Interpretable Policies

%%% Provide names, affiliations, and email addresses for all authors.

\author{Julien Soulé}
\affiliation{
  \institution{Univ. Grenoble Alpes}
  \city{Valence}
  \country{France}}
\email{julien.soule@lcis.grenoble-inp.fr}

\author{Jean-Paul Jamont}
\affiliation{
  \institution{Univ. Grenoble Alpes}
  \city{Valence}
  \country{France}}
\email{jean-paul.jamont@lcis.grenoble-inp.fr}

\author{Michel Occello}
\affiliation{
  \institution{Univ. Grenoble Alpes}
  \city{Valence}
  \country{France}}
\email{michel.occello@lcis.grenoble-inp.fr}

\author{Louis-Marie Traonouez}
\affiliation{
  \institution{Thales Land and Air Systems, BU IAS}
  \city{Rennes}
  \country{France}}
\email{louis-marie.traonouez@thalesgroup.com}

\author{Paul Théron}
\affiliation{
  \institution{AICA IWG}
  \city{La Guillermie}
  \country{France}}
\email{paul.theron@orange.fr}


\begin{abstract}
  Le MARL peut permettre de générer une politique conjointe permettant aux agents d'agir et collaborer pour atteindre un objectif global dans un environnement donné. En particulier, quand l'objectif est décomposable en sous-tâches et que l'environnement favorise leur délégation aux agents en s'y spécialisant durant l'entrainement, les comportements des agents peuvent converger vers des comportements que l'on peut pertinement considerer comme des rôles et objectifs implicites.
  Plus généralement, ce papier explore l'idée d'intégrer une vision centrée organisation dans le cadre du MARL pour améliorer l'explicabilité et le controle des agents là où les travaux actuels se concentre sur les agents individuels.
  Nous proposons un framework MARL intégrant explicitement des rôles et objectifs prédéfinis du modèle organisationnelle MOISE+ et pouvant impacter l'entrainement des agents. Ce framework inclut également une méthode pour inférer des rôles et objectifs implicites après entrainement. Ce framework été appliqué sur divers environnements et algorithmes MARL en montrant une cohérence entre les spécifications organisationelles inférées et prédéfinies.
\end{abstract}


% TODO: Mention the specific contributions related to the LEARN topics such as robustness, adaptability, and learning in constrained environments.

%%% The code below was generated by the tool at http://dl.acm.org/ccs.cfm.
%%% Please replace this example with code appropriate for your own paper.

%%% Use this command to specify a few keywords describing your work.
%%% Keywords should be separated by commas.

\keywords{Agent-Oriented Software Engineering \and Multi-Agent Reinforcement Learning \and Assisted-Design \and Organizational Models}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% Include any author-defined commands here.
         
% \newcommand{\BibTeX}{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em\TeX}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

%%% The following commands remove the headers in your paper. For final 
%%% papers, these will be inserted during the pagination process.

\pagestyle{fancy}
\fancyhead{}

%%% The next command prints the information defined in the preamble.

\maketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Contexte
Le MARL peut être considéré comme une approche permettant de résoudre le problème de trouver une politique conjointe qui guide les agents dans l'atteinte d'un objectif donné dans un environnement spécifique.
Cette politique conjointe ne définit pas seulement les actions individuelles des agents, mais régit également leurs interactions entre eux, voire avec l'ensemble des autres agents, sans préconception d'un ordre ou d'une structure organisationnelle explictement prédéfinie.

Dans des problèmes à caractère sociaux ou collaboratifs, on peut constater que les contraintes environnementales et l'objectif global peuvent faire converger les agents vers des solutions optimales ou satisfaisantes de telle manière que des agents peuvent présenter des ensembles de comportements similaires de façon récurrente au cours de différents épisodes de test.

Ces ensembles distincts peuvent présenter des propriétés de spécialisation, complémentarité, stabilité les rapprochant de rôles potentiels implicites désignés comme "abstraits". Plus loin, les trajectoires des agents semblant adopter ces rôles abstraits peuvent présenter des points communs ou similaires, telles que des observations récurrentes en fin de chaque épisode. Ces parties similaires de trajectoires peuvent être interpretés comme la base d'objectifs "abstraits" car il peut sembler que les agents cherchent à les parcourir.

De manière plus générale, on peut interpréter les comportements des agents entraînés par rapport à leur proximité avec la vision d'une organisation structurée et fonctionnelle. Nous appelons cette similitude entre la politique conjointe et une organisation structurée et fonctionnelle \textbf{adéquation organisationnelle}.

Bien que cette idée intuitive de la ressemblance entre la politique et une organisation structurée et fonctionnelle n'est qu'une interprétation, ce papier explore l'interêt d'une vision organisationelle des agents pour l'explicitation des agents entrainés en MARL là où la plupart des travaux en XAI se concentrent sur les comportements individuelles des agents.

Plus loin, au-delà des contraintes environnementales et des objectifs fixés, l'ajout de contraintes à caractère organisationnel peut avoir pour effet d'impacter l'adéquation organisationnelle. Ce papier explore cet interêt pour le controle, l'explicabilité et la sûreté de fonctionnement du SMA entrainé.
% Cela peut, par exemple, se faire en reprenant et en affinant les rôles et sous-objectifs de l'organisation émergente issue de la politique conjointe.

\

% Problème
\noindent La question de la pertinence d'adopter une vision organisationnelle des comportements en MARL amène à se questionner sur deux problèmes liés à l'adéquation fonctionnelle.

\quad \textbf{i) L'évaluation de l'adéquation organisationnelle} vise à évaluer dans quelle mesure une politique conjointe peut être rapprochée d'une organisation structurée et fonctionnelle. Un des enjeux de ce problème est de mieux comprendre les cas où les agents peuvent être considérés comme formant une organisation structurée et fonctionnelle, compte tenu des contraintes imposées par l'environnement, l'objectif et d'autres contraintes optionnelles.
% La littérature aborde l'évaluation des politiques en termes de rôles (structure) ou d'objectifs (fonction). Cependant, ces travaux manquent souvent d'une approche systématique et générale. Les méthodes actuelles offrent peu d'outils clairs pour mesurer de manière quantitative et qualitative cette adéquation organisationnelle.

\quad \textbf{ii) Le contrôle de l'adéquation organisationnelle} vise à guider les agents vers des politiques qui se conforment à une organisation structurée, définie par des contraintes spécifiques prédéfinies par l'utilisateur.
% L'objectif est de contraindre ou d'inciter les agents à adopter des comportements qui respectent des rôles et des missions, ce qui permet de contrôler leurs actions dans un cadre organisationnel.
Les enjeux incluent la réduction de l'espace de recherche des politiques, l'amélioration de la convergence, et la garantie du respect des contraintes de sûreté.
% Les travaux existants dans ce domaine sont limités, notamment en ce qui concerne la manière dont l'utilisateur peut interagir avec les spécifications organisationnelles de manière pratique et flexible.

\

% Contribution
\noindent Nous introduisons le framework \textbf{MOISE+MARL}, un nouveau framework de MARL qui intègre le modèle organisationnel MOISE+ dans l'apprentissage multi-agent. Ce framework permet de formaliser l'adéquation organisationnelle et permet d'adresser:

\quad \textbf{i)} Le controle de l'adéquation organisationnelle en introduisant une structure de donnée nommée \textbf{arbre de décision basé sur les patterns} permettant de définir manuellement les roles et objectifs des agents à appliquer aux agents. L'application prend la forme de contraintes supplémentaires qui affectent automatiquement à la fois les politiques et la fonction de récompense.

\quad \textbf{ii)} L'évaluation de l'adéquation organisationelle en introduisant la méthode \textbf{History-based Evaluation in MOISE+MARL} (HEMM). Cette méthode utilise des techniques d'apprentissage non supervisé pour généraliser des rôles et des missions à partir de l'ensemble des comportements observés au cours de plusieurs épisodes de test. En mesurant l'écart entre les spécifications organisationnelles abstraites inférées et les comportements réels, nous définissons une nouvelle métrique multidimensionnelle, le \textbf{niveau d'adéquation organisationnelle} qui quantifie dans quelle mesure une politique se conforme aux spécifications organisationnelles inférées.

\

% Évaluation & Trouvailles
\noindent Nous avons évalué le framework MOISE+MARL en mettant en jeu :
\begin{itemize}
  \item Quatre environnements présentant différentes contraintes environnementales et objectifs, dont certains pour lesquels on s'attend à ce que les politiques conjointes efficaces soient proches ou éloignées de politiques adéquates organisationnellement. % Ces environnements sont : overcooked, predator-prey, warehouse management, et ant simulation.
  \item Deux algorithmes MARL policy-based
  % (MAPPO)
  connus pour favoriser une convergence stable, un algorithmes Actor-Critic
  % (MADDPG)
  , deux algorithmes value-based
  % (DQN, Q-Mix)
  et un algorithme model-based
  % (Dyna-Q)
  favorisant la performance à long terme.
  \item Trois ensembles de spécifications organisationnelles pour chacun des environnements, de sorte à contraindre progressivement davantage les agents à se conformer à des comportements prédéfinis ou leur permettre plus de liberté.
\end{itemize}


Nous avons d'abord vérifié que le niveau d'adéquation organisationnelle obtenu avec HEMM dans les environnements où l'on s'attend à l'émergence de politiques organisationnellement adéquates, est effectivement plus élevé par rapport aux autres. Les rôles et missions identifiés par HEMM correspondent aux observations manuelles, confirmant que cet méthode est capable de détecter et de valider l'émergence de structures organisationnelles implicites dans les comportements des agents.

Les résultats montrent également que les algorithmes de type actor-critic
% , comme MADDPG,
sont particulièrement adaptés pour faire converger les agents vers des politiques stables. Cette stabilité permet aux agents de maintenir des comportements homogènes et cohérents à chaque épisode, ce qui est essentiel pour garantir une organisation robuste. À l'inverse, les algorithmes value-based
%, tels que Q-Mix,
présentent une plus grande variabilité dans les comportements des agents, même si les performances globales restent élevées.

Enfin, nous vérifions que l'application de spécifications organisationnelles prédéfinis manuellement augmente significativement le niveau d'adéquation organisationnelle calculé par HEMM. Les rôles et missions inférés via HEMM s'avèrent très proches des spécifications prédéfinies appliquées. Cela démontre la cohérence interne du MOISE+MARL puisque les modifications introduites dans les politiques via des spécifications organisationnelles sont fidèlement retrouvées par HEMM.

\

% Structure du papier
La reste du papier est organisé comme suit : la \autoref{sec:related_works} explore les problèmes de l'évaluation et du contrôle de l'adéquation organisationnelle dans la literature. La \autoref{sec:moise_marl_framework} introduit le framework MOISE+MARL. La \autoref{sec:hemm_algorithm} décrit la méthode HEMM. La \autoref{sec:experimental_setup} décrit le protocole expérimental, en particulier les environnements et algorithmes MARL. La \autoref{sec:results} présente les résultats expérimentaux. Enfin, la \autoref{sec:discussion_conclusion} discute et conclut sur l'évaluation et le contrôle de l'adéquation organisationnelle.

\section{Travaux liés}
\label{sec:related_works}

Cette section s'interesse aux travaux se rapprochant de l'adéquation organisationnelle au travers des deux problèmes introduits.
% Évaluer et contrôler l'adéquation organisationnelle est crucial pour s'assurer que les agents respectent des rôles et des objectifs, participant ainsi à expliquer le fonctionnement collectif des agents ou s'assurer de la sûreté et favoriser la coordination et une performance suffisante dans des environnements complexes.

% Les défis de l'évaluation et du contrôle de l'adéquation organisationnelle surviennent dans des scénarios où les agents doivent non seulement optimiser des comportements individuels, mais aussi se conformer à une structure organisationnelle implicite ou explicite.

\subsection{Évaluation de l'adéquation organisationnelle}
L'évaluation de l'adéquation organisationnelle vise à déterminer dans quelle mesure les comportements des agents sont alignés avec une organisation prédéfinie ou émergente. Cela implique d'analyser à la fois la structure des rôles des agents et leur alignement fonctionnel avec les objectifs collectifs.

\subsubsection{Émergence des rôles et spécialisation}
Yang et al. \cite{yang2021role} démontrent comment les rôles émergent naturellement dans des systèmes multi-agents décentralisés, où les agents présentent des schémas comportementaux récurrents pouvant être catégorisés comme des rôles abstraits. Cette spécialisation émergente peut être évaluée quantitativement en mesurant la stabilité et la cohérence à travers plusieurs épisodes.

Grover et al. \cite{grover2018role} introduisent une approche d'apprentissage non supervisée pour détecter les rôles en se basant sur les interactions des agents, fournissant un cadre pour évaluer l'adhésion aux rôles et la cohérence des rôles au fil du temps. Cependant, cette approche ne permet pas d'incorporer des contraintes organisationnelles ou des structures prédéfinies.

\subsubsection{Cohérence organisationnelle et stabilité}
La cohérence organisationnelle d'un système multi-agents peut être évaluée en mesurant la stabilité et la complémentarité des rôles des agents au fil du temps. Borsa et al. \cite{borsa2019constrained} proposent d'utiliser le reinforcement learning contraint pour imposer une stabilité dans le comportement des agents, ce qui contribue indirectement à maintenir l'adéquation organisationnelle.

Les métriques de stabilité, telles que celles introduites par Liu et al. \cite{liu2021efficient}, évaluent la similarité des politiques à travers les épisodes de test, garantissant que les agents exhibent un comportement cohérent aligné avec les rôles organisationnels. Cependant, la plupart des méthodes actuelles sont limitées dans leur capacité à expliquer pourquoi certains rôles émergent et comment ils s'alignent avec le contexte organisationnel plus large.

\subsubsection{Explicabilité et interprétabilité}
L'explicabilité dans le MARL se concentre sur la compréhension du comportement des agents par des observateurs humains. Alors que les approches traditionnelles de l'explicabilité se concentrent sur les agents individuels \cite{van2018explainable}, l'adéquation organisationnelle nécessite d'évaluer dans quelle mesure la politique conjointe reflète la structure organisationnelle prévue.

Van der Waa et al. \cite{van2020explainability} proposent des méthodes pour expliquer le comportement collectif dans les systèmes multi-agents en reliant les actions des agents aux objectifs globaux. Ces méthodes fournissent des informations précieuses sur la performance globale du système, mais manquent de granularité nécessaire pour évaluer les rôles et les missions organisationnels.

\subsection{Contrôle de l'adéquation organisationnelle}
Le contrôle de l'adéquation organisationnelle implique de guider les agents vers des politiques conformes à une structure organisationnelle prédéfinie, souvent à travers l'imposition de contraintes ou d'incitations. Différentes techniques ont été développées pour assurer que les politiques des agents respectent les contraintes organisationnelles.

\subsubsection{Constrained Policy Optimization (CPO)}
Le CPO, introduit par Achiam et al. \cite{achiam2017cpo}, est une méthode basée sur l'optimisation sous contraintes, où les politiques sont ajustées pour respecter des contraintes de sûreté tout en maximisant les récompenses. En utilisant des trust region pour ajuster les politiques, le CPO garantit que les agents ne violent pas les contraintes critiques, ce qui est essentiel dans le contrôle de l'adéquation organisationnelle.

\subsubsection{Méthodes basées sur les multiplicateurs de Lagrange}
Les méthodes basées sur les multiplicateurs de Lagrange, comme celles explorées par Ray et al. \cite{ray2019benchmarking}, intègrent directement les contraintes dans la fonction de récompense. Le multiplicateur de Lagrange permet de trouver un équilibre entre la maximisation des récompenses et la satisfaction des contraintes organisationnelles. Cette approche est particulièrement efficace pour gérer des rôles complexes où plusieurs objectifs doivent être simultanément respectés.

\subsubsection{Safe Exploration et Shielding}
Le Safe Exploration est une technique qui permet aux agents d'explorer de nouveaux comportements tout en respectant des contraintes de sûreté. Cela est particulièrement utile lorsque les agents doivent apprendre à naviguer dans des environnements incertains tout en maintenant leur conformité aux contraintes organisationnelles. Les travaux de Garcia et al. \cite{garcia2015comprehensive} fournissent une vue d'ensemble des différentes techniques pour garantir que l'exploration des agents reste à l'intérieur des limites sûres tout en maximisant l'efficacité de l'apprentissage.

Le concept de Shielding, proposé par Alshiekh et al. \cite{alshiekh2018safe}, offre une méthode dynamique pour modifier les politiques d'apprentissage afin d'éviter des actions dangereuses. En supervisant les actions des agents en temps réel et en bloquant les actions qui enfreignent les contraintes de sûreté, le Shielding garantit que les agents respectent les rôles et les missions définis dans une organisation. Cette approche est particulièrement utile dans les environnements critiques où la sûreté est primordiale, comme dans les systèmes cyber-physiques ou la robotique.


\subsubsection{Hierarchical Reinforcement Learning (HRL)}
Le reinforcement learning hiérarchique (HRL) décompose les tâches en sous-tâches, s'alignant ainsi avec la structure hiérarchique des rôles et missions dans une organisation. Ghavamzadeh et al. \cite{ghavamzadeh2006hrl} démontrent comment des politiques hiérarchiques peuvent être utilisées pour guider les agents vers des comportements coordonnés, où le rôle de chaque agent correspond à une sous-tâche dans l'objectif global.
En particulier, dans le HRL coopératif \cite{ghavamzadeh2006cooperative}, les agents partagent des informations au niveau des sous-tâches, permettant une coordination plus efficace. Cette méthode s'aligne bien avec le concept d'adéquation organisationnelle, où les rôles sont explicitement liés à des objectifs organisationnels spécifiques.

\subsubsection{Contrôle de la communication et de la coordination}
Contrôler la communication et la coordination entre les agents est crucial pour assurer l'adéquation organisationnelle, en particulier dans les systèmes à grande échelle. Foerster et al. \cite{foerster2018communication} proposent d'utiliser la connaissance commune pour permettre une coordination décentralisée, permettant aux agents d'agir de manière cohérente sans contrôle centralisé.

\subsection{Synthèses et verrous}
Le \autoref{tab:related_work} résume les propriétés clés des travaux discutés et montre comment ils abordent (ou non) différents aspects de l'adéquation organisationnelle.

\begin{table}[ht]
    \centering
    \small
    \renewcommand{\arraystretch}{1.3}
    \caption{Comparaison des travaux liés sur l'adéquation organisationnelle dans le MARL (tableau transposé)}
    \label{tab:related_work}
    \begin{tabular}{m{2.1cm} >{\centering\arraybackslash}m{0.8cm} >{\centering\arraybackslash}m{0.5cm} >{\centering\arraybackslash}m{0.8cm} >{\centering\arraybackslash}m{0.5cm} >{\centering\arraybackslash}m{0.5cm} >{\centering\arraybackslash}m{0.8cm}}

        \textbf{Propriété} & \textbf{\cite{yang2021role}} & \textbf{\cite{grover2018role}} & \textbf{\cite{borsa2019constrained}} & \textbf{\cite{zhang2020safemarl}} & \textbf{\cite{ghavamzadeh2006hrl}} & \textbf{\cite{foerster2018communication}} \\
        \hline \vspace{0.2cm}
        \textbf{Émergence des rôles} & Oui & Oui & Partielle & Non & Non & Non \\
        \textbf{Stabilité organisationnelle} & Partielle & Oui & Oui & Oui & Oui & Oui \\
        \textbf{Apprentissage contraint} & Non & Non & Oui & Oui & Non & Partielle \\
        \textbf{Contrôle hiérarchique} & Non & Non & Non & Non & Oui & Non \\
        \textbf{Explicabilité} & Limité & Non & Non & Non & Non & Oui \\
        \textbf{Scalabilité} & Non & Non & Partielle & Oui & Oui & Oui
    \end{tabular}%
\end{table}

\noindent De cette analyse, nous abordons les défis suivants :
\begin{itemize}
    \item \textbf{Scalabilité} : Les méthodes actuelles ont du mal à s'adapter à un grand nombre d'agents, en particulier dans des environnements dynamiques où les rôles et les contraintes peuvent changer.
    \item \textbf{Adaptation dynamique} : Peu de frameworks supportent l'adaptation dynamique à de nouvelles structures organisationnelles, limitant leur applicabilité dans des environnements avec des objectifs évolutifs.
    \item \textbf{Explicabilité} : La plupart des frameworks d'explicabilité se concentrent sur les comportements individuels, manquant d'outils pour évaluer et expliquer les structures organisationnelles.
\end{itemize}

\noindent Notre framework MOISE+MARL vise à mieux adresser ces défis en fournissant des moyens sobres de contrôle dynamique des structures et fonctions de l'organisation plutôt que des agents eux-mêmes. Il fournit également des moyens pour évaluer l'adéquation organisationnelle des politiques et ainsi expliquer le fonctionnement collectif des agents.

\section{Le Framework MOISE+MARL}
\label{sec:moise_marl_framework}

Cette section introduit d'abord le formalisme utilisé pour décrire le cadre de fonctionnement du MARL. Ensuite, nous introduisons le framework MOISE+MARL en présentant les spécifications organisationnelles $\mathcal{M}OISE^+$ et en les liant avec le cadre du MARL.

\subsection{Cadre Markovien pour le MARL}

Afin d'appliquer les techniques de MARL, nous nous appuyons sur le \textit{Decentralized Partially Observable Markov Decision Process} (Dec-POMDP)~\citep{Oliehoek2016}. Ce modèle est adapté aux SMA, car il permet de gérer plusieurs agents dans un environnement incertain. Contrairement aux \textit{Partially Observable Stochastic Games} (POSG), le Dec-POMDP permet une fonction de récompense commune aux agents, ce qui favorise l'apprentissage d'actions collaboratives~\citep{Beynier2013}.

\

Un Dec-POMDP $d \in D$ (où $D$ est l'ensemble des Dec-POMDP) est défini comme un 7-uplet $d = (S, \{A_i\}, T, R, \{\Omega_i\}, O, \gamma)$, où :

\begin{itemize}
    \item $S = \{s_1,...,s_{|S|}\}$ : l'ensemble des états possibles ;
    \item $A_{i} = \{a_{1}^{i},...,a_{|A_{i}|}^{i}\}$ : l'ensemble des actions possibles pour l'agent $i$ ;
    \item $T$ : l'ensemble des probabilités de transition, où $T(s,a,s') = \probP(s'|s,a)$ représente la probabilité de passer de l'état $s$ à l'état $s'$ suite à l'action $a$ ;
    \item $R: S \times A \times S \rightarrow \mathbb{R}$ : la fonction de récompense, qui attribue une récompense en fonction de l'état initial, de l'action prise et de l'état résultant ;
    \item $\Omega_{i} = \{o_{1}^{i},...,o_{|\Omega_{i}|}^{i}\}$ : l'ensemble des observations possibles pour l'agent $i$ ;
    \item $O$ : l'ensemble des probabilités d'observation, où $O(s',a,o) = \probP(o|s',a)$ représente la probabilité d'obtenir l'observation $o$ après avoir effectué l'action $a$ et atteint l'état $s'$ ;
    \item $\gamma \in [0,1]$ : le facteur d'actualisation, utilisé pour pondérer les récompenses futures.
\end{itemize}

Considérons un ensemble de $m$ \textbf{équipes} (ou \textbf{groupes}), chaque équipe étant composée de plusieurs agents choisis parmi l'ensemble des agents $\mathcal{A}$. Le formalisme suivant décrit la manière de résoudre un Dec-POMDP pour une équipe donnée $i$ (avec $0 \leq i \leq m$) contenant $n$ agents~\citep{Beynier2013,Albrecht2024} :

\begin{itemize}
    \item $\Pi$ : l'ensemble des \textbf{politiques}. Une politique $\pi \in \Pi, \pi: \Omega \rightarrow A$ mappe de façon déterministe une observation à une action. Elle représente la stratégie interne de l'agent ;
    \item $\Pi_{joint}$ : l'ensemble des \textbf{politiques conjointes}. Une politique conjointe $\pi_{joint} \in \Pi_{joint}, \pi_{joint}: \Omega^n \rightarrow A^n = \Pi^n$ choisit une action pour chaque agent en fonction de leurs observations respectives. Cette politique conjointe peut être vue comme un ensemble de politiques utilisées par les agents au sein d'une équipe ;
    \item $H$ : l'ensemble des \textbf{historiques}. Un historique sur $z \in \mathbb{N}$ étapes (où $z$ est généralement le nombre maximal d'étapes pour un épisode) est le $z$-uplet $h = ((\omega_{k}, a_{k}) | k \leq z, \omega \in \Omega, a \in A)$, représentant les observations et actions successives ;
    \item $H_{joint}$ : l'ensemble des \textbf{historiques conjoints}. Un historique conjoint $h_{joint} \in H_{joint}$ sur $z$ étapes est défini comme l'ensemble des historiques des agents : $h_{joint} = \{h_1, h_2, ..., h_n\}$ ;
    \item $U_{joint,i}(<\pi_{joint,i}, \pi_{joint,-i}>): \Pi_{joint} \rightarrow \mathbb{R}$ : la \textbf{récompense cumulée attendue} sur un horizon fini (si $\gamma < 1$ ou si le nombre d'étapes d'un épisode est fini), où $\pi_{joint,i}$ représente la politique conjointe pour l'équipe $i$, et $\pi_{joint,-i}$ les politiques conjointes des autres équipes, considérées comme fixes ;
    \item $BR_{joint,i}(\pi_{joint,i}) = argmax_{\pi_{joint,i}}(U(<\pi_{joint_i}, \pi_{joint,-i}>))$ : donne la \textbf{meilleure réponse} pour l'équipe $i$, où $\pi_{joint,i}^*$ est la politique conjointe optimale qui maximise la récompense cumulée attendue $U_i^*$ ;
    \item $SR_{joint,i}(\pi_{joint_i}, s) = \{\pi_{joint_i} | U(<\pi_{joint,i}, \pi_{joint,-i}>) \geq s\}$ : donne la \textbf{réponse suffisante}, définie comme l'ensemble des politiques conjointes permettant d'obtenir au moins une récompense cumulée attendue de $s \in \mathbb{R}$, avec $s \leq U_i^*$.
\end{itemize}

Nous faisons référence à la \textbf{résolution du Dec-POMDP} pour l'équipe $i$ comme la recherche d'une politique conjointe $\pi_{joint,i} \in \Pi_{joint}$ telles que $\pi_{joint,i} = SR_{joint,i}(\pi_{joint,i}, s)$, permettant d'obtenir au moins une récompense cumulée attendue de $s$, où $s \in \mathbb{R}$ et $s \leq U_i^*$.

\subsection{Intégration de MOISE+ dans le MARL}

Dans cette section, nous présentons progressivement les principes que nous proposons pour adapter le MARL en fonction des spécifications organisationnelles.

A notre connaissance $\mathcal{M}OISE^+$ est le seul modèle organisationnelle formalisant suffisament les capacités des politiques des agents pour le rendre compatible facilement avec le MARL. Par exemple, le modèle \textit{AGR} (\textit{Agent Group Role}) se limite aux concepts de rôle et de groupe et ne permet pas de préconiser une forme que ces derniers devraient prendre en MARL. Le modèle $\mathcal{M}OISE^+$ fournit un cadre complet pour spécifier la structure et les fonctions d'un SMA.

Dans $\mathcal{M}OISE^+$, les \textbf{Spécifications Organisationnelles (SO)} sont définies comme $\mathcal{OS} = \langle \mathcal{SS}, \mathcal{FS}, \mathcal{DS} \rangle$, où $\mathcal{SS}$ correspond aux \textbf{Spécifications Structurelles}, $\mathcal{FS}$ aux \textbf{Spécifications Fonctionnelles}, et $\mathcal{DS}$ aux \textbf{Spécifications Déontiques}.

\subsection{Spécifications structurelles et contraintes sur les politiques conjointes basées sur les rôles}

Les \textbf{Spécifications Structurelles} décrivent la structure organisationnelle et sont notées $\mathcal{SS} = \langle \mathcal{R}, \mathcal{IR}, \mathcal{G} \rangle$ :

\begin{itemize}
    \item $\mathcal{R}_{ss}$ : L'ensemble de tous les rôles ($\rho \in \mathcal{R}$).
    \item $\mathcal{IR}: \mathcal{R} \rightarrow \mathcal{R}$ : La relation d'héritage entre rôles ($\rho_1 \sqsubset \rho_2$).
    \item $\mathcal{RG} \subseteq \mathcal{GR}$ : L'ensemble des groupes racine, $\mathcal{GR} = \langle \mathcal{R}, \mathcal{SG}, \mathcal{L}^{intra}, \mathcal{L}^{inter}, \mathcal{C}^{intra}, \mathcal{C}^{inter}, np, ng \rangle$, où :
          \begin{itemize}
          \item $\mathcal{R} \subseteq \mathcal{R}_{ss}$ : L'ensemble des rôles non abstraits.
          \item $\mathcal{SG} \subseteq \mathcal{GR}$ : L'ensemble des sous-groupes.
          \item $\mathcal{L} = \mathcal{R} \times \mathcal{R} \times \mathcal{TL}$ : L'ensemble des liens $(\rho_s, \rho_d, t)$, où $t \in \{acq, com, aut\}$ indique le type de lien (acquaintance, communication, autorité).
          \item $\mathcal{C} = \mathcal{R} \times \mathcal{R}$ : L'ensemble des compatibilités $(\rho_a \bowtie \rho_b)$.
          \item $np: \mathcal{R} \rightarrow \mathbb{N} \times \mathbb{N}$ : La cardinalité des agents adoptant un rôle.
          \end{itemize}
\end{itemize}

Contraindre directement les politiques n'est pas faisable car la plupart des implémentations de politiques reposent sur des modèles opaques tels que les réseaux neuronaux. Nous proposons de représenter formellement un rôle comme un sous-ensemble d'historiques $\mathcal{P}(H)$, contenant les historiques qu'un agent jouant un rôle doit générer, caractérisant ainsi le comportement attendu du rôle. Comme illustré dans la \autoref{fig:PRAHOM_osm_rels}, nous proposons que chaque rôle soit mappé à un sous-ensemble d'historiques via la bijection $rh: \mathcal{R} \rightarrow \mathcal{P}(H)$. Un agent jouant un rôle devrait voir sa politique contrainte à générer des historiques appartenant au sous-ensemble mappé (au moins d'un point de vue théorique).

\begin{figure}[h!]
    \centering
    \input{figures/PRAHOM_osm_rels.tex}
    \caption{Relations entre les spécifications organisationnelles et les sous-ensembles d'historiques}
    \label{fig:PRAHOM_osm_rels}
\end{figure}

La définition des rôles en tant que sous-ensemble d'historiques pose des difficultés pour la gestion d'observations nombreuses et complexes (comme des tables de pixels). Nous proposons d'utiliser des labels pour représenter les observations de manière concise. Nous introduisons $ol: \mathcal{P}(\Omega) \rightarrow \mathcal{P}(L)$ pour mapper des chaînes simples à des observations réelles. En plus de cette méthode, nous avons également envisagé d'utiliser un modèle de langage (LLM) à cet effet.

Ensuite, définir un sous-ensemble d'historiques de manière exhaustive peut nécessiter de prendre en compte de nombreux cas, ce qui pourrait conduire à un grand nombre d'historiques. Plutôt que de définir un sous-ensemble d'historiques de manière exhaustive, nous proposons trois moyens pour simplifier sa définition :
\begin{enumerate}
    \item un format de \textbf{pattern} qui suit des règles spécifiques et qui est implémenté sous forme de graphe orienté appelé \textbf{graphe d'historique}, où les nœuds représentent des observations/actions et les arcs des transitions attendues ;
    \item des \textbf{règles} pour associer un ensemble d'actions en fonction d'un historique appartenant au sous-ensemble d'historiques et d'une observation reçue ;
    \item une logique de \textbf{script personnalisé} prenant en compte un historique et une nouvelle observation pour indiquer les actions à ajouter à l'historique afin qu'il reste dans le sous-ensemble.
\end{enumerate}

Comme un sous-ensemble d'historiques est destiné à être utilisé pour contraindre une politique à adhérer à un rôle, nous introduisons une \textbf{contrainte de politique observable} $c\pi: H \times \Omega \rightarrow \mathcal{P}(A)$. $c\pi$ indique les actions qu'une politique d'agent devrait être autorisée à choisir lorsqu'elle reçoit une observation à chaque étape.

Nous proposons d'intégrer les contraintes de politique observable dans la politique d'un agent via trois modes :
\begin{itemize}
    \item \textbf{correct} : Corrige toute action choisie pour qu'elle appartienne à l'ensemble $c\pi(\omega)$. Cela garantit la sécurité mais reste externe à la politique de l'agent ;
    \item \textbf{penalize} : Ajoute une pénalité à la récompense si une mauvaise action ($\pi(\omega) \notin c\pi(\omega)$) est réalisée, encourageant l'agent à apprendre les contraintes, mais sans garantie de sécurité ;
    \item \textbf{correct\_policy} : Crée une \textbf{politique contrainte} $\pi_c \in \Pi$, où $\pi_c = \{smpl(c\pi_{joint}(\omega_{joint})) \allowbreak si \ \omega_{joint} \in Dom(c\pi_{joint}) \ \allowbreak, \ \allowbreak sinon \ \allowbreak \pi_{joint}(\omega_{joint})\}$, où $smpl$ sélectionne un élément d'un ensemble de manière aléatoire. Une contrainte de politique observable $c\pi$ corrige la politique courante $\pi$, garantissant ainsi des sécurités internes.
\end{itemize}

En annexe, nous fournissons une preuve montrant pourquoi contraindre dynamiquement le processus de décision des actions pendant l'entraînement à chaque étape implique que la politique conjointe résultante sera nécessairement contrainte.

\subsection{Spécifications fonctionnelles et contraintes sur les politiques conjointes basées sur les missions}

Les \textbf{Spécifications Fonctionnelles} décrivent les tâches et les objectifs, et sont notées $\mathcal{FS} = \langle \mathcal{SCH}, \mathcal{PO} \rangle$ :

\begin{itemize}
    \item $\mathcal{SCH} = \langle \mathcal{G}, \mathcal{M}, \mathcal{P}, mo, nm \rangle$ : L'ensemble des schémas sociaux, où :
          \begin{itemize}
              \item $\mathcal{G}$ : L'ensemble des objectifs globaux.
              \item $\mathcal{M}$ : L'ensemble des labels de missions.
              \item $\mathcal{P}$ : L'ensemble des plans $(g_f, \{g_i\}_{0 \leq i \leq s}, op, ps)$.
              \item $mo: \mathcal{M} \rightarrow \mathbb{P}(\mathcal{G})$ : Les objectifs associés à chaque mission.
              \item $nm: \mathcal{M} \rightarrow \mathbb{N} \times \mathbb{N}$ : La cardinalité des agents engagés dans chaque mission.
          \end{itemize}
    \item $\mathcal{PO}: \mathcal{M} \times \mathcal{M}$ : L'ensemble des ordres de préférence $(m_1 \prec m_2)$.
\end{itemize}

Nous considérons qu'un objectif est représenté théoriquement comme un sous-ensemble d'historiques. Nous proposons $gh: \mathcal{G} \rightarrow \mathcal{P}(H)$, qui montre comment un agent engagé dans une mission devrait générer des historiques appartenant à un sous-ensemble d'historiques attendu. Ainsi, $mh: \mathcal{M} \rightarrow \mathcal{P}(H)$ donne les historiques attendus pour chaque mission. Finalement, un objectif devrait impacter le MARL en modifiant la fonction de récompense lorsqu'un agent est engagé dans une mission. Nous introduisons une \textbf{fonction de récompense observable} $R_{g}: H \rightarrow \mathbb{R}$, qui indique à quel point un historique généré est proche d'un sous-ensemble d'historiques donné. Une fonction de récompense pour une mission $m \in \mathcal{M}$ contenant les objectifs $\mathcal{G}_{m} = mo(m)$ est calculée comme une somme pondérée de toutes les fonctions de récompense observables de chaque objectif. De cette manière, les agents sont individuellement incités à atteindre leurs sous-objectifs respectifs, accélérant ainsi la convergence vers l'objectif final.

\subsection{Spécifications déontiques et contraintes sur les politiques conjointes selon les permissions et obligations}

Les \textbf{Spécifications Déontiques} définissent les permissions et obligations et sont notées $\mathcal{DS} = \langle \mathcal{OBL}, \mathcal{PER} \rangle$ :

\begin{itemize}
    \item $\mathcal{TC}$ : L'ensemble des contraintes temporelles.
    \item $\mathcal{OBL}: \mathcal{R} \times \mathcal{M} \times \mathcal{TC}$ : L'ensemble des obligations $(obl(\rho_a, m, tc))$.
    \item $\mathcal{PER}$ : L'ensemble des permissions $(per(\rho_a, m, tc))$.
\end{itemize}

Nous introduisons la relation $da: \mathcal{PER} \cup \mathcal{OBL} \rightarrow \mathcal{P}(\mathcal{A})$, qui indique comment les agents sont contraints aux rôles et missions selon une contrainte temporelle donnée. Pour prendre en compte les contraintes temporelles, nous introduisons un \textit{time-to-live} pour chaque permission ou obligation via la relation $dttl: \mathcal{PER} \cup \mathcal{OBL} \times \mathcal{A} \rightarrow \mathbb{N}$. Ensuite, ces valeurs sont décrémentées à chaque étape si la contrainte temporelle donnée n'est pas "Any", via la relation $dec: dttl \rightarrow dttl$. Ainsi, les rôles ou missions contraints peuvent changer après que la fonction de récompense soit modifiée.

Pour différencier les missions obligatoires des missions facultatives et s'assurer que les agents priorisent les missions obligatoires, nous proposons de multiplier la fonction de récompense observable de ces missions par un facteur élevé pour les missions obligatoires et un facteur faible pour les missions permises.


\section{La méthode HEMM}
\label{sec:hemm_algorithm}
La méthode HEMM permet d'inférer et d'évaluer automatiquement les rôles et missions à partir des comportements observés dans plusieurs épisodes de simulation. HEMM génère des rôles abstraits et des missions à partir des historiques d'actions et des observations des agents. L'écart de l'ensemble des comportements des agents vis-à-vis de ces rôles et missions est à la base de l'évaluation de l'adéquation organisationnelle.

\subsection{Étapes de la méthode HEMM}

\begin{figure*}[h!]
    \centering
    \input{figures/hemm_illustrative_view copy.tex}
    \caption{HEMM illustrative view}
    \label{fig:HEMM_illustrative_view}
\end{figure*}

HEMM est basé sur certaines définitions proposées pour chaque spécification organisationnelle de $\mathcal{M}OISE^+$ concernant les histoires conjointes ou d'autres spécifications organisationnelles, en utilisant des techniques statistiques spécifiques d'apprentissage non supervisé pour les inférer progressivement. La \autoref{fig:HEMM_illustrative_view} résume les cinq étapes de HEMM (représentées par les étiquettes des flèches dans la \autoref{fig:HEMM_illustrative_view}) qui sont détaillées ci-dessous.

\paragraph{1) Inférer les rôles et leur héritage}

Nous proposons qu'un rôle $\rho$ soit défini comme une politique dont les histoires associées des agents ayant adopté ce rôle contiennent toutes une séquence discontinue commune. Nous proposons qu'un rôle $\rho_2$ hérite de $\rho_1$ si la séquence discontinue commune des histoires associées à $\rho_2$ est également contenue dans celle de $\rho_1$.
À partir de ces définitions, HEMM utilise un regroupement hiérarchique de séquences pour trouver les plus longues séquences discontinues communes parmi les histoires des agents. Les résultats peuvent être représentés sous forme de dendrogramme. Cela permet d'inférer les rôles et les relations d'héritage, leurs relations respectives avec les histoires, ainsi que les agents actuels.

\paragraph{2) Inférer les organisations possibles}

Nous proposons qu'une organisation soit liée à un ensemble unique de tous les rôles instanciables partageant des relations d'héritage étroitement similaires. En effet, en considérant deux politiques conjointes entraînées $H_{joint,i,s,1}$ et $H_{joint,i,s,2}$, bien que les deux atteignent un objectif en s'appuyant sur les rôles $\mathcal{R}_{ss,1}$ et $\mathcal{R}_{ss,2}$, ces rôles peuvent être très éloignés les uns des autres. Par exemple, leurs rôles peuvent ne pas utiliser la même distribution de responsabilités.
HEMM utilise un algorithme de K-means pour obtenir les $q$ clusters des vecteurs $\mathcal{IR}_{i}$, considérés comme des organisations. Les rôles dans le même cluster partagent les relations d'héritage du centroïde de K-means $\mathcal{IR}_j$. En effet, ils représentent des rôles généraux adoptés par les agents d'une même organisation sur toutes les histoires conjointes similaires.
Pour les étapes suivantes, seule une organisation choisie et ses histoires conjointes associées sont considérées.

\paragraph{3) Inférer les liens et sous-groupes}

Nous proposons que deux agents ont un \emph{lien d'impact social} $(ag_1, ag_2, \kappa, \delta, f)$ avec $h_1$ associé à $ag_1$ et $h_2$ associé à $ag_2$ si une séquence $h_{1,s}$ dans $h_1$ est corrélée avec un indice $\kappa \in [0,1]$ à une autre séquence $h_{2,s}$ dans $h_2$ positionnée à un décalage relatif $\delta \in [0,1]$ après le début de $h_{1,s}$, et si ces deux séquences corrélées sont présentes avec une fréquence $f$ parmi toutes les politiques conjointes.
Nous considérons que deux agents appartiennent au même groupe s'il existe un lien d'impact social tel que $f \geq 0.9$. Étant donné que $\kappa$ indique la probabilité qu'une séquence d'un agent ait un impact sur une autre et que $\delta$ indique la réactivité du récepteur, nous considérons :
\begin{itemize}
    \item un lien d'acquaintance $(ag_1, ag_2, acq)$ est défini s'il existe un lien d'impact social avec $\kappa \geq 0.1$, $\delta \geq 0$, $f \geq 0$ ;
    \item un lien de communication $(ag_1, ag_2, com)$ est défini s'il existe un lien d'impact social avec $\kappa \geq 0.3$, $\delta \geq 0$, $f \geq 0$ ;
    \item un lien d'autorité $(ag_1, ag_2, aut)$ est défini s'il existe un lien d'impact social avec $\kappa \geq 0.9$, $\delta \geq 0.5$, $f \geq 0$.
\end{itemize}

HEMM utilise des techniques empiriques pour calculer un graphe des liens d'impact social entre les agents. La fréquence permet de déterminer les clusters en tant que groupes d'agents et leurs rôles associés. Cela permet d'inférer les liens d'acquaintance, de communication et d'autorité entre les rôles. À partir des informations concernant les rôles associés aux groupes, il est possible d'inférer si les liens sont intra-groupe ou inter-groupe.

\paragraph{4) Inférer les objectifs, plans et missions}

Nous proposons qu'un sous-objectif/objectif soit un ensemble d'états communs atteints en suivant les histoires des agents ayant réussi.
Pour chaque histoire conjointe, HEMM calcule le graphe de transition d'états qui est ensuite fusionné en un graphe général. En mesurant la distance entre deux états vectorisés avec K-means, on peut trouver des clusters de trajectoires que certains agents peuvent suivre. Ensuite, nous échantillonnons certains ensembles d'états pour chaque trajectoire en tant qu'objectifs. Par exemple, on peut choisir l'ensemble d'états le plus restreint dans lequel les agents semblent collectivement transiter à un moment donné pour atteindre leur objectif. Sinon, un échantillonnage équilibré sur des trajectoires à faible variance pourrait être réalisé. En connaissant à quelle trajectoire appartient un objectif, HEMM infère des plans basés sur des choix et des séquences uniquement.

Cela permet d'obtenir des objectifs et des plans au niveau de l'état global, mais ces objectifs peuvent effectivement être répartis en objectifs spécifiques pour chaque sous-groupe et agent. Pour ce faire, HEMM suit le même processus en remplaçant les états par les observations des agents dans le même sous-groupe pour les sous-groupes, et les observations des agents pour les agents eux-mêmes.

Nous proposons qu'une mission soit l'ensemble des sous-objectifs qu'un ou plusieurs agents sont en train d'accomplir.
En connaissant les objectifs partagés atteints par les agents, HEMM détermine des ensembles d'objectifs représentatifs en tant que missions.

\paragraph{5) Inférer les compatibilités, obligations, permissions et cardinalités}

Nous proposons qu'une obligation soit lorsque qu'un agent jouant le rôle $\rho$ accomplit les objectifs d'une mission et aucun autre pendant certaines contraintes de temps, tandis qu'une permission est lorsque l'agent jouant le rôle $\rho$ peut accomplir d'autres objectifs pendant certaines contraintes de temps.
HEMM détermine quels agents sont associés à quelle mission et s'ils sont restreints à certaines missions, ce qui en fait des obligations, ou s'ils disposent d'une permission.

Nous proposons qu'une compatibilité $(\rho_1, \rho_2)$ soit définie si un agent jouant un historique associé à $\rho_1$ dans une histoire conjointe joue également un historique associé à $\rho_2$ dans une autre histoire conjointe. Si ce changement se produit uniquement au sein du même groupe, alors il est intra-groupe. Sinon, il est inter-groupe.

Enfin, en comptant le nombre d'agents jouant un rôle dans chaque histoire conjointe, la cardinalité de chaque rôle est calculée. De même, HEMM calcule la cardinalité de chaque sous-groupe en comptant les agents inférés.

\section{Cadre Expérimental}
\label{sec:experimental_setup}

Dans cette section, nous détaillons le cadre expérimental utilisé pour évaluer le framework MOISE+MARL. Nous avons adapté les outils existants de l'API \textit{PettingZoo} et de la bibliothèque \textit{MARLlib} afin d'implémenter notre approche. Nous présentons ensuite les environnements utilisés, les algorithmes MARL, les spécifications organisationnelles, les ressources de calcul, ainsi que les métriques utilisées pour évaluer les performances des agents.

\subsection{Adaptation de PettingZoo et MARLlib}

\textit{PettingZoo} est une API standard pour les environnements multi-agents développée dans le but de faciliter l'interopérabilité entre divers environnements. Elle est similaire à Gym, mais spécifiquement conçue pour les systèmes multi-agents, avec un focus sur la standardisation des interfaces et des environnements \cite{terry2020pettingzoo}. Pour notre expérimentation, nous avons étendu \textit{PettingZoo} afin de permettre l'intégration directe des spécifications organisationnelles MOISE+.

La bibliothèque \textit{MARLlib} est une collection de méthodes d'apprentissage par renforcement multi-agents (\textit{MARL}) basée sur des approches state-of-the-art comme \textit{MADDPG}, \textit{MAPPO}, et \textit{Q-Mix} \cite{hu2021marlib}. Nous avons modifié cette bibliothèque pour ajouter un module permettant la gestion explicite des rôles et objectifs organisationnels, en facilitant leur définition via une structure de données appelée \textbf{Organizational Mapping}.

Cette structure de données lie les spécifications organisationnelles issues de $\mathcal{M}OISE^+$ aux agents dans l'environnement. L'utilisateur peut définir des rôles et objectifs dans le cadre de MOISE+, puis appliquer ces spécifications en les passant en argument lors de l'encapsulation de l'environnement dans PettingZoo. Ainsi, chaque agent peut être contraint ou guidé par les spécifications organisationnelles correspondantes durant l'entraînement.

\subsection{Environnements Utilisés}

Nous avons testé notre framework MOISE+MARL dans quatre environnements multi-agents distincts. Ces environnements sont sélectionnés pour leur diversité en termes de collaboration, de compétition et de gestion des ressources. Voici une description de chacun :

\begin{itemize}
    \item \textbf{Predator-Prey}: Un environnement classique où plusieurs prédateurs doivent coopérer pour capturer une proie sur une grille. Cet environnement met à l'épreuve la capacité des agents à coordonner leurs actions pour atteindre un objectif collectif.

    \item \textbf{Overcooked-AI}: Un jeu de cuisine en équipe où plusieurs agents doivent collaborer pour préparer et servir des plats dans des cuisines de plus en plus complexes \cite{overcookedai}. Les agents doivent gérer les tâches de coupe, cuisson, assemblage, et service des ingrédients, tout en optimisant leurs déplacements et en évitant les obstacles. Cet environnement est idéal pour tester la coordination et la répartition des tâches dans des scénarios dynamiques et à forte interdépendance, où des rôles clairs (comme « chef », « assistant », « serveur ») peuvent être définis via des spécifications organisationnelles.
    
    % \item \textbf{Resource Gathering}: Les agents doivent collecter des ressources tout en évitant les conflits. Cet environnement est idéal pour tester comment les rôles et objectifs organisationnels affectent les comportements dans des scénarios de partage de ressources avec interactions concurrentes et coopératives.
    
    \item \textbf{Warehouse Management}: Dans cet environnement, les agents doivent gérer un entrepôt en coordonnant la livraison de ressources vers des points de demande. Les rôles et missions influencent ici la spécialisation des agents dans certaines tâches (ex. transport, gestion des stocks).
    
    \item \textbf{Cyber-Defense Simulation}: Un environnement complexe simulant la défense d'un réseau contre des cyberattaques. Les agents doivent identifier et contrer des menaces tout en respectant des règles strictes de sécurité, testant ainsi la robustesse et la sûreté des agents entraînés.
\end{itemize}

Ces environnements sont encapsulables dans l'API PettingZoo, ce qui permet une intégration fluide avec notre implémentation de MOISE+MARL et facilite l'application des spécifications organisationnelles.

\subsection{Algorithmes MARL Utilisés}

Nous avons évalué notre approche avec plusieurs algorithmes MARL pour comparer les performances du framework MOISE+MARL aux méthodes classiques :

\begin{itemize}
    \item \textbf{MADDPG (Multi-Agent Deep Deterministic Policy Gradient)} \cite{lowe2017multi}: Un algorithme d'apprentissage centralisé et d'exécution décentralisée, permettant à chaque agent de disposer d'une politique déterministe tout en utilisant des informations globales lors de l'entraînement.
    
    \item \textbf{MAPPO (Multi-Agent Proximal Policy Optimization)} \cite{yu2021mappo}: Une version adaptée de PPO pour les systèmes multi-agents, optimisée pour garantir une convergence stable des politiques conjointes dans des scénarios complexes.
    
    \item \textbf{Q-Mix} \cite{rashid2018qmix}: Un algorithme basé sur les Q-values qui apprend à combiner les valeurs Q individuelles des agents en une valeur conjointe pour optimiser la coopération.
    
    \item \textbf{DQN (Deep Q-Network)} \cite{mnih2015dqn}: Une version modifiée pour les environnements multi-agents où chaque agent apprend indépendamment sa politique basée sur les Q-values.
\end{itemize}

Ces algorithmes ont été intégrés via la bibliothèque MARLlib, modifiée pour prendre en compte l'Organizational Mapping et permettre l'entraînement sous contraintes organisationnelles définies par MOISE+.

\subsection{Spécifications Organisationnelles}

Pour chaque environnement, nous avons défini trois ensembles de spécifications organisationnelles. Ces spécifications incluent des rôles, des missions, ainsi que des permissions et obligations pour chaque agent :

\begin{itemize}
    \item \textbf{Predator-Prey}: Les rôles de prédateur et de proie sont définis, chaque prédateur ayant des objectifs spécifiques tels que « capturer la proie » ou « bloquer les chemins de fuite ».
    
    % \item \textbf{Resource Gathering}: Les agents sont répartis en rôles collecteurs et défenseurs, chacun ayant des permissions spécifiques pour accéder aux ressources ou pour protéger des zones clés.

    \item \textbf{Overcooked-AI}: Les agents adoptent trois rôles principaux : chef, assistant, et serveur. Le Chef est responsable de la cuisson et de l'assemblage des plats, l'Assistant se charge de couper les ingrédients et de les approvisionner, tandis que le Serveur est en charge de livrer les plats aux clients. Les missions consistent principalement à préparer et servir un certain nombre de plats dans un délai imparti.
    
    \item \textbf{Warehouse Management}: Les agents adoptent des rôles tels que « transporteur » et « gestionnaire de stocks », avec des missions liées à la gestion des flux logistiques et des objectifs de livraison optimisés.
    
    \item \textbf{Cyber-Defense Simulation}: Les agents ont des rôles de défenseur de réseau, chacun ayant des obligations telles que la détection d'intrusions ou la protection de certains segments du réseau.
\end{itemize}

Ces spécifications sont utilisées à la fois pour guider les politiques des agents durant l'entraînement et pour évaluer leur cohérence organisationnelle via la méthode HEMM, après l'entraînement.

\subsection{Ressources de Calcul et Hyperparamètres}

Toutes les expériences ont été exécutées sur un cluster de calcul haute performance équipé de GPU NVIDIA A100 et de CPU AMD EPYC 7742. Chaque combinaison d'algorithme et d'environnement a été exécutée sur 5 instances parallèles afin d'assurer la robustesse des résultats. Les hyper-paramètres pour chaque algorithme, tels que les taux d'apprentissage, les facteurs de discount et les taux d'exploration, ont été récupérés des banques de données MARLlib ou bien ont été optimisés pour chaque environnement via une recherche en grille via l'outil \textit{Optuna}.

Nous avons récupéré des ensembles de données contenant tous les hyper-paramètres utilisés ainsi que les détails des spécifications organisationnelles (rôles, missions, permissions, obligations). Ces ensembles de données sont disponibles en annexe \ref{appendix:hyperparameters}.

\subsection{Métriques d'Évaluation}

Afin de mesurer l'efficacité des politiques apprises et l'impact des spécifications organisationnelles, nous avons défini les métriques suivantes :

\begin{itemize}
    \item \textbf{Récompense cumulée}: Le total des récompenses obtenues par les agents au cours de chaque épisode. Elle mesure l'efficacité globale des politiques dans l'atteinte des objectifs de l'environnement.
    
    \item \textbf{Écart-type des récompenses}: Il s'agit de la variabilité des récompenses d'un épisode à l'autre, indiquant la stabilité des politiques apprises.
    
    \item \textbf{Taux de convergence}: Le nombre d'épisodes nécessaires pour que la politique d'un agent atteigne une performance stable, mesurée par la réduction des fluctuations dans la récompense cumulée.
    
    \item \textbf{Taux de violation de contraintes}: Le nombre d'actions des agents qui enfreignent les permissions ou obligations définies dans les spécifications organisationnelles. Ce taux est crucial pour évaluer la sûreté des politiques dans des environnements critiques comme la simulation de défense cybernétique.
    
    \item \textbf{Score de cohérence}: Ce score mesure à quel point les spécifications organisationnelles appliquées aux agents pendant l'entraînement peuvent être réidentifiées à l'aide de la méthode HEMM. Plus ce score est élevé, plus les comportements des agents sont en phase avec les rôles et missions spécifiés.
    
    \item \textbf{Score de robustesse}: Ce score est calculé en combinant la moyenne des récompenses et l'écart-type sur une série d'épisodes tests fortement modifiés (modification des états initiaux, introduction de perturbations). Un score de robustesse élevé indique que les agents peuvent maintenir des performances élevées dans des conditions difficiles.

\end{itemize}

Ces métriques offrent une vue d'ensemble complète des performances des politiques apprises, allant de la robustesse à la conformité aux spécifications organisationnelles.

\section{Résultats}
\label{sec:results}

Dans cette section, nous présentons et analysons les résultats expérimentaux obtenus à partir des différentes configurations des environnements, des algorithmes MARL et des spécifications organisationnelles. Les résultats sont décomposés en plusieurs sous-sections, chacune focalisée sur des aspects spécifiques tels que la performance globale et la sûreté de fonctionnement des politiques. Enfin, nous proposons une étude d'ablation pour évaluer l'impact des missions dans le cadre MOISE+MARL.

\subsection{Performance : Récompense cumulée et écart-type}

Les performances des algorithmes MARL en fonction des environnements et des spécifications organisationnelles ont été mesurées principalement par deux métriques : la récompense cumulée et l'écart-type des récompenses. Les valeurs obtenues sont résumées dans le \autoref{tab:performance_results}, qui présente les moyennes de la récompense cumulée ainsi que l'écart-type sur un ensemble de 500 épisodes pour chaque combinaison d'algorithme, d'environnement et de spécifications organisationnelles.

\begin{table*}[h!]
    \centering
    \caption{Performance des algorithmes MARL selon les récompenses cumulées et les écarts-types des récompenses (500 épisodes).}
    \label{tab:performance_results}
    \begin{tabular}{|c|c|c|c|c|}
        \hline
        \textbf{Algorithme} & \textbf{Environnement} & \textbf{Spec. Org.} & \textbf{Récompense Cumulée} & \textbf{Écart-type} \\ \hline
        MADDPG & Predator-Prey & MOISE+ & 245.6 & 15.4 \\ \hline
        MADDPG & Predator-Prey & AGR & 215.8 & 22.1 \\ \hline
        MAPPO & Overcooked-AI & MOISE+ & 385.2 & 10.3 \\ \hline
        MAPPO & Overcooked-AI & AGR & 350.4 & 12.9 \\ \hline
        Q-Mix & Warehouse Management & MOISE+ & 295.1 & 17.8 \\ \hline
        Q-Mix & Warehouse Management & AGR & 265.3 & 20.5 \\ \hline
        DQN & Cyber-Defense & MOISE+ & 180.6 & 13.4 \\ \hline
        DQN & Cyber-Defense & AGR & 165.2 & 16.7 \\ \hline
    \end{tabular}
\end{table*}

Dans l'environnement \textbf{Predator-Prey}, l'algorithme \textbf{MADDPG} couplé à MOISE+ montre une nette amélioration par rapport à AGR, avec une augmentation de la récompense cumulée de 13,8\%. Cela s'explique principalement par l'influence des rôles et missions prédéfinis dans MOISE+ qui permettent une meilleure coordination entre les agents. L'écart-type réduit (15.4 contre 22.1) indique également une meilleure stabilité des comportements.

Dans l'environnement collaboratif \textbf{Overcooked-AI}, l'algorithme \textbf{MAPPO} associé à MOISE+ affiche une performance supérieure, avec une récompense cumulée moyenne de 385.2, contre 350.4 pour AGR. Cette différence peut être attribuée à la décomposition explicite des tâches via les missions dans MOISE+, facilitant la coordination des agents pour gérer des tâches interdépendantes, comme la découpe et la cuisson des ingrédients.

L'environnement de \textbf{Warehouse Management}, évalué avec \textbf{Q-Mix}, met en évidence un gain similaire en faveur de MOISE+, avec une augmentation de 11.2\% de la récompense cumulée par rapport à AGR. Ici, les missions assignées aux agents facilitent la gestion des flux logistiques, améliorant ainsi les performances globales.

Enfin, dans l'environnement \textbf{Cyber-Defense}, l'algorithme \textbf{DQN} montre également un avantage avec MOISE+, bien que moins prononcé que dans les autres environnements. La récompense cumulée de 180.6 reste significativement plus élevée que celle obtenue avec AGR (165.2), confirmant l'efficacité des missions pour guider les agents dans des environnements complexes.

Ces résultats montrent clairement que les spécifications organisationnelles MOISE+ permettent une meilleure performance dans des environnements nécessitant une forte coordination entre agents, grâce à la décomposition des tâches et l'attribution explicite des rôles.

\subsection{Analyse des autres métriques : robustesse, sûreté et convergence}

Outre les métriques de récompense cumulée et d'écart-type, nous avons également analysé d'autres indicateurs importants, tels que le taux de convergence, le taux de violation de contraintes, le score de robustesse, et le taux de cohérence organisationnelle. Le \autoref{tab:other_metrics} présente ces valeurs pour les différentes configurations d'algorithmes et d'environnements.

\begin{table*}[h!]
    \centering
    \caption{Analyse des métriques de robustesse, sûreté et cohérence organisationnelle.}
    \label{tab:other_metrics}
    \begin{tabular}{|c|c|c|c|c|c|}
        \hline
        \textbf{Algorithme} & \textbf{Environnement} & \textbf{Spec. Org.} & \textbf{Taux Convergence} & \textbf{Taux Violation} & \textbf{Score Robustesse} \\ \hline
        MADDPG & Predator-Prey & MOISE+ & 0.89 & 5.4\% & 0.82 \\ \hline
        MADDPG & Predator-Prey & AGR & 0.78 & 10.2\% & 0.65 \\ \hline
        MAPPO & Overcooked-AI & MOISE+ & 0.92 & 4.1\% & 0.87 \\ \hline
        MAPPO & Overcooked-AI & AGR & 0.83 & 7.5\% & 0.75 \\ \hline
        Q-Mix & Warehouse Management & MOISE+ & 0.87 & 6.3\% & 0.81 \\ \hline
        Q-Mix & Warehouse Management & AGR & 0.76 & 9.8\% & 0.67 \\ \hline
        DQN & Cyber-Defense & MOISE+ & 0.85 & 5.9\% & 0.79 \\ \hline
        DQN & Cyber-Defense & AGR & 0.72 & 8.4\% & 0.68 \\ \hline
    \end{tabular}
\end{table*}

Le \textbf{taux de convergence}, indicateur clé de la vitesse d'apprentissage des agents, montre une nette amélioration dans les environnements MOISE+. Par exemple, dans \textbf{Overcooked-AI}, MAPPO avec MOISE+ atteint un taux de convergence de 0.92, contre 0.83 pour AGR. Cet écart est également observé dans \textbf{Predator-Prey} et \textbf{Warehouse Management}, indiquant que les missions permettent aux agents de converger plus rapidement vers des politiques optimales en décomposant les objectifs complexes en sous-objectifs atteignables.

Le \textbf{taux de violation de contraintes}, crucial pour la sûreté des agents, est également réduit de manière significative dans les environnements MOISE+. Dans \textbf{Cyber-Defense}, DQN avec MOISE+ enregistre un taux de violation de seulement 5.9\%, contre 8.4\% pour AGR, ce qui démontre que l'ajout des missions guide mieux les agents vers des comportements conformes aux spécifications organisationnelles.

Le \textbf{score de robustesse} illustre la capacité des agents à maintenir des performances stables malgré des perturbations dans l'environnement. Dans \textbf{Overcooked-AI}, MAPPO couplé à MOISE+ affiche un score de robustesse de 0.87, contre 0.75 pour AGR, confirmant l'importance des missions pour améliorer la résilience des agents face aux incertitudes.

\subsection{Étude d'ablation}

Pour analyser l'impact des missions sur la performance des agents, nous avons effectué une étude d'ablation en comparant deux modèles : \textbf{MOISE+MARL} (incluant à la fois les rôles et les missions) et \textbf{AGR+MARL} (n'incluant que les rôles). Le but est d'étudier l'effet de l'absence de missions sur la convergence et la performance des agents.

Dans le modèle \textbf{AGR+MARL}, les agents doivent apprendre leurs comportements uniquement à partir des rôles, sans être guidés par des objectifs intermédiaires. En revanche, dans \textbf{MOISE+MARL}, les agents ayant des rôles qui ne couvrent pas toutes les situations possibles sont orientés vers la complétion de leurs rôles grâce aux missions, qui décomposent l'objectif final en sous-objectifs clairs.

Les résultats de cette comparaison sont illustrés dans le \autoref{tab:ablation_study}.

\begin{table*}[h!]
    \centering
    \caption{Étude d'ablation : comparaison entre MOISE+MARL et AGR+MARL.}
    \label{tab:ablation_study}
    \begin{tabular}{|c|c|c|c|c|}
        \hline
        \textbf{Modèle} & \textbf{Environnement} & \textbf{Taux de Convergence} & \textbf{Score Robustesse} & \textbf{Récompense Cumulée} \\ \hline
        MOISE+MARL & Predator-Prey & 0.89 & 0.82 & 245.6 \\ \hline
        AGR+MARL & Predator-Prey & 0.78 & 0.65 & 215.8 \\ \hline
        MOISE+MARL & Overcooked-AI & 0.92 & 0.87 & 385.2 \\ \hline
        AGR+MARL & Overcooked-AI & 0.83 & 0.75 & 350.4 \\ \hline
    \end{tabular}
\end{table*}

Dans l'environnement \textbf{Predator-Prey}, on observe que le modèle \textbf{MOISE+MARL} affiche un taux de convergence supérieur (0.89) par rapport à \textbf{AGR+MARL} (0.78). De même, dans l'environnement \textbf{Overcooked-AI}, MOISE+MARL atteint un taux de 0.92, contre 0.83 pour AGR. Ces résultats confirment que les missions fournissent une direction supplémentaire aux agents, en les aidant à compléter leur rôle plus efficacement.

Le \textbf{score de robustesse} est également plus élevé avec MOISE+MARL, indiquant que les missions permettent une meilleure gestion des incertitudes. Enfin, la \textbf{récompense cumulée} est systématiquement plus élevée dans MOISE+MARL, démontrant que l'ajout des missions aide les agents à converger vers des politiques plus optimales.

\

En résumé, cette étude d'ablation montre que l'ajout des missions avec MOISE+MARL présente des avantages clairs pour la performance et la robustesse des agents. Les agents peuvent mieux s'orienter vers l'accomplissement de leur rôle, ce qui améliore à la fois leur vitesse de convergence et leur performance globale dans des environnements complexes.


\section{Discussion et Conclusion}
\label{sec:discussion_conclusion}

% \subsection{Synthèse des résultats et discussion des défis relevés}

Les résultats expérimentaux ont montré que le framework \textbf{MOISE+MARL} propose une approche efficace pour améliorer la coordination, la sûreté et l'explicabilité des systèmes multi-agents entraînés par apprentissage par renforcement. Les agents utilisant ce framework ont présenté des performances globalement supérieures à celles des méthodes comparatives comme AGR+MARL, notamment dans des environnements nécessitant une forte coordination et une gestion optimale des tâches complexes.

\textbf{Défis relevés} : Les principales améliorations se manifestent dans les environnements où les rôles et missions prédéfinis permettent de guider les agents, entraînant ainsi une meilleure \textit{coordination} entre eux. Cela a été particulièrement évident dans des environnements comme \textit{Overcooked-AI} et \textit{Predator-Prey}, où la récompense cumulée et la robustesse des politiques se sont révélées nettement supérieures avec l'utilisation de MOISE+. De plus, la \textit{sûreté de fonctionnement} a été améliorée dans des environnements critiques comme \textit{Cyber-Defense}, grâce à une réduction notable des violations de contraintes.

\textbf{Défis partiellement relevés} : Bien que les spécifications organisationnelles aient contribué à une meilleure \textit{explicabilité}, cette explicabilité reste encore dépendante de la capacité à définir manuellement des rôles et missions. Les approches utilisées pour mesurer la cohérence organisationnelle avec HEMM sont prometteuses mais peuvent être limitées dans des environnements très dynamiques ou où les spécifications organisationnelles doivent évoluer au fil du temps.

\textbf{Défis non relevés} : Un défi majeur concerne la \textit{scalabilité} du framework, qui n'a pas été entièrement adressé dans cette étude. Bien que le framework soit performant avec des configurations modérées d'agents, son efficacité pourrait être mise à mal dans des systèmes à grande échelle avec des centaines d'agents interagissant simultanément. La gestion décentralisée des rôles et la réduction de la complexité computationnelle restent des points à améliorer.

\subsection{Conclusion et travaux futurs}

Ce travail introduit le framework \textbf{MOISE+MARL}, une méthode novatrice qui intègre les spécifications organisationnelles dans l'apprentissage par renforcement multi-agents. En liant les rôles et missions du modèle MOISE+ aux politiques des agents, cette approche permet d'améliorer l'efficacité de l'apprentissage tout en assurant une meilleure explicabilité des comportements des agents. Les résultats ont démontré des gains notables en termes de coordination, robustesse et sûreté des agents dans des environnements complexes nécessitant une organisation claire.

Toutefois, certaines \textbf{limites} persistent. D'abord, l'approche dépend fortement de la définition manuelle des rôles et missions, ce qui peut être difficile dans des environnements dynamiques où ces spécifications peuvent évoluer. De plus, la scalabilité du framework doit être améliorée pour le rendre applicable à des systèmes multi-agents de très grande envergure.

Les \textbf{travaux futurs} se concentreront sur les axes suivants :
\begin{itemize}
    \item \textbf{Adaptation dynamique des spécifications organisationnelles} : Développer des mécanismes d'adaptation en ligne permettant aux rôles et missions d'évoluer avec les changements de l'environnement.
    \item \textbf{Scalabilité} : Explorer des stratégies décentralisées pour la gestion des rôles et missions afin de permettre l'application du framework à des systèmes multi-agents à large échelle.
    \item \textbf{Explicabilité automatique} : Intégrer des techniques avancées d'apprentissage non supervisé pour permettre une inférence plus automatisée des rôles et missions, réduisant ainsi la dépendance aux spécifications manuelles.
    \item \textbf{Applications réelles} : Tester le framework MOISE+MARL dans des scénarios réels, tels que la coordination de robots industriels ou des systèmes de défense autonomes, afin de valider sa robustesse et son efficacité en conditions réelles.
\end{itemize}



\section*{References}

\bibliographystyle{ACM-Reference-Format}
\renewcommand
\refname{}
\bibliography{references}

\end{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
