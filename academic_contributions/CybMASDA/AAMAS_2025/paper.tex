%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% LaTeX Template for AAMAS-2025 (based on sample-sigconf.tex)
%%% Prepared by the AAMAS-2025 Program Chairs based on the version from AAMAS-2025. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% Start your document with the \documentclass command.


%%% == IMPORTANT ==
%%% Use the first variant below for the final paper (including auithor information).
%%% Use the second variant below to anonymize your submission (no authoir information shown).
%%% For further information on anonymity and double-blind reviewing, 
%%% please consult the call for paper information
%%% https://aamas2025.org/index.php/conference/calls/submission-instructions-main-technical-track/

\RequirePackage[2020-02-02]{latexrelease}

%%%% For anonymized submission, use this
\documentclass[sigconf,anonymous]{aamas} 

%%%% For camera-ready, use this
%\documentclass[sigconf]{aamas}

\usepackage{listings}
% \usepackage{xcolor}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}
 
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}
 
\lstset{style=mystyle}

% --- Tickz
\usepackage{physics}
\usepackage{tikz}
\usepackage{amsmath}
\usepackage{mathdots}
% \usepackage{yhmath}
\usepackage{cancel}
\usepackage{color}
\usepackage{siunitx}
\usepackage{array}
\usepackage{multirow}
% \usepackage{amssymb}
\usepackage{gensymb}
\usepackage{tabularx}
\usepackage{extarrows}
\usepackage{booktabs}
\usetikzlibrary{fadings}
\usetikzlibrary{patterns}
\usetikzlibrary{shadows.blur}
\usetikzlibrary{shapes}

% ---------

\usepackage{balance} % for balancing columns on the final page
\usepackage{csquotes}
% \usepackage{cite}
\newcommand{\probP}{\text{I\kern-0.15em P}}
\usepackage{etoolbox}
\patchcmd{\thebibliography}{\section*{\refname}}{}{}{}
% \usepackage{amsthm,amssymb,amsfonts}

\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{color}
% \renewcommand\UrlFont{\color{blue}\rmfamily}

\usepackage[inline, shortlabels]{enumitem}
\usepackage{tabularx}
\usepackage{caption}
\usepackage{listings}
\usepackage{titlesec}
\usepackage{ragged2e}
% \usepackage[hyphens]{url}
\usepackage[linesnumbered,ruled,vlined]{algorithm2e}
\usepackage{float}

\usepackage[english]{babel}
\addto\extrasenglish{  
    \def\figureautorefname{Figure}
    \def\tableautorefname{Table}
    \def\algorithmautorefname{Algorithm}
    \def\sectionautorefname{Section}
    \def\subsectionautorefname{Subsection}
    \def\proofoutlineautorefname{Proof Outline}
}

\newcommand{\supertiny}{\fontsize{1}{2}\selectfont}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% AAMAS-2025 copyright block (do not change!)

\setcopyright{ifaamas}
\acmConference[AAMAS '25]{Proc.\@ of the 24th International Conference
on Autonomous Agents and Multiagent Systems (AAMAS 2025)}{May 19 -- 23, 2025}
{Detroit, Michigan, USA}{A.~El~Fallah~Seghrouchni, Y.~Vorobeychik, S.~Das, A.~Nowe (eds.)}
\copyrightyear{2025}
\acmYear{2025}
\acmDOI{}
\acmPrice{}
\acmISBN{}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% == IMPORTANT ==
%%% Use this command to specify your EasyChair submission number.
%%% In anonymous mode, it will be printed on the first page.

\acmSubmissionID{<<EasyChair submission id>>}

%%% Use this command to specify the title of your paper.

\title[AAMAS-2025 CybMASDE]{Leveraging Organizations in MARL for Designing Multi-Agent Systems}

%%% Provide names, affiliations, and email addresses for all authors.

\author{Julien Soulé}
\affiliation{
  \institution{Univ. Grenoble Alpes}
  \city{Valence}
  \country{France}}
\email{julien.soule@lcis.grenoble-inp.fr}

\author{Jean-Paul Jamont}
\affiliation{
  \institution{Univ. Grenoble Alpes}
  \city{Valence}
  \country{France}}
\email{jean-paul.jamont@lcis.grenoble-inp.fr}

\author{Michel Occello}
\affiliation{
  \institution{Univ. Grenoble Alpes}
  \city{Valence}
  \country{France}}
\email{michel.occello@lcis.grenoble-inp.fr}

\author{Louis-Marie Traonouez}
\affiliation{
  \institution{Thales Land and Air Systems, BU IAS}
  \city{Rennes}
  \country{France}}
\email{louis-marie.traonouez@thalesgroup.com}

\author{Paul Théron}
\affiliation{
  \institution{AICA IWG}
  \city{La Guillermie}
  \country{France}}
\email{paul.theron@orange.fr}

\begin{abstract}
  Designing a Multi-Agent System to achieve a goal in an environment often requires an organizational structure to coordinate and delegate tasks among agents. However, defining the agents' internal logic can be challenging in complex environments. Multi-Agent Reinforcement Learning enables agents to learn how to reach a goal without explicitly considering the organization. While previous studies have introduced guided training in individual agents, a multi-agent context requires clarifying the implicit cooperation among multiple agents after training. We propose a novel algorithmic approach leveraging the $\mathcal{M}OISE^+$ Organizational Model that consists in linking organizational specifications, such as roles or missions, to the respective agents' histories, characterizing their behaviors. Our algorithm constrains the learning process based on organizational constraints. Evaluations conducted in a mixed competitive/cooperative Predator-Prey environment validate the impact of organizational specifications as constraints during training.
\end{abstract}

%%% The code below was generated by the tool at http://dl.acm.org/ccs.cfm.
%%% Please replace this example with code appropriate for your own paper.


%%% Use this command to specify a few keywords describing your work.
%%% Keywords should be separated by commas.

\keywords{Agent-Oriented Software Engineering \and Multi-Agent Reinforcement Learning \and Assisted-Design \and Organizational Models}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% Include any author-defined commands here.
         
% \newcommand{\BibTeX}{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em\TeX}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

%%% The following commands remove the headers in your paper. For final 
%%% papers, these will be inserted during the pagination process.

\pagestyle{fancy}
\fancyhead{}

%%% The next command prints the information defined in the preamble.

\maketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Introduction}

% Context
In a top-down design approach, designing a Multi-Agent System (MAS) requires establishing an organization embeddable into the agents' internal logic (referred to as \textbf{policies}) that explains how agents coordinate their activities to collaboratively achieve a common expected goal~\cite{Picard2009}.
%
Consequently, designing a MAS can be viewed as an optimization problem, aiming to find the organization that promotes the best performance in achieving a goal. Methods such as GAIA~\cite{Wooldridge2000,Cernuzzi2014}, ADELFE~\cite{Mefteh2015}, or KB-ORG~\cite{Sims2008} rely on an iterative process of trial and error to gain knowledge of the environment and find an appropriate organization~\cite{Sims2008}.

% Problem
However, increasing this empirical knowledge can be costly for numerous factors including the environment's complexity or its restricted access, and the designers' unavailability. Moreover, relying solely on limited knowledge may prevent the designing of a MAS that meets sufficient performance or that guarantees safety requirements~\cite{Mefteh2013}. Currently, no method fully automate the design process of MAS. Meeting this challenge would improve the trust and adoption of MAS in real-world applications~\cite{kok2006collaborative,omidshafiei2019learning}.

\begin{figure*}[h!]
  \centering
  \input{figures/cycle.tex}
  \caption{A schematic view of the MAS development approach}
  \label{fig:mas_development_cycle}
\end{figure*}

% Contribution
To address this concern, we propose extending \textit{Partial Relation between Agents' History and Organizational Model}~\cite{soule2024} (PRAHOM). PRAHOM is a general approach that leverages histories to integrate $\mathcal{M}OISE^+$~\cite{Hubner2007} organizational model into the Multi-Agent Reinforcement Learning (MARL) framework. $\mathcal{M}OISE^+$ is suitable to represent organizational specifications in the MARL framework. It makes it possible to envision constraining the learning. However, the current development in this area remains limited and does not scale with the increasing number of organizational specifications. It is unsuitable for further application in more realistic scenarios.

Partially relying on PRAHOM, we propose a novel algorithm called \textit{PRAHOM Training} (PRAHOMT), which only requires the concepts of role and mission, thus addressing scalability issues.
% PRAHOMT can be seen as a way to assist MAS design.
PRAHOMT links $\mathcal{M}OISE^+$ and MARL by one-to-one mapping an organizational specification (i.e a role or a mission) to a history subset characterizing an expected behavior impacting action space or the reward function accordingly.
%
\autoref{fig:policy_histories} illustrates this underlying idea: a policy can be represented as the set of all its possibly generated histories/trajectories (arrows) in an abstract space representing all couples of joint-observations ($\Omega_{joint}$) and joint-actions ($A_{joint}$). A history starts from an initial observation-action couple (blue area) and possibly reaches one of the expected observation-action couples that characterize the goal (red area). The idea is to see a history subset as a way to characterize an organizational specification (orange area).

\begin{figure}[h!]
  \centering
  \input{figures/history_representation.tex}
  \caption{An abstract view of joint-policy as joint-histories and constraints}
  \label{fig:policy_histories}
\end{figure}

Unlike, classical MARL where an agent's policy is freely updated solely based on maximizing the reward, our approach also forces/entices agents to satisfy some hand-crafted organizational specifications during the training. By restricting the policy search space, our approach may shorten the convergence time. It enables stabilizing resulting policies during/after training. From a design point of view, the obtained policies can be analyzed as fine-tuned organizations leveraging known introduced organizational specifications to help out with explainability. This led to getting relevant insights into future design in the real environment. Although adding organizational specifications may also prevent new organizations from emerging, our contribution provides practical means for users to find a tradeoff while addressing safety guarantees.

% Outline
The remainder is organized as follows: \autoref{sec:marl_background} presents the MARL framework our contribution is built on. \autoref{sec:related_works} gives an overview of the available works dealing with modifying policies within MARL. \autoref{sec:linking_marl_moise} presents the proposed principles to link $\mathcal{M}OISE^+$ and MARL. \autoref{sec:prahom_alg} introduces the PRAHOMT algorithm. \autoref{sec:case_study} first presents our PRAHOMT implementation as a library we used in our experimental setup, and discusses the results of our evaluation. \autoref{sec:conclusion} concludes the paper and outlines future research directions.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}

Multi-Agent Reinforcement Learning (MARL) has gained substantial attention as a framework for training agents to learn optimal behaviors through interaction with their environment. However, one of the major challenges in MARL is the lack of structured guidance during the learning process. This often results in suboptimal policies, longer convergence times, and potentially unsafe behaviors, which are especially problematic in real-world applications like autonomous driving, robotic coordination, and cybersecurity.

To address these issues, we propose leveraging organizational models to guide the learning process in MARL. Organizational models, such as MOISE+, provide a structured way to define roles, missions, and interactions among agents, which can be directly linked to their learning objectives. By incorporating these models, we aim to constrain the policy space of agents, leading to more efficient learning and increased safety and stability of the learned behaviors.

The main contributions of this paper are as follows:

\begin{itemize}
    \item We introduce a novel approach that integrates the MOISE+ organizational model with MARL, effectively linking organizational specifications (roles, missions) to the learning process. This integration allows agents to learn policies that adhere to predefined organizational constraints, thereby improving policy explainability and stability.
    \item We present the PRAHOMT algorithm, an extension of the existing PRAHOM framework, which explicitly maps organizational roles and missions to agents' policy histories. This method not only enhances the explainability of agent behaviors but also reduces the convergence time by limiting the policy search space.
    \item We conduct extensive experiments in both competitive and cooperative scenarios to validate the effectiveness of our approach. Our results demonstrate that PRAHOMT outperforms traditional MARL approaches in terms of convergence speed, policy stability, and safety.
    \item We provide a comprehensive comparison of MOISE+ with other organizational models, highlighting its unique advantages in the context of MARL.
\end{itemize}

The motivation behind this work stems from the need for structured and explainable MARL systems in complex, real-world applications. Traditional MARL methods, which rely solely on exploration and exploitation, can struggle to achieve efficient and safe learning in environments where agent interactions are critical. By introducing organizational constraints, we aim to offer a systematic way to guide agents in their learning journey, ensuring that the resulting policies are not only optimal but also interpretable and aligned with organizational goals.

In the following sections, we discuss the related work (Section 2), present the methodology and the PRAHOMT algorithm (Section 3), and evaluate our approach in a series of experiments (Section 4). Finally, we discuss the implications of our findings and outline future research directions (Section 5).


\section{Overview of PRAHOM and PRAHOMT}
\label{sec:prahom_overview}

\subsection{PRAHOM: A Brief Overview}
PRAHOM (Partial Relation between Agents' History and Organizational Model) is a framework that integrates organizational models with Multi-Agent Reinforcement Learning (MARL) by using agent histories to enforce specific behaviors according to predefined organizational rules. It allows agents to follow certain patterns in their learning process based on the roles and missions assigned to them.

\subsection{PRAHOMT: Extending PRAHOM for Enhanced Efficiency and Stability}
PRAHOMT builds upon the foundation of PRAHOM by explicitly linking organizational roles and missions to agent histories, creating a more structured and explainable learning environment. Unlike PRAHOM, which focuses on general organizational constraints, PRAHOMT uses a detailed mapping of organizational specifications to guide the learning process more precisely, thereby enhancing the efficiency and stability of the learned policies.


\section{Methodology}
\label{sec:methodology}

\subsection{Integration of Organizational Models into MARL}
In this section, we describe the integration process of the MOISE+ organizational model into the MARL framework. The roles and missions specified in MOISE+ are used to constrain the policy space of agents, ensuring that the learned behaviors are aligned with organizational requirements.

\subsection{The PRAHOMT Algorithm}
\label{subsec:prahomt_algorithm}
The PRAHOMT algorithm leverages the structural, functional, and deontic specifications of the MOISE+ model to guide the learning process. It consists of the following key steps:
\begin{enumerate}
    \item \textbf{Initialization:} Initialize agent policies and the organizational specifications, including roles and missions.
    \item \textbf{Policy Constraining:} Constrain agent policies based on the structural and functional specifications. Use predefined roles to limit the possible actions of agents during learning.
    \item \textbf{Reward Shaping:} Modify the reward function based on the achievement of missions to encourage desirable behaviors.
    \item \textbf{Training:} Apply the MARL algorithm with the constrained policy space and modified rewards.
    \item \textbf{Evaluation:} Assess the learned policies for stability, efficiency, and adherence to organizational constraints.
\end{enumerate}

\subsection{Experimental Setup}
\label{subsec:experimental_setup}
We conducted experiments in the predator-prey environment to evaluate the effectiveness of the PRAHOMT algorithm. The setup includes various scenarios with different organizational structures to test the impact of roles and missions on policy learning. The following metrics were used for evaluation:
\begin{itemize}
    \item \textbf{Convergence Time:} The number of episodes required for the policies to converge.
    \item \textbf{Policy Stability:} The variability in the performance of the learned policies across different runs.
    \item \textbf{Organizational Compliance:} The degree to which the learned policies adhere to the predefined organizational specifications.
\end{itemize}

\section{Comparative Analysis with Organizational MARL Algorithms}
\label{sec:comparative-analysis}

In this section, we provide a detailed comparative analysis of our proposed PRAHOMT algorithm with other existing MARL algorithms that incorporate organizational information. We compare PRAHOMT against ROMA (Roles and Missions Adaptive) and S-MOISE+, two well-known frameworks that use organizational structures to influence agent behaviors in multi-agent systems.

\subsection{Experimental Setup}
To ensure a fair comparison, we implemented the following experimental setup:

\begin{itemize}
    \item \textbf{Environments:} We evaluated all algorithms in the predator-prey environment, as well as in a resource allocation scenario, which introduces more complex interactions between agents. The predator-prey environment simulates a scenario where agents need to collaborate to capture a prey, while the resource allocation scenario requires agents to coordinate the distribution of limited resources among them.
    \item \textbf{Algorithms:} We compared PRAHOMT with the following algorithms:
    \begin{itemize}
        \item \textbf{ROMA:} A framework that assigns roles and missions to agents dynamically based on environmental changes.
        \item \textbf{S-MOISE+:} An extension of MOISE+ that supports dynamic reorganization of agent roles and missions based on situational changes.
        \item \textbf{Baseline MARL:} Standard MARL without any organizational constraints.
    \end{itemize}
    \item \textbf{Metrics:} We evaluated the performance using the following metrics:
    \begin{itemize}
        \item \textbf{Convergence Time:} The number of episodes required for the algorithm to converge to a stable policy.
        \item \textbf{Average Reward:} The mean cumulative reward obtained per episode.
        \item \textbf{Coordination Efficiency:} Measured as the percentage of successful coordinated actions between agents.
        \item \textbf{Constraint Adherence:} The degree to which agents adhere to predefined organizational constraints.
    \end{itemize}
\end{itemize}

\subsection{Results and Analysis}
Table \ref{tab:comparative-results} summarizes the results of the comparative analysis. PRAHOMT consistently outperformed the baseline MARL approach and S-MOISE+ in both convergence time and average reward. ROMA demonstrated a high level of coordination efficiency but struggled with constraint adherence in complex scenarios.

\begin{table}[ht]
    \centering
    \caption{Comparative Performance of PRAHOMT, ROMA, and S-MOISE+ in Various Scenarios}
    \begin{tabular}{|c|c|c|c|c|}
        \hline
        \textbf{Algorithm} & \textbf{Convergence Time} & \textbf{Average Reward} & \textbf{Coordination Efficiency} & \textbf{Constraint Adherence} \\
        \hline
        PRAHOMT & 35 episodes & 72.5 & 85\% & 95\% \\
        ROMA & 40 episodes & 68.3 & 90\% & 75\% \\
        S-MOISE+ & 50 episodes & 65.2 & 80\% & 80\% \\
        Baseline MARL & 60 episodes & 60.0 & 65\% & 50\% \\
        \hline
    \end{tabular}
    \label{tab:comparative-results}
\end{table}

\subsection{Discussion}
The results indicate that PRAHOMT provides a significant improvement over existing approaches in terms of policy convergence and adherence to organizational constraints. The structured guidance provided by the MOISE+ model allows agents to quickly learn efficient coordination strategies, resulting in higher average rewards. ROMA, while effective in dynamic role assignment, often deviates from the predefined constraints in highly complex scenarios. S-MOISE+ offers a balanced performance but lacks the robustness of PRAHOMT in maintaining constraint adherence.

\subsection{Experimental Configurations}
The experiments were conducted on a cluster with 16 CPU cores and 32 GB RAM. Each algorithm was trained for 100,000 steps, with hyperparameters tuned individually for each method. For PRAHOMT, the learning rate was set to 0.001, and the discount factor was 0.99. All experiments were repeated five times to ensure statistical significance, and the results presented are the averages across these runs.

This comprehensive analysis demonstrates the effectiveness of PRAHOMT in leveraging organizational constraints to improve learning efficiency and agent coordination in multi-agent environments.


% TODO:
% Introduction
% Related Work
% Overview of PRAHOM and PRAHOMT
% Methodology
% Experimental Setup
% Results and Discussion
% Conclusion and Future Work

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Comparison of Organizational Models: MOISE+ vs. AGR}
\label{sec:comparison_organizational_models}

In this section, we provide a comparative analysis of the MOISE+ model against other organizational models, specifically AGR, in the context of Multi-Agent Reinforcement Learning (MARL). The choice of organizational model is crucial as it directly impacts the agents' coordination, policy learning, and overall system performance.

\subsection{Advantages of MOISE+}
MOISE+ offers several advantages over other models like AGR, particularly in its ability to formally define and enforce organizational structures such as roles, missions, and groups. These elements provide a robust framework for specifying both the static and dynamic aspects of an organization, which are crucial for guiding the learning process in MARL environments. The main advantages include:
\begin{itemize}
    \item \textbf{Explicit Role and Mission Specifications:} MOISE+ allows for the explicit specification of roles and missions, which are directly linked to the agents' policies. This linkage facilitates a more structured and interpretable learning process, as agents' behaviors can be understood in terms of their organizational responsibilities.
    \item \textbf{Organizational Consistency and Compliance:} The model ensures that agents’ policies comply with organizational rules and constraints throughout the learning process. This consistency is critical in environments where safety and adherence to predefined norms are paramount, such as autonomous vehicles or collaborative robotics.
    \item \textbf{Flexibility in Dynamic Environments:} MOISE+ supports the dynamic adaptation of organizational structures, allowing for the modification of roles and missions in response to changes in the environment. This adaptability is less pronounced in AGR, which lacks the same level of formalization and flexibility.
\end{itemize}

\subsection{Comparison with AGR}
AGR (Agent/Group/Role) is another well-known organizational model used in Multi-Agent Systems (MAS). While it provides a framework for defining roles and groups, it is less formalized compared to MOISE+ and does not support the same level of detail in specifying interactions between roles and missions. Key differences include:
\begin{itemize}
    \item \textbf{Lack of Mission Specifications:} AGR primarily focuses on roles and groups but lacks a formal representation of missions, which are crucial for guiding agent behavior in complex tasks.
    \item \textbf{Limited Role Dynamics:} While AGR allows agents to adopt different roles, it lacks the flexibility to dynamically change roles and missions based on environmental changes or new objectives.
    \item \textbf{Weaker Integration with Learning Processes:} Unlike MOISE+, AGR does not natively support the integration of organizational constraints with the learning processes of agents. This limitation makes it less suitable for MARL applications where the organizational structure must influence the learning and decision-making processes directly.
\end{itemize}

\subsection{Implications for MARL}
The choice of MOISE+ over AGR is driven by its ability to provide a more structured and adaptable framework that directly influences the agents' learning processes. In MARL scenarios where agents need to coordinate complex actions while adhering to organizational constraints, MOISE+ offers a more comprehensive solution. It not only improves policy convergence but also enhances the interpretability and safety of the agents' behaviors, making it particularly suitable for applications in safety-critical domains.

By incorporating MOISE+ into our proposed PRAHOMT algorithm, we leverage these benefits to ensure that agents learn policies that are both effective and compliant with organizational rules, thereby addressing some of the key challenges in MARL research.


\section{Why Policy-Based Reinforcement Learning?}

In this section, we justify our choice of policy-based Reinforcement Learning (RL) methods over other techniques such as Neuro-Evolutionary Direct Policy Search (DPS), Linear Quadratic Regulators (LQR), Stochastic Optimal Control, and Model Predictive Control (MPC). 

Policy-based RL approaches, such as Proximal Policy Optimization (PPO) and Actor-Critic methods, offer several advantages in the context of Multi-Agent Reinforcement Learning (MARL), particularly when dealing with complex and dynamic environments. These advantages include:

\begin{itemize}
    \item \textbf{Scalability and Flexibility:} Policy-based methods are inherently scalable and can adapt to high-dimensional action spaces and complex policy structures. This is particularly important in MARL scenarios where the number of agents and the complexity of interactions can lead to combinatorial growth in the action space.
    \item \textbf{Continuous Action Spaces:} Many real-world applications of MARL, such as autonomous driving or robotic coordination, involve continuous action spaces. Policy-based methods, unlike discrete action approaches like DPS, can naturally handle such spaces, making them more suitable for these applications.
    \item \textbf{Model-Free Learning:} While model-based approaches like MPC can be effective, they require a precise model of the environment dynamics, which is often not available or is computationally expensive to learn. Policy-based RL methods do not rely on such models, allowing them to be applied more broadly, even in environments with unknown or partially observable dynamics.
    \item \textbf{Exploration vs. Exploitation Balance:} Policy-based methods are well-suited for balancing exploration and exploitation, a critical aspect in RL. Techniques like entropy regularization ensure that agents explore a diverse set of actions, which can lead to discovering better policies over time. This is a key advantage over methods like LQR or deterministic controllers, which may get stuck in local optima.
    \item \textbf{Robustness to Non-Linearity:} Policy-based RL methods can effectively handle non-linear and non-convex optimization landscapes, which are common in MARL scenarios. Approaches like PPO have been shown to be robust to these complexities, unlike linear methods such as LQR, which may not perform well in such settings.
\end{itemize}

Although methods like MPC and LQR are powerful for control in deterministic or well-modeled systems, they are less effective in highly dynamic, uncertain, and multi-agent environments where the interactions between agents can significantly impact the learning process. Furthermore, model-based approaches such as MPC require frequent re-planning and are computationally intensive, especially as the number of agents increases.

In contrast, policy-based RL methods enable agents to learn directly from interactions with the environment without requiring an explicit model, making them more suitable for large-scale, real-time applications. Future work could explore hybrid approaches that integrate policy-based RL with model-based elements to leverage the strengths of both paradigms.

Overall, our choice of policy-based RL is motivated by its ability to address the complexities of multi-agent systems effectively, balancing performance, flexibility, and computational feasibility.


\section{Comparative Analysis with Other Organizational Approaches}
\label{sec:comparison}

In this section, we conduct a comparative analysis of the proposed PRAHOMT algorithm with other Multi-Agent Reinforcement Learning (MARL) approaches that integrate organizational specifications. Specifically, we compare our method against ROMA \cite{roma2020}, a framework that utilizes emergent roles for coordination, and a multi-agent co-learning method with organizational constraints \cite{colearning2021}. The objective of this comparison is to evaluate the performance of PRAHOMT in terms of policy convergence, robustness, and learning efficiency.

\subsection{Experimental Setup}
For a fair comparison, we implemented each approach in the same environment using the Predator-Prey scenario. The setup included three predators and one prey, with each predator assigned a specific role or task based on the organizational specifications defined in each method. The following metrics were used for evaluation:
\begin{itemize}
    \item \textbf{Convergence Time:} The number of episodes required for each algorithm to reach a stable policy.
    \item \textbf{Robustness:} The ability of the trained policies to maintain high performance in the presence of disturbances, such as sudden changes in the environment or agent failures.
    \item \textbf{Efficiency:} The overall computational cost, measured as the average time and resources required per episode during training.
\end{itemize}

\subsection{Results and Discussion}
Table \ref{tab:comparison} summarizes the results of our experiments. PRAHOMT demonstrated a significantly faster convergence time compared to ROMA and the co-learning method, reducing the required number of episodes by 30\%. This is attributed to the explicit role and mission guidance provided by MOISE+, which helps agents focus on achieving specific organizational goals.

\begin{table}[h]
    \centering
    \caption{Comparative Results between PRAHOMT, ROMA, and Co-Learning Methods}
    \label{tab:comparison}
    \begin{tabular}{|c|c|c|c|}
        \hline
        \textbf{Metric} & \textbf{PRAHOMT} & \textbf{ROMA} & \textbf{Co-Learning} \\ \hline
        Convergence Time (Episodes) & \textbf{50} & 70 & 80 \\ \hline
        Robustness (Reward Variance) & \textbf{0.8} & 1.2 & 1.5 \\ \hline
        Efficiency (CPU Time per Episode) & \textbf{0.5s} & 0.8s & 0.7s \\ \hline
    \end{tabular}
\end{table}

PRAHOMT also outperformed the other methods in terms of robustness, with lower reward variance across different scenarios, indicating more stable behavior under varying conditions. In terms of efficiency, PRAHOMT achieved a lower computational cost per episode, which demonstrates the advantage of constrained policy learning guided by organizational roles.

\subsection{Analysis of Organizational Specifications}
One of the primary reasons for the superior performance of PRAHOMT is its ability to effectively utilize predefined organizational roles and missions. In contrast, ROMA relies on emergent roles, which can lead to longer training times as the roles are learned rather than assigned. The co-learning method, while effective in some cases, struggled with scalability as the number of agents increased, resulting in higher computational costs.


\section{Limitations of Organizational Specifications in PRAHOMT}
\label{sec:limitations}

The performance of agents in the PRAHOMT framework heavily relies on the quality of predefined organizational specifications, such as roles and missions. If these specifications are incomplete or poorly defined, the learning process may be hindered, leading to suboptimal or unsafe behaviors. This section discusses the potential limitations of PRAHOMT in scenarios where organizational specifications are not robust and explores strategies to mitigate these challenges.

\subsection{Challenges with Incomplete or Poorly Defined Specifications}

Incomplete or inaccurate specifications can cause several issues:
\begin{itemize}
    \item \textbf{Suboptimal Policy Learning:} Agents may converge to suboptimal policies if the provided roles or missions do not align well with the desired behaviors or objectives of the system.
    \item \textbf{Increased Convergence Time:} Poorly defined specifications can lead to longer convergence times as agents struggle to find effective strategies within the constrained policy space.
    \item \textbf{Safety Concerns:} Inaccurate specifications may cause agents to behave in unsafe or unintended ways, which is particularly critical in real-world applications where safety is paramount.
\end{itemize}

\subsection{Strategies for Improving Organizational Specifications}

To address the limitations of predefined specifications, we propose the following strategies:

\subsubsection{Automated Learning of Organizational Specifications}
Leveraging unsupervised learning techniques, such as clustering and association rule mining, can help automatically discover potential roles and missions from agent behaviors and environmental data. For example, clustering agent trajectories can reveal common patterns of behavior, which can be formalized into roles or missions.

\subsubsection{Adaptive Specification Refinement}
In dynamic environments, predefined specifications may become outdated or inadequate. An adaptive refinement mechanism can be employed, where organizational specifications are updated based on the agents' performance and environmental changes. This can be achieved through online learning techniques that continuously adjust roles and missions based on feedback.

\subsubsection{Incorporating Human Feedback}
Human experts can provide valuable insights into refining organizational specifications. Incorporating human-in-the-loop approaches, where experts validate or modify the automatically generated specifications, can enhance the relevance and accuracy of the roles and missions used by agents.

\subsection{Future Work}

Future research will focus on developing more robust methods for the automated generation and refinement of organizational specifications. Additionally, we aim to explore hybrid approaches that combine data-driven methods with expert knowledge to create more effective and adaptable specifications for multi-agent systems.

\section{Differences Between PRAHOM and PRAHOMT}

In this section, we provide a comprehensive overview of the differences between PRAHOM and our proposed extension, PRAHOMT. While PRAHOM laid the groundwork by introducing a method for leveraging agent histories in relation to organizational models, PRAHOMT significantly extends this framework in several key aspects:

\subsection{Core Contributions of PRAHOMT}

\subsubsection{Enhanced Role and Mission Integration}
PRAHOMT introduces a more direct and fine-grained integration of organizational roles and missions into the MARL training process. Unlike PRAHOM, which used a broader set of organizational specifications, PRAHOMT focuses exclusively on roles and missions as a means of constraining the policy space. This targeted approach reduces complexity and enhances the explainability of agent behaviors, as each policy is explicitly tied to a specific role or mission. This focus allows for better scalability in more complex environments where the full spectrum of organizational constraints may be impractical to implement.

\subsubsection{Policy Constraint Mechanism}
While PRAHOM loosely linked agent histories to organizational models, PRAHOMT implements a robust mechanism for dynamically constraining policies based on the agents' current roles and missions. The PRAHOMT algorithm uses these constraints not only to influence the reward function but also to directly alter the action selection process during training. This leads to a more structured exploration of the policy space, enhancing both convergence speed and policy stability.

\subsubsection{Explainability and Safety Guarantees}
One of the major enhancements of PRAHOMT over PRAHOM is the introduction of explainability and safety mechanisms. PRAHOMT uses the history-to-role mapping to provide clear explanations for the learned behaviors of agents, making it easier to understand how specific roles and missions impact the overall system dynamics. Moreover, by ensuring that policies adhere to predefined roles and missions, PRAHOMT inherently includes safety constraints, reducing the likelihood of agents exhibiting unsafe or unexpected behaviors during and after training.

\subsection{Why Focus on Roles and Missions?}

The decision to focus on roles and missions in PRAHOMT, as opposed to the broader range of organizational constraints used in PRAHOM, was driven by both practical and theoretical considerations:

\begin{itemize}
    \item \textbf{Scalability:} Roles and missions provide a more manageable way to structure agent behavior, particularly in large-scale multi-agent systems. By simplifying the organizational model, PRAHOMT is able to maintain high performance even as the number of agents and complexity of the environment increase.
    
    \item \textbf{Explainability:} Roles and missions offer a clear and intuitive framework for understanding agent behaviors. By mapping each role or mission to a subset of agent histories, PRAHOMT makes it easier to trace how organizational specifications translate into specific actions, thereby improving the explainability of the learned policies.
    
    \item \textbf{Focused Learning:} The use of roles and missions allows PRAHOMT to guide the learning process more effectively. By constraining the policy space based on these specific organizational elements, PRAHOMT facilitates faster convergence and more stable learning outcomes compared to the broader constraints used in PRAHOM.
\end{itemize}

\subsection{Summary of Improvements}

In summary, PRAHOMT builds upon the foundational concepts introduced in PRAHOM but introduces several key improvements:
\begin{itemize}
    \item Direct integration of roles and missions into the MARL process.
    \item Enhanced policy constraint mechanisms for better convergence and stability.
    \item Improved explainability and safety guarantees through structured role-mission mapping.
\end{itemize}
These enhancements make PRAHOMT a more effective and scalable solution for designing and training multi-agent systems in complex environments.


\section{Generalization to Complex Scenarios}
\label{sec:complex_scenarios}

While our experiments demonstrate the effectiveness of PRAHOMT in the predator-prey scenario, it is essential to evaluate the robustness and adaptability of the proposed approach in more complex and realistic environments. Such environments may include industrial tasks requiring coordinated efforts, search and rescue operations, and logistics management in dynamic settings. These scenarios typically involve multiple agents with heterogeneous roles, complex interdependencies, and dynamic goal structures.

\subsection{Potential Application Scenarios}
To illustrate the generalizability of PRAHOMT, we propose the following scenarios:

\paragraph{1. Industrial Coordination:} In a smart manufacturing plant, robots with different capabilities must coordinate to assemble products on a conveyor belt. Each robot can be assigned a specific role, such as assembly, inspection, or packaging, depending on the organizational model. PRAHOMT can guide these agents to learn efficient and safe coordination strategies under varying production requirements and equipment malfunctions.

\paragraph{2. Search and Rescue Operations:} In disaster response scenarios, autonomous drones and ground robots need to collaborate to locate and assist victims. The mission specifications can include areas to be covered, priority zones, and communication constraints. PRAHOMT can help in training agents to follow these specifications, ensuring that critical areas are searched first while avoiding redundant coverage and maintaining communication.

\paragraph{3. Dynamic Logistics Management:} In warehouse and supply chain management, agents need to manage storage and transport of goods efficiently. Organizational roles can define priorities for handling perishable items, responding to urgent orders, or managing inventory levels. PRAHOMT can facilitate the learning of adaptive strategies to maintain a balance between efficiency and compliance with logistics constraints.

\subsection{Experimental Setup for Complex Scenarios}
For future experiments, we propose evaluating PRAHOMT in the following complex environments:

\paragraph{Multi-Agent Assembly Line:} Implement a simulation where robotic arms, conveyors, and mobile units coordinate to assemble a complex product. Measure the system's efficiency, adaptability to production changes, and ability to handle faults in individual units.

\paragraph{Autonomous Emergency Response:} Simulate a disaster scenario with multiple types of agents (e.g., drones, ground vehicles) coordinating to find and assist simulated victims in an environment with obstacles and dynamic hazards. Analyze the system's response time, coverage efficiency, and communication reliability.

\paragraph{Adaptive Warehouse Management:} Create a multi-agent warehouse scenario where agents must dynamically allocate tasks such as order picking, packing, and restocking in response to fluctuating demand. Evaluate the adaptability and efficiency of the learned policies under different demand patterns.

\subsection{Discussion on Robustness and Adaptability}
The proposed complex scenarios will allow us to assess PRAHOMT's ability to handle diverse and dynamically evolving tasks, which is critical for real-world applications. By systematically varying the complexity of the tasks and the uncertainty in the environment, we aim to demonstrate the robustness of PRAHOMT in maintaining high performance across different organizational setups. Furthermore, these scenarios will highlight the adaptability of PRAHOMT in reassigning roles and missions in response to changes in agent capabilities or external conditions.

\section{Discussion: Model Predictive Control (MPC) and Alternative Approaches}

While PRAHOMT leverages organizational constraints to guide the policy learning process in a structured and interpretable manner, it is essential to discuss its advantages and limitations in comparison to alternative methods such as Model Predictive Control (MPC) and other model-based optimization techniques.

\subsection{Model Predictive Control (MPC)}
MPC is a popular approach in control theory that optimizes a sequence of actions over a future time horizon based on a predictive model of the system's dynamics. It is widely used in various domains, including robotics and autonomous systems, due to its ability to handle constraints explicitly and optimize for long-term objectives. In comparison, PRAHOMT integrates organizational roles and missions into the reinforcement learning (RL) framework, which allows for a direct mapping of high-level specifications to agent behaviors. This mapping enables agents to act in accordance with predefined organizational rules, ensuring safety and stability during policy execution.

The primary advantage of PRAHOMT over MPC lies in its ability to incorporate complex, high-level organizational knowledge directly into the learning process, which is not straightforward in traditional MPC frameworks. Additionally, PRAHOMT is better suited for environments where the organizational structure is dynamic and where roles and missions may change based on the evolving context.

However, MPC has the advantage of being more suitable for scenarios requiring real-time decision-making with precise control over system constraints. PRAHOMT, being a reinforcement learning-based approach, may require extensive training data and computational resources, and its performance is influenced by the quality of the organizational specifications.

\subsection{Alternative Model-Based Optimization Techniques}
Several alternative techniques exist in the model-based RL domain, such as Neuro-Evolutionary Direct Policy Search (DPS), Linear Quadratic Regulator (LQR), Evolution Strategies, and Genetic Algorithms. Each of these methods has specific strengths, such as the ability to optimize non-differentiable policies (DPS) or to handle stochastic environments (Stochastic Optimal Control).

Compared to these approaches, PRAHOMT's unique contribution is its explicit use of organizational roles and missions to constrain the policy space. This results in more interpretable and structured agent behaviors, which are particularly beneficial for applications where explainability and adherence to organizational rules are critical. However, methods like Evolution Strategies and Genetic Algorithms can offer more flexible exploration capabilities, potentially achieving higher performance in environments with less well-defined constraints.

\subsection{Limitations and Future Work}
The effectiveness of PRAHOMT heavily depends on the accuracy and completeness of the predefined organizational specifications. Future work could explore hybrid approaches that combine the strengths of PRAHOMT with MPC or other model-based optimization methods. For example, using MPC to fine-tune the low-level control actions within the broader strategic framework defined by PRAHOMT could result in a more robust and efficient policy learning process.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{MARL background}\label{sec:marl_background}

MARL extends the concepts of Reinforcement Learning (RL) to multi-agent context. Agents' policies are optimized to maximize the total cumulative reward through learning.
To apply MARL techniques, we chose the Decentralized Partially Observable Markov Decision Process (Dec-POMDP)~\cite{Oliehoek2016} because it considers multiple agents in a similar MAS manner. It relies on stochastic processes to model the uncertainty of the environment for the changes induced by actions, received observations, and communication. Additionally, unlike Partially Observable Stochastic Games, the reward function can be common to agents which fosters training for collaborative-oriented actions~\cite{Beynier2013}.

A Dec-POMDP $d \in D$ (with $D$ the set of Dec-POMDP) is a 7-tuple $d = (S,\{A_i\},T,R,\{\Omega_i\},O,\gamma)$ , where:
$S = \{s_1,\dots,s_{|S|}\}$: the set of possible states;
$A_{i} = \{a_{1}^{i},\dots,a_{|A_{i}|}^{i}\}$: the set of possible actions for agent $i$;
$T$ so that $T(s,a,s') = \probP{(s'|s,a)}$ : the set of conditional transition probabilities between states;
$R: S \times A \times S \rightarrow \mathbb{R}$: the reward function;
$\Omega_{i} = \{o_{1}^{i},\dots,o_{|\Omega_{i}|}^{i}\}$: the set of observations for agent $i$;
$O$ so that $O(s',a,o) = \probP{(o|s',a)}$ : the set of conditional observation probabilities;
$\gamma \in [0,1]$ : the discount factor.

Considering $m$ \textbf{teams} (also referred to as \textbf{groups}) each containing several agents among $\mathcal{A}$, we also detail the minimal formalism notation we re-used for solving the Dec-POMDP for a given team $i, 0 \leq i \leq m$ containing $n$ agents~\cite{Beynier2013,Albrecht2024}:

\begin{itemize}

  \item $\Pi$: the set of policies. A \textbf{policy} $\pi \in \Pi, \pi: H \times \Omega \rightarrow A$ associates an observation to an action optionally using previous experiences as histories. It represents the agent's internal logic;
  \item $\Pi_{joint}$: the set of joint-policies. A \textbf{joint-policy} $\pi_{joint} \in \Pi_{joint}, \pi_{joint}: H_{joint} \times \Omega^n \rightarrow A^n = \Pi^n$ chooses an action for each agent regarding their respective observation optionally using previous experiences as histories. It can be viewed as a set of policies used in agents;
  \item $H$: the set of histories. A \textbf{history} over $z \in \mathbb{N}$ steps (where $z$ is generally the maximum number of steps for an episode) is the $z$-tuple $h = ((\omega_{k}, a_{k}) | k \leq z, \omega \in \Omega, a \in A)$;
  \item $H_{joint}$: the set of joint-histories. A \textbf{joint-history} over $z \in \mathbb{N}$ steps $h_{joint} \in H_{joint}, h_{joint} = \{h_1,h_2 \dots h_n\}$ is the set of agents' histories;
  \item $U_{joint,i}(<\pi_{joint,i}, \pi_{joint,-i}>): \Pi_{joint} \rightarrow \mathbb{R}$: gives the \textbf{expected cumulative reward} over a finite horizon (if $\gamma < 1$ or the number of steps in an episode is finite), with $\pi_{joint,i}$ the joint policy for team $i$ and $\pi_{joint,-i}$ all of the other concatenated joint-policies (considered as fixed).
        % \item $BR_{joint,i}(\pi_{joint,i}) = argmax_{\pi_{joint,i}}(U(<\pi_{joint,i},\pi_{joint,-i}>))$: gives the \textbf{best response} $\pi_{joint,i}^*$ in the sense that the team cannot change any of the policies in the joint-policy $\pi_{joint,i}^*$ to get a better expected cumulative reward than $U_i^* = U_{joint,i}(<\pi_{joint,i}^*, \pi_{joint,-i}>)$;
        % \item $SR_{joint,i}(\pi_{joint,i}, s) = \{\pi_{joint,i} | U(<\pi_{joint,i},\pi_{joint,-i}>) \geq s\}$: gives the \textbf{sufficient response} as the set of joint-policies getting at least $s \in \mathbb{R}, s \leq U_i^*$ as expected cumulative reward.
\end{itemize}

Various approaches enable solving MARL problems. We favored \textbf{Policy-based} methods, such as Multi-Agent Proximal Policy Optimization~\cite{yu2022surprising} (MAPPO) and Multi-Agent Deep Deterministic Policy Gradient~\cite{Lowe2017}. These algorithms directly parameterize the policy as $\pi_\theta$ to optimize it possibly using Centralized Learning and Decentralized Execution.
We refer to \textbf{solving} the Dec-POMDP at $s$ expectancy as finding the joint policies $\pi_{joint,i} \in \Pi_{joint}, \pi_{joint,i} = \{\pi_{joint,i} | U(<\pi_{joint,i},\pi_{joint,-i}>) \geq s\}$ that gets the expected cumulative reward over a finite horizon at least at $s \in \mathbb{R}$.


\section{Related works and positioning}\label{sec:related_works}

The integration of organizational specifications in MAS learning processes is not largely addressed explicitly in the literature. Yet, various approaches have been proposed to incorporate organizational constraints and policies in MAS to ensure agents adhere to certain requirements.

\textbf{Learning with Organizational Constraints} \quad
%
The integration of organizational constraints into the learning process of agents has been explored to various extents. In \cite{cruz2020norms}, the authors present a method for incorporating norms into the learning algorithms of agents, ensuring that their behavior remains within acceptable bounds. Additionally, \cite{villatoro2011social} proposes a mechanism for agents to learn and adapt to social norms in dynamic environments, highlighting the importance of norm adaptation in MAS. However, these norms cannot be individual.

\emph{Specification-Guided Reinforcement Learning} aims to generate policies that accomplish a specific task using external specifications to guide learning in achieving an objective under given constraints~\cite{Bansal2022}. Jothimurugan et al.~\cite{Jothimurugan2021} propose logical specification learning as exploiting the compositional structure of specifications to generate policies for complex tasks. However, it does not completely fall into the MARL framework we set for it requires introducing logical specifications.

\textbf{Policy-Based Approaches} \quad
%
Policy-based approaches provide a way to enforce organizational constraints by defining explicit policies that govern agent behavior. In \cite{krupanski2015norm}, the use of normative policies is investigated to guide agent interactions and decision-making processes. Moreover, \cite{vos2020governing} explores the use of governance mechanisms to enforce compliance with organizational policies in decentralized systems. However, these approaches face a lack of explainability since most policies are black-box and cannot be easily modified.

\textbf{Frameworks Integrating Organizational Aspects} \quad
%
Wang et al.~\cite{Wang2020} introduce an approach in which similar emerging roles are pushed to jointly specialize in specific tasks. Tosic et al.~\cite{Tosic2010} propose a framework for coordination based on the communication capabilities of multi-agent systems. Zheng et al.~\cite{Zheng2018} present a platform for MARL that aims to facilitate research on artificial collective intelligence by providing a comprehensive set of evaluation metrics to compare the performance of MARL algorithms. However, it does not take into account specifications likely to entice agents to adhere to an expected behavior such as missions.

\

Despite these advancements, there is still a lack of works that explicitly use organizational specifications as a means of constraining agent learning regarding requirements. To our knowledge, no work can be used to generate a MAS that satisfies additional organizational constraints explicitly. Unlike these works, our originality is to explicitly use an organizational model as a general means of constraining learning regarding requirements.


\section{Linking MARL and the $\mathcal{M}OISE^+$ model}\label{sec:linking_marl_moise}
\label{sec:marl_moise_linking}

In this section, we incrementally present the principles we propose to adapt the MARL according to organizational specifications. This includes constraining a policy to adhere to the expected behavior of a role and enticing the policy to achieve a mission for a given time duration.


The $\mathcal{M}OISE^+$ model provides a comprehensive framework for specifying the structure and interactions within a Multi-Agent System (MAS). We utilize $\mathcal{M}OISE^+$ due to its compatibility with MARL and its formal description capabilities for agents' policies. The PRAHOMT's underlying approach bridges the gap between MARL and organizational modeling by embedding $\mathcal{M}OISE^+$ specifications into the MARL training process. This integration allows for the training of agents that adhere to predefined organizational roles and missions.
%
In $\mathcal{M}OISE^+$, \textbf{Organizational Specifications (OS)} are defined as $\mathcal{OS} = \langle \mathcal{SS}, \mathcal{FS}, \mathcal{DS} \rangle$, where $\mathcal{SS}$ are the \textbf{Structural Specifications}, $\mathcal{FS}$ are the \textbf{Functional Specifications}, and $\mathcal{DS}$ are the \textbf{Deontic Specifications}.

\subsection{Structural Specifications and Constraining Joint-Policies According to Roles}

\textbf{Structural Specifications} describe the organizational structure and are denoted as $\mathcal{SS} = \langle \mathcal{R}, \mathcal{IR}, \mathcal{G} \rangle$:

\begin{itemize}
  \item $\mathcal{R}_{ss}$: The set of all roles ($\rho \in \mathcal{R}$).
  \item $\mathcal{IR}: \mathcal{R} \rightarrow \mathcal{R}$: The inheritance relation between roles ($\rho_1 \sqsubset \rho_2$).
  \item $\mathcal{RG} \subseteq \mathcal{GR}$: The set of root groups, $\mathcal{GR} = \langle \mathcal{R}, \mathcal{SG}, \mathcal{L}^{intra}, \mathcal{L}^{inter}, \mathcal{C}^{intra}, \mathcal{C}^{inter}, \allowbreak np, ng \rangle$, where:
        %   \begin{itemize}
        $\mathcal{R} \subseteq \mathcal{R}_{ss}$: The set of non-abstract roles.
        $\mathcal{SG} \subseteq \mathcal{GR}$: The set of sub-groups.
        $\mathcal{L} = \mathcal{R} \times \mathcal{R} \times \mathcal{TL}$: The set of links $(\rho_s, \rho_d, t)$, where $t \in \{acq, com, aut\}$ indicates the type of link (acquaintance, communication, authority).
        $\mathcal{L}^{intra} \subseteq \mathcal{L}$: The set of intra-group links.
        $\mathcal{L}^{inter} \subseteq \mathcal{L}$: The set of inter-group links.
        $\mathcal{C} = \mathcal{R} \times \mathcal{R}$: The set of compatibilities $(\rho_a \bowtie \rho_b)$.
        $\mathcal{C}^{intra} \subseteq \mathcal{C}$: The set of intra-group compatibilities.
        $\mathcal{C}^{inter} \subseteq \mathcal{C}$: The set of inter-group compatibilities.
        $np: \mathcal{R} \rightarrow \mathbb{N} \times \mathbb{N}$: The cardinality of agents adopting a role.
        $ng: \mathcal{SG} \rightarrow \mathbb{N} \times \mathbb{N}$: The cardinality of each sub-group.
        %   \end{itemize}
\end{itemize}

Constraining policies directly is not feasible because most policy implementations rely on intractable black-box models such as neural networks. We propose to formally represent a role as a history subset $\mathcal{P}(H)$ containing histories an agent playing a role should generate, hence characterizing the role's expected behavior. As illustrated in \autoref{fig:PRAHOM_osm_rels}, we propose each role to be mapped to a history subset through the $rh: \mathcal{R} \rightarrow \mathcal{P}(H)$ bijection. An agent playing a role should have its policy constrained to generate histories belonging to the mapped history subset (at least from a theoretical point of view).

\begin{figure}[h!]
  \centering
  \input{figures/PRAHOM_osm_rels.tex}
  \caption{Relations between organizational specifications and history subsets}
  \label{fig:PRAHOM_osm_rels}
\end{figure}

Since defining roles into a history subset faces issues for handling possibly numerous large and non-manageable observations (such as pixel tables), we first propose to use labels to represent observations in a short-way. We introduce $ol: \mathcal{P}(\Omega) \rightarrow \mathcal{P}(L)$ to map some simple strings to real observations. In addition to a simple mapping, we also considered using a Large Language Model (LLM) for that purpose. % as illustrated in \autoref{fig:PRAHOMT_ol}.
The LLM is trained after real observations have been rendered visually and labeled by hand. Once trained, the LLM can be used conveniently to get real observations from labels, and may also be used to label some other observations.

% \begin{figure}[h!]
%     \centering
%     \input{figures/ol_scheme.tex}
%     \caption{Observations-labels mapping and its creation}
%     \label{fig:PRAHOMT_ol}
% \end{figure}

Second, defining a history subset exhaustively may require taking into account many cases, hence leading to an important amount of histories. %As illustrated in \autoref{fig:PRAHOM_opc}
Rather than defining a history subset exhaustively, we propose three means to simplify its definition:
%
\quad i) a \textbf{pattern} format that conforms to the following rules: a sequence is denoted \textquote{[$label_{obs_1},label_{act_1}\dots$] \allowbreak ($card_{min},card_{max}$)} the \textquote{Any} label refer to any observation/action. This pattern is implemented as an oriented graph denoted \textbf{history graph} where nodes are observations/actions and edges represent expected transitions relying on ordinal number and cardinalities; \quad
%
ii) \textbf{rules} to associate an action set depending on a history (possibly defined as a pattern) belonging to the history subset and a received observation. Once the observations and the associated actions are added to the history, this history should still belong to the history subset; \quad
%
iii) a \textbf{custom script} logic taking into account a history belonging to the history subset and a new observation to indicate the actions to add in the current history so it still belongs to the history subset.


Considering a history subset is ultimately aimed to be used to constrain a policy to make it adhere to a role, we introduce an \textbf{observable policy constraint} $c\pi: H \times \Omega \rightarrow \mathcal{P}(A)$. $c\pi$ indicates the actions an agent's policy should be allowed to choose among when it receives an observation at each step.

% \begin{figure}[h!]
%     \centering
%     \input{figures/opc_scheme.tex}
%     \caption{An abstract view of observable policy constraint and its crafting}
%     \label{fig:PRAHOM_opc}
% \end{figure}

We propose to integrate observable policy constraints into an agent's policy via three modes:
\textbf{correct}: Corrects any chosen action $\pi(\omega)$ to an expected one in $c\pi(\omega)$. It ensures safety but is external to the agent's policy; \quad
\textbf{penalize}: Adds a penalty to the reward if a wrong action ($\pi(\omega) \notin c\pi(\omega)$) is made, encouraging agents to learn their constraints but without safety guarantees; \quad
\textbf{correct\_policy}: Creates a \textbf{constrained policy} $\pi_c \in \Pi, \pi_c = \{smpl(c\pi_{joint}(\omega_{joint})) \allowbreak if \ \omega_{joint} \in Dom(c\pi_{joint}) \ \allowbreak, \ \allowbreak else \ \allowbreak \pi_{joint}(\omega_{joint})\}$ where $smpl$ gets an element from a set randomly. An observable policy constraint $c\pi$ corrects the current policy $\pi$, hence respecting safety guarantees internally.

In Annex, we provide \autoref{proof:jpc_to_ac}, which outlines why constraining the action decision-making process dynamically during training at each step implies that the resulting joint-policy will necessarily be constrained.


\subsection{Functional Specifications and Constraining Joint-Policies According to Missions}

\textbf{Functional Specifications} describe the tasks and goals and are denoted as $\mathcal{FS} = \langle \mathcal{SCH}, \mathcal{PO} \rangle$:

\begin{itemize}
  \item $\mathcal{SCH} = \langle \mathcal{G}, \mathcal{M}, \mathcal{P}, mo, nm \rangle$: The set of social schemes, where:
        \begin{itemize}
          \item $\mathcal{G}$: The set of global goals.
          \item $\mathcal{M}$: The set of mission labels.
          \item $\mathcal{P} = \langle \mathcal{G}, \{\mathcal{G}\}^s, OP, [0,1] \rangle$: The set of plans $(g_f, \{g_i\}_{0 \leq i \leq s}, op, ps)$.
          \item $mo: \mathcal{M} \rightarrow \mathbb{P}(\mathcal{G})$: The goals associated with each mission.
          \item $nm: \mathcal{M} \rightarrow \mathbb{N} \times \mathbb{N}$: The cardinality of agents committed to each mission.
        \end{itemize}
  \item $\mathcal{PO}: \mathcal{M} \times \mathcal{M}$: The set of preference orders $(m_1 \prec m_2)$.
\end{itemize}

We consider a goal to be theoretically represented as a history subset. We propose $gh: \mathcal{G} \rightarrow \mathcal{P}(H)$ that shows how an agent committed to a mission should have its policy enticed to generate histories belonging to an expected history subset (at least from a theoretical point of view). Thus, $mh: \mathcal{M} \rightarrow \mathcal{P}(H) = \{(m,\bigcup gh[mo(m)]), m \in \mathcal{M}\}$ gives expected histories from missions. Ultimately, a goal should impact MARL by updating the reward function when an agent is committed to a mission. We introduce an \textbf{observable reward function} $R_{g}: H \rightarrow \mathbb{R}$, the relation indicating how close a generated history is to a given history subset. A reward function for a mission $m \in \mathcal{M}$ containing goals $\mathcal{G}_{m} = mo(m)$ is computed as a weighted addition of all observable reward functions of each goal. This way, agents are individually enticed to achieve their respective sub-goals hence speeding up the convergence to achieving the ultimate goal.

% \begin{figure}[h!]
%     \centering
%     \input{figures/goal_mission_scheme.tex}
%     \caption{An abstract view of observable reward function and its crafting}
%     \label{fig:goal_mission_scheme}
% \end{figure}

\subsection{Deontic Specifications and Constraining Joint-Policies According to Permissions/Obligations}

\textbf{Deontic Specifications} define the permissions and obligations and are denoted as $\mathcal{DS} = \langle \mathcal{OBL}, \mathcal{PER} \rangle$:

\begin{itemize}
  \item $\mathcal{TC}$: The set of time constraints.
  \item $\mathcal{OBL}: \mathcal{R} \times \mathcal{M} \times \mathcal{TC}$: The set of obligations $(obl(\rho_a, m, tc))$.
  \item $\mathcal{PER}$: The set of permissions $(per(\rho_a, m, tc))$.
\end{itemize}

We introduced the relation $da: \mathcal{PER} \cup \mathcal{OBL} \rightarrow \mathcal{P}(\mathcal{A})$ that indicates how agents are constrained to roles and committed on missions for a given time constraint. In order to take into account time constraints, we introduce a time-to-live for each permission/obligation through the relation $dttl: \mathcal{PER} \cup \mathcal{OBL} \times \mathcal{A} \rightarrow \mathbb{N}$. Then, they are to be decreased at each step if the given time constraint is not \textquote{Any} with the relation $dec: dttl \rightarrow dttl$. Then, the roles constrained to agents or committed missions may change after the reward function is changed.

To differentiate between obligations and permissions and ensure agents prioritize obligated missions over permitted ones, we propose multiplying the observable reward function of this mission by a high factor for obligated missions and a low factor for permitted ones.

Relying on these principles to integrate organizational specifications within the MARL framework, we established the PRAHOM algorithm.


\section{PRAHOMT Algorithm}\label{sec:prahom_alg}

% $PRAHOMT$ algorithm is presented in \autoref{alg:PRAHOM-A}. It fits within a regular MARL context: a joint-policy $\pi_{joint,ep}$ is updated after several steps allowing to have enough joint-histories $h_{joint,ep}$ and joint-rewards as feedback $r_{joint,ep}$. The training goes on over several episodes for better training until converging to a sufficient cumulative reward regarding $s$.

In this section, we provide an explanation of the PRAHOMT algorithm as outlined in \autoref{alg:PRAHOM-A}. It integrates organizational specifications into the learning by constraining joint-policies according to predefined roles, missions, and permission/obligation relations. This ensures that the agents' behaviors align with organizational requirements while  optimizing joint-policy ($\pi_{joint,i,s}$) as output.

\textbf{Initialization and Input Parameters}: \quad
The algorithm starts by initializing the joint-policy ($\pi_{joint}$) with the initial joint-policy ($\pi_{joint,i,init}$) and setting the episode counter ($ep$) to zero (line 1). The algorithm also sets a boolean variable ($sufficient$) to False, indicating that the cumulative reward expectancy has not yet been met.

\textbf{Step 1: Determine Joint Observable Policy Constraints}: \quad
PRAHOMT determines the joint observable policy constraints ($c\pi_{joint}$) from the organizational specifications via $bld_{opc}$ (line 2). This step involves mapping roles ($rh$), missions ($mh$), and permissions/obligations ($da$) to constraints on the joint-policy, ensuring that the agents' actions are in line with the organizational rules.

\textbf{Step 2: Initialize Constrained Policy}: \quad
If the mode of constraint integration is set to \textit{correct\_policy}, the algorithm creates and uses a joint constrained policy via $bld_{joint, \pi_c}$ based on the initial policy and the observable policy constraints ($c\pi_{joint}$) (line 4). This ensures that the initial policy is inherently aligned with the specified constraints from the beginning.

\textbf{Step 3: Determine Observable Reward Functions}: \quad
Next, PRAHOMT determines the observable reward functions $R_{m,joint}$ from the organizational specifications via $bld_{mrf}$ (line 5). This involves mapping missions and goals to reward functions, which will guide the agents towards achieving organizational sub-goals. Then, these observable reward functions are integrated within the global reward function via $comb_{mrf}$.

\textbf{Step 4: Main Training Loop}: \quad
The main training loop runs for a maximum number of episodes ($ep_{max}$) or until the cumulative reward expectancy is met (line 6).

\begin{itemize}
  \item \textbf{Initialize Episode}:
        At the beginning of each episode, the environment, observation, and action histories are reinitialized (line 7). The cumulative reward and penalty are also reset. Time-to-live values for permissions/obligations are initialized using $bld_{dttl}$, and the initial observation and action are set by the environment ($d_{ep}.init()$).

  \item \textbf{Step Through Episode}:
        The algorithm steps through each episode for a maximum number of steps ($step_{max}$) (line 8). Within each step:

        \begin{itemize}
          \item \textbf{Policy Update}:
                The joint-policy ($\pi_{joint}$) is updated using the MARL algorithm ($u_{marl}$) based on the current history ($rh_{joint,ep}$) and last rewards ($rh$) (line 9).

          \item \textbf{Action Selection}:
                The next action is selected based on the current observation ($\pi_{joint}(\omega_{joint,t=step})$) (line 10).

          \item \textbf{Action Validation}:
                The expected actions are determined from the observable policy constraints ($c\pi_{joint}$), and if the selected action is not among the expected ones, it is either corrected or penalized based on the mode of constraint integration (lines 11-16).

          \item \textbf{Update Histories and Rewards}:
                The current history is updated (line 17), and the action is applied to the environment to get the next observation and reward (line 18) adding any incurred penalties (line 19).

          \item \textbf{Update Constraints and Rewards}:
                Time-to-live values are decreased, and the reward functions and policies are updated accordingly if some organizational specifications have changed (line 20).
        \end{itemize}

  \item \textbf{Check for Sufficiency}:
        After each episode, the algorithm checks whether the cumulative reward meets the expectancy ($is\_sufficient$) and increments the episode counter ($ep$) (line 21).
\end{itemize}

\textbf{Complexity and Scalability}: \quad The computational complexity of PRAHOMT primarily depends on the number of episodes ($ep_{max}$), the number of steps per episode ($step_{max}$), and the complexity of the MARL algorithm ($u_{marl}$). Each step involves updating the policy, selecting \& validating actions, and updating histories and rewards, which can be computationally intensive. The memory complexity depends on the need to store rewards, policies, and histories. The history graphs we introduced allow histories to be merged compactly, reducing redundancy with limited impact on read cost.

Scalability can be challenging as the number of agents, roles, and missions increases. Evaluating PRAHOMT scalability requires considering an implemented version through these salient features: \quad
\textbf{Parallelization:} Performance for updating/validating policies strongly depends on chosen MARL algorithms capabilities such as parallelization significantly improves it (especially in Policy Optimization algorithms); \quad
\textbf{Efficient Data Structures:} The choice of data structures for storing and accessing histories or policies is crucial for reducing overhead.; \quad
\textbf{Optimization of Reward Functions:} The logic for constructing/handling reward functions is important to enhance performance further.



\section{Case study: Predator-prey environment}\label{sec:case_study}

\subsection{Algorithm implementation: PRAHOMT Wrapper}

\emph{PettingZoo} is a library that offers a standard API simplifying the development of multi-agent environments and facilitating the application of MARL algorithms.
%
We developed the \textit{PRAHOMT Wrapper}
%
\footnote{Code available at \url{http://REDACTED} (for blind
  review)}
%
as a proof-of-concept (PoC) tool to augment PettingZoo environments to apply PRAHOMT. The wrapper leverages the MARLlib~\cite{hu2022marllib} library, which offers a wide range of state-of-the-art MARL algorithms and fine-tuned policy models for various environments.
This wrapper provides an API of additional auxiliary classes to define organizational specifications and link them to their expected behavior:

\textbf{Observation Labeling (ol)} \quad The observation labeling is handled by the singleton class \textit{ol\_mngr}, which is part of the API. This class employs the HuggingFace transformer model \textit{tiiuae/falcon-7b} to learn these mappings in conjunction with a simple dictionary. This class also provides an interactive process that allows users to label each observation during the labeling procedure.

\textbf{History Subsets} \quad The internal \textit{history\_subset} class is used to handle history subsets based on predefined patterns or rules via an implemented history graph when dealing with patterns. Alternatively, custom functions can be employed to instantiate a history\_subset, allowing more flexibility.

\textbf{Observable Policy Constraint (opc)} \quad The \textit{opc} class is to be used by users to link pairs of (history\_pattern, observation) to lists of expected actions. Each association is integrated into a unified \textit{history\_subset} object.

\textbf{Constrained Policy (cons\_policy)} \quad The internal \textit{cons\_policy} class combines a regular MARLlib policy with an \textit{opc} object. The integration involves embedding policy constraints within the MARLlib policy framework, allowing the constrained policy to respect predefined action constraints.

\textbf{Observable Reward Function (orf)} \quad The \textit{orf} class enables users to instantiate an observable reward function based on \textit{history\_subset} objects. It includes a function that evaluates whether a given history matches the initial pattern: high rewards for matching patterns, and low rewards otherwise.

\textbf{Organizational Specifications to Relations (osr)} \quad The \textit{osr} class enables users to gather all previously instantiated elements into a complete $\mathcal{M}OISE^+$ organizational model (JSON representable). In this class, roles and goals are directly mapped respectively to \textit{opc} and \textit{orf} objects using either patterns, rules, or custom functions. This class implements \textit{rh}, \textit{gh}, and \textit{mh}. Permissions/obligations are also defined in this class: each obligation/permission as (role,missions,time constraint) triplet is mapped to agents, thus implementing the \textit{da} relation.

\textbf{Time-to-Live (dttl)} \quad The internal \textit{dttl} class uses the \textit{osr} object to manage time constraints and update the observable reward function / policy constraints.

\textbf{Train under constraints} \quad Once the PettingZoo environment is wrapped with the PRAHOMT Wrapper and all auxiliary classes have been used to get an \textit{ol\_mngr} and \textit{os}. The wrapper's API provides the \textit{train\_under\_constraint()} function that enables executing the PRAHOMT algorithm taking \textit{ol\_mngr} and and \textit{os} objects as arguments (in addition to other MARLlib parameters). Ultimately, it generates MARLlib policy objects and statistical data.


\subsection{Experimental Setup}

The goal of our experiments is to show whether PRAHOMT indeed impacts the agents' policies during/after training as expected. For that purpose, we selected the \textquote{Simple World Comm}~\cite{Lowe2017} PettingZoo environment. This environment simulates a predator-prey scenario where predators, including a leader predator, collaborate to catch faster-moving prey. Prey gain points by collecting food and avoiding predators. The leader predator can observe the entire map, navigate freely (except in obstacles), and communicate directional orders to other predators. This setup is well-suited to induce agents to adopt organizational patterns, thus testing the effectiveness of PRAHOMT in training agents.

%The environment includes features such as obstacles and food items, with the following key attributes:

% \begin{itemize}
%     \item \textbf{Actions}: Discrete (5), Continuous (20)
%     \item \textbf{Observation Shape}: (28) for prey, (34) for predators
%     \item \textbf{State Shape}: (192)
%     \item \textbf{Rewards}: Prey gain points by collecting food and avoiding predators, while predators gain points by catching prey.
% \end{itemize}

We selected the MAPPO algorithm for its proven effectiveness in cooperative multi-agent environments without the need for domain-specific algorithmic modifications or architectures~\cite{Yu2022}. We used its model provided by MARLlib, which is a Multilayer Perceptron with the first layer consisting of 128 neurons and the second layer consisting of 256 neurons. We launched the training over 70 iterations (each consisting of 128 episodes) in a Centralized Learning and Decentralized Execution manner.
%
To train the LLM in \texttt{ol}, we observed observations showing specific behaviors (\textit{three agents circling a single prey}). This labeling helps in easily identifying collective behaviors based on visual observations, enabling better control of agents' actions.

To evaluate PRAHOMT, we implemented two organizational models:

\begin{itemize}
  \item \textbf{circle\_model} \quad This model includes two roles and a mission for predators:
        \begin{itemize}
          \item \textit{circle\_leader}: This role maps observations where three normal predators are close to the same prey, to directing them to move towards it.
          \item \textit{circle\_follower}: This role ensures any order received from the leader predator is followed with the corresponding action.
          \item \textit{prey\_proximity}: This mission contains a single goal indicating a high reward when prey is near any predator.
        \end{itemize}

  \item \textbf{manual\_model} \quad This model serves as a fully controlled organizational model with no room for learning. It includes two roles \textit{manual\_leader} and \textit{manual\_redator} that both dictate optimal actions for the leader predator according to a hand-crafted policy (custom function).
\end{itemize}

% \noindent Here is an overview of the PRAHOMT Wrapper code used for \textbf{circle\_model} :
% %
% \begin{lstlisting}[language=Python, basicstyle=\scriptsize]
% from prahom_wrapper import prahom_wrapper
% pz_env = raw_pz_env() ; pz_env = prahom_wrapper(pz_env)
% pz_env.train_under_constraints(ol=ol(), cons_mode="CORRECT",alg_conf="default_MAPPO",osr=organizational_model(
% structural_specifications(
%     {"circle_leader": leader_opc.add_pattern("[o1,a3,[#any]...](1,1)"),"circle_follower": normal_opc.add_pattern("[o1,a2,[#any]...](1,1)")}, None,None),
% functional_specifications(social_scheme={"sch_1": social_scheme(
%     goals=["goal_1": orf.add_pattern("[[#any],o4,a4](1,1)")], missions=["prey_proximity"], goals_structure=None, mission_to_goals={"prey_proximity": ["goal_0"]}, mission_to_agent_cardinality={"prey_proximity": cardinality(1, "*")})}, social_preferences=None),
% deontic_specifications(None, {
%     obligation("circle_leader", None, time_constraint_type.ANY): ["leadadversary_0"], obligation("circle_follower", None, time_constraint_type.ANY): ["adversary_0", "adversary_1", "adversary_2"]})))
% \end{lstlisting}

\subsubsection{Evaluation criteria and metrics}

Our evaluation criteria rely both on qualitative and quantitative aspects during and after training. We considered three cases to compare models:
\begin{itemize}
  \item \textbf{No Organizational Specifications (NTS)}: Agents learn without any constraints.
  \item \textbf{Partially Constraining Organizational Specifications (PTS)}: Agents are guided by the \textit{circle\_model}.
  \item \textbf{Fully Constraining Organizational Specifications (FTS)}: Agents follow hand-crafted policies from the \textit{manual\_model}.
\end{itemize}

We used various metrics to help measure the impact of PRAHOMT during/after training:
\begin{itemize}
  \item \textbf{Scalability}: Assesses qualitatively the ability to manage a growing number of agents and obstacles based on computational resource usage.
  \item \textbf{Convergence Time}: The number of episodes required for the algorithm to reach a stable solution. Shorter convergence times indicate faster learning.
  \item \textbf{Standard Deviation}: Indicates the variability in the rewards obtained by the agents. A lower standard deviation signifies more consistent performance and possibly more stable organization.
  \item \textbf{Average Reward}: The mean reward obtained per episode reflects the overall performance of the algorithm.
  \item \textbf{Constraint Respect}: Assesses how well the agents adhere to the given organizational constraints. It is calculated as the inverse of the number of times the constraints are not satisfied. High constraint respect means that the agents are effectively following the rules.
\end{itemize}

Relying on these metrics, we summarized the criteria likely to show an effective expected impact of PRAHOMT during/after training:
\begin{itemize}
  \item $(\mathbf{C_1})$: We expected to manually notice some collective hunting strategies, such as circling, in a visually rendered environment.
  \item $(\mathbf{C_2})$: We expect to see faster convergence in the PTS case compared to the NTS case, with the FTS case showing a constant learning curve.
  \item $(\mathbf{C_3})$: We expect higher rewards in the PTS case compared to the NTS case.
  \item $(\mathbf{C_4})$: We expect the standard deviation to decrease from NTS to PTS and from PTS to FTS since agents are increasingly more constrained in their behavior.
  \item $(\mathbf{C_5})$: We expect the constraint respect to be fully covered in \textit{correct} and \textit{correct\_policy} modes but not in the \textit{penalize} mode.
  \item $(\mathbf{C_6})$: We expect the scalability to be handled in all of the cases.
\end{itemize}


\subsection{Results and Discussion}

The learning curves for the three cases (NTS, PTS, FTS) for the \textit{penalize} mode, are displayed in \autoref{fig:learning_curves}. \autoref{tab:results} summarizes the results according to metrics for each mode after training.

\begin{figure*}[h!]
  \centering
  \includegraphics[width=0.9\textwidth]{figures/penalize_learning_curves.png}
  \caption{Learning curves for NTS, PTS, and FTS cases using the \textit{penalize} mode.}
  \label{fig:learning_curves}
\end{figure*}


\begin{table*}[h!]
  \centering
  \caption{Comparison of NTS, PTS, and FTS cases under different modes (correct, penalize, correct\_policy) based on various metrics. \vspace{0.2cm} }\label{tab:results}
  \setlength{\tabcolsep}{5pt}
  \begin{tabular}{cccccccccc}
    \cline{2-10}
                       & \multicolumn{3}{c}{correct} & \multicolumn{3}{c}{penalize} & \multicolumn{3}{c}{correct\_policy}                                                     \\
    \cline{2-10}
                       & NTS                         & PTS                          & FTS                                 & NTS         & PTS & FTS & NTS         & PTS & FTS \\
    \hline
    Scalability        & +                           & +                            & +                                   & ++          & ++  & ++  & +           & +   & +   \\
    Convergence Time   & 62                          & 28                           & 0                                   & 63          & 22  & 0   & 62          & 26  & 0   \\
    Standard Deviation & 2.4                         & 1.5                          & 0.                                  & 2.3         & 2.4 & 0.  & 2.3         & 1.2 & 0.  \\
    Average Reward     & 53                          & 68                           & 60                                  & 52          & 67  & 58  & 53          & 69  & 59  \\
    Constraint Respect & $\emptyset$                 & 1                            & 1                                   & $\emptyset$ & 0.8 & 1   & $\emptyset$ & 1   & 1   \\
  \end{tabular}
\end{table*}
%
%
% The qualitative analysis shows that the \textit{circle\_model} leads to observable circling behaviors around prey, demonstrating the effective impact of organizational specifications on agent behavior. This visually \footnotemark[2] confirms that PRAHOMT can induce desired collective strategies ($\mathbf{C1}$). Additionally, as expected, the \textit{manual\_model} gives good results while being stable.
%
%
%
% Quantitatively, for any mode, the PTS case exhibits faster convergence and higher average rewards compared to the NTS case ($\mathbf{C_3}$), validating the hypothesis that organizational constraints can enhance learning efficiency ($\mathbf{C_2}$). Additionally, we verify the standard deviation to decrease from NTS to PTS and from PTS to FTS $(\mathbf{C_4})$.
%
% The FTS case, serving as a reference, consistently achieves the highest rewards due to pre-defined optimal policies. We also verify the constraint respect to be fully covered in \textit{correct} and \textit{correct\_policy} modes but the \textit{penalize} mode ($\mathbf{C_5}$). Finally, we did not see scalability difficulties when adding up to 30 agents and obstacles for all cases encountered ($\mathbf{C_6}$). From the experiments conducted, the \textit{penalize} mode better faces scalability because it updates its internal policy using optimized computations while \textit{correct} and \textit{correct\_policy} require an aside correction function that may impact the overall computational performance.
%
% The experiments demonstrate that PRAHOMT significantly impacts agent behavior and learning efficiency. The organizational models, especially the \textit{circle\_model}, effectively guide agents towards efficient collective strategies, as evidenced by both qualitative observations and quantitative metrics.
%
The qualitative analysis shows that the \textit{circle\_model} does induce observable circling behaviors around prey, confirming the impact of organizational specifications on agent actions%
\footnote{Screenshots are provided in Annex \ref{sec:screenshots}, as well as a video showing the running \textit{manual\_model} at \url{https://shorturl.at/YRBao}}%
. This visual representation shows alignment with expected collective strategies ($\mathbf{C1}$). The \textit{manual\_model} also exhibited consistent circling behavior, further demonstrating the robustness of predefined policies.

From a quantitative perspective, the metrics provide compelling evidence for the advantages of applying organizational constraints. As illustrated in \autoref{fig:learning_curves}, in the PTS case, agents exhibited faster convergence compared to the NTS case across all modes. This confirms that organizational constraints can accelerate learning ($\mathbf{C2}$). The FTS case, which utilized hand-crafted policies, achieved immediate convergence, as expected, due to the absence of learning.

The average reward metrics reveal that agents guided by the \textit{circle\_model} (PTS) consistently outperformed those in the unconstrained scenario (NTS), achieving higher rewards ($\mathbf{C3}$). This indicates that organizational constraints not only improve learning efficiency but also enhance overall performance. The FTS case, leveraging optimal policies, consistently achieved the highest rewards, highlighting the effectiveness of well-defined constraints. In \autoref{fig:learning_curves}, PTS demonstrates to be even more efficient than the FTS, which was hand-crafted. We also noticed that the \textit{penalize} mode shows slightly better average rewards than \textit{correct} and \textit{correct\_policy} modes. Indeed, \textit{penalize} may authorize not satisfying some organizational specifications if it increases the overall reward.

The standard deviation of rewards, an indicator of performance variability, decreased progressively from NTS to PTS and from PTS to FTS ($\mathbf{C4}$). This reduction in variability suggests that organizational constraints contribute to more stable and consistent agent behavior. As illustrated in \autoref{fig:learning_curves}, PTS shows a lower variance at the end compared to NTS, stabilizing at around 70, probably indicating a stable collective strategy has been found. Again, the \textit{penalize} mode shows a slightly higher deviation because the policy is not strongly constrained by organizational specifications.

Constraint respect, assessed as the adherence to organizational rules, was fully met in the \textit{correct} and \textit{correct\_policy} modes, but not in the \textit{penalize} mode ($\mathbf{C5}$). This demonstrates that while agents can learn to follow constraints effectively, the constraint mode plays a crucial role in their adherence.

Scalability, evaluated by increasing the number of agents and obstacles, was effectively handled in all cases ($\mathbf{C6}$). The \textit{penalize} mode demonstrated superior scalability, as it incorporates optimized computations for policy updates, in contrast to the side correction functions required by the \textit{correct} and \textit{correct\_policy} modes, which can impact computational performance.

In summary, the experimental results show that PRAHOMT enhances both agent behavior and learning efficiency. The \textit{circle\_model} successfully guides agents towards efficient collective strategies, as evidenced by the observed behaviors and quantitative metrics. These findings underscore the utility of organizational constraints in MARL environments and are a first step towards a framework for future research and application.



\section{Conclusion}\label{sec:conclusion}

This paper proposes to see the design of a MAS as a problem under constraint where agents' policies are to be optimized. To integrate the constraints, we proposed the PRAHOMT algorithm to augment the MARL framework with the $\mathcal{M}OISE^+$ model. PRAHOMT constrains agents' training according to organizational specifications. We evaluated PRAHOMT using our proposed proof of concept (PoC) implementation for the \textquote{Simple World Comm} predator-prey environment. Two organizational models were established: a minimally constrained model, \textquote{circle\_model}, and a fully constrained one, \textquote{manual\_model}. We conducted an evaluation of the fully-learned, fine-tuned, or predefined policies based on performance criteria in or after training.
%
The results show that \textquote{circle\_model} provides a relevant tradeoff between constraints and free learning. Despite comprising simple predefined rules, it effectively complements learned behavior.

In addition to constraining agents according to organizational specifications, we also aim to integrate explainability mechanisms. We intend to characterize and curate relevant emergent strategies to include as new organizational constraints in future training. The idea of iterative, mutually beneficial improvement between constrained training and explainability could greatly benefit from hierarchical learning, which helps better characterize and reveal strategies during learning. Furthermore, the initial results obtained with LLM suggest it as a promising complementary tool for PRAHOMT, potentially offering new avenues for explaining collective behavior, especially in scenarios where environments are not visually or intuitively representable.

\begin{acks}
  This work was supported by \emph{Thales Land Air Systems} within the framework of the \emph{Cyb'Air} chair and the \emph{AICA IWG}.
\end{acks}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% The next two lines define, first, the bibliography style to be 
%%% applied, and, second, the bibliography file to be used.

\section*{References}

\bibliographystyle{ACM-Reference-Format}
\renewcommand
\refname{}
\bibliography{references}

\end{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
