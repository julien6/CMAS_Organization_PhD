%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% LaTeX Template for AAMAS-2025 (based on sample-sigconf.tex)
%%% Prepared by the AAMAS-2025 Program Chairs based on the version from AAMAS-2025. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% Start your document with the \documentclass command.

%%%% For anonymized submission, use this
\documentclass[sigconf,anonymous]{aamas} 

%%%% For camera-ready, use this
%\documentclass[sigconf]{aamas}

\usepackage{listings}
% \usepackage{xcolor}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}
 
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2,
    inputencoding = utf8,  % Input encoding
    extendedchars = true,  % Extended ASCII
    literate      =        % Support additional characters
    {á}{{\'a}}1  {é}{{\'e}}1  {í}{{\'i}}1 {ó}{{\'o}}1  {ú}{{\'u}}1
    {Á}{{\'A}}1  {É}{{\'E}}1  {Í}{{\'I}}1 {Ó}{{\'O}}1  {Ú}{{\'U}}1
    {à}{{\`a}}1  {è}{{\`e}}1  {ì}{{\`i}}1 {ò}{{\`o}}1  {ù}{{\`u}}1
    {À}{{\`A}}1  {È}{{\`E}}1  {Ì}{{\`I}}1 {Ò}{{\`O}}1  {Ù}{{\`U}}1
    {ä}{{\"a}}1  {ë}{{\"e}}1  {ï}{{\"i}}1 {ö}{{\"o}}1  {ü}{{\"u}}1
    {Ä}{{\"A}}1  {Ë}{{\"E}}1  {Ï}{{\"I}}1 {Ö}{{\"O}}1  {Ü}{{\"U}}1
    {â}{{\^a}}1  {ê}{{\^e}}1  {î}{{\^i}}1 {ô}{{\^o}}1  {û}{{\^u}}1
    {Â}{{\^A}}1  {Ê}{{\^E}}1  {Î}{{\^I}}1 {Ô}{{\^O}}1  {Û}{{\^U}}1
    {œ}{{\oe}}1  {Œ}{{\OE}}1  {æ}{{\ae}}1 {Æ}{{\AE}}1  {ß}{{\ss}}1
    {ẞ}{{\SS}}1  {ç}{{\c{c}}}1 {Ç}{{\c{C}}}1 {ø}{{\o}}1  {Ø}{{\O}}1
    {å}{{\aa}}1  {Å}{{\AA}}1  {ã}{{\~a}}1  {õ}{{\~o}}1 {Ã}{{\~A}}1
    {Õ}{{\~O}}1  {ñ}{{\~n}}1  {Ñ}{{\~N}}1  {¿}{{?`}}1  {¡}{{!`}}1
    {„}{\quotedblbase}1 {“}{\textquotedblleft}1 {–}{$-$}1
    {°}{{\textdegree}}1 {º}{{\textordmasculine}}1 {ª}{{\textordfeminine}}1
    {£}{{\pounds}}1  {©}{{\copyright}}1  {®}{{\textregistered}}1
    {«}{{\guillemotleft}}1  {»}{{\guillemotright}}1  {Ð}{{\DH}}1  {ð}{{\dh}}1
    {Ý}{{\'Y}}1    {ý}{{\'y}}1    {Þ}{{\TH}}1    {þ}{{\th}}1    {Ă}{{\u{A}}}1
    {ă}{{\u{a}}}1  {Ą}{{\k{A}}}1  {ą}{{\k{a}}}1  {Ć}{{\'C}}1    {ć}{{\'c}}1
    {Č}{{\v{C}}}1  {č}{{\v{c}}}1  {Ď}{{\v{D}}}1  {ď}{{\v{d}}}1  {Đ}{{\DJ}}1
    {đ}{{\dj}}1    {Ė}{{\.{E}}}1  {ė}{{\.{e}}}1  {Ę}{{\k{E}}}1  {ę}{{\k{e}}}1
    {Ě}{{\v{E}}}1  {ě}{{\v{e}}}1  {Ğ}{{\u{G}}}1  {ğ}{{\u{g}}}1  {Ĩ}{{\~I}}1
    {ĩ}{{\~\i}}1   {Į}{{\k{I}}}1  {į}{{\k{i}}}1  {İ}{{\.{I}}}1  {ı}{{\i}}1
    {Ĺ}{{\'L}}1    {ĺ}{{\'l}}1    {Ľ}{{\v{L}}}1  {ľ}{{\v{l}}}1  {Ł}{{\L{}}}1
    {ł}{{\l{}}}1   {Ń}{{\'N}}1    {ń}{{\'n}}1    {Ň}{{\v{N}}}1  {ň}{{\v{n}}}1
    {Ő}{{\H{O}}}1  {ő}{{\H{o}}}1  {Ŕ}{{\'{R}}}1  {ŕ}{{\'{r}}}1  {Ř}{{\v{R}}}1
    {ř}{{\v{r}}}1  {Ś}{{\'S}}1    {ś}{{\'s}}1    {Ş}{{\c{S}}}1  {ş}{{\c{s}}}1
    {Š}{{\v{S}}}1  {š}{{\v{s}}}1  {Ť}{{\v{T}}}1  {ť}{{\v{t}}}1  {Ũ}{{\~U}}1
    {ũ}{{\~u}}1    {Ū}{{\={U}}}1  {ū}{{\={u}}}1  {Ů}{{\r{U}}}1  {ů}{{\r{u}}}1
    {Ű}{{\H{U}}}1  {ű}{{\H{u}}}1  {Ų}{{\k{U}}}1  {ų}{{\k{u}}}1  {Ź}{{\'Z}}1
    {ź}{{\'z}}1    {Ż}{{\.Z}}1    {ż}{{\.z}}1    {Ž}{{\v{Z}}}1  {ž}{{\v{z}}}1
    % ¿ and ¡ are not correctly displayed if inconsolata font is used
    % together with the lstlisting environment. Consider typing code in
    % external files and using \lstinputlisting to display them instead.   
}
 
\lstset{style=mystyle}

% --- Tickz
\usepackage{physics}
\usepackage{tikz}
\usepackage{amsmath}
\usepackage{mathdots}
% \usepackage{yhmath}
\usepackage{cancel}
\usepackage{color}
\usepackage{siunitx}
\usepackage{array}
\usepackage{multirow}
% \usepackage{amssymb}
\usepackage{gensymb}
\usepackage{tabularx}
\usepackage{extarrows}
\usepackage{booktabs}
\usetikzlibrary{fadings}
\usetikzlibrary{patterns}
\usetikzlibrary{shadows.blur}
\usetikzlibrary{shapes}

% ---------

\usepackage{balance} % for balancing columns on the final page
\usepackage{csquotes}
% \usepackage{cite}
\newcommand{\probP}{\text{I\kern-0.15em P}}
\usepackage{etoolbox}
\patchcmd{\thebibliography}{\section*{\refname}}{}{}{}
% \usepackage{amsthm,amssymb,amsfonts}

\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{color}
% \renewcommand\UrlFont{\color{blue}\rmfamily}

\usepackage[inline, shortlabels]{enumitem}
\usepackage{tabularx}
\usepackage{caption}
\usepackage{listings}
\usepackage{titlesec}
\usepackage{ragged2e}
% \usepackage[hyphens]{url}
\usepackage[linesnumbered,ruled,vlined]{algorithm2e}
% \usepackage{float}

\usepackage[english]{babel}
\addto\extrasenglish{  
    \def\figureautorefname{Figure}
    \def\tableautorefname{Table}
    \def\algorithmautorefname{Algorithm}
    \def\sectionautorefname{Section}
    \def\subsectionautorefname{Subsection}
    \def\proofoutlineautorefname{Proof Outline}
}

\newcommand{\supertiny}{\fontsize{1}{2}\selectfont}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% AAMAS-2025 copyright block (do not change!)

\setcopyright{ifaamas}
\acmConference[AAMAS '25]{Proc.\@ of the 24th International Conference
on Autonomous Agents and Multiagent Systems (AAMAS 2025)}{May 19 -- 23, 2025}
{Detroit, Michigan, USA}{A.~El~Fallah~Seghrouchni, Y.~Vorobeychik, S.~Das, A.~Nowe (eds.)}
\copyrightyear{2025}
\acmYear{2025}
\acmDOI{}
\acmPrice{}
\acmISBN{}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% == IMPORTANT ==
%%% Use this command to specify your EasyChair submission number.
%%% In anonymous mode, it will be printed on the first page.

\acmSubmissionID{<<EasyChair submission id>>}

%%% Use this command to specify the title of your paper.

\title[AAMAS-2025 CybMASDE]{Une Méthode pour Assister la Conception d'un Système Multi-Agent en Utilisant l'Apprentissage par Renforcement}

%%% Provide names, affiliations, and email addresses for all authors.

\author{Julien Soulé}
\affiliation{
  \institution{Univ. Grenoble Alpes}
  \city{Valence}
  \country{France}}
\email{julien.soule@lcis.grenoble-inp.fr}

\author{Jean-Paul Jamont}
\affiliation{
  \institution{Univ. Grenoble Alpes}
  \city{Valence}
  \country{France}}
\email{jean-paul.jamont@lcis.grenoble-inp.fr}

\author{Michel Occello}
\affiliation{
  \institution{Univ. Grenoble Alpes}
  \city{Valence}
  \country{France}}
\email{michel.occello@lcis.grenoble-inp.fr}

\author{Louis-Marie Traonouez}
\affiliation{
  \institution{Thales Land and Air Systems, BU IAS}
  \city{Rennes}
  \country{France}}
\email{louis-marie.traonouez@thalesgroup.com}

\author{Paul Théron}
\affiliation{
  \institution{AICA IWG}
  \city{La Guillermie}
  \country{France}}
\email{paul.theron@orange.fr}

\begin{abstract}
  Les méthodes AOSE reposent généralement sur la connaissance et l'expertise du concepteur afin d'orienter le développement vers un SMA qui atteint un objectif dans un environnement donné. Les récents travaux en MARL laissent entrevoir une approche plus automatisée de l'exploration de l'espace de conception.
  Nous proposons la méthode SAMMASD (Semi-Automated MARL-based MAS Design) pour la conception et le déploiement de SMA. Elle s'appuie pour cela sur l'intégration de spécifications organisationnelles en MARL. Cette méthode s'articule autour de quatre phases. La première consiste à modéliser l'environnement réel, les objectifs et contraintes additionnelles, telles que des exigences de fonctionnement, dans une simulation. La seconde phase exploite plusieurs algorithmes MARL afin d'apprendre des politiques satisfaisantes et stables dans l'atteinte des objectifs sous les contraintes exprimées précedement. La troisième phase applique une analyse des comportements pour inférer les rôles et objectifs émergents, et pour générer des "plans détaillés". Enfin, la phase de développement permet de déployer automatiquement ces politiques dans des environnements réels après une vérification en environnement émulé.
  SAMMASD est illustré sur un scénario de gestion de flux en entrepôt par des robots. Son utilisation montre que plusieurs SMA efficaces et sûrs sont générés facilitant le processus de conception.
\end{abstract}



%%% The code below was generated by the tool at http://dl.acm.org/ccs.cfm.
%%% Please replace this example with code appropriate for your own paper.


%%% Use this command to specify a few keywords describing your work.
%%% Keywords should be separated by commas.

\keywords{Agent-Oriented Software Engineering \and Multi-Agent Reinforcement Learning \and Assisted-Design \and Organizational Models}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% Include any author-defined commands here.
         
% \newcommand{\BibTeX}{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em\TeX}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

%%% The following commands remove the headers in your paper. For final 
%%% papers, these will be inserted during the pagination process.

\pagestyle{fancy}
\fancyhead{}

%%% The next command prints the information defined in the preamble.

\maketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}

Les systèmes multi-agents (SMA) sont au cœur de nombreuses applications, allant de la gestion d'entrepôts automatisés à la cybersécurité. Ces systèmes doivent permettre à plusieurs agents autonomes d'agir de manière coordonnée afin d'atteindre des objectifs communs dans des environnements complexes et dynamiques. Cependant, la conception de tels systèmes présente des défis majeurs en termes de robustesse, de sûreté et d'explicabilité des comportements des agents.

Les méthodes actuelles d'apprentissage par renforcement multi-agent (MARL) ont montré leur efficacité pour générer des politiques optimales dans des environnements dynamiques. Toutefois, ces approches manquent souvent de mécanismes explicites pour imposer des contraintes organisationnelles et garantir que les agents respectent des normes de sûreté essentielles pour des environnements critiques, comme la logistique en entrepôts ou la défense cybernétique.

Cet article présente une nouvelle méthode, \textbf{SAMMASD} (Semi-Automated MARL-based MAS Design), qui combine le framework \textbf{MOISE+MARL} avec l'algorithme d'évaluation \textbf{HEMM} pour inférer et appliquer des structures organisationnelles aux politiques apprises par les agents. L'intégration de \textbf{MOISE+}, un modèle organisationnel orienté multi-agents, permet de contraindre l'apprentissage des agents en définissant des rôles et des missions, ce qui favorise l'émergence de comportements interprétables et conformes aux exigences de sûreté.

Le reste de cet article est structuré comme suit : la \autoref{sec:related_works} présente les travaux connexes dans le domaine du MARL et des modèles organisationnels pour SMA. La \autoref{sec:approach} décrit en détail les quatre phases de la méthode SAMMASD. Ensuite, la \autoref{sec:evaluation} discute des résultats expérimentaux obtenus dans un scénario de gestion de flux d'entrepôt. Enfin, la \autoref{sec:conclusion} conclut et propose des pistes de travail futures.

\begin{figure*}[h!]
  \centering
  \input{figures/cycle.tex}
  \caption{Cycle de vie d'un SMA conçu avec SAMMASD.}
  \label{fig:cycle}
\end{figure*}

\section{Travaux connexes}
\label{sec:related_works}
La conception de systèmes multi-agents (SMA) assistée par des modèles organisationnels a été largement étudiée dans la littérature. En particulier, le modèle \textbf{MOISE+}, introduit par Gleizes et al. \cite{gleize2008moise}, propose une structure formelle pour spécifier les rôles, les missions, et les relations entre agents dans un SMA. Ces spécifications permettent de guider le développement de systèmes complexes en assurant une cohérence entre les objectifs collectifs et les comportements individuels des agents. Cependant, MOISE+ ne fournit pas de mécanismes pour intégrer directement l'apprentissage des politiques par renforcement, une lacune que notre travail cherche à combler.

D'un autre côté, les méthodes d'apprentissage par renforcement multi-agent (\textbf{MARL}) ont connu un essor ces dernières années, avec des approches telles que \textbf{MADDPG} (Multi-Agent Deep Deterministic Policy Gradient) \cite{lowe2017multi} et \textbf{Q-Mix} \cite{rashid2018qmix}. Ces algorithmes permettent aux agents d'apprendre des politiques optimales en collaboration ou en compétition dans des environnements partiellement observables. Néanmoins, ces méthodes se concentrent principalement sur la performance des politiques sans se soucier des contraintes organisationnelles ou des besoins en explicabilité, ce qui limite leur applicabilité dans des contextes nécessitant une coordination stricte entre agents.

Des travaux récents ont exploré des méthodologies agiles pour le développement de SMA, en combinant des approches orientées agent avec des pratiques agiles \cite{winikoff2021agile}. Ces méthodes visent à faciliter l'itération rapide dans la conception de SMA, mais elles n'intègrent pas directement les dynamiques d'apprentissage en MARL. Notre approche s'inscrit dans cette lignée en combinant l'apprentissage automatisé via MARL avec des spécifications organisationnelles, comblant ainsi le fossé entre les processus d'apprentissage et les exigences de conception organisationnelle.

\section{Approche de conception organisationnelle proposée pour les SMA}
\label{sec:approach}

\subsection{Aperçu général}

Notre méthode, SAMMASD, s'articule autour de quatre phases principales : (1) la modélisation de l'environnement et des contraintes, (2) l'apprentissage des politiques via \textbf{MOISE+MARL}, (3) l'analyse des comportements avec \textbf{HEMM}, et (4) le développement et le déploiement du SMA. Cette approche permet de guider le processus d'apprentissage des agents en respectant des contraintes organisationnelles strictes, tout en garantissant l'efficacité des politiques apprises. Le cycle de vie d'un SMA conçu avec SAMMASD est illustré dans la \autoref{fig:cycle}.

\subsection{Phase 1 : Modélisation}

L'étape de modélisation vise à créer un modèle simulé qui capture fidèlement les dynamiques et les contraintes de l'environnement réel, tout en incluant des spécifications organisationnelles et des objectifs. Ce modèle servira de base pour entraîner les agents dans un environnement contrôlé. Cette étape est cruciale pour s'assurer que les agents apprennent dans un contexte sécurisé et optimisé.

\paragraph{Entrées}
\begin{itemize}
    \item \textbf{Environnement réel} : Informations détaillées sur l'environnement cible, comprenant les états possibles, les transitions et les caractéristiques de l'environnement.
    \item \textbf{Description du problème} : Définition des objectifs à atteindre sous forme de fonction de récompense.
    \item \textbf{Contraintes additionnelles} : Exigences spécifiques à respecter, pouvant inclure des normes, règles d'organisation, ou contraintes de sûreté.
\end{itemize}

\paragraph{Processus}
\begin{enumerate}
    \item \textbf{Création du Modèle Simulé} :\\
    Si un modèle simulé existe déjà, il sera utilisé pour intégrer l'objectif et les contraintes. Sinon, on crée un modèle simulé en collectant des traces de l'environnement réel ou d'une copie sécurisée de celui-ci.
    
    \item \textbf{Incorporation des Contraintes MOISE+MARL} :\\
    Utilisation des spécifications de \textbf{MOISE+} pour structurer les rôles et missions des agents via les contraintes \textbf{OAC} et \textbf{TRF}.
    
    \item \textbf{Formulation du Modèle de Simulation} :\\
    Enrichissement du modèle par l'ajout des contraintes additionnelles pour encadrer le comportement des agents dans un cadre étendu du Dec-POMDP.
\end{enumerate}

\paragraph{Sortie}
Un modèle \textbf{MOISE+MARL} intégrant l'environnement simulé, l'objectif (fonction de récompense), et les contraintes additionnelles (OAC, TRF).

\RestyleAlgo{ruled}
\SetKwComment{Comment}{// }{}

\begin{algorithm}[hbt!]
  \caption{Étape de Modélisation pour SAMMASD}\label{alg:modeling}

  \SetAlgoLined
\KwIn{%
    $real\_env$ : Environnement réel avec états et transitions,\\
    $problem\_desc$ : Description du problème avec fonction de récompense,\\
    $additional\_constraints$ : Contraintes organisationnelles et de sûreté (TRF, OAC)
}
\KwOut{%
    $model$ : Un modèle MOISE+MARL avec l'environnement simulé, l'objectif, et les contraintes
}

% Étape 1: Créer ou utiliser un modèle simulé
\If{SimulatedModelExists($real\_env$)}{
    $simulated\_env \gets$ GetExistingSimulatedModel($real\_env$)\;
}
\Else{
    $safe\_env\_copy \gets$ CreateSafeCopy($real\_env$)\;
    $traces \gets$ CollectTraces($safe\_env\_copy$)\;
    $simulated\_env \gets$ GenerateSimulatedModel($traces$)\;
}

% Étape 2: Ajouter l'objectif et les contraintes
$simulated\_env$.AddObjective($problem\_desc.objective\_function$)\;
\ForEach{$constraint$ in $additional\_constraints$}{
    \eIf{$constraint.Type$ == "Soft"}{
        ApplyRewardShaping($simulated\_env$, $constraint$)\;
    }{
        ApplyShielding($simulated\_env$, $constraint$)\;
    }
}

% Étape 3: Incorporer les spécifications MOISE+MARL
$oac \gets$ GenerateOAC($simulated\_env$, $problem\_desc.roles$)\;
$trf \gets$ GenerateTRF($simulated\_env$, $problem\_desc.missions$)\;
$model \gets$ MOISE\_MARL\_Model($simulated\_env$, $oac$, $trf$)\;

\Return{$model$}

\end{algorithm}


Ce processus établit un environnement d'apprentissage sécurisé et guidé pour les agents, en s'assurant qu'ils respectent les contraintes définies tout en poursuivant leurs objectifs de manière efficace.


\subsubsection{Formalisation de MOISE+MARL}

Le framework **MOISE+MARL** enrichit le cadre de l'**Apprentissage par Renforcement Multi-Agent (MARL)** en intégrant des spécifications organisationnelles qui structurent et contrôlent les comportements des agents. En utilisant les rôles et missions de **MOISE+**, ce formalisme permet de guider les agents dans un environnement multi-agent selon des contraintes organisationnelles.

\paragraph{Description Globale}

La structure formelle de **MOISE+MARL** repose sur l'ajout de deux ensembles fondamentaux dans le cadre Dec-POMDP : 

\begin{itemize}
    \item \textbf{Observation-Actions Constraint (OAC)} : Un ensemble de relations modélisant les rôles en associant chaque observation d'un agent à un ensemble d'actions autorisées. Cela contraint les comportements possibles selon les rôles organisationnels.
    \item \textbf{Trajectory-based Reward Function (TRF)} : Un ensemble de relations associant des trajectoires d'actions à des récompenses, définissant ainsi les missions par des objectifs précis que les agents doivent atteindre.
\end{itemize}

\paragraph{Formalisation Complète}

Pour lier les structures MOISE+ aux contraintes d'apprentissage MARL, nous définissons les relations suivantes :

\begin{itemize}
    \item \textbf{OAC (Observation-Actions Constraint)} : 
    \begin{itemize}
        \item $\text{OAC} = \{ oac_1, oac_2, \dots, oac_k \}$
        \item Chaque relation \( oac \in \text{OAC} \) est définie par une fonction \( oac: \Omega \rightarrow \mathcal{P}(A) \), associant chaque observation \( o \in \Omega \) à un ensemble d'actions \( A \) autorisées.
    \end{itemize}
    \item \textbf{TRF (Trajectory-based Reward Function)} : 
    \begin{itemize}
        \item $\text{TRF} = \{ trf_1, trf_2, \dots, trf_m \}$
        \item Chaque relation \( trf \in \text{TRF} \) est une fonction \( trf: H \rightarrow \mathbb{R} \), attribuant une récompense à chaque trajectoire \( h \in H \).
    \end{itemize}
    \item \textbf{Déontiques (Relation DA)} :
    \begin{itemize}
        \item La relation \( \text{DA}: \mathcal{D} \rightarrow \mathcal{P}(\mathcal{A}) \) relie les spécifications déontiques aux agents, permettant de définir les permissions et obligations selon les rôles et missions.
    \end{itemize}
    \item \textbf{Relation Role-OAC (ROAC)} :
    \begin{itemize}
        \item La relation \( \text{ROAC}: \mathcal{R} \rightarrow \text{OAC} \) associe chaque rôle de MOISE+ à une relation dans OAC, encadrant les actions des agents pour chaque rôle.
    \end{itemize}
    \item \textbf{Relation Objective-TRF (OTRF)} :
    \begin{itemize}
        \item La relation \( \text{OTRF}: \mathcal{G} \rightarrow \text{TRF} \) lie les objectifs aux relations de TRF, en représentant les objectifs sous forme de récompenses dans le MARL.
    \end{itemize}
\end{itemize}

\paragraph{Intérêt du Formalisme MOISE+MARL}

Ce formalisme guide l'apprentissage multi-agent en imposant des contraintes basées sur des rôles et des missions organisationnels. Il permet de mieux structurer les comportements d'agents dans des systèmes multi-agents complexes, assurant ainsi un alignement avec les normes organisationnelles et facilitant la sûreté et l'explicabilité des politiques apprises.


%%%%%%%%%

\subsection{Comparaison des Méthodes de Modélisation Automatisée}

Dans cette sous-section, nous présentons une analyse des différentes méthodes permettant de modéliser un environnement cible de manière fidèle et automatisée. Les méthodes considérées sont l'Identification de Systèmes, l'Apprentissage par Imitation, la Modélisation Approximative (Surrogate Modeling), et le Jumeau Numérique (Digital Twin). Nous utilisons des critères spécifiques pour évaluer ces méthodes en fonction de leur adéquation aux besoins de la modélisation automatisée.

\paragraph{Critères d'Évaluation}

\begin{itemize}
    \item \textbf{Niveau d'Intervention Manuelle (NIM)} : La méthode nécessite-t-elle que l'utilisateur développe ou configure manuellement une partie du modèle simulé ?
    \item \textbf{Niveau de Modélisation Mathématique (NMM)} : La méthode inclut-elle des modèles mathématiques pour représenter l'environnement ?
    \item \textbf{Applicabilité Générale (AG)} : La méthode peut-elle modéliser une large variété d'environnements sans restrictions spécifiques ?
    \item \textbf{Requiert une Version Émulée (RVE)} : La méthode nécessite-t-elle une copie émulée de l'environnement cible pour générer ou valider le modèle ?
    \item \textbf{Mise à Jour Automatique (MUA)} : La méthode permet-elle de mettre à jour le modèle automatiquement sans intervention humaine ?
    \item \textbf{Automatisation Complète (AC)} : La méthode offre-t-elle une procédure entièrement automatisée pour construire et gérer le modèle ?
    \item \textbf{Adaptabilité en Temps Réel (ATR)} : La méthode peut-elle s'adapter en temps réel aux changements de l'environnement cible ?
    \item \textbf{Niveau de Fidélité au Système Réel (NFS)} : La méthode fournit-elle un modèle avec un haut niveau de fidélité par rapport au système cible ?
    \item \textbf{Exigences en Données (ED)} : Quel est le volume et le type de données requis pour construire et maintenir le modèle ?
    \item \textbf{Coût Computationnel (CC)} : Quelles sont les exigences en termes de ressources de calcul pour générer et mettre à jour le modèle ?
\end{itemize}

\paragraph{Comparaison des Méthodes}

Le tableau \ref{tab:comparaison-methodes} présente une comparaison des méthodes d'Identification de Systèmes, d'Apprentissage par Imitation, de Modélisation Approximative, et de Jumeau Numérique en fonction des critères mentionnés.


\begin{table*}[h!]
\centering
\caption{Comparaison des méthodes de modélisation automatisée selon différents critères}
\begin{tabular}{|p{5cm}|c|c|c|c|}
\hline
\textbf{Critères}                   & \textbf{System Identification} & \textbf{Imitation Learning} & \textbf{Surrogate Modeling} & \textbf{Digital Twin} \\
\hline
Niveau d'Intervention Manuelle (NIM)    & Moyen                         & Bas                          & Moyen                       & Haut                 \\
Niveau de Modélisation Mathématique (NMM) & Élevé                         & Faible                       & Élevé                       & Moyen                \\
Applicabilité Générale (AG)             & Moyenne                       & Élevée                       & Moyenne                     & Moyenne              \\
Requiert une Version Émulée (RVE)       & Non                           & Non                          & Non                         & Oui                  \\
Mise à Jour Automatique (MUA)           & Moyenne                       & Élevée                       & Moyenne                     & Élevée               \\
Automatisation Complète (AC)            & Moyenne                       & Moyenne                      & Moyenne                     & Variable             \\
Adaptabilité en Temps Réel (ATR)        & Faible                        & Faible                       & Moyenne                     & Élevée               \\
Niveau de Fidélité au Système Réel (NFS) & Moyen                         & Bas                          & Moyen                       & Élevé                \\
Exigences en Données (ED)               & Élevées                       & Moyennes                     & Élevées                     & Élevées              \\
Coût Computationnel (CC)                & Élevé                         & Moyen                        & Moyen                       & Élevé                \\
\hline
\end{tabular}
\label{tab:comparaison-methodes}
\end{table*}

\paragraph{Explications des Comparaisons}

Les méthodes d'\textbf{Identification de Systèmes} et de \textbf{Jumeau Numérique} sont connues pour leur fidélité accrue à l'environnement réel, mais elles nécessitent souvent des ajustements manuels et un coût computationnel élevé. L'\textbf{Apprentissage par Imitation} est plus automatisé et nécessite moins de données mathématiques, bien qu'il soit souvent moins précis. La \textbf{Modélisation Approximative} offre un bon compromis pour des applications générales avec des exigences computationnelles moyennes, mais elle peut nécessiter une intervention manuelle pour ajuster les modèles aux changements de l'environnement.


\subsection{Phase 2 : Résolution}
Dans cette phase, nous utilisons des algorithmes \textbf{MARL} pour résoudre le problème formalisé dans l'environnement simulé. Les algorithmes comme \textbf{MADDPG} et \textbf{MAPPO} sont utilisés pour apprendre des politiques qui respectent à la fois les objectifs de performance et les contraintes organisationnelles définies par \textbf{MOISE+}. L'intégration de \textbf{MOISE+} dans le processus d'apprentissage permet de restreindre l'espace de recherche des politiques, rendant ainsi le processus plus efficace et garantissant la sûreté du système.

\begin{itemize}
  \item \textbf{Entrées :} Problème formalisé, contraintes organisationnelles.
  \item \textbf{Sorties :} Politiques conjointes optimales pour les agents.
  \item \textbf{Processus :} Utilisation de \textbf{MARL} pour apprendre des politiques tout en respectant les rôles et missions prédéfinis.
\end{itemize}

\subsection{Phase 3 : Analyse}
Une fois les politiques apprises, elles sont analysées à l'aide de \textbf{HEMM}, un algorithme qui infère les rôles et les missions des agents en se basant sur leurs comportements observés. L'analyse permet d'identifier les écarts entre les rôles inférés et les rôles prédéfinis dans \textbf{MOISE+}, offrant ainsi une évaluation de l'adéquation organisationnelle du SMA. Les résultats de cette analyse sont ensuite traduits en plans détaillés, qui seront utilisés pour le développement final du système.

\begin{itemize}
  \item \textbf{Entrées :} Politiques conjointes des agents.
  \item \textbf{Sorties :} Rôles et missions inférés, plan détaillé des comportements.
  \item \textbf{Processus :} Utilisation de \textbf{HEMM} pour analyser les politiques apprises et générer des plans interprétables.
\end{itemize}

\subsection{Phase 4 : Développement et déploiement}
Dans la phase finale, les plans générés sont utilisés pour développer et déployer le SMA dans l'environnement réel. Nous utilisons l'outil \textbf{CybMASDE} pour faciliter l'intégration des politiques apprises dans les agents du système. Avant le déploiement réel, des tests sont effectués dans un environnement émulé pour vérifier que le SMA respecte bien les contraintes de sûreté et les objectifs de performance. Cette phase garantit que les agents fonctionnent comme prévu dans l'environnement réel tout en respectant les rôles et missions spécifiés.

\section{Évaluation}
\label{sec:evaluation}

\subsection{CybMASDE : un environnement de développement pour l'approche}
Pour évaluer notre méthode, nous avons développé un outil nommé \textbf{CybMASDE} qui soutient chaque phase du processus de conception. \textbf{CybMASDE} propose une interface pour modéliser des environnements complexes, définir des objectifs, et spécifier des contraintes organisationnelles. Il intègre également des algorithmes \textbf{MARL} pour l'apprentissage des politiques et des outils d'analyse pour l'inférence des rôles. CybMASDE permet de visualiser les rôles inférés et de faciliter le déploiement des SMA dans des environnements réels après une validation en simulation.

\subsection{Résultats expérimentaux}
Nous avons appliqué SAMMASD à un scénario de gestion de flux de paquets dans un entrepôt, où des robots doivent collaborer pour organiser les flux de marchandises tout en respectant des contraintes de sécurité et d'efficacité. Les résultats montrent que la méthode permet de générer des politiques conjointes efficaces, avec une réduction significative du temps de conception et une amélioration de la robustesse des systèmes déployés.

\begin{itemize}
  \item \textbf{Environnement :} Gestion de flux dans un entrepôt simulé.
  \item \textbf{Objectifs :} Optimiser les flux de marchandises tout en respectant les contraintes de sûreté.
  \item \textbf{Métriques :} Temps de conception, robustesse des politiques, respect des contraintes organisationnelles.
\end{itemize}

Les résultats montrent que les agents entraînés via \textbf{MOISE+MARL} respectent les contraintes organisationnelles définies et optimisent les objectifs de performance, tout en générant des comportements interprétables grâce à l'analyse par \textbf{HEMM}.

\section{Conclusion}
\label{sec:conclusion}
Cet article a présenté une nouvelle méthode, \textbf{SAMMASD}, qui permet de concevoir et déployer des systèmes multi-agents en combinant des spécifications organisationnelles avec des techniques d'apprentissage par renforcement. La méthode s'articule autour de quatre phases clés, allant de la modélisation de l'environnement à l'analyse des politiques apprises et au déploiement des systèmes réels. Les résultats expérimentaux montrent que cette approche réduit le temps de conception et améliore la robustesse des SMA tout en garantissant l'explicabilité des comportements des agents.

Les travaux futurs porteront sur l'extension de \textbf{SAMMASD} à des environnements encore plus complexes, notamment dans des contextes critiques tels que la cybersécurité, et sur l'intégration de nouveaux algorithmes d'apprentissage pour améliorer l'efficacité des politiques apprises.

\bibliographystyle{ACM-Reference-Format}
\renewcommand\refname{}
\bibliography{references}

\end{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
