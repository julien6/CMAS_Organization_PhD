%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% LaTeX Template for AAMAS-2025 (based on sample-sigconf.tex)
%%% Prepared by the AAMAS-2025 Program Chairs based on the version from AAMAS-2025. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% Start your document with the \documentclass command.


%%% == IMPORTANT ==
%%% Use the first variant below for the final paper (including auithor information).
%%% Use the second variant below to anonymize your submission (no authoir information shown).
%%% For further information on anonymity and double-blind reviewing, 
%%% please consult the call for paper information
%%% https://aamas2025.org/index.php/conference/calls/submission-instructions-main-technical-track/

%%%% For anonymized submission, use this
\documentclass[sigconf,anonymous]{aamas} 

%%%% For camera-ready, use this
%\documentclass[sigconf]{aamas}

\usepackage{listings}
% \usepackage{xcolor}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}
 
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}
 
\lstset{style=mystyle}

% --- Tickz
\usepackage{physics}
\usepackage{tikz}
\usepackage{amsmath}
\usepackage{mathdots}
% \usepackage{yhmath}
\usepackage{cancel}
\usepackage{color}
\usepackage{siunitx}
\usepackage{array}
\usepackage{cuted}
\usepackage{multirow}
% \usepackage{amssymb}
\usepackage{gensymb}
\usepackage{tabularx}
\usepackage{extarrows}
\usepackage{booktabs}
\usetikzlibrary{fadings}
\usetikzlibrary{patterns}
\usetikzlibrary{shadows.blur}
\usetikzlibrary{shapes}

% ---------

\usepackage{balance} % for balancing columns on the final page
\usepackage{csquotes}
% \usepackage{cite}
\newcommand{\probP}{I\kern-0.15em P}
\usepackage{etoolbox}
\patchcmd{\thebibliography}{\section*{\refname}}{}{}{}
% \usepackage{amsthm,amssymb,amsfonts}

\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{color}
% \renewcommand\UrlFont{\color{blue}\rmfamily}

\usepackage[inline, shortlabels]{enumitem}
\usepackage{tabularx}
\usepackage{caption}
\usepackage{listings}
\usepackage{titlesec}
\usepackage{ragged2e}
% \usepackage[hyphens]{url}
\usepackage[linesnumbered,ruled,vlined]{algorithm2e}
\usepackage{float}

\usepackage[english]{babel}
\addto\extrasenglish{  
    \def\figureautorefname{Figure}
    \def\tableautorefname{Table}
    \def\algorithmautorefname{Algorithm}
    \def\sectionautorefname{Section}
    \def\subsectionautorefname{Subsection}
    \def\proofoutlineautorefname{Proof Outline}
}

\newcommand{\supertiny}{\fontsize{1}{2}\selectfont}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% AAMAS-2025 copyright block (do not change!)

\setcopyright{ifaamas}
\acmConference[AAMAS '25]{Proc.\@ of the 24th International Conference
on Autonomous Agents and Multiagent Systems (AAMAS 2025)}{May 19 -- 23, 2025}
{Detroit, Michigan, USA}{A.~El~Fallah~Seghrouchni, Y.~Vorobeychik, S.~Das, A.~Nowe (eds.)}
\copyrightyear{2025}
\acmYear{2025}
\acmDOI{}
\acmPrice{}
\acmISBN{}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% == IMPORTANT ==
%%% Use this command to specify your EasyChair submission number.
%%% In anonymous mode, it will be printed on the first page.

\acmSubmissionID{<<EasyChair submission id>>}

%%% Use this command to specify the title of your paper.

\title[AAMAS-2025 CybMASDE]{A Method for Assisting Multi-Agent System Design Using Reinforcement Learning}

%%% Provide names, affiliations, and email addresses for all authors.

\author{Julien Soulé}
\affiliation{
  \institution{Univ. Grenoble Alpes}
  \city{Valence}
  \country{France}}
\email{julien.soule@lcis.grenoble-inp.fr}

\author{Jean-Paul Jamont}
\affiliation{
  \institution{Univ. Grenoble Alpes}
  \city{Valence}
  \country{France}}
\email{jean-paul.jamont@lcis.grenoble-inp.fr}

\author{Michel Occello}
\affiliation{
  \institution{Univ. Grenoble Alpes}
  \city{Valence}
  \country{France}}
\email{michel.occello@lcis.grenoble-inp.fr}

\author{Louis-Marie Traonouez}
\affiliation{
  \institution{Thales Land and Air Systems, BU IAS}
  \city{Rennes}
  \country{France}}
\email{louis-marie.traonouez@thalesgroup.com}

\author{Paul Théron}
\affiliation{
  \institution{AICA IWG}
  \city{La Guillermie}
  \country{France}}
\email{paul.theron@orange.fr}

\begin{abstract}
    Agent-Oriented Software Engineering (AOSE) methods typically rely on the designer's expertise to guide the development of a multi-agent system (MAS) that meets specific objectives within a given environment. Recent advancements in Multi-Agent Reinforcement Learning (MARL) suggest a more automated approach to exploring the design space. We introduce the SAMMASD (Semi-Automated MARL-based MAS Design) method, which integrates organizational specifications into MARL for MAS design and deployment. This method consists of four phases. The first phase models the real-world environment, objectives, and additional constraints—such as operational requirements—into a simulation. The second phase leverages multiple MARL algorithms to learn stable policies that achieve these objectives within the defined constraints. The third phase conducts a behavior analysis to infer emergent roles and objectives and to generate detailed "blueprints" for implementation. Finally, the development phase enables the automatic deployment of these policies in real-world environments, following validation in an emulated setting. We demonstrate SAMMASD in a warehouse flow management scenario involving robot agents, illustrating how the method can produce efficient and reliable MAS designs that streamline the design process.
\end{abstract}



%%% The code below was generated by the tool at http://dl.acm.org/ccs.cfm.
%%% Please replace this example with code appropriate for your own paper.


%%% Use this command to specify a few keywords describing your work.
%%% Keywords should be separated by commas.

\keywords{Agent-Oriented Software Engineering \and Multi-Agent Reinforcement Learning \and Assisted-Design \and Organizational Models}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% Include any author-defined commands here.
         
% \newcommand{\BibTeX}{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em\TeX}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

%%% The following commands remove the headers in your paper. For final 
%%% papers, these will be inserted during the pagination process.

\pagestyle{fancy}
\fancyhead{}

%%% The next command prints the information defined in the preamble.

\maketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}

Les systèmes multi-agents (SMA) sont au cœur de nombreuses applications, allant de la gestion d'entrepôts automatisés à la cybersécurité. Ces systèmes doivent permettre à plusieurs agents autonomes d'agir de manière coordonnée afin d'atteindre des objectifs communs dans des environnements complexes et dynamiques. Cependant, la conception de tels systèmes présente des défis majeurs en termes de robustesse, de sûreté et d'explicabilité des comportements des agents.

Les méthodes actuelles d'apprentissage par renforcement multi-agent (MARL) ont montré leur efficacité pour générer des politiques optimales dans des environnements dynamiques. Toutefois, ces approches manquent souvent de mécanismes explicites pour imposer des contraintes organisationnelles et garantir que les agents respectent des normes de sûreté essentielles pour des environnements critiques, comme la logistique en entrepôts ou la défense cybernétique.

Cet article présente une nouvelle méthode, \textbf{SAMMASD} (Semi-Automated MARL-based MAS Design), qui combine le framework \textbf{$mathcal{M}OISE^+$MARL} avec l'algorithme d'évaluation \textbf{HEMM} pour inférer et appliquer des structures organisationnelles aux politiques apprises par les agents. L'intégration de \textbf{$mathcal{M}OISE^+$}, un modèle organisationnel orienté multi-agents, permet de contraindre l'apprentissage des agents en définissant des rôles et des missions, ce qui favorise l'émergence de comportements interprétables et conformes aux exigences de sûreté.

Le reste de cet article est structuré comme suit : la \autoref{sec:related_works} présente les travaux connexes dans le domaine du MARL et des modèles organisationnels pour SMA. La \autoref{sec:apprragh} décrit en détail les quatre phases de la méthode SAMMASD. Ensuite, la \autoref{sec:evaluation} discute des résultats expérimentaux obtenus dans un scénario de gestion de flux d'entrepôt. Enfin, la \autoref{sec:conclusion} conclut et propose des pistes de travail futures.

\section{Travaux connexes}
\label{sec:related_works}
La conception de systèmes multi-agents (SMA) assistée par des modèles organisationnels a été largement étudiée dans la littérature. En particulier, le modèle \textbf{$mathcal{M}OISE^+$}, introduit par Gleizes et al. \cite{gleize2008moise}, propose une structure formelle pour spécifier les rôles, les missions, et les relations entre agents dans un SMA. Ces spécifications permettent de guider le développement de systèmes complexes en assurant une cohérence entre les objectifs collectifs et les comportements individuels des agents. Cependant, $mathcal{M}OISE^+$ ne fournit pas de mécanismes pour intégrer directement l'apprentissage des politiques par renforcement, une lacune que notre travail cherche à combler.

D'un autre côté, les méthodes d'apprentissage par renforcement multi-agent (\textbf{MARL}) ont connu un essor ces dernières années, avec des approches telles que \textbf{MADDPG} (Multi-Agent Deep Deterministic Policy Gradient) \cite{lowe2017multi} et \textbf{Q-Mix} \cite{rashid2018qmix}. Ces algorithmes permettent aux agents d'apprendre des politiques optimales en collaboration ou en compétition dans des environnements partiellement observables. Néanmoins, ces méthodes se concentrent principalement sur la performance des politiques sans se soucier des contraintes organisationnelles ou des besoins en explicabilité, ce qui limite leur applicabilité dans des contextes nécessitant une coordination stricte entre agents.

Des travaux récents ont exploré des méthodologies agiles pour le développement de SMA, en combinant des approches orientées agent avec des pratiques agiles \cite{winikoff2021agile}. Ces méthodes visent à faciliter l'itération rapide dans la conception de SMA, mais elles n'intègrent pas directement les dynamiques d'apprentissage en MARL. Notre approche s'inscrit dans cette lignée en combinant l'apprentissage automatisé via MARL avec des spécifications organisationnelles, comblant ainsi le fossé entre les processus d'apprentissage et les exigences de conception organisationnelle.

\section{Approche de conception organisationnelle proposée pour les SMA}
\label{sec:apprragh}

Le coeur de la méthode SAMMASD est de considerer la conception d'un SMA comme un problème d'optimisation sous contraintes. La variable à optimiser est alors la politique conjointe pour maximiser la récompense cumulée sous des contraintes additionnelles données. SAMMASD inclue des spécifications organisationnelle en MARL comme contraintes additionnelles permettant à l'utilisateur de controler au moins partiellement non-seulement des comportements individuels des agents, mais aussi de pouvoir gérer l'ensemble des agents en ne jouant que sur celles-ci. Nous formalisons cette approche en proposant le framework MOISE+MARL.

\subsection{\textbf{Framework MOISE+MARL}}

L'objectif est alors de guider l'apprentissage des agents en respectant des contraintes organisationnelles sous la forme de role et d'objectifs, tout en garantissant une efficacité suffisante des politiques apprises dans leur environment.
Nous formalisons cette idée en proposant le framework MOISE+MARL qui combine le formalisme d'un Decentralized Partially Observable Markov Decision Process (Dec-POMDP) avec le modèle organisationnel $\mathcal{M}OISE^+$.

\textbf{Dec-POMDP} \quad défini par un tuple $D = \langle N, S, A, P, R, \Omega, O, \gamma \rangle$, où :
%
\begin{itemize*}[label={},itemjoin={; }]
    \item $\mathcal{A}$, l'ensemble des $N \in \mathbb{N}$ agents
    \item $S$, l'ensemble des états possibles de l'environment (agents compris)
    \item $A = \times_{i=1}^N A_i $, l'ensemble des actions possibles pour chaque agent $i$
    \item $T: S \times A^N \to S$, la fonction de transition d'états qui définit le prochain état $s'$ à partir d'un état $s$ sous l'action conjointe $a$
    \item $R: S \times A^N \times  S \to \mathbb{R}$, la fonction de récompense associant une récompense $r$ à chaque transition $(s, a, s')$
    \item $\Omega = \times_{i=1}^N \Omega_i $ est l'ensemble des observations possibles pour chaque agent $i$
    \item $O: S \times A \to \Omega$ est la fonction d'observation qui définit la prochaine observation $\omega$ d'un agent lorsque ce dernier applique une action $a$ dans l'état $s$
    \item $\gamma \in [0, 1] $ est le facteur d'actualisation.
\end{itemize*}

$\mathbf{\mathcal{M}OISE^+}$ \quad défini par un tuple $\mathcal{OS} = \langle \mathcal{SS}, \mathcal{FS}, \mathcal{DS} \rangle$. Ici, nous présentons une version minimale du formalisme de $\mathcal{M}OISE^+$ :
\begin{itemize*}[label={},itemjoin={; }]
    \item $\mathcal{SS} = \langle \mathcal{R} \rangle$, les spécifications structurelles sont définies par un ensemble de rôles $\mathcal{R}$
    \item $\mathcal{FS} = \langle \mathcal{G}, \mathcal{M}, mo \rangle$, les spécifications fonctionnelles sont un ensemble d'objectifs $\mathcal{G}$, un ensemble de mission $\mathcal{M}$ et une fonction $mo: \mathcal{M} \to \mathcal{P}(\mathcal{G} \times [0,1])$ qui associe une mission $m \in \mathcal{M}$ à un ensemble d'objectifs chacun pondéré par un poids $\{(g_1,w_0), (g_2,w_1)\dots\}, w_i \in [0,1], g_i \in \mathcal{G}$
    \item $\mathcal{DS} = \mathcal{R} \times \mathcal{M} \times T_c \times \{0,1\}$, les spécifications déontiques comme un ensemble de 4-uplets $(\rho_a, m, \allowbreak t_c, p)$, signifiant respectivement qu'un agent jouant le rôle $\rho_a \in \mathcal{R}$ est autorisé (si $p = 0$) ou obligé (si $p = 1$) à s'engager dans la mission $m \in \mathcal{M}, \ m \subseteq \mathcal{G}$ pour une contrainte temporelle donnée $t_c \in \mathcal{TC}, t_c = \mathcal{P}(N)$ spécifiant une période pendant laquelle la permission/obligation est valide.
\end{itemize*}

\textbf{Constraint Guides} \quad sont trois nouvelles relations introduites pour décrire la logique des rôles et objectifs $\mathcal{M}OISE^+$ dans le formalisme Dec-POMDP:

\begin{itemize}[label={},itemjoin={; }]
    \item \textbf{Role Action Guide} (RAG) \quad $rag: \Omega \rightarrow \mathcal{P}(A \times \mathbb{R})$, la relation qui permet de modéliser un rôle comme un ensemble de règles qui à chaque observation $\omega \in \Omega$ reçu par l'agent associe des actions attendues $A \in \mathcal{P}(A)$ chacune associée à une dureté de contrainte $ch \in [0,1]$ ($ch = 0$ par défaut). En restreignant le choix de la prochaine action parmi celles autorisées, l'agent est forcé d'adherer au comportement attendu du rôle
    \item \textbf{Role Reward Guide} (RRG) \quad $rrg: \Omega \times A \to \mathbb{R} = \{r_m \text{ if } a \in A, a \notin A_\omega, \ \omega \in \Omega, \ rag(\omega) = A_\omega \times \mathbb{R_\omega}; \text{ else } 0\}$, la relation qui permet de modéliser un rôle en ajoutant un malus $r_m$ à la récompense globale si l'action choisie par l'agent $a \in A$ n'est pas autorisée pour inciter l'agent à adherer au comportement attendu d'un role
    \item \textbf{Goal Reward Guide} (GRG) \quad $grg: H \rightarrow \mathbb{R}$, la relation qui permet de modéliser un objectif comme une contrainte molle en ajoutant un bonus $r_b \in \mathbb{R}$ à la récompense globale si la trajectoire de l'agent $h \in H$ contient une sous-séquence $h_g \in H_g$ caractéristique de l'objectif de sorte à inciter l'agent à trouver un moyen pour l'atteindre.
\end{itemize}

Enfin, pour lier les spécifications organisationnelles de $\mathcal{M}OISE^+$ aux "Constraint Guides" et aux agents, nous introduisons les relations utilitaires suivantes :
\begin{itemize}
    \item \textbf{Agent to Role} (AR) \quad $ar: \mathcal{A} \to \mathcal{R}$, la relation bijective qui relie un agent à un rôle ;
    \item \textbf{Role to Constraint Guide} (RCG) \quad $rcg: \mathcal{R} \rightarrow rag \cup rrg$, la relation qui associe chaque rôle de $\mathcal{M}OISE^+$ à une relation $rag$ ou $rrg$, forçant/incitant l'agent à suivre les actions attendu pour le role $\rho \in \mathcal{R}$ ;
    \item \textbf{Goal to Constraint Guide} (GCG) \quad $gcg: \mathcal{G} \rightarrow grg$ la relation qui lie les objectifs aux relations de trf, en représentant les objectifs sous forme de récompenses dans le MARL.
\end{itemize}

Résoudre un problème MOISE+MARL implique de trouver une politique conjointe qui maximise la fonction de valeur $V^\pi$ qui représente la récompense cumulative espérée en partant d'un état initial $s \in S$ et en suivant la politique conjointe $\pi$, en appliquant succesivement les action conjointe $a \in A^N$ sous les contraintes additonnelles des "Constraint Guides". La formalisation de la résolution est décrite dans la \hyperref[eq:value_function]{Definition 1} en adaptant la définition de la fonction de valeur pour les rôles (en rouge) et les missions (en bleu), impactant ainsi l'espace des actions et la récompense.

% \begin{gather*}
%   (1) \quad V^\pi(s_t) = \hspace{-0.4cm} \sum_{\textcolor{red}{ \substack{a_{t} \in rac(A)}
%   }}{\hspace{-0.4cm} \pi(a_{t} | \omega_t)} \sum_{s_{t+1} \in S}{T(s_{t+1} | s_t, a_{t})[R(s_t,a_{t},s_{t+1})} \\
%    + \ \hspace{-0.cm} \allowbreak
%   \textcolor{blue}{rrc(h_{t+1})} + \textcolor{red}{mrc(\omega_t, a_t)} + V^{\pi}(s_{t+1})]
% \end{gather*}

\begin{figure*}[h!]
  \label{eq:value_function}
  \raggedright
  \textbf{Definition 1 : Fonction de valeur adaptée aux "Constraint Guides" pour un agent donné $i$}
  \begin{gather*}
    \text{ \quad \quad}V^\pi(s_t) = \hspace{-0.75cm} \sum_{\textcolor{red}{ \substack{a_{t} \in A \text{ if } rn() < ch_{t}, \\ 
    a_{t} \in A_{t} \text{ else}}
    }}{\hspace{-0.7cm} \pi(a_{t} | \omega_t)} \sum_{s_{t+1} \in S}{T(s_{t+1} | s_t, a_{t})[R(s_t,a_{t},s_{t+1}) + \hspace{-0.1cm} \textcolor{blue}{ \sum_{m \in \mathcal{M}_i}{ \hspace{-0.1cm} v_m(t) \times \frac{grg_m(h_{t+1})}{1 - p + \epsilon} } } + \textcolor{red}{ch_t \times rrg(\omega_t,a_{t+1})} + V^{\pi}(s_{t+1})]}
  \end{gather*}  
  %
  \textcolor{red}{\[\text{With } rag(\omega_t) = A_{t} \times \mathbb{R} \text{, } (a_t, ch_{t}) \in A_{t} \times \mathbb{R} \text{ ; } \text{ and } rn: \emptyset \to [0,1[ \text{, a uniform random function}\]}
  %
  \textcolor{blue}{
  \begin{gather*}
  \text{With } \omega_t = O(\omega_t | s_t, a_t) \text{ ; } h_t = \{h_0 = \langle \rangle, h_{t+1} = \langle h_t, (\omega_{t+1}, a_{t+1}) \rangle \} \text{ ; } grg_m(h) = \hspace{-0.8cm} \sum_{(grg_i,w_i) \in mo(m)}{\hspace{-0.8cm} w_i \times grg_i(h)} \text{ ; } \epsilon \in \mathbb{R}_{>0} \text{ ; } \\
  v_m(t) = \{ 1 \text{ if } t \in t_c \text{ ; else } 0 \} \text{ ; } \text{ and } \mathcal{M}_i = \{m_j | (ar(i),m_j,t_c,p) \in \mathcal{M}\}
  \end{gather*}
  }

  \end{figure*}

A un instant $t \in \mathbb{N}$, on considère un agent $i$ contraint à un rôle $\rho = ar(i)$. Pour chaque spécification déontique associé valide temporellement $d_i = (\rho,m_i,t_{c_i},p_i)$ (telles que $v_{m_i}(t) = t \in t_{c_i}$), l'agent est permis (si $p_i = 0$) ou obligé (si $p_i = 1$) de s'engager à la mission $m_i \in \mathcal{M}, \mathcal{G}_{m_i} = mo(m_i), n \in \mathbb{N}$.
%
D'abord, à partir de l'observation reçue $\omega_t$, l'agent doit choisir une action soit : dans l'ensemble des actions attendues du rôle $A_t$ si une valeur aléatoire est inférieur à la dureté de contrainte de role $ch_t$; soit dans l'ensemble de toutes les actions $A$ sinon. Si $ch_t = 1$ le rôle est fortement contraint à l'agent et faiblement sinon.
%
Ensuite, l'action est appliquée dans l'état courant $s_t$ pour passer à l'état suivant $s_{t+1}$, générer l'observation suivante $\omega_{t+1}$, et générer la récompense. La récompense est la somme de la récompense globale avec les pénalités et bonus obtenus par les spécifications organisationnelles: \quad i) la somme des bonus des objectifs associés à chaque mission associé (via les "Goal Reward Guide"), valide temporellement (via $v_m(t)$), pondérés par le poids associé ($\frac{1}{1-p+\epsilon}$) ; \quad ii) la pénalité associée au rôle (via les ""Role Reward Guide") pondérée par la dureté de contrainte au role.

En considérant plusieurs agents respectivement contraints à des rôles et permis/autorisés à s'engager respectivement à une mission.


\subsection{Aperçu général}

Notre méthode, SAMMASD, s'articule autour de quatre phases principales : (1) la modélisation de l'environnement, de l'objectif et des contraintes organisationnelles selon le framework proposé, (2) l'apprentissage des politiques via plusieurs algorithmes MARL, (3) l'analyse des comportements et l'inférence de spécifications organisationnelles avec une méthode proposée, et (4) le développement et le déploiement du SMA. Cette approche permet de guider le processus d'apprentissage des agents en respectant des contraintes organisationnelles strictes, tout en garantissant l'efficacité des politiques apprises. Le cycle de vie d'un SMA conçu avec SAMMASD est illustré dans la \autoref{fig:cycle}.

% Description formelle des phases


\begin{figure*}[h!]
  \centering
  \input{figures/cycle.tex}
  \caption{Cycle de vie d'un SMA conçu avec SAMMASD.}
  \label{fig:cycle}
\end{figure*}

\subsection{Phase 1 : Modélisation}

L'étape de modélisation vise à créer un modèle simulé qui capture fidèlement les dynamiques et les contraintes de l'environnement cible, définir des spécifications organisationnelles et des objectifs. Ce modèle servira de base pour entraîner les agents dans un environnement contrôlé. Cette étape est cruciale pour s'assurer que les agents apprennent dans une simulation fidèle à la réalité du système cible dans un cadre sécurisé et facilitant la recherche d'une solution. % Le processus de cette phase est décrit dans \autoref{alg:modeling}.

Cette phase prend les éléments suivants en entrée :
\begin{itemize}
    \item \textbf{Environnement} : Une copie "émulée" de l'environnement réel ou l'environment cible lui-même si cela est possible.
    \item \textbf{Description du problème} : La description détaillée des objectifs à atteindre pour les agents, c'est à dire les états recherchés.
    \item \textbf{Contraintes additionnelles} : Des exigences spécifiques à respecter, pouvant inclure des normes, règles d'organisation, ou contraintes de sûreté.
\end{itemize}

Une fois les élements collectés, la modélisation suit les étapes suivantes : 


\paragraph{\textbf{1) Modélisation de l'environnement simulé}} \quad

\noindent Nous définissons la modélisation de l'environment comme l'élaboration d'une fonction d'observation approximée $\hat{O}: S \times A \to \Omega, \hat{O}(s_t,a_t) = \omega_{t+1}$ telle que $|\hat{O} \cap O| \geq f$, où $f \in \mathbb{R}$ est la fidélité à l'environment réel décrit par $O$.
Il s'agit pour le concepteur de reproduire fidèlement la logique de l'environnement qui amène l'agent à recevoir des observations à chaque transition d'état. 

Le tableau \ref{tab:comparaison-methodes} présente une comparaison des méthodes d'Identification de Systèmes, d'Apprentissage par Imitation, de Modélisation Approximative, et de Jumeau Numérique en fonction des critères mentionnés.

\begin{table*}[h!]
\centering
\caption{Comparaison des méthodes de modélisation automatisée selon différents critères}
\begin{tabular}{|p{5cm}|c|c|c|c|}
\hline
\textbf{Critères}                   & \textbf{System Identification} & \textbf{Imitation Learning} & \textbf{Surrogate Modeling} & \textbf{Digital Twin} \\
\hline
Niveau d'Intervention Manuelle (NIM)    & Moyen                         & Bas                          & Moyen                       & Haut                 \\
Niveau de Modélisation Mathématique (NMM) & Élevé                         & Faible                       & Élevé                       & Moyen                \\
Applicabilité Générale (AG)             & Moyenne                       & Élevée                       & Moyenne                     & Moyenne              \\
Requiert une Version Émulée (RVE)       & Non                           & Non                          & Non                         & Oui                  \\
Mise à Jour Automatique (MUA)           & Moyenne                       & Élevée                       & Moyenne                     & Élevée               \\
Automatisation Complète (AC)            & Moyenne                       & Moyenne                      & Moyenne                     & Variable             \\
Adaptabilité en Temps Réel (ATR)        & Faible                        & Faible                       & Moyenne                     & Élevée               \\
Niveau de Fidélité au Système Réel (NFS) & Moyen                         & Bas                          & Moyen                       & Élevé                \\
Exigences en Données (ED)               & Élevées                       & Moyennes                     & Élevées                     & Élevées              \\
Coût Computationnel (CC)                & Élevé                         & Moyen                        & Moyen                       & Élevé                \\
\hline
\end{tabular}
\label{tab:comparaison-methodes}
\end{table*}

Chacune de ces méthodes se distingue par son approche et son niveau de fidélité par rapport au système cible. L'Identification de Systèmes s'appuie sur une modélisation mathématique précise qui repose sur l'observation et l'ajustement de paramètres en fonction des données collectées. Cela implique généralement une intervention humaine significative, notamment pour les environnements où les dynamiques sont connues et représentables par des équations. En revanche, l'Apprentissage par Imitation ne requiert pas nécessairement de modèle mathématique, mais plutôt une observation du comportement pour que le modèle reproduise celui-ci. Cette méthode est efficace pour capturer des comportements complexes, mais peut être limitée en précision et nécessiter de nombreux exemples pour générer des résultats robustes.

D'autre part, la Modélisation Approximative et les Jumeaux Numériques sont des méthodes adaptées à des contextes où la complexité du système ou les exigences de mise à jour automatique sont élevées. La Modélisation Approximative génère des représentations simplifiées à partir de techniques statistiques ou de l'apprentissage automatique, permettant d'optimiser des systèmes complexes à moindre coût de calcul. Bien qu'elle offre une grande flexibilité, elle peut être moins précise qu'un modèle fondé sur des équations exactes. Le Jumeau Numérique, quant à lui, crée une réplique numérique du système réel, synchronisée en temps réel, permettant des mises à jour continues et une fidélité accrue au système cible. Cependant, la création d'un jumeau numérique est plus coûteuse en termes de collecte de données et de ressources, et nécessite souvent une version émulée du système cible. En somme, le choix de la méthode doit prendre en compte les besoins en termes de précision, de flexibilité, et de capacité d'automatisation, afin de sélectionner la solution la plus adaptée à l'application visée.

Dans notre méthode, nous favorisons les techniques de "Imitation Learning" car beaucoup de ces techniques ne requierent pas d'intervention humaine, sont applicables à une majorité d'environnements tout en permettant de capturer la complexité de l'environment après un entrainement suffisant. Bien que ces techniques peuvent présenter un manque de lisibilité en particulier pour ceux basés sur des architectures de réseaux de neuronne, nous ne considérons que la fidélité comme critère principal de notre méthode.

Dans cette optique, la méthode suggère de procéder comme suit: des agents observateurs (possiblement humains) sont déployés pour collecter des traces de l'environnement réel ou d'une copie sécurisée. Ces traces sont rassemblées et utilisées pour entrainer un modèle basé sur un Recurrent Neural Networks (RNN). Cette architecture est particulierement adapté pour prédire la prochaine observation en étant optimisé pour apprendre à partir de séquence.

\paragraph{\textbf{2) Formulation de la fonction de récompense}} \quad

\noindent Nous définissons la formulation de la fonction de récompense comme : \quad i) la recherche d'une description claire des différents états recherchés et sa description comme des (sous-)trajectoires plus généralement afin de caractériser au mieux l'objectif global ; \quad ii) la recherche d'un moyen de mesurer seulement la distance entre l'état courant et ces trajectoires voulus. L'utilisateur doit donc mettre en place la logique d'une fonction de récompense qu'il estime être la plus adaptée pour atteindre les objectifs fixés.

La recherche d'une description claire des états recherchés n'est pas toujours trivial en fonction des environnements. Bien que consideré comme admise dans notre méthode, cette recherche peut être appuyée par des techniques d'Inverse Reinforcement Learning (IRL) consistant à s'inspirer des comportements observés des agents dans un environment réel pour en détérminer l'objectif final. D'autre part, nous préconisons la vision d'un objectif comme indépendant des agents. C'est pourquoi l'objectif ne doit pas chercher à influencer directement les agents afin qu'ils adoptent un comportement attendu (il s'agit plutôt de la vision d'un rôle). Une fois établies, ces différents états voulus peuvent être présentés comme un ensemble de (sous-)trajectoires.

Concernant la recherche d'un moyen de mesurer la distance entre l'état courant et l'ensemble des trajctoires voulus, nous suggérons d'utiliser la séquence commune la plus longue entre la (sous-)trajectoire caractérisant l'objectif et l'historique courant des agents. Cette mesure a l'avantage d'être simple et pertinente lorsque la séquence associée à la trajectoire d'un objectif est continue (ce qui est le cas dans nos experimentations). Néanmoins, elle moins facilement applicable lorsque la séquence associée à l'objectif est discontinue. D'autres mesures issues des travaux sur les séries temporelles sont également envisageables.

\paragraph{\textbf{3) Formulation des spécifications $mathcal{M}OISE^+$MARL}} \quad

\noindent Nous définissons la formulation des spécifications organizationnelles comme l'établissement des relation : \quad i) $ar$, liant les agents à des rôles ; \quad ii) $rcg$

Les contraintes organisationnelles et les sous-objectifs sont formalisés en utilisant le framework $mathcal{M}OISE^+$MARL. Les rôles et les missions sont définis pour guider les agents dans leur apprentissage.

\

Au terme de cette phase, la sortie comprend les éléments suivants:
\begin{itemize}
    \item \textbf{Un modèle de l'environnement} sous forme d'une fonction d'approximation de la fonction d'observation ;
    \item \textbf{Une fonction de récompense} indiquant objectivement aux agents à quel point ils se trouvent proche ou éloigné de l'objectif global ;
    \item \textbf{Un ensemble de spécifications organisationnelles} décrites comme 
\end{itemize}

\subsection{Phase 2 : Résolution}
Dans cette phase, nous utilisons des algorithmes \textbf{MARL} pour résoudre le problème formalisé dans l'environnement simulé. Les algorithmes comme \textbf{MADDPG} et \textbf{MAPPO} sont utilisés pour apprendre des politiques qui respectent à la fois les objectifs de performance et les contraintes organisationnelles définies par \textbf{$mathcal{M}OISE^+$}. L'intégration de \textbf{$mathcal{M}OISE^+$} dans le processus d'apprentissage permet de restreindre l'espace de recherche des politiques, rendant ainsi le processus plus efficace et garantissant la sûreté du système.

\begin{itemize}
  \item \textbf{Entrées :} Problème formalisé, contraintes organisationnelles.
  \item \textbf{Sorties :} Politiques conjointes optimales pour les agents.
  \item \textbf{Processus :} Utilisation de \textbf{MARL} pour apprendre des politiques tout en respectant les rôles et missions prédéfinis.
\end{itemize}

\subsection{Phase 3 : Analyse}
Une fois les politiques apprises, elles sont analysées à l'aide de \textbf{HEMM}, un algorithme qui infère les rôles et les missions des agents en se basant sur leurs comportements observés. L'analyse permet d'identifier les écarts entre les rôles inférés et les rôles prédéfinis dans \textbf{$mathcal{M}OISE^+$}, offrant ainsi une évaluation de l'adéquation organisationnelle du SMA. Les résultats de cette analyse sont ensuite traduits en plans détaillés, qui seront utilisés pour le développement final du système.

\begin{itemize}
  \item \textbf{Entrées :} Politiques conjointes des agents.
  \item \textbf{Sorties :} Rôles et missions inférés, plan détaillé des comportements.
  \item \textbf{Processus :} Utilisation de \textbf{HEMM} pour analyser les politiques apprises et générer des plans interprétables.
\end{itemize}

\subsection{Phase 4 : Développement et déploiement}
Dans la phase finale, les plans générés sont utilisés pour développer et déployer le SMA dans l'environnement réel. Nous utilisons l'outil \textbf{CybMASDE} pour faciliter l'intégration des politiques apprises dans les agents du système. Avant le déploiement réel, des tests sont effectués dans un environnement émulé pour vérifier que le SMA respecte bien les contraintes de sûreté et les objectifs de performance. Cette phase garantit que les agents fonctionnent comme prévu dans l'environnement réel tout en respectant les rôles et missions spécifiés.

\section{Évaluation}
\label{sec:evaluation}

\subsection{CybMASDE : un environnement de développement pour l'approche}
Pour évaluer notre méthode, nous avons développé un outil nommé \textbf{CybMASDE} qui soutient chaque phase du processus de conception. \textbf{CybMASDE} propose une interface pour modéliser des environnements complexes, définir des objectifs, et spécifier des contraintes organisationnelles. Il intègre également des algorithmes \textbf{MARL} pour l'apprentissage des politiques et des outils d'analyse pour l'inférence des rôles. CybMASDE permet de visualiser les rôles inférés et de faciliter le déploiement des SMA dans des environnements réels après une validation en simulation.

\subsection{Résultats expérimentaux}
Nous avons appliqué SAMMASD à un scénario de gestion de flux de paquets dans un entrepôt, où des robots doivent collaborer pour organiser les flux de marchandises tout en respectant des contraintes de sécurité et d'efficacité. Les résultats montrent que la méthode permet de générer des politiques conjointes efficaces, avec une réduction significative du temps de conception et une amélioration de la robustesse des systèmes déployés.

\begin{itemize}
  \item \textbf{Environnement :} Gestion de flux dans un entrepôt simulé.
  \item \textbf{Objectifs :} Optimiser les flux de marchandises tout en respectant les contraintes de sûreté.
  \item \textbf{Métriques :} Temps de conception, robustesse des politiques, respect des contraintes organisationnelles.
\end{itemize}

Les résultats montrent que les agents entraînés via \textbf{$mathcal{M}OISE^+$MARL} respectent les contraintes organisationnelles définies et optimisent les objectifs de performance, tout en générant des comportements interprétables grâce à l'analyse par \textbf{HEMM}.

\section{Conclusion}
\label{sec:conclusion}
Cet article a présenté une nouvelle méthode, \textbf{SAMMASD}, qui permet de concevoir et déployer des systèmes multi-agents en combinant des spécifications organisationnelles avec des techniques d'apprentissage par renforcement. La méthode s'articule autour de quatre phases clés, allant de la modélisation de l'environnement à l'analyse des politiques apprises et au déploiement des systèmes réels. Les résultats expérimentaux montrent que cette approche réduit le temps de conception et améliore la robustesse des SMA tout en garantissant l'explicabilité des comportements des agents.

Les travaux futurs porteront sur l'extension de \textbf{SAMMASD} à des environnements encore plus complexes, notamment dans des contextes critiques tels que la cybersécurité, et sur l'intégration de nouveaux algorithmes d'apprentissage pour améliorer l'efficacité des politiques apprises.

\section*{References}

\bibliographystyle{ACM-Reference-Format}
\renewcommand
\refname{}
\bibliography{references}

\end{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
