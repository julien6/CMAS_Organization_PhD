%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% LaTeX Template for AAMAS-2025 (based on sample-sigconf.tex)
%%% Prepared by the AAMAS-2025 Program Chairs based on the version from AAMAS-2025. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% Start your document with the \documentclass command.


%%% == IMPORTANT ==
%%% Use the first variant below for the final paper (including auithor information).
%%% Use the second variant below to anonymize your submission (no authoir information shown).
%%% For further information on anonymity and double-blind reviewing, 
%%% please consult the call for paper information
%%% https://aamas2025.org/index.php/conference/calls/submission-instructions-main-technical-track/

%%%% For anonymized submission, use this
\documentclass[sigconf,anonymous]{aamas} 

%%%% For camera-ready, use this
%\documentclass[sigconf]{aamas}

\usepackage{listings}
% \usepackage{xcolor}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}
 
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}
 
\lstset{style=mystyle}

% --- Tickz
\usepackage{physics}
\usepackage{tikz}
\usepackage{amsmath}
\usepackage{mathdots}
% \usepackage{yhmath}
\usepackage{cancel}
\usepackage{color}
\usepackage{siunitx}
\usepackage{array}
\usepackage{multirow}
% \usepackage{amssymb}
\usepackage{gensymb}
\usepackage{tabularx}
\usepackage{extarrows}
\usepackage{booktabs}
\usetikzlibrary{fadings}
\usetikzlibrary{patterns}
\usetikzlibrary{shadows.blur}
\usetikzlibrary{shapes}

% ---------

\usepackage{balance} % for balancing columns on the final page
\usepackage{csquotes}
% \usepackage{cite}
\newcommand{\probP}{I\kern-0.15em P}
\usepackage{etoolbox}
\patchcmd{\thebibliography}{\section*{\refname}}{}{}{}
% \usepackage{amsthm,amssymb,amsfonts}

\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{color}
% \renewcommand\UrlFont{\color{blue}\rmfamily}

\usepackage[inline, shortlabels]{enumitem}
\usepackage{tabularx}
\usepackage{caption}
\usepackage{listings}
\usepackage{titlesec}
\usepackage{ragged2e}
% \usepackage[hyphens]{url}
\usepackage[linesnumbered,ruled,vlined]{algorithm2e}
\usepackage{float}

\usepackage[english]{babel}
\addto\extrasenglish{  
    \def\figureautorefname{Figure}
    \def\tableautorefname{Table}
    \def\algorithmautorefname{Algorithm}
    \def\sectionautorefname{Section}
    \def\subsectionautorefname{Subsection}
    \def\proofoutlineautorefname{Proof Outline}
}

\newcommand{\supertiny}{\fontsize{1}{2}\selectfont}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% AAMAS-2025 copyright block (do not change!)

\setcopyright{ifaamas}
\acmConference[AAMAS '25]{Proc.\@ of the 24th International Conference
on Autonomous Agents and Multiagent Systems (AAMAS 2025)}{May 19 -- 23, 2025}
{Detroit, Michigan, USA}{A.~El~Fallah~Seghrouchni, Y.~Vorobeychik, S.~Das, A.~Nowe (eds.)}
\copyrightyear{2025}
\acmYear{2025}
\acmDOI{}
\acmPrice{}
\acmISBN{}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% == IMPORTANT ==
%%% Use this command to specify your EasyChair submission number.
%%% In anonymous mode, it will be printed on the first page.

\acmSubmissionID{<<EasyChair submission id>>}

%%% Use this command to specify the title of your paper.

\title[AAMAS-2025 CybMASDE]{A Method for Assisting Multi-Agent System Design Using Reinforcement Learning}

%%% Provide names, affiliations, and email addresses for all authors.

\author{Julien Soulé}
\affiliation{
  \institution{Univ. Grenoble Alpes}
  \city{Valence}
  \country{France}}
\email{julien.soule@lcis.grenoble-inp.fr}

\author{Jean-Paul Jamont}
\affiliation{
  \institution{Univ. Grenoble Alpes}
  \city{Valence}
  \country{France}}
\email{jean-paul.jamont@lcis.grenoble-inp.fr}

\author{Michel Occello}
\affiliation{
  \institution{Univ. Grenoble Alpes}
  \city{Valence}
  \country{France}}
\email{michel.occello@lcis.grenoble-inp.fr}

\author{Louis-Marie Traonouez}
\affiliation{
  \institution{Thales Land and Air Systems, BU IAS}
  \city{Rennes}
  \country{France}}
\email{louis-marie.traonouez@thalesgroup.com}

\author{Paul Théron}
\affiliation{
  \institution{AICA IWG}
  \city{La Guillermie}
  \country{France}}
\email{paul.theron@orange.fr}

\begin{abstract}
    Agent-Oriented Software Engineering (AOSE) methods typically rely on the designer's expertise to guide the development of a multi-agent system (MAS) that meets specific objectives within a given environment. Recent advancements in Multi-Agent Reinforcement Learning (MARL) suggest a more automated approach to exploring the design space. We introduce the SAMMASD (Semi-Automated MARL-based MAS Design) method, which integrates organizational specifications into MARL for MAS design and deployment. This method consists of four phases. The first phase models the real-world environment, objectives, and additional constraints—such as operational requirements—into a simulation. The second phase leverages multiple MARL algorithms to learn stable policies that achieve these objectives within the defined constraints. The third phase conducts a behavior analysis to infer emergent roles and objectives and to generate detailed "blueprints" for implementation. Finally, the development phase enables the automatic deployment of these policies in real-world environments, following validation in an emulated setting. We demonstrate SAMMASD in a warehouse flow management scenario involving robot agents, illustrating how the method can produce efficient and reliable MAS designs that streamline the design process.
\end{abstract}



%%% The code below was generated by the tool at http://dl.acm.org/ccs.cfm.
%%% Please replace this example with code appropriate for your own paper.


%%% Use this command to specify a few keywords describing your work.
%%% Keywords should be separated by commas.

\keywords{Agent-Oriented Software Engineering \and Multi-Agent Reinforcement Learning \and Assisted-Design \and Organizational Models}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% Include any author-defined commands here.
         
% \newcommand{\BibTeX}{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em\TeX}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

%%% The following commands remove the headers in your paper. For final 
%%% papers, these will be inserted during the pagination process.

\pagestyle{fancy}
\fancyhead{}

%%% The next command prints the information defined in the preamble.

\maketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}

Les systèmes multi-agents (SMA) sont au cœur de nombreuses applications, allant de la gestion d'entrepôts automatisés à la cybersécurité. Ces systèmes doivent permettre à plusieurs agents autonomes d'agir de manière coordonnée afin d'atteindre des objectifs communs dans des environnements complexes et dynamiques. Cependant, la conception de tels systèmes présente des défis majeurs en termes de robustesse, de sûreté et d'explicabilité des comportements des agents.

Les méthodes actuelles d'apprentissage par renforcement multi-agent (MARL) ont montré leur efficacité pour générer des politiques optimales dans des environnements dynamiques. Toutefois, ces approches manquent souvent de mécanismes explicites pour imposer des contraintes organisationnelles et garantir que les agents respectent des normes de sûreté essentielles pour des environnements critiques, comme la logistique en entrepôts ou la défense cybernétique.

Cet article présente une nouvelle méthode, \textbf{SAMMASD} (Semi-Automated MARL-based MAS Design), qui combine le framework \textbf{$mathcal{M}OISE^+$MARL} avec l'algorithme d'évaluation \textbf{HEMM} pour inférer et appliquer des structures organisationnelles aux politiques apprises par les agents. L'intégration de \textbf{$mathcal{M}OISE^+$}, un modèle organisationnel orienté multi-agents, permet de contraindre l'apprentissage des agents en définissant des rôles et des missions, ce qui favorise l'émergence de comportements interprétables et conformes aux exigences de sûreté.

Le reste de cet article est structuré comme suit : la \autoref{sec:related_works} présente les travaux connexes dans le domaine du MARL et des modèles organisationnels pour SMA. La \autoref{sec:apprragh} décrit en détail les quatre phases de la méthode SAMMASD. Ensuite, la \autoref{sec:evaluation} discute des résultats expérimentaux obtenus dans un scénario de gestion de flux d'entrepôt. Enfin, la \autoref{sec:conclusion} conclut et propose des pistes de travail futures.

\section{Travaux connexes}
\label{sec:related_works}
La conception de systèmes multi-agents (SMA) assistée par des modèles organisationnels a été largement étudiée dans la littérature. En particulier, le modèle \textbf{$mathcal{M}OISE^+$}, introduit par Gleizes et al. \cite{gleize2008moise}, propose une structure formelle pour spécifier les rôles, les missions, et les relations entre agents dans un SMA. Ces spécifications permettent de guider le développement de systèmes complexes en assurant une cohérence entre les objectifs collectifs et les comportements individuels des agents. Cependant, $mathcal{M}OISE^+$ ne fournit pas de mécanismes pour intégrer directement l'apprentissage des politiques par renforcement, une lacune que notre travail cherche à combler.

D'un autre côté, les méthodes d'apprentissage par renforcement multi-agent (\textbf{MARL}) ont connu un essor ces dernières années, avec des approches telles que \textbf{MADDPG} (Multi-Agent Deep Deterministic Policy Gradient) \cite{lowe2017multi} et \textbf{Q-Mix} \cite{rashid2018qmix}. Ces algorithmes permettent aux agents d'apprendre des politiques optimales en collaboration ou en compétition dans des environnements partiellement observables. Néanmoins, ces méthodes se concentrent principalement sur la performance des politiques sans se soucier des contraintes organisationnelles ou des besoins en explicabilité, ce qui limite leur applicabilité dans des contextes nécessitant une coordination stricte entre agents.

Des travaux récents ont exploré des méthodologies agiles pour le développement de SMA, en combinant des approches orientées agent avec des pratiques agiles \cite{winikoff2021agile}. Ces méthodes visent à faciliter l'itération rapide dans la conception de SMA, mais elles n'intègrent pas directement les dynamiques d'apprentissage en MARL. Notre approche s'inscrit dans cette lignée en combinant l'apprentissage automatisé via MARL avec des spécifications organisationnelles, comblant ainsi le fossé entre les processus d'apprentissage et les exigences de conception organisationnelle.

\section{Approche de conception organisationnelle proposée pour les SMA}
\label{sec:apprragh}

\begin{figure*}[h!]
    \centering
    \input{figures/cycle.tex}
    \caption{Cycle de vie d'un SMA conçu avec SAMMASD.}
    \label{fig:cycle}
  \end{figure*}

\subsection{Aperçu général}

Le coeur de la méthode SAMMASD est de considerer la conception d'un SMA comme un problème d'optimisation sous contraintes. La variable à optimiser est alors la politique conjointe pour maximiser la récompense cumulée sous des contraintes additionnelles données. SAMMASD inclue des spécifications organisationnelle en MARL comme contraintes additionnelles permettant à l'utilisateur de controler au moins partiellement non-seulement des comportements individuels des agents, mais aussi de pouvoir gérer l'ensemble des agents en jouant sur celles-ci.

L'objectif est alors de guider l'apprentissage des agents en respectant des contraintes organisationnelles sous la forme de role et d'objectifs, tout en garantissant une efficacité suffisante des politiques apprises dans leur environment.
Nous formalisons cette idée en proposant le framework MOISE+MARL qui combine le formalisme d'un Decentralized Partially Observable Markov Decision Process (Dec-POMDP) avec le modèle organisationnel $\mathcal{M}OISE^+$.

Un \textbf{Dec-POMDP} est formalisé par un tuple $D = \langle N, S, A, P, R, \Omega, O, \gamma \rangle$, où :
\begin{itemize}
    \item $\mathcal{A}$, l'ensemble des $N \in \mathbb{N}$ agents;
    \item $S$, l'ensemble des états possibles de l'environment (agents compris);
    \item $A = \times_{i=1}^N A_i $, l'ensemble des actions possibles pour chaque agent $i$;
    \item $T: S \times A^N \to S$, la fonction de transition d'états qui définit le prochain état $s'$ à partir d'un état $s$ sous l'action conjointe $a$;
    \item $R: S \times A^N \times  S \to \mathbb{R}$, la fonction de récompense associant une récompense $r$ à chaque transition $(s, a, s')$;
    \item $\Omega = \times_{i=1}^N \Omega_i $ est l'ensemble des observations possibles pour chaque agent $i$;
    \item $O: S \times A \to \Omega$ est la fonction d'observation qui définit la prochaine observation $\omega$ d'un agent lorsque ce dernier applique une action $a$ dans l'état $s$;
    \item $\gamma \in [0, 1] $ est le facteur d'actualisation.
\end{itemize}

Le modèle $\mathcal{M}OISE^+$ est formalisé par un tuple $\mathcal{OS} = \langle \mathcal{SS}, \mathcal{FS}, \mathcal{DS} \rangle$. Ici, nous présentons une version minimale du formalisme de $\mathcal{M}OISE^+$ :
\begin{itemize}
    \item $\mathcal{SS} = \langle \mathcal{R} \rangle$, les spécifications structurelles sont définies par un ensemble de rôles $\mathcal{R}$;
    \item $\mathcal{FS} = \langle \mathcal{G}, \mathcal{M}, mo \rangle$, les spécifications fonctionnelles sont un ensemble d'objectifs $\mathcal{G}$, un ensemble de mission $\mathcal{M}$ et une fonction $mo$ qui associe une mission à un ensemble d'objectifs ;
    \item $\mathcal{DS} = \langle \mathcal{OBL}, \mathcal{PER} \rangle$, les spécifications déontiques constitué des permissions $\mathcal{PER}$ et obligations $\mathcal{OBL}$ comme deux ensemble de 3-uplet $(\rho_a, m, tc)$, signifiant respectivement qu'un agent jouant le rôle $\rho_a \in \mathcal{R}$ est autorisé ou obligé à s'engager dans la mission $m \in \mathcal{M}$ pour une contrainte temporelle donnée $tc \in \mathcal{TC}$. Une contrainte temporelle $tc \in \mathcal{TC}$ spécifie un ensemble de périodes pendant lesquelles une permission ou obligation est valide.% ($Any \in \mathcal{TC}$ signifie \textquote{tout le temps}).
\end{itemize}

Nous introduisons trois nouvelles relations designées comme "Constraint Guides" pour déterminer la logique des rôles et objectifs $\mathcal{\mathcal{M}OISE^+}$ dans le formalisme Dec-POMDP:

\begin{itemize}
    \item \textbf{Role Action Guide} (RAG) \quad $rag: \Omega \rightarrow \mathcal{P}(A \times \mathbb{R})$, la relation qui permet de modéliser un rôle comme un ensemble de règles qui à chaque observation $\omega \in \Omega$ reçu par l'agent associe des actions attendues $A \in \mathcal{P}(A)$ chacune associée à une dureté de contrainte $ch \in [0,1]$ ($ch = 0$ par défaut). En restreignant le choix de la prochaine action à celles autorisées, l'agent est forcé d'adherer au comportement attendu du rôle ;
    \item \textbf{Role Reward Guide} (RRG) \quad $rrg: \Omega \times A \to \mathbb{R} = \{r_m \text{ if } a \in A, a \notin A_\omega, \ \omega \in \Omega, \ rag(\omega) = A_\omega \times \mathbb{R_\omega}; \text{ else } 0\}$, la relation qui permet de modéliser un rôle en ajoutant un malus $r_m$ à la récompense globale si l'action choisie par l'agent $a \in A$ n'est pas autorisée pour inciter l'agent à adherer au comportement attendu d'un role ;
    \item \textbf{Goal Reward Guide} (GRG) \quad $grg: H \rightarrow \mathbb{R}$, la relation qui permet de modéliser un objectif comme une contrainte molle en ajoutant un bonus $r_b \in \mathbb{R}$ à la récompense globale si la trajectoire de l'agent $h \in H$ contient une sous-séquence $h_g \in H_g$ caractéristique de l'objectif de sorte à inciter l'agent à trouver un moyen pour l'atteindre.
\end{itemize}

Enfin, pour lier les spécifications organisationnelles de $\mathcal{M}OISE^+$ aux "Constraint Guides" et aux agents, nous introduisons les relations suivantes :
\begin{itemize}
    \item \textbf{Deontic specifications to Agents} (DA) \quad $da: \mathcal{D} \rightarrow \mathcal{P}(\mathcal{A})$, la relation qui relie les spécifications déontiques aux agents, permettant de définir les permissions et obligations selon les rôles et missions.
    \item \textbf{Role to Constraint Guide} (RCG) \quad $rcg: \mathcal{R} \rightarrow rag \cup rrg$, la relation qui associe chaque rôle de $\mathcal{M}OISE^+$ à une relation $rag$ ou $rrg$, forçant/incitant l'agent à suivre les actions attendu pour le role $\rho \in \mathcal{R}$.
    \item \textbf{Goal to Constraint Guide} (GCG) \quad $gcg: \mathcal{G} \rightarrow grg$ la relation qui lie les objectifs aux relations de trf, en représentant les objectifs sous forme de récompenses dans le MARL.
\end{itemize}

Résoudre un problème MOISE+MARL implique de trouver une politique conjointe qui maximise la fonction de valeur $V_\pi$ qui représente la récompense cumulative espérée en partant d'un état initial $s \in S$ et en suivant la politique conjointe $\pi$, en appliquant succesivement les action conjointe $a \in A^N$ sous les contraintes additonnelles des "Constraint Guides" impactant l'espace des actions et la récompense :

\begin{figure*}
\[
  \sum_{\substack{
    a_{t+1} \in A_{t+1} \text{ if } rn() \leq ch_{t+1}, \\ 
    a_{t+1} \in A \text{ else, } \\
    rag(\omega_t) = A_{t+1} \times \mathbb{R} = ach \\
    ach(a_{t+1}) = ch_{t+1}
}}{\pi(a_{t+1} | \omega_t)} \sum_{s_{t+1} \in S}{T(s_{t+1} | s_t, a_{t+1}) [R(s_t,a_{t+1},s_{t+1}) + gcg(h_{t+1}) + (1-ch) \times rrg(\omega_t,a_{t+1})]}
\]
\end{figure*}

Avec $rn: \emptyset \to [0,1]$, une fonction random uniforme

\[
  \pi^* = argmax_{\pi}(V^\pi(s_0)), \ \forall s_0 \in S_0 \subseteq S
\]

Notre méthode, SAMMASD, s'articule autour de quatre phases principales : (1) la modélisation de l'environnement, de l'objectif et des contraintes organisationnelles selon le framework proposé, (2) l'apprentissage des politiques via plusieurs algorithmes MARL, (3) l'analyse des comportements et l'inférence de spécifications organisationnelles avec une méthode proposée, et (4) le développement et le déploiement du SMA. Cette approche permet de guider le processus d'apprentissage des agents en respectant des contraintes organisationnelles strictes, tout en garantissant l'efficacité des politiques apprises. Le cycle de vie d'un SMA conçu avec SAMMASD est illustré dans la \autoref{fig:cycle}.

% Description formelle des phases

\subsection{Phase 1 : Modélisation}

L'étape de modélisation vise à créer un modèle simulé qui capture fidèlement les dynamiques et les contraintes de l'environnement cible, tout en incluant des spécifications organisationnelles et des objectifs. Ce modèle servira de base pour entraîner les agents dans un environnement contrôlé. Cette étape est cruciale pour s'assurer que les agents apprennent dans une simulation fidèle à la réalité du système cible dans un cadre sécurisé et facilitant la recherche d'une solution. Le processus de cette phase est décrit dans \autoref{alg:modeling}.

Cette phase prend les éléments suivants en entrée :
\begin{itemize}
    \item \textbf{Environnement} : Une copie "émulée" de l'environnement réel ou l'environment cible lui-même si cela est possible.
    \item \textbf{Description du problème} : La description détaillée des objectifs à atteindre pour les agents, c'est à dire les états recherchés.
    \item \textbf{Contraintes additionnelles} : Des exigences spécifiques à respecter, pouvant inclure des normes, règles d'organisation, ou contraintes de sûreté.
\end{itemize}

Une fois les élements collectés, la modélisation suit les étapes suivantes :

\subsubsection{Modélisation de l'environnement simulé}
  
Des agents observateurs sont déployés pour collecter des traces de l'environnement réel ou de sa copie sécurisée. Ces traces sont utilisées pour générer un modèle simulé de l'environnement en utilisant une technique de \textit{System Identification}.

\subsubsection{Formulation de la fonction de récompense}

L'utilisateur doit mettre en place la logique d'une fonction de récompense qu'il estime être la plus adaptée pour atteindre les objectifs fixés. 

\subsubsection{Formulation des spécifications $mathcal{M}OISE^+$MARL}

Les contraintes organisationnelles et les sous-objectifs sont formalisés en utilisant le framework $mathcal{M}OISE^+$MARL. Les rôles et les missions sont définis pour guider les agents dans leur apprentissage.




\RestyleAlgo{ruled}
\SetKwComment{Comment}{// }{}

\begin{algorithm}[hbt!]
  \caption{Étape de Modélisation pour SAMMASD}\label{alg:modeling}

  \SetAlgoLined
\KwIn{%
    $real\_env$ : Environnement réel avec états et transitions,\\
    $problem\_desc$ : Description du problème avec fonction de récompense,\\
    $additional\_constraints$ : Contraintes organisationnelles et de sûreté (trf, rag)
}
\KwOut{%
    $model$ : Un modèle $mathcal{M}OISE^+$MARL avec l'environnement simulé, l'objectif, et les contraintes
}

% Étape 1: Créer ou utiliser un modèle simulé
\If{SimulatedModelExists($real\_env$)}{
    $simulated\_env \gets$ GetExistingSimulatedModel($real\_env$)\;
}
\Else{
    $safe\_env\_copy \gets$ CreateSafeCopy($real\_env$)\;
    $traces \gets$ CollectTraces($safe\_env\_copy$)\;
    $simulated\_env \gets$ GenerateSimulatedModel($traces$)\;
}

% Étape 2: Ajouter l'objectif et les contraintes
$simulated\_env$.AddObjective($problem\_desc.objective\_function$)\;
\ForEach{$constraint$ in $additional\_constraints$}{
    \eIf{$constraint.Type$ == "Soft"}{
        ApplyRewardShaping($simulated\_env$, $constraint$)\;
    }{
        ApplyShielding($simulated\_env$, $constraint$)\;
    }
}

% Étape 3: Incorporer les spécifications $mathcal{M}OISE^+$MARL
$rag \gets$ Generaterag($simulated\_env$, $problem\_desc.roles$)\;
$trf \gets$ Generatetrf($simulated\_env$, $problem\_desc.missions$)\;
$model \gets$ MOISE\_MARL\_Model($simulated\_env$, $rag$, $trf$)\;

\Return{$model$}

\end{algorithm}


Ce processus établit un environnement d'apprentissage sécurisé et guidé pour les agents, en s'assurant qu'ils respectent les contraintes définies tout en poursuivant leurs objectifs de manière efficace.




\subsubsection{Comparaison des Méthodes de Modélisation Automatisée}

Dans cette sous-section, nous présentons une analyse des différentes méthodes permettant de modéliser un environnement cible de manière fidèle et automatisée. Les méthodes considérées sont l'Identification de Systèmes, l'Apprentissage par Imitation, la Modélisation Approximative (Surrogate Modeling), et le Jumeau Numérique (Digital Twin). Nous utilisons des critères spécifiques pour évaluer ces méthodes en fonction de leur adéquation aux besoins de la modélisation automatisée.

\paragraph{Critères d'Évaluation}

\begin{itemize}
    \item \textbf{Niveau d'Intervention Manuelle (NIM)} : La méthode nécessite-t-elle que l'utilisateur développe ou configure manuellement une partie du modèle simulé ?
    \item \textbf{Niveau de Modélisation Mathématique (NMM)} : La méthode inclut-elle des modèles mathématiques pour représenter l'environnement ?
    \item \textbf{Applicabilité Générale (AG)} : La méthode peut-elle modéliser une large variété d'environnements sans restrictions spécifiques ?
    \item \textbf{Requiert une Version Émulée (RVE)} : La méthode nécessite-t-elle une copie émulée de l'environnement cible pour générer ou valider le modèle ?
    \item \textbf{Mise à Jour Automatique (MUA)} : La méthode permet-elle de mettre à jour le modèle automatiquement sans intervention humaine ?
    \item \textbf{Automatisation Complète (AC)} : La méthode offre-t-elle une procédure entièrement automatisée pour construire et gérer le modèle ?
    \item \textbf{Adaptabilité en Temps Réel (ATR)} : La méthode peut-elle s'adapter en temps réel aux changements de l'environnement cible ?
    \item \textbf{Niveau de Fidélité au Système Réel (NFS)} : La méthode fournit-elle un modèle avec un haut niveau de fidélité par rapport au système cible ?
    \item \textbf{Exigences en Données (ED)} : Quel est le volume et le type de données requis pour construire et maintenir le modèle ?
    \item \textbf{Coût Computationnel (CC)} : Quelles sont les exigences en termes de ressources de calcul pour générer et mettre à jour le modèle ?
\end{itemize}

\paragraph{Comparaison des Méthodes}

Le tableau \ref{tab:comparaison-methodes} présente une comparaison des méthodes d'Identification de Systèmes, d'Apprentissage par Imitation, de Modélisation Approximative, et de Jumeau Numérique en fonction des critères mentionnés.


\begin{table*}[h!]
\centering
\caption{Comparaison des méthodes de modélisation automatisée selon différents critères}
\begin{tabular}{|p{5cm}|c|c|c|c|}
\hline
\textbf{Critères}                   & \textbf{System Identification} & \textbf{Imitation Learning} & \textbf{Surrogate Modeling} & \textbf{Digital Twin} \\
\hline
Niveau d'Intervention Manuelle (NIM)    & Moyen                         & Bas                          & Moyen                       & Haut                 \\
Niveau de Modélisation Mathématique (NMM) & Élevé                         & Faible                       & Élevé                       & Moyen                \\
Applicabilité Générale (AG)             & Moyenne                       & Élevée                       & Moyenne                     & Moyenne              \\
Requiert une Version Émulée (RVE)       & Non                           & Non                          & Non                         & Oui                  \\
Mise à Jour Automatique (MUA)           & Moyenne                       & Élevée                       & Moyenne                     & Élevée               \\
Automatisation Complète (AC)            & Moyenne                       & Moyenne                      & Moyenne                     & Variable             \\
Adaptabilité en Temps Réel (ATR)        & Faible                        & Faible                       & Moyenne                     & Élevée               \\
Niveau de Fidélité au Système Réel (NFS) & Moyen                         & Bas                          & Moyen                       & Élevé                \\
Exigences en Données (ED)               & Élevées                       & Moyennes                     & Élevées                     & Élevées              \\
Coût Computationnel (CC)                & Élevé                         & Moyen                        & Moyen                       & Élevé                \\
\hline
\end{tabular}
\label{tab:comparaison-methodes}
\end{table*}

\paragraph{Explications des Comparaisons}

Les méthodes d'\textbf{Identification de Systèmes} et de \textbf{Jumeau Numérique} sont connues pour leur fidélité accrue à l'environnement réel, mais elles nécessitent souvent des ajustements manuels et un coût computationnel élevé. L'\textbf{Apprentissage par Imitation} est plus automatisé et nécessite moins de données mathématiques, bien qu'il soit souvent moins précis. La \textbf{Modélisation Approximative} offre un bon compromis pour des applications générales avec des exigences computationnelles moyennes, mais elle peut nécessiter une intervention manuelle pour ajuster les modèles aux changements de l'environnement.

Les étapes de cette approche sont les suivantes :
\begin{enumerate}
    \item \textbf{Collecte de Traces} : Les agents collectent des séquences de transitions $(s, a, s')$, des observations $\omega$, et des récompenses $r$.
    \item \textbf{Estimation des Paramètres} : Les paramètres $(\theta, \phi, \rho)$ sont estimés par l'application de techniques d'identification.
    \item \textbf{Modélisation Révisée} : Les fonctions $(\hat{P}, \hat{O}, \hat{R})$ sont ajustées afin d'obtenir un modèle simulé fidèle à l'environnement.
    \item \textbf{Entraînement et Validation} : Le modèle simulé est utilisé pour entraîner les agents et valider leurs politiques dans un environnement de test avant un éventuel déploiement dans l'environnement réel.
\end{enumerate}


%%%%%%%%%%

\subsection{Phase 2 : Résolution}
Dans cette phase, nous utilisons des algorithmes \textbf{MARL} pour résoudre le problème formalisé dans l'environnement simulé. Les algorithmes comme \textbf{MADDPG} et \textbf{MAPPO} sont utilisés pour apprendre des politiques qui respectent à la fois les objectifs de performance et les contraintes organisationnelles définies par \textbf{$mathcal{M}OISE^+$}. L'intégration de \textbf{$mathcal{M}OISE^+$} dans le processus d'apprentissage permet de restreindre l'espace de recherche des politiques, rendant ainsi le processus plus efficace et garantissant la sûreté du système.

\begin{itemize}
  \item \textbf{Entrées :} Problème formalisé, contraintes organisationnelles.
  \item \textbf{Sorties :} Politiques conjointes optimales pour les agents.
  \item \textbf{Processus :} Utilisation de \textbf{MARL} pour apprendre des politiques tout en respectant les rôles et missions prédéfinis.
\end{itemize}

\subsection{Phase 3 : Analyse}
Une fois les politiques apprises, elles sont analysées à l'aide de \textbf{HEMM}, un algorithme qui infère les rôles et les missions des agents en se basant sur leurs comportements observés. L'analyse permet d'identifier les écarts entre les rôles inférés et les rôles prédéfinis dans \textbf{$mathcal{M}OISE^+$}, offrant ainsi une évaluation de l'adéquation organisationnelle du SMA. Les résultats de cette analyse sont ensuite traduits en plans détaillés, qui seront utilisés pour le développement final du système.

\begin{itemize}
  \item \textbf{Entrées :} Politiques conjointes des agents.
  \item \textbf{Sorties :} Rôles et missions inférés, plan détaillé des comportements.
  \item \textbf{Processus :} Utilisation de \textbf{HEMM} pour analyser les politiques apprises et générer des plans interprétables.
\end{itemize}

\subsection{Phase 4 : Développement et déploiement}
Dans la phase finale, les plans générés sont utilisés pour développer et déployer le SMA dans l'environnement réel. Nous utilisons l'outil \textbf{CybMASDE} pour faciliter l'intégration des politiques apprises dans les agents du système. Avant le déploiement réel, des tests sont effectués dans un environnement émulé pour vérifier que le SMA respecte bien les contraintes de sûreté et les objectifs de performance. Cette phase garantit que les agents fonctionnent comme prévu dans l'environnement réel tout en respectant les rôles et missions spécifiés.

\section{Évaluation}
\label{sec:evaluation}

\subsection{CybMASDE : un environnement de développement pour l'approche}
Pour évaluer notre méthode, nous avons développé un outil nommé \textbf{CybMASDE} qui soutient chaque phase du processus de conception. \textbf{CybMASDE} propose une interface pour modéliser des environnements complexes, définir des objectifs, et spécifier des contraintes organisationnelles. Il intègre également des algorithmes \textbf{MARL} pour l'apprentissage des politiques et des outils d'analyse pour l'inférence des rôles. CybMASDE permet de visualiser les rôles inférés et de faciliter le déploiement des SMA dans des environnements réels après une validation en simulation.

\subsection{Résultats expérimentaux}
Nous avons appliqué SAMMASD à un scénario de gestion de flux de paquets dans un entrepôt, où des robots doivent collaborer pour organiser les flux de marchandises tout en respectant des contraintes de sécurité et d'efficacité. Les résultats montrent que la méthode permet de générer des politiques conjointes efficaces, avec une réduction significative du temps de conception et une amélioration de la robustesse des systèmes déployés.

\begin{itemize}
  \item \textbf{Environnement :} Gestion de flux dans un entrepôt simulé.
  \item \textbf{Objectifs :} Optimiser les flux de marchandises tout en respectant les contraintes de sûreté.
  \item \textbf{Métriques :} Temps de conception, robustesse des politiques, respect des contraintes organisationnelles.
\end{itemize}

Les résultats montrent que les agents entraînés via \textbf{$mathcal{M}OISE^+$MARL} respectent les contraintes organisationnelles définies et optimisent les objectifs de performance, tout en générant des comportements interprétables grâce à l'analyse par \textbf{HEMM}.

\section{Conclusion}
\label{sec:conclusion}
Cet article a présenté une nouvelle méthode, \textbf{SAMMASD}, qui permet de concevoir et déployer des systèmes multi-agents en combinant des spécifications organisationnelles avec des techniques d'apprentissage par renforcement. La méthode s'articule autour de quatre phases clés, allant de la modélisation de l'environnement à l'analyse des politiques apprises et au déploiement des systèmes réels. Les résultats expérimentaux montrent que cette approche réduit le temps de conception et améliore la robustesse des SMA tout en garantissant l'explicabilité des comportements des agents.

Les travaux futurs porteront sur l'extension de \textbf{SAMMASD} à des environnements encore plus complexes, notamment dans des contextes critiques tels que la cybersécurité, et sur l'intégration de nouveaux algorithmes d'apprentissage pour améliorer l'efficacité des politiques apprises.

\section*{References}

\bibliographystyle{ACM-Reference-Format}
\renewcommand
\refname{}
\bibliography{references}

\end{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
