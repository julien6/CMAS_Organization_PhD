%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% LaTeX Template for AAMAS-2025 (based on sample-sigconf.tex)
%%% Prepared by the AAMAS-2025 Program Chairs based on the version from AAMAS-2025. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% Start your document with the \documentclass command.

%%%% For anonymized submission, use this
\documentclass[sigconf,anonymous]{aamas} 

%%%% For camera-ready, use this
%\documentclass[sigconf]{aamas}

\usepackage{listings}
% \usepackage{xcolor}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}
 
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2,
    inputencoding = utf8,  % Input encoding
    extendedchars = true,  % Extended ASCII
    literate      =        % Support additional characters
    {á}{{\'a}}1  {é}{{\'e}}1  {í}{{\'i}}1 {ó}{{\'o}}1  {ú}{{\'u}}1
    {Á}{{\'A}}1  {É}{{\'E}}1  {Í}{{\'I}}1 {Ó}{{\'O}}1  {Ú}{{\'U}}1
    {à}{{\`a}}1  {è}{{\`e}}1  {ì}{{\`i}}1 {ò}{{\`o}}1  {ù}{{\`u}}1
    {À}{{\`A}}1  {È}{{\`E}}1  {Ì}{{\`I}}1 {Ò}{{\`O}}1  {Ù}{{\`U}}1
    {ä}{{\"a}}1  {ë}{{\"e}}1  {ï}{{\"i}}1 {ö}{{\"o}}1  {ü}{{\"u}}1
    {Ä}{{\"A}}1  {Ë}{{\"E}}1  {Ï}{{\"I}}1 {Ö}{{\"O}}1  {Ü}{{\"U}}1
    {â}{{\^a}}1  {ê}{{\^e}}1  {î}{{\^i}}1 {ô}{{\^o}}1  {û}{{\^u}}1
    {Â}{{\^A}}1  {Ê}{{\^E}}1  {Î}{{\^I}}1 {Ô}{{\^O}}1  {Û}{{\^U}}1
    {œ}{{\oe}}1  {Œ}{{\OE}}1  {æ}{{\ae}}1 {Æ}{{\AE}}1  {ß}{{\ss}}1
    {ẞ}{{\SS}}1  {ç}{{\c{c}}}1 {Ç}{{\c{C}}}1 {ø}{{\o}}1  {Ø}{{\O}}1
    {å}{{\aa}}1  {Å}{{\AA}}1  {ã}{{\~a}}1  {õ}{{\~o}}1 {Ã}{{\~A}}1
    {Õ}{{\~O}}1  {ñ}{{\~n}}1  {Ñ}{{\~N}}1  {¿}{{?`}}1  {¡}{{!`}}1
    {„}{\quotedblbase}1 {“}{\textquotedblleft}1 {–}{$-$}1
    {°}{{\textdegree}}1 {º}{{\textordmasculine}}1 {ª}{{\textordfeminine}}1
    {£}{{\pounds}}1  {©}{{\copyright}}1  {®}{{\textregistered}}1
    {«}{{\guillemotleft}}1  {»}{{\guillemotright}}1  {Ð}{{\DH}}1  {ð}{{\dh}}1
    {Ý}{{\'Y}}1    {ý}{{\'y}}1    {Þ}{{\TH}}1    {þ}{{\th}}1    {Ă}{{\u{A}}}1
    {ă}{{\u{a}}}1  {Ą}{{\k{A}}}1  {ą}{{\k{a}}}1  {Ć}{{\'C}}1    {ć}{{\'c}}1
    {Č}{{\v{C}}}1  {č}{{\v{c}}}1  {Ď}{{\v{D}}}1  {ď}{{\v{d}}}1  {Đ}{{\DJ}}1
    {đ}{{\dj}}1    {Ė}{{\.{E}}}1  {ė}{{\.{e}}}1  {Ę}{{\k{E}}}1  {ę}{{\k{e}}}1
    {Ě}{{\v{E}}}1  {ě}{{\v{e}}}1  {Ğ}{{\u{G}}}1  {ğ}{{\u{g}}}1  {Ĩ}{{\~I}}1
    {ĩ}{{\~\i}}1   {Į}{{\k{I}}}1  {į}{{\k{i}}}1  {İ}{{\.{I}}}1  {ı}{{\i}}1
    {Ĺ}{{\'L}}1    {ĺ}{{\'l}}1    {Ľ}{{\v{L}}}1  {ľ}{{\v{l}}}1  {Ł}{{\L{}}}1
    {ł}{{\l{}}}1   {Ń}{{\'N}}1    {ń}{{\'n}}1    {Ň}{{\v{N}}}1  {ň}{{\v{n}}}1
    {Ő}{{\H{O}}}1  {ő}{{\H{o}}}1  {Ŕ}{{\'{R}}}1  {ŕ}{{\'{r}}}1  {Ř}{{\v{R}}}1
    {ř}{{\v{r}}}1  {Ś}{{\'S}}1    {ś}{{\'s}}1    {Ş}{{\c{S}}}1  {ş}{{\c{s}}}1
    {Š}{{\v{S}}}1  {š}{{\v{s}}}1  {Ť}{{\v{T}}}1  {ť}{{\v{t}}}1  {Ũ}{{\~U}}1
    {ũ}{{\~u}}1    {Ū}{{\={U}}}1  {ū}{{\={u}}}1  {Ů}{{\r{U}}}1  {ů}{{\r{u}}}1
    {Ű}{{\H{U}}}1  {ű}{{\H{u}}}1  {Ų}{{\k{U}}}1  {ų}{{\k{u}}}1  {Ź}{{\'Z}}1
    {ź}{{\'z}}1    {Ż}{{\.Z}}1    {ż}{{\.z}}1    {Ž}{{\v{Z}}}1  {ž}{{\v{z}}}1
    % ¿ and ¡ are not correctly displayed if inconsolata font is used
    % together with the lstlisting environment. Consider typing code in
    % external files and using \lstinputlisting to display them instead.   
}
 
\lstset{style=mystyle}

% --- Tickz
\usepackage{physics}
\usepackage{tikz}
\usepackage{amsmath}
\usepackage{mathdots}
% \usepackage{yhmath}
\usepackage{cancel}
\usepackage{color}
\usepackage{siunitx}
\usepackage{array}
\usepackage{multirow}
% \usepackage{amssymb}
\usepackage{gensymb}
\usepackage{tabularx}
\usepackage{extarrows}
\usepackage{booktabs}
\usetikzlibrary{fadings}
\usetikzlibrary{patterns}
\usetikzlibrary{shadows.blur}
\usetikzlibrary{shapes}

% ---------

\usepackage{balance} % for balancing columns on the final page
\usepackage{csquotes}
% \usepackage{cite}
\newcommand{\probP}{\text{I\kern-0.15em P}}
\usepackage{etoolbox}
\patchcmd{\thebibliography}{\section*{\refname}}{}{}{}
% \usepackage{amsthm,amssymb,amsfonts}

\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{color}
% \renewcommand\UrlFont{\color{blue}\rmfamily}

\usepackage[inline, shortlabels]{enumitem}
\usepackage{tabularx}
\usepackage{caption}
\usepackage{listings}
\usepackage{titlesec}
\usepackage{ragged2e}
% \usepackage[hyphens]{url}
\usepackage[linesnumbered,ruled,vlined]{algorithm2e}
% \usepackage{float}

\usepackage[english]{babel}
\addto\extrasenglish{  
    \def\figureautorefname{Figure}
    \def\tableautorefname{Table}
    \def\algorithmautorefname{Algorithm}
    \def\sectionautorefname{Section}
    \def\subsectionautorefname{Subsection}
    \def\proofoutlineautorefname{Proof Outline}
}

\newcommand{\supertiny}{\fontsize{1}{2}\selectfont}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% AAMAS-2025 copyright block (do not change!)

\setcopyright{ifaamas}
\acmConference[AAMAS '25]{Proc.\@ of the 24th International Conference
on Autonomous Agents and Multiagent Systems (AAMAS 2025)}{May 19 -- 23, 2025}
{Detroit, Michigan, USA}{A.~El~Fallah~Seghrouchni, Y.~Vorobeychik, S.~Das, A.~Nowe (eds.)}
\copyrightyear{2025}
\acmYear{2025}
\acmDOI{}
\acmPrice{}
\acmISBN{}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% == IMPORTANT ==
%%% Use this command to specify your EasyChair submission number.
%%% In anonymous mode, it will be printed on the first page.

\acmSubmissionID{<<EasyChair submission id>>}

%%% Use this command to specify the title of your paper.

\title[AAMAS-2025 CybMASDE]{Une Méthode pour Assister la Conception d'un Système Multi-Agent en Utilisant l'Apprentissage par Renforcement}

%%% Provide names, affiliations, and email addresses for all authors.

\author{Julien Soulé}
\affiliation{
  \institution{Univ. Grenoble Alpes}
  \city{Valence}
  \country{France}}
\email{julien.soule@lcis.grenoble-inp.fr}

\author{Jean-Paul Jamont}
\affiliation{
  \institution{Univ. Grenoble Alpes}
  \city{Valence}
  \country{France}}
\email{jean-paul.jamont@lcis.grenoble-inp.fr}

\author{Michel Occello}
\affiliation{
  \institution{Univ. Grenoble Alpes}
  \city{Valence}
  \country{France}}
\email{michel.occello@lcis.grenoble-inp.fr}

\author{Louis-Marie Traonouez}
\affiliation{
  \institution{Thales Land and Air Systems, BU IAS}
  \city{Rennes}
  \country{France}}
\email{louis-marie.traonouez@thalesgroup.com}

\author{Paul Théron}
\affiliation{
  \institution{AICA IWG}
  \city{La Guillermie}
  \country{France}}
\email{paul.theron@orange.fr}

\begin{abstract}
  Les méthodes AOSE reposent généralement sur la connaissance et l'expertise du concepteur afin d'orienter le développement vers un SMA qui atteint un objectif dans un environnement donné. Les récents travaux en MARL laissent entrevoir une approche plus automatisée de l'exploration de l'espace de conception.
  Nous proposons la méthode SAMMASD (Semi-Automated MARL-based MAS Design) pour la conception et le déploiement de SMA. Elle s'appuie pour cela sur l'intégration de spécifications organisationnelles en MARL. Cette méthode s'articule autour de quatre phases. La première consiste à modéliser l'environnement réel, les objectifs et contraintes additionnelles, telles que des exigences de fonctionnement, dans une simulation. La seconde phase exploite plusieurs algorithmes MARL afin d'apprendre des politiques satisfaisantes et stables dans l'atteinte des objectifs sous les contraintes exprimées précedement. La troisième phase applique une analyse des comportements pour inférer les rôles et objectifs émergents, et pour générer des "plans détaillés". Enfin, la phase de développement permet de déployer automatiquement ces politiques dans des environnements réels après une vérification en environnement émulé.
  SAMMASD est illustré sur un scénario de gestion de flux en entrepôt par des robots. Son utilisation montre que plusieurs SMA efficaces et sûrs sont générés facilitant le processus de conception.
\end{abstract}



%%% The code below was generated by the tool at http://dl.acm.org/ccs.cfm.
%%% Please replace this example with code appropriate for your own paper.


%%% Use this command to specify a few keywords describing your work.
%%% Keywords should be separated by commas.

\keywords{Agent-Oriented Software Engineering \and Multi-Agent Reinforcement Learning \and Assisted-Design \and Organizational Models}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% Include any author-defined commands here.
         
% \newcommand{\BibTeX}{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em\TeX}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

%%% The following commands remove the headers in your paper. For final 
%%% papers, these will be inserted during the pagination process.

\pagestyle{fancy}
\fancyhead{}

%%% The next command prints the information defined in the preamble.

\maketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}

Les systèmes multi-agents (SMA) sont au cœur de nombreuses applications, allant de la gestion d'entrepôts automatisés à la cybersécurité. Ces systèmes doivent permettre à plusieurs agents autonomes d'agir de manière coordonnée afin d'atteindre des objectifs communs dans des environnements complexes et dynamiques. Cependant, la conception de tels systèmes présente des défis majeurs en termes de robustesse, de sûreté et d'explicabilité des comportements des agents.

Les méthodes actuelles d'apprentissage par renforcement multi-agent (MARL) ont montré leur efficacité pour générer des politiques optimales dans des environnements dynamiques. Toutefois, ces approches manquent souvent de mécanismes explicites pour imposer des contraintes organisationnelles et garantir que les agents respectent des normes de sûreté essentielles pour des environnements critiques, comme la logistique en entrepôts ou la défense cybernétique.

Cet article présente une nouvelle méthode, \textbf{SAMMASD} (Semi-Automated MARL-based MAS Design), qui combine le framework \textbf{$mathcal{M}OISE^+$MARL} avec l'algorithme d'évaluation \textbf{HEMM} pour inférer et appliquer des structures organisationnelles aux politiques apprises par les agents. L'intégration de \textbf{$mathcal{M}OISE^+$}, un modèle organisationnel orienté multi-agents, permet de contraindre l'apprentissage des agents en définissant des rôles et des missions, ce qui favorise l'émergence de comportements interprétables et conformes aux exigences de sûreté.

Le reste de cet article est structuré comme suit : la \autoref{sec:related_works} présente les travaux connexes dans le domaine du MARL et des modèles organisationnels pour SMA. La \autoref{sec:approach} décrit en détail les quatre phases de la méthode SAMMASD. Ensuite, la \autoref{sec:evaluation} discute des résultats expérimentaux obtenus dans un scénario de gestion de flux d'entrepôt. Enfin, la \autoref{sec:conclusion} conclut et propose des pistes de travail futures.

\begin{figure*}[h!]
  \centering
  \input{figures/cycle.tex}
  \caption{Cycle de vie d'un SMA conçu avec SAMMASD.}
  \label{fig:cycle}
\end{figure*}

\section{Travaux connexes}
\label{sec:related_works}
La conception de systèmes multi-agents (SMA) assistée par des modèles organisationnels a été largement étudiée dans la littérature. En particulier, le modèle \textbf{$mathcal{M}OISE^+$}, introduit par Gleizes et al. \cite{gleize2008moise}, propose une structure formelle pour spécifier les rôles, les missions, et les relations entre agents dans un SMA. Ces spécifications permettent de guider le développement de systèmes complexes en assurant une cohérence entre les objectifs collectifs et les comportements individuels des agents. Cependant, $mathcal{M}OISE^+$ ne fournit pas de mécanismes pour intégrer directement l'apprentissage des politiques par renforcement, une lacune que notre travail cherche à combler.

D'un autre côté, les méthodes d'apprentissage par renforcement multi-agent (\textbf{MARL}) ont connu un essor ces dernières années, avec des approches telles que \textbf{MADDPG} (Multi-Agent Deep Deterministic Policy Gradient) \cite{lowe2017multi} et \textbf{Q-Mix} \cite{rashid2018qmix}. Ces algorithmes permettent aux agents d'apprendre des politiques optimales en collaboration ou en compétition dans des environnements partiellement observables. Néanmoins, ces méthodes se concentrent principalement sur la performance des politiques sans se soucier des contraintes organisationnelles ou des besoins en explicabilité, ce qui limite leur applicabilité dans des contextes nécessitant une coordination stricte entre agents.

Des travaux récents ont exploré des méthodologies agiles pour le développement de SMA, en combinant des approches orientées agent avec des pratiques agiles \cite{winikoff2021agile}. Ces méthodes visent à faciliter l'itération rapide dans la conception de SMA, mais elles n'intègrent pas directement les dynamiques d'apprentissage en MARL. Notre approche s'inscrit dans cette lignée en combinant l'apprentissage automatisé via MARL avec des spécifications organisationnelles, comblant ainsi le fossé entre les processus d'apprentissage et les exigences de conception organisationnelle.

\section{Approche de conception organisationnelle proposée pour les SMA}
\label{sec:approach}

\subsection{Aperçu général}

Le coeur de la méthode SAMMASD est de considerer la conception d'un SMA comme un problème d'optimisation sous contraintes. La variable à optimiser est alors la politique conjointe pour maximiser la récompense cumulée sous des contraintes additionnelles données. En suivant cette approche, nous avons choisi d'adopter une approche incluant des spécifications organisationnelle en MARL comme contraintes additionnelles. Ces spécifications organisationnelles permettent à l'utilisateur de laisser un certain degré de controle non-seulement des comportements individuels des agents, mais aussi de pouvoir gérer l'ensemble des agents en jouant sur celles-ci.
L'objectif est alors de guider l'apprentissage des agents en respectant des contraintes organisationnelles sous la forme de role et d'objectifs, tout en garantissant chercher une efficacité suffisante des politiques apprises.

Notre méthode, SAMMASD, s'articule autour de quatre phases principales : (1) la modélisation de l'environnement, de l'objectif et des contraintes organisationnelles selon le framework proposé, (2) l'apprentissage des politiques via plusieurs algorithmes MARL, (3) l'analyse des comportements et l'inférence de spécifications organisationnelles avec une méthode proposée, et (4) le développement et le déploiement du SMA. Cette approche permet de guider le processus d'apprentissage des agents en respectant des contraintes organisationnelles strictes, tout en garantissant l'efficacité des politiques apprises. Le cycle de vie d'un SMA conçu avec SAMMASD est illustré dans la \autoref{fig:cycle}.

% Description formelle des phases

\subsection{Phase 1 : Modélisation}

L'étape de modélisation vise à créer un modèle simulé qui capture fidèlement les dynamiques et les contraintes de l'environnement cible, tout en incluant des spécifications organisationnelles et des objectifs. Ce modèle servira de base pour entraîner les agents dans un environnement contrôlé. Cette étape est cruciale pour s'assurer que les agents apprennent dans une simulation fidèle à la réalité du système cible dans un cadre sécurisé et facilitant la recherche d'une solution.

Cette phase prend en entré les éléments suivants :
\begin{itemize}
    \item \textbf{Environnement} : Une copie "émulée" de l'environnement réel ou l'environment cible lui-même si cela est possible.
    \item \textbf{Description du problème} : La description détaillée des objectifs à atteindre pour les agents, c'est à dire les états recherchés.
    \item \textbf{Contraintes additionnelles} : Des exigences spécifiques à respecter, pouvant inclure des normes, règles d'organisation, ou contraintes de sûreté.
\end{itemize}

Une fois les élements collectés, la modélisation suit les étapes suivantes :
\begin{enumerate}
  \item \textit{Modélisation de l'environnement simulé}: Des agents observateurs sont déployés pour collecter des traces de l'environnement réel ou de sa copie sécurisée. Ces traces sont utilisées pour générer un modèle simulé de l'environnement en utilisant une technique de \textit{System Identification}.
  \item \textit{Formulation de la fonction de récompense} : L'utilisateur doit mettre en place la logique d'une fonction de récompense qu'il estime être la plus adaptée pour atteindre les objectifs fixés. 
  \item \textit{Formulation des spécifications $mathcal{M}OISE^+$MARL} : Les contraintes organisationnelles et les sous-objectifs sont formalisés en utilisant le framework $mathcal{M}OISE^+$MARL. Les rôles et les missions sont définis pour guider les agents dans leur apprentissage.
\end{enumerate}

\subsubsection{Modélisation de l'environnement simulé}

Un \textbf{Dec-POMDP} est généralement formalisé par un tuple $d = \langle N, S, A, P, R, \Omega, O, \gamma \rangle$, où :
\begin{itemize}
    \item \( N \) est l'ensemble des agents.
    \item \( S \) est l'ensemble des états possibles du système.
    \item \( A = \times_{i=1}^N A_i \) est l'ensemble des actions possibles pour chaque agent \(i\).
    \item \( P : S \times A \times S \to [0, 1] \) est la fonction de transition d'états, qui définit la probabilité de passer d'un état \(s\) à un état \(s'\) sous l'action \(a\).
    \item \( R : S \times A \to \mathbb{R} \) est la fonction de récompense.
    \item \( \Omega = \times_{i=1}^N \Omega_i \) est l'ensemble des observations possibles pour chaque agent \(i\).
    \item \( O : S \times A \times \Omega \to [0, 1] \) est la fonction d'observation, qui définit la probabilité qu'une observation \(\omega\) soit perçue après une transition \(s \to s'\) sous l'action \(a\).
    \item \( \gamma \in [0, 1] \) est le facteur d'actualisation.
\end{itemize}

Un ensemble de spécifications tirées du modèle $\mathcal{M}OISE^+$ est noté $\mathcal{OS} = \langle \mathcal{SS}, \mathcal{FS}, \mathcal{DS} \rangle$, où $\mathcal{SS}$ sont les \textbf{Spécifications Structurelles}, $\mathcal{FS}$ sont les \textbf{Spécifications Fonctionnelles}, et $\mathcal{DS}$ sont les \textbf{Spécifications Déontiques}.

\paragraph{\textbf{Spécifications Structurelles (SS)}} se réfèrent aux moyens structurés laissés aux agents pour atteindre un objectif, que nous notons $\mathcal{SS} = \langle \mathcal{R}, \mathcal{IR}, \mathcal{G} \rangle$, où :

\begin{itemize}
    \item $\mathcal{R}_{ss}$ : l'ensemble de tous les rôles (noté $\rho \in \mathcal{R}$) ;
    \item $\mathcal{IR} : \mathcal{R} \rightarrow \mathcal{R}$ : la relation d'héritage entre rôles ($\mathcal{IR}(\rho_1) = \rho_2$ signifie que $\rho_1$ hérite de $\rho_2$, noté également $\rho_1 \sqsubset \rho_2$) ;
    \item $\mathcal{RG} \subseteq \mathcal{GR}$ : l'ensemble des groupes racines, $\mathcal{GR} = \langle \mathcal{R}, \mathcal{SG}, \mathcal{L}^{intra}, \mathcal{L}^{inter}, \mathcal{C}^{intra}, \mathcal{C}^{inter}, np, ng \rangle$, l'ensemble de tous les groupes, où :
          \begin{itemize}
              \item $\mathcal{R} \subseteq \mathcal{R}_{ss}$ : l'ensemble des rôles non abstraits ;
              \item $\mathcal{SG} \subseteq \mathcal{GR}$ : l'ensemble des sous-groupes ;
              \item $\mathcal{L} = \mathcal{R} \cross \mathcal{R} \cross \mathcal{TL}$ : l'ensemble des liens. Un lien est un 3-uplet $(\rho_s,\rho_d,t) \in \mathcal{L}$ (noté aussi $link(\rho_s,\rho_d,t))$, où $\rho_{s}$ est le rôle source, $\rho_{d}$ est le rôle destination, et $t \in \mathcal{TL}, \mathcal{TL} = \{acq, com, aut\}$ est le type de lien :
                    \begin{itemize}
                        \item Si $t = acq$ (acquaintance), les agents jouant le rôle source $\rho_{\mathrm{s}}$ sont autorisés à avoir une représentation des agents jouant le rôle destination $\rho_{d}$ ;
                        \item Si $t = com$ (communication), les agents $\rho_{\mathrm{s}}$ peuvent communiquer avec les agents $\rho_{d}$ ;
                        \item Si $t = aut$ (authority), les agents $\rho_{\mathrm{s}}$ ont autorité sur les agents $\rho_{d}$, nécessitant des liens d'acquaintance et de communication.
                    \end{itemize}
              \item $\mathcal{L}^{intra} \subseteq \mathcal{L}$ : l'ensemble des liens intra-groupes ;
              \item $\mathcal{L}^{inter} \subseteq \mathcal{L}$ : l'ensemble des liens inter-groupes ;
              \item $\mathcal{C} = \mathcal{R} \cross \mathcal{R}$ : l'ensemble des compatibilités. Une compatibilité est un couple $(a,b) \in \mathcal{C}$ (noté aussi $\rho_a \bowtie \rho_b$), ce qui signifie que les agents jouant le rôle $\rho_a \in \mathcal{R}$ peuvent également jouer le rôle $\rho_b \in \mathcal{R}$ ;
              \item $\mathcal{C}^{intra} \subseteq \mathcal{C}$ : l'ensemble des compatibilités intra-groupes ;
              \item $\mathcal{C}^{inter} \subseteq \mathcal{C}$ : l'ensemble des compatibilités inter-groupes ;
              \item $np : \mathcal{R} \rightarrow \mathbb{N} \times \mathbb{N}$ : la relation donnant la cardinalité des agents adoptant un rôle ;
              \item $ng : \mathcal{SG} \rightarrow \mathbb{N} \times \mathbb{N}$ : la relation donnant la cardinalité de chaque sous-groupe.
          \end{itemize}
\end{itemize}

\paragraph{\textbf{Spécifications Fonctionnelles (FS)}} désignent globalement les tâches et objectifs que les agents doivent atteindre, que nous notons $\mathcal{FS} = \langle \mathcal{SCH}, \mathcal{PO} \rangle$, où :

\begin{itemize}
    \item $\mathcal{SCH} = \langle\mathcal{G}, \mathcal{M}, \mathcal{P}, mo, nm \rangle$ : l'ensemble des \textbf{schémas sociaux}, où :
          \begin{itemize}
              \item $\mathcal{G}$ est l'ensemble des objectifs globaux ;
              \item $\mathcal{M}$ est l'ensemble des labels de mission ;
              \item $\mathcal{P} = \langle \mathcal{G}, \{\mathcal{G}\}^s, OP, [0,1] \rangle, s \in \mathbb{N}^*$ est l'ensemble des plans qui construisent la structure arborescente des objectifs. Un plan $p \in \mathcal{P}$ est un 4-uplet $p=(g_f,\{g_i\}_{0 \leq i \leq s}, op, ps)$, ce qui signifie que l'objectif $g_f$ est atteint si certains des sous-objectifs $g_i$ sont atteints avec une probabilité de succès $ps$ et selon l'opérateur $op$ :
                    \begin{itemize}
                        \item si $op = sequence$, les $g_i$ peuvent seulement être atteints dans le même ordre séquentiellement ;
                        \item si $op = choice$, un seul des $g_i$ doit être atteint ;
                        \item si $op = parallel$, les $g_i$ peuvent être atteints séquentiellement ou simultanément.
                    \end{itemize}
              \item $mo : \mathcal{M} \rightarrow \mathbb{P}(\mathcal{G})$ : spécifie l'ensemble des objectifs associés à une mission ;
              \item $nm : \mathcal{M} \rightarrow \mathbb{N} \times \mathbb{N}$ : la cardinalité des agents engagés pour chaque mission.
          \end{itemize}
    \item $\mathcal{PO} : \mathcal{M} \cross \mathcal{M}$ : l'ensemble des \textbf{ordres de préférence}. Un ordre de préférence est un couple $(m_1, m_2)$, ce qui signifie que si un agent est autorisé à s'engager dans $m_1$ et $m_2$ en même temps, il a une préférence sociale pour $m_1$.
\end{itemize}

\paragraph{\textbf{Spécifications Déontiques (DS)}} se réfèrent à la façon dont les SS sont utilisées pour atteindre les FS, que nous notons $\mathcal{DS} = \langle \mathcal{OBL}, \mathcal{PER} \rangle$, l'ensemble des spécifications déontiques, où :

\begin{itemize}
    \item $\mathcal{TC}$ : l'ensemble des \textbf{contraintes temporelles}. Une contrainte temporelle $tc \in \mathcal{TC}$ spécifie un ensemble de périodes pendant lesquelles une permission ou obligation est valide ($Any \in \mathcal{TC}$ signifie \textquote{tout le temps}) ;
    \item $\mathcal{OBL} : \mathcal{R} \cross \mathcal{M} \cross \mathcal{TC}$ : l'ensemble des \textbf{obligations}. Une obligation est un 3-uplet $(\rho_a, m, tc)$, signifiant qu'un agent jouant le rôle $\rho_a \in \mathcal{R}$ est obligé de s'engager dans la mission $m \in \mathcal{M}$ pour une contrainte temporelle donnée $tc \in \mathcal{TC}$ ;
    \item $\mathcal{PER}$ : l'ensemble des \textbf{permissions}. Une permission est un 3-uplet $(\rho_a, m, tc)$, signifiant qu'un agent jouant le rôle $\rho_a \in \mathcal{R}$ est autorisé à s'engager dans la mission $m \in \mathcal{M}$ pour une contrainte temporelle donnée $tc \in \mathcal{TC}$.
\end{itemize}


\subsubsection{Formulation de la fonction de récompense}

\subsubsection{Formulation des spécifications $mathcal{M}OISE^+$MARL}




Le framework \textbf{$mathcal{M}OISE^+$MARL} enrichit le cadre du MARL en intégrant des spécifications organisationnelles qui structurent et contrôlent les comportements des agents. En utilisant les rôles et missions de \textbf{$mathcal{M}OISE^+$}, ce formalisme permet de guider les agents dans un environnement multi-agent selon des contraintes organisationnelles.

\paragraph{Description Globale}

La structure formelle de \textbf{$mathcal{M}OISE^+$MARL} repose sur l'ajout de deux ensembles fondamentaux dans le cadre Dec-POMDP : 

\begin{itemize}
    \item \textbf{Observation-Actions Constraint (OAC)} : Un ensemble de relations modélisant les rôles en associant chaque observation d'un agent à un ensemble d'actions autorisées. Cela contraint les comportements possibles selon les rôles organisationnels.
    \item \textbf{Trajectory-based Reward Function (TRF)} : Un ensemble de relations associant des trajectoires d'actions à des récompenses, définissant ainsi les missions par des objectifs précis que les agents doivent atteindre.
\end{itemize}

\paragraph{Formalisation Complète}

Pour lier les structures $mathcal{M}OISE^+$ aux contraintes d'apprentissage MARL, nous définissons les relations suivantes :

\begin{itemize}
    \item \textbf{OAC (Observation-Actions Constraint)} : 
    \begin{itemize}
        \item $\text{OAC} = \{ oac_1, oac_2, \dots, oac_k \}$
        \item Chaque relation \( oac \in \text{OAC} \) est définie par une fonction \( oac: \Omega \rightarrow \mathcal{P}(A) \), associant chaque observation \( o \in \Omega \) à un ensemble d'actions \( A \) autorisées.
    \end{itemize}
    \item \textbf{TRF (Trajectory-based Reward Function)} : 
    \begin{itemize}
        \item $\text{TRF} = \{ trf_1, trf_2, \dots, trf_m \}$
        \item Chaque relation \( trf \in \text{TRF} \) est une fonction \( trf: H \rightarrow \mathbb{R} \), attribuant une récompense à chaque trajectoire \( h \in H \).
    \end{itemize}
    \item \textbf{Déontiques (Relation DA)} :
    \begin{itemize}
        \item La relation \( \text{DA}: \mathcal{D} \rightarrow \mathcal{P}(\mathcal{A}) \) relie les spécifications déontiques aux agents, permettant de définir les permissions et obligations selon les rôles et missions.
    \end{itemize}
    \item \textbf{Relation Role-OAC (ROAC)} :
    \begin{itemize}
        \item La relation \( \text{ROAC}: \mathcal{R} \rightarrow \text{OAC} \) associe chaque rôle de $mathcal{M}OISE^+$ à une relation dans OAC, encadrant les actions des agents pour chaque rôle.
    \end{itemize}
    \item \textbf{Relation Objective-TRF (OTRF)} :
    \begin{itemize}
        \item La relation \( \text{OTRF}: \mathcal{G} \rightarrow \text{TRF} \) lie les objectifs aux relations de TRF, en représentant les objectifs sous forme de récompenses dans le MARL.
    \end{itemize}
\end{itemize}

\paragraph{Intérêt du Formalisme $mathcal{M}OISE^+$MARL}





\paragraph{Processus}
\begin{enumerate}
    \item \textbf{Création du Modèle Simulé} :\\
    Si un modèle simulé existe déjà, il sera utilisé pour intégrer l'objectif et les contraintes. Sinon, on crée un modèle simulé en collectant des traces de l'environnement réel ou d'une copie sécurisée de celui-ci.
    
    \item \textbf{Incorporation des Contraintes $mathcal{M}OISE^+$MARL} :\\
    Utilisation des spécifications de \textbf{$mathcal{M}OISE^+$} pour structurer les rôles et missions des agents via les contraintes \textbf{OAC} et \textbf{TRF}.
    
    \item \textbf{Formulation du Modèle de Simulation} :\\
    Enrichissement du modèle par l'ajout des contraintes additionnelles pour encadrer le comportement des agents dans un cadre étendu du Dec-POMDP.
\end{enumerate}

\paragraph{Sortie}
Un modèle \textbf{$mathcal{M}OISE^+$MARL} intégrant l'environnement simulé, l'objectif (fonction de récompense), et les contraintes additionnelles (OAC, TRF).

\RestyleAlgo{ruled}
\SetKwComment{Comment}{// }{}

\begin{algorithm}[hbt!]
  \caption{Étape de Modélisation pour SAMMASD}\label{alg:modeling}

  \SetAlgoLined
\KwIn{%
    $real\_env$ : Environnement réel avec états et transitions,\\
    $problem\_desc$ : Description du problème avec fonction de récompense,\\
    $additional\_constraints$ : Contraintes organisationnelles et de sûreté (TRF, OAC)
}
\KwOut{%
    $model$ : Un modèle $mathcal{M}OISE^+$MARL avec l'environnement simulé, l'objectif, et les contraintes
}

% Étape 1: Créer ou utiliser un modèle simulé
\If{SimulatedModelExists($real\_env$)}{
    $simulated\_env \gets$ GetExistingSimulatedModel($real\_env$)\;
}
\Else{
    $safe\_env\_copy \gets$ CreateSafeCopy($real\_env$)\;
    $traces \gets$ CollectTraces($safe\_env\_copy$)\;
    $simulated\_env \gets$ GenerateSimulatedModel($traces$)\;
}

% Étape 2: Ajouter l'objectif et les contraintes
$simulated\_env$.AddObjective($problem\_desc.objective\_function$)\;
\ForEach{$constraint$ in $additional\_constraints$}{
    \eIf{$constraint.Type$ == "Soft"}{
        ApplyRewardShaping($simulated\_env$, $constraint$)\;
    }{
        ApplyShielding($simulated\_env$, $constraint$)\;
    }
}

% Étape 3: Incorporer les spécifications $mathcal{M}OISE^+$MARL
$oac \gets$ GenerateOAC($simulated\_env$, $problem\_desc.roles$)\;
$trf \gets$ GenerateTRF($simulated\_env$, $problem\_desc.missions$)\;
$model \gets$ MOISE\_MARL\_Model($simulated\_env$, $oac$, $trf$)\;

\Return{$model$}

\end{algorithm}


Ce processus établit un environnement d'apprentissage sécurisé et guidé pour les agents, en s'assurant qu'ils respectent les contraintes définies tout en poursuivant leurs objectifs de manière efficace.


\subsubsection{Intégration de la System Identification dans un Dec-POMDP}

Dans le cadre d'un \textbf{Dec-POMDP} enrichi par la \textbf{System Identification}, nous utilisons le formalisme suivant, où chaque composante du modèle est ajustée à partir de données collectées pour reproduire les dynamiques de l'environnement cible. 

\subsubsection{Ajout de la System Identification dans le Dec-POMDP}

La \textbf{System Identification} permet d'enrichir ce formalisme en estimant le modèle de transition, les observations et les récompenses à partir de traces collectées. Ainsi, nous obtenons les ajustements suivants :

\paragraph{Modèle de Transition \(\hat{P}\)}
La fonction de transition estimée est définie par :
\[
\hat{P}(s' \mid s, a, \theta) \approx P(s' \mid s, a)
\]
où \(\theta\) représente les paramètres de la transition, ajustés pour minimiser l'écart entre les transitions réelles et estimées.

\paragraph{Fonction d'Observation \(\hat{O}\)}
De même, l'observation est approchée par :
\[
\hat{O}(\omega \mid s', a, \phi) \approx O(\omega \mid s', a)
\]
où \(\phi\) est le vecteur de paramètres qui permet d'estimer les probabilités d'observation sur la base des traces collectées.

\paragraph{Fonction de Récompense \(\hat{R}\)}
La fonction de récompense est également ajustée :
\[
\hat{R}(s, a, \rho) \approx R(s, a)
\]
où \(\rho\) sont les paramètres de récompense qui permettent de reproduire les incitations spécifiques de l'environnement cible.

\subsubsection{Comparaison des Méthodes de Modélisation Automatisée}

Dans cette sous-section, nous présentons une analyse des différentes méthodes permettant de modéliser un environnement cible de manière fidèle et automatisée. Les méthodes considérées sont l'Identification de Systèmes, l'Apprentissage par Imitation, la Modélisation Approximative (Surrogate Modeling), et le Jumeau Numérique (Digital Twin). Nous utilisons des critères spécifiques pour évaluer ces méthodes en fonction de leur adéquation aux besoins de la modélisation automatisée.

\paragraph{Critères d'Évaluation}

\begin{itemize}
    \item \textbf{Niveau d'Intervention Manuelle (NIM)} : La méthode nécessite-t-elle que l'utilisateur développe ou configure manuellement une partie du modèle simulé ?
    \item \textbf{Niveau de Modélisation Mathématique (NMM)} : La méthode inclut-elle des modèles mathématiques pour représenter l'environnement ?
    \item \textbf{Applicabilité Générale (AG)} : La méthode peut-elle modéliser une large variété d'environnements sans restrictions spécifiques ?
    \item \textbf{Requiert une Version Émulée (RVE)} : La méthode nécessite-t-elle une copie émulée de l'environnement cible pour générer ou valider le modèle ?
    \item \textbf{Mise à Jour Automatique (MUA)} : La méthode permet-elle de mettre à jour le modèle automatiquement sans intervention humaine ?
    \item \textbf{Automatisation Complète (AC)} : La méthode offre-t-elle une procédure entièrement automatisée pour construire et gérer le modèle ?
    \item \textbf{Adaptabilité en Temps Réel (ATR)} : La méthode peut-elle s'adapter en temps réel aux changements de l'environnement cible ?
    \item \textbf{Niveau de Fidélité au Système Réel (NFS)} : La méthode fournit-elle un modèle avec un haut niveau de fidélité par rapport au système cible ?
    \item \textbf{Exigences en Données (ED)} : Quel est le volume et le type de données requis pour construire et maintenir le modèle ?
    \item \textbf{Coût Computationnel (CC)} : Quelles sont les exigences en termes de ressources de calcul pour générer et mettre à jour le modèle ?
\end{itemize}

\paragraph{Comparaison des Méthodes}

Le tableau \ref{tab:comparaison-methodes} présente une comparaison des méthodes d'Identification de Systèmes, d'Apprentissage par Imitation, de Modélisation Approximative, et de Jumeau Numérique en fonction des critères mentionnés.


\begin{table*}[h!]
\centering
\caption{Comparaison des méthodes de modélisation automatisée selon différents critères}
\begin{tabular}{|p{5cm}|c|c|c|c|}
\hline
\textbf{Critères}                   & \textbf{System Identification} & \textbf{Imitation Learning} & \textbf{Surrogate Modeling} & \textbf{Digital Twin} \\
\hline
Niveau d'Intervention Manuelle (NIM)    & Moyen                         & Bas                          & Moyen                       & Haut                 \\
Niveau de Modélisation Mathématique (NMM) & Élevé                         & Faible                       & Élevé                       & Moyen                \\
Applicabilité Générale (AG)             & Moyenne                       & Élevée                       & Moyenne                     & Moyenne              \\
Requiert une Version Émulée (RVE)       & Non                           & Non                          & Non                         & Oui                  \\
Mise à Jour Automatique (MUA)           & Moyenne                       & Élevée                       & Moyenne                     & Élevée               \\
Automatisation Complète (AC)            & Moyenne                       & Moyenne                      & Moyenne                     & Variable             \\
Adaptabilité en Temps Réel (ATR)        & Faible                        & Faible                       & Moyenne                     & Élevée               \\
Niveau de Fidélité au Système Réel (NFS) & Moyen                         & Bas                          & Moyen                       & Élevé                \\
Exigences en Données (ED)               & Élevées                       & Moyennes                     & Élevées                     & Élevées              \\
Coût Computationnel (CC)                & Élevé                         & Moyen                        & Moyen                       & Élevé                \\
\hline
\end{tabular}
\label{tab:comparaison-methodes}
\end{table*}

\paragraph{Explications des Comparaisons}

Les méthodes d'\textbf{Identification de Systèmes} et de \textbf{Jumeau Numérique} sont connues pour leur fidélité accrue à l'environnement réel, mais elles nécessitent souvent des ajustements manuels et un coût computationnel élevé. L'\textbf{Apprentissage par Imitation} est plus automatisé et nécessite moins de données mathématiques, bien qu'il soit souvent moins précis. La \textbf{Modélisation Approximative} offre un bon compromis pour des applications générales avec des exigences computationnelles moyennes, mais elle peut nécessiter une intervention manuelle pour ajuster les modèles aux changements de l'environnement.

Les étapes de cette approche sont les suivantes :
\begin{enumerate}
    \item \textbf{Collecte de Traces} : Les agents collectent des séquences de transitions \((s, a, s')\), des observations \(\omega\), et des récompenses \(r\).
    \item \textbf{Estimation des Paramètres} : Les paramètres \((\theta, \phi, \rho)\) sont estimés par l'application de techniques d'identification.
    \item \textbf{Modélisation Révisée} : Les fonctions \((\hat{P}, \hat{O}, \hat{R})\) sont ajustées afin d'obtenir un modèle simulé fidèle à l'environnement.
    \item \textbf{Entraînement et Validation} : Le modèle simulé est utilisé pour entraîner les agents et valider leurs politiques dans un environnement de test avant un éventuel déploiement dans l'environnement réel.
\end{enumerate}


%%%%%%%%%%

\subsection{Phase 2 : Résolution}
Dans cette phase, nous utilisons des algorithmes \textbf{MARL} pour résoudre le problème formalisé dans l'environnement simulé. Les algorithmes comme \textbf{MADDPG} et \textbf{MAPPO} sont utilisés pour apprendre des politiques qui respectent à la fois les objectifs de performance et les contraintes organisationnelles définies par \textbf{$mathcal{M}OISE^+$}. L'intégration de \textbf{$mathcal{M}OISE^+$} dans le processus d'apprentissage permet de restreindre l'espace de recherche des politiques, rendant ainsi le processus plus efficace et garantissant la sûreté du système.

\begin{itemize}
  \item \textbf{Entrées :} Problème formalisé, contraintes organisationnelles.
  \item \textbf{Sorties :} Politiques conjointes optimales pour les agents.
  \item \textbf{Processus :} Utilisation de \textbf{MARL} pour apprendre des politiques tout en respectant les rôles et missions prédéfinis.
\end{itemize}

\subsection{Phase 3 : Analyse}
Une fois les politiques apprises, elles sont analysées à l'aide de \textbf{HEMM}, un algorithme qui infère les rôles et les missions des agents en se basant sur leurs comportements observés. L'analyse permet d'identifier les écarts entre les rôles inférés et les rôles prédéfinis dans \textbf{$mathcal{M}OISE^+$}, offrant ainsi une évaluation de l'adéquation organisationnelle du SMA. Les résultats de cette analyse sont ensuite traduits en plans détaillés, qui seront utilisés pour le développement final du système.

\begin{itemize}
  \item \textbf{Entrées :} Politiques conjointes des agents.
  \item \textbf{Sorties :} Rôles et missions inférés, plan détaillé des comportements.
  \item \textbf{Processus :} Utilisation de \textbf{HEMM} pour analyser les politiques apprises et générer des plans interprétables.
\end{itemize}

\subsection{Phase 4 : Développement et déploiement}
Dans la phase finale, les plans générés sont utilisés pour développer et déployer le SMA dans l'environnement réel. Nous utilisons l'outil \textbf{CybMASDE} pour faciliter l'intégration des politiques apprises dans les agents du système. Avant le déploiement réel, des tests sont effectués dans un environnement émulé pour vérifier que le SMA respecte bien les contraintes de sûreté et les objectifs de performance. Cette phase garantit que les agents fonctionnent comme prévu dans l'environnement réel tout en respectant les rôles et missions spécifiés.

\section{Évaluation}
\label{sec:evaluation}

\subsection{CybMASDE : un environnement de développement pour l'approche}
Pour évaluer notre méthode, nous avons développé un outil nommé \textbf{CybMASDE} qui soutient chaque phase du processus de conception. \textbf{CybMASDE} propose une interface pour modéliser des environnements complexes, définir des objectifs, et spécifier des contraintes organisationnelles. Il intègre également des algorithmes \textbf{MARL} pour l'apprentissage des politiques et des outils d'analyse pour l'inférence des rôles. CybMASDE permet de visualiser les rôles inférés et de faciliter le déploiement des SMA dans des environnements réels après une validation en simulation.

\subsection{Résultats expérimentaux}
Nous avons appliqué SAMMASD à un scénario de gestion de flux de paquets dans un entrepôt, où des robots doivent collaborer pour organiser les flux de marchandises tout en respectant des contraintes de sécurité et d'efficacité. Les résultats montrent que la méthode permet de générer des politiques conjointes efficaces, avec une réduction significative du temps de conception et une amélioration de la robustesse des systèmes déployés.

\begin{itemize}
  \item \textbf{Environnement :} Gestion de flux dans un entrepôt simulé.
  \item \textbf{Objectifs :} Optimiser les flux de marchandises tout en respectant les contraintes de sûreté.
  \item \textbf{Métriques :} Temps de conception, robustesse des politiques, respect des contraintes organisationnelles.
\end{itemize}

Les résultats montrent que les agents entraînés via \textbf{$mathcal{M}OISE^+$MARL} respectent les contraintes organisationnelles définies et optimisent les objectifs de performance, tout en générant des comportements interprétables grâce à l'analyse par \textbf{HEMM}.

\section{Conclusion}
\label{sec:conclusion}
Cet article a présenté une nouvelle méthode, \textbf{SAMMASD}, qui permet de concevoir et déployer des systèmes multi-agents en combinant des spécifications organisationnelles avec des techniques d'apprentissage par renforcement. La méthode s'articule autour de quatre phases clés, allant de la modélisation de l'environnement à l'analyse des politiques apprises et au déploiement des systèmes réels. Les résultats expérimentaux montrent que cette approche réduit le temps de conception et améliore la robustesse des SMA tout en garantissant l'explicabilité des comportements des agents.

Les travaux futurs porteront sur l'extension de \textbf{SAMMASD} à des environnements encore plus complexes, notamment dans des contextes critiques tels que la cybersécurité, et sur l'intégration de nouveaux algorithmes d'apprentissage pour améliorer l'efficacité des politiques apprises.

\bibliographystyle{ACM-Reference-Format}
\renewcommand\refname{}
\bibliography{references}

\end{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
