%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% LaTeX Template for AAMAS-2025 (based on sample-sigconf.tex)
%%% Prepared by the AAMAS-2025 Program Chairs based on the version from AAMAS-2025. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% Start your document with the \documentclass command.


%%% == IMPORTANT ==
%%% Use the first variant below for the final paper (including author information).
%%% Use the second variant below to anonymize your submission (no author information shown).
%%% For further information on anonymity and double-blind reviewing, 
%%% please consult the call for paper information
%%% https://aamas2025.org/index.php/conference/calls/submission-instructions-main-technical-track/

%%%% For anonymized submission, use this
\documentclass[sigconf,anonymous]{aamas} 

%%%% For camera-ready, use this
%\documentclass[sigconf]{aamas}

\usepackage{listings}
% \usepackage{xcolor}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}
 
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}
 
\lstset{style=mystyle}

% --- Tickz
\usepackage{physics}
\usepackage{tikz}
\usepackage{amsmath}
\usepackage{mathdots}
% \usepackage{yhmath}
\usepackage{cancel}
\usepackage{color}
\usepackage{siunitx}
\usepackage{array}
\usepackage{cuted}
\usepackage{multirow}
% \usepackage{amssymb}
\usepackage{gensymb}
\usepackage{tabularx}
\usepackage{extarrows}
\usepackage{booktabs}
\usetikzlibrary{fadings}
\usetikzlibrary{patterns}
\usetikzlibrary{shadows.blur}
\usetikzlibrary{shapes}

% ---------

\usepackage{balance} % for balancing columns on the final page
\usepackage{csquotes}
% \usepackage{cite}
\newcommand{\probP}{I\kern-0.15em P}
\usepackage{etoolbox}
\patchcmd{\thebibliography}{\section*{\refname}}{}{}{}
% \usepackage{amsthm,amssymb,amsfonts}

\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{color}
% \renewcommand\UrlFont{\color{blue}\rmfamily}

\usepackage[inline, shortlabels]{enumitem}
\usepackage{tabularx}
\usepackage{caption}
\usepackage{listings}
\usepackage{titlesec}
\usepackage{ragged2e}
% \usepackage[hyphens]{url}
\usepackage[linesnumbered,ruled,vlined]{algorithm2e}
\usepackage{float}

\usepackage[english]{babel}
\addto\extrasenglish{  
    \def\figureautorefname{Figure}
    \def\tableautorefname{Table}
    \def\algorithmautorefname{Algorithm}
    \def\sectionautorefname{Section}
    \def\subsectionautorefname{Subsection}
    \def\proofoutlineautorefname{Proof Outline}
}

\DeclareMathAlphabet\mathbfcal{OMS}{cmsy}{b}{n}

\newcommand{\supertiny}{\fontsize{1}{2}\selectfont}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% AAMAS-2025 copyright block (do not change!)

\setcopyright{ifaamas}
\acmConference[AAMAS '25]{Proc.\@ of the 24th International Conference
on Autonomous Agents and Multiagent Systems (AAMAS 2025)}{May 19 -- 23, 2025}
{Detroit, Michigan, USA}{A.~El~Fallah~Seghrouchni, Y.~Vorobeychik, S.~Das, A.~Nowe (eds.)}
\copyrightyear{2025}
\acmYear{2025}
\acmDOI{}
\acmPrice{}
\acmISBN{}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% == IMPORTANT ==
%%% Use this command to specify your EasyChair submission number.
%%% In anonymous mode, it will be printed on the first page.

\acmSubmissionID{<<EasyChair submission id>>}

%%% Use this command to specify the title of your paper.

\title[AAMAS-2025 CybMASDE]{A Method for Assisting Multi-agent System Design Using Reinforcement Learning}

%%% Provide names, affiliations, and email addresses for all authors.

\author{Julien Soulé}
\affiliation{
  \institution{Univ. Grenoble Alpes}
  \city{Valence}
  \country{France}}
\email{julien.soule@lcis.grenoble-inp.fr}

\author{Jean-Paul Jamont}
\affiliation{
  \institution{Univ. Grenoble Alpes}
  \city{Valence}
  \country{France}}
\email{jean-paul.jamont@lcis.grenoble-inp.fr}

\author{Michel Occello}
\affiliation{
  \institution{Univ. Grenoble Alpes}
  \city{Valence}
  \country{France}}
\email{michel.occello@lcis.grenoble-inp.fr}

\author{Louis-Marie Traonouez}
\affiliation{
  \institution{Thales Land and Air Systems, BU IAS}
  \city{Rennes}
  \country{France}}
\email{louis-marie.traonouez@thalesgroup.com}

\author{Paul Théron}
\affiliation{
  \institution{AICA IWG}
  \city{La Guillermie}
  \country{France}}
\email{paul.theron@orange.fr}

\begin{abstract}
    Agent-oriented Software Engineering (AOSE) methods typically rely on the designer's expertise to guide the development of a multi-agent system (MAS) that meets specific objectives within a given environment. Recent advancements in Multi-Agent Reinforcement Learning (MARL) suggest a more automated approach to exploring the design space. We introduce the SAMMASD (Semi-Automated MARL-based MAS Design) method, which integrates organizational specifications into MARL for MAS design and deployment. This method consists of four phases. The first phase models the real-world environment, objectives, and additional constraints—such as operational requirements—into a simulation. The second phase leverages multiple MARL algorithms to learn stable policies that achieve these objectives within the defined constraints. The third phase conducts a behavior analysis to infer emergent roles and objectives and to generate detailed "blueprints" for implementation. Finally, the development phase enables the automatic deployment of these policies in real-world environments, following validation in an emulated setting. We demonstrate SAMMASD in a warehouse flow management scenario involving robot agents, illustrating how the method can produce efficient and reliable MAS designs that streamline the design process.
\end{abstract}



%%% The code below was generated by the tool at http://dl.acm.org/ccs.cfm.
%%% Please replace this example with code appropriate for your own paper.


%%% Use this command to specify a few keywords describing your work.
%%% Keywords should be separated by commas.

\keywords{Agent-oriented Software Engineering, Multi-Agent Reinforcement Learning, Assisted-Design, Organizational Models}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% Include any author-defined commands here.
         
% \newcommand{\BibTeX}{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em\TeX}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

%%% The following commands remove the headers in your paper. For final 
%%% papers, these will be inserted during the pagination process.

\pagestyle{fancy}
\fancyhead{}

%%% The next command prints the information defined in the preamble.

\maketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}

Designing multi-agent systems (MAS) that can operate autonomously and efficiently in complex and dynamic environments is a significant challenge within the field of Agent-oriented Software Engineering (AOSE). MAS are widely used across various applications, such as warehouse management, cybersecurity, and other domains where multiple agents must collaborate to achieve shared objectives. An important aspect of MAS design is ensuring that agent behaviors align with organizational requirements, including safety, explainability, and adaptability, while also allowing for efficient and robust operation in uncertain environments.

Traditional AOSE methodologies often rely heavily on expert knowledge to define organizational structures and guide agent behaviors, which can limit their scalability and adaptability to changing environments. Furthermore, while Multi-Agent reinforcement learning (MARL) has shown promise for automating aspects of MAS design, it frequently lacks mechanisms for incorporating explicit organizational constraints, which are essential for ensuring that agent behaviors remain interpretable and safe.

To address these limitations, we introduce a novel method, the \textbf{SAMMASD} (Semi-Automated MARL-based MAS Design) method, which integrates organizational specifications into MARL for the automated design and deployment of MAS. SAMMASD leverages the \textbf{MOISE+MARL} framework, which combines the organizational modeling capabilities of $\mathcal{M}OISE^+$ with the learning power of MARL. This integration allows for the enforcement of organizational constraints throughout the learning process, leading to the development of policies that are both effective and compliant with specified roles and objectives.

SAMMASD consists of four main phases:
\begin{enumerate*}[label=\roman*,itemjoin={; \quad}]
  \item modeling the target environment, objectives, and organizational constraints to create a simulation that faithfully reflects the system's dynamics
  \item training agent policies using a variety of MARL algorithms to optimize for defined objectives
  \item analyzing agent behaviors to infer roles, missions, and organizational structures, ensuring compliance with the \textbf{MOISE+MARL} specifications
  \item deploying the MAS into real-world environments, with steps for ensuring safety and adaptability.
\end{enumerate*}

In this paper, we apply the SAMMASD method within a simulated warehouse flow management scenario, utilizing the \textbf{CybMASDE} (Cyber Multi-agent System Development Environment) tool to facilitate each phase. We evaluate the method's effectiveness based on output compliance, ease of use, automation, verification, and adaptability. Our findings demonstrate that SAMMASD offers a comprehensive approach to MAS design that bridges the gap between traditional AOSE methods and the emerging capabilities of MARL, while providing pathways for further refinement in real-time adaptability and deployment automation.

The remainder of this paper is organized as follows: \autoref{sec:related_works} presents related works in AOSE and MARL methodologies. \autoref{sec:sammasd_presentation} details the SAMMASD method and the MOISE+MARL framework. \autoref{sec:evaluation} discusses the experimental results from the warehouse management scenario. Finally, \autoref{sec:conclusion_perspectives} concludes with insights on the contributions of SAMMASD and directions for future work.

\section{Related Works}
\label{sec:related_works}

The AOSE field aims to offer structured methodologies for designing, developing, and managing MAS. These methods typically integrate role-based specifications, organizational models, and structured interaction patterns to guide agent behaviors within complex systems. While these methodologies provide robust frameworks, they often rely on user input, limiting automation.

Among the prominent AOSE methodologies, GAIA \cite{gaia1998} organizes MAS design around roles and interaction protocols, requiring some manual setup to define these elements. ADELFE \cite{adelfe2002} supports MAS design in dynamic environments, offering adaptability but still requiring user-defined specifications to configure adaptive behaviors. INGENIAS \cite{ingenias2004}, another key approach, includes modeling tools to develop MAS with explicit role-based structures, yet it also depends on user involvement to align agent behaviors with organizational objectives.

The KB-ORG methodology \cite{kborg2001}, which leverages predefined organizational templates, allows for more automation by reducing the need for custom role definitions, though it still requires configuration when adapting to specific organizational structures. Overall, these AOSE methodologies necessitate some level of manual configuration, making full automation unachievable. Despite their robust structures, they cannot fully support adaptive and scalable MAS deployment without user intervention.

However, recent research in MARL may provide insights into a more automated MAS design under various constraints. While these studies do not constitute complete methodologies, they introduce innovative concepts that align with our goal. For instance, Hammar's work, \emph{"An Online Framework for Adapting Security Policies in Dynamic IT Environments"} \cite{hammar2019}, applies MARL to adapt security policies dynamically, demonstrating near-complete automation in changing environments. This aligns with our goal of reducing manual involvement while accommodating evolving organizational requirements.

Studies like those by Thomas \cite{thomas2023} and Iocchi \cite{iocchi2023} address the use of MARL to enhance MAS adaptability in industrial contexts. They emphasize automated learning frameworks that minimize manual tuning and allow MAS to adjust to shifting objectives and constraints. These studies also underscore the need for role-based organizational constraints to ensure agents adhere to safety and operational guidelines. The article by Ning and Xie \cite{ning2024} provides a broad survey of MARL applications, offering insights into recent MARL strategies for addressing challenges like scalability, coordination, safety, and interpretability. These works fit within our aim to build an adaptive, safe, and interpretable MAS with reduced manual effort.

Furthermore, one can mention works in Constrained Policy Optimization \cite{zhao2024} to automate adherence to organizational roles, reducing manual effort. Safe Exploration techniques \cite{melcer2024} help agents learn safely in dynamic environments, ensuring adaptability without sacrificing safety. Shielding methods, as demonstrated by ElSayed-Aly et al. \cite{elsayed2021}, further automate the enforcement of safety standards by preventing unsafe actions during training and deployment. Together, these techniques advance our aim to automate MAS design while meeting safety and organizational constraints.

Another critical element in MAS design is transparency in agent behavior. Research by Krajna et al. \cite{krajna2022} highlights the importance of explainability in MARL systems, proposing frameworks that embed transparency into agent decision-making. This ensures MAS not only meets safety standards but also offers insights into agent behavior, which is crucial for environments requiring adherence to strict safety protocols and the ability to explain decisions to stakeholders.

In conclusion, while traditional AOSE methodologies lay the groundwork for MAS design, recent MARL research introduces features like automation, adaptability, safety, and explainability, which are central to developing our method.


\section{Proposed Organizational Design Approach for MAS}
\label{sec:sammasd_presentation}

The core of the SAMMASD method is to consider the design of a MAS as a constrained optimization problem. The variable to be optimized is the \textbf{joint policy}, which represents the internal logic of the agents in determining their next actions. The goal is to maximize the cumulative reward under the given constraints. SAMMASD includes organizational specifications in MARL as additional constraints, allowing the user to control not only individual agent behaviors but also to manage the entire MAS through these specifications. We formalize this approach by proposing the MOISE+MARL framework and then using it to construct the four phases of our method.

\subsection{\textbf{MOISE+MARL Framework}}

The objective is to guide agent learning while adhering to organizational constraints in the form of roles and objectives, while ensuring sufficient efficiency of the learned policies within their environment. We formalize this concept by proposing the MOISE+MARL framework, which combines the formalism of a Decentralized Partially Observable Markov Decision Process (Dec-POMDP) with the $\mathcal{M}OISE^+$ organizational model.

\textbf{Dec-POMDP} \quad defined by a tuple $D = \langle N, S, A, P, R, \Omega, O, \gamma \rangle$, where:
%
\begin{itemize*}[label={},itemjoin={; }]
    \item $\mathcal{A}$ is the set of $N \in \mathbb{N}$ agents
    \item $S$, the set of possible states of the environment (including agents)
    \item $A = \times_{i=1}^N A_i $, the set of possible actions for each agent $i$
    \item $T: S \times A^N \to S$, the state transition function defining the next state $s'$ given a state $s$ under joint action $a$
    \item $R: S \times A^N \times  S \to \mathbb{R}$, the reward function assigning a reward $r$ to each transition $(s, a, s')$
    \item $\Omega = \times_{i=1}^N \Omega_i $ is the set of possible observations for each agent $i$
    \item $O: S \times A \to \Omega$ is the observation function defining the next observation $\omega$ of an agent when taking action $a$ in state $s$
    \item $\gamma \in [0, 1] $ is the discount factor.
\end{itemize*}

$\mathbf{\mathcal{M}OISE^+}$ \quad defined by a tuple $\mathcal{OS} = \langle \mathcal{SS}, \mathcal{FS}, \mathcal{DS} \rangle$. Here, we present a minimal version of the $\mathcal{M}OISE^+$ formalism:
\begin{itemize*}[label={},itemjoin={; }]
    \item $\mathcal{SS} = \langle \mathcal{R} \rangle$, structural specifications defined by a set of roles $\mathcal{R}$
    \item $\mathcal{FS} = \langle \mathcal{G}, \mathcal{M}, mo \rangle$, functional specifications are a set of objectives $\mathcal{G}$, a set of missions $\mathcal{M}$, and a function $mo: \mathcal{M} \to \mathcal{P}(\mathcal{G} \times [0,1])$ associating a mission $m \in \mathcal{M}$ with a set of objectives, each weighted by a value $\{(g_1,w_0), (g_2,w_1)\dots\}, w_i \in [0,1], g_i \in \mathcal{G}$
    \item $\mathcal{DS} = \mathcal{R} \times \mathcal{M} \times T_c \times \{0,1\}$, deontic specifications as a set of quadruples $(\rho_a, m, \allowbreak t_c, p)$, meaning that an agent playing the role $\rho_a \in \mathcal{R}$ is permitted (if $p = 0$) or obligated (if $p = 1$) to engage in mission $m \in \mathcal{M}, \ m \subseteq \mathcal{G}$ for a given time constraint $t_c \in \mathcal{TC}, t_c = \mathcal{P}(N)$ specifying a period during which permission/obligation is valid.
\end{itemize*}

The \textbf{Constraint Guides} \quad are three new relations introduced to describe the logic of the roles and objectives of $\mathcal{M}OISE^+$ in the Dec-POMDP formalism:

\begin{itemize}
    \item \textbf{Role Action Guide} (RAG) \quad $rag: H \times \Omega \rightarrow \mathcal{P}(A \times \mathbb{R})$, the relation that models a role as a set of rules which, for each pair consisting of a history $h \in H$ and an observation received by the agent $\omega \in \Omega$, associates expected actions $A \in \mathcal{P}(A)$ each associated with a constraint hardness $ch \in [0,1]$ ($ch = 1$ by default). By restricting the choice of the next action among those authorized, the agent is forced to adhere to the expected behavior of the role.
    \item \textbf{Role Reward Guide} (RRG) \quad $rrg: H \times \Omega \times A \to \mathbb{R} = \{r_m \text{ if } a \notin A_\omega \text{, } rag(h, \omega) = A_\omega \times \mathbb{R} \text{, } h \in H; \text{ else } 0\}$, the relation that models a role by adding a penalty $r_m$ to the global reward if the last action chosen by the agent $a \in A$ is not authorized. This is intended to encourage the agent to adhere to the expected behavior of a role.
    \item \textbf{Goal Reward Guide} (GRG) \quad $grg: H \rightarrow \mathbb{R}$, the relation that models a goal as a soft constraint by adding a bonus $r_b \in \mathbb{R}$ to the global reward if the agent's history $h \in H$ contains a characteristic sub-sequence $h_g \in H_g$ of the goal, encouraging the agent to find a way to reach it.
\end{itemize}

% \begin{gather*}
%   (1) \quad V^\pi(s_t) = \hspace{-0.4cm} \sum_{\textcolor{red}{ \substack{a_{t} \in rac(A)}
%   }}{\hspace{-0.4cm} \pi(a_{t} | \omega_t)} \sum_{s_{t+1} \in S}{T(s_{t+1} | s_t, a_{t})[R(s_t,a_{t},s_{t+1})} \\
%    + \ \hspace{-0.cm} \allowbreak
%   \textcolor{blue}{rrc(h_{t+1})} + \textcolor{red}{mrc(\omega_t, a_t)} + V^{\pi}(s_{t+1})]
% \end{gather*}

\begin{figure*}[h!]
  \label{eq:single_value_function}
  \raggedright
  \textbf{\quad Definition 1: Value function adapted to "Constraint Guides" in AEC.}
  \begin{gather*}
    \text{\quad \quad} V^{\pi^j}(s_t) = \hspace{-0.75cm} \sum_{\textcolor{red}{ \substack{a_{t} \in A \text{ if } rn() < ch_{t}, \\ 
    a_{t} \in A_{t} \text{ else}}
    }}{\hspace{-0.7cm} \pi_i(a_{t} | \omega_t)} \sum_{s_{t+1} \in S}{\hspace{-0.1cm} T(s_{t+1} | s_t, a_{t})[R(s_t,a_{t},s_{t+1}) + \hspace{-0.1cm} \textcolor{blue}{ \sum_{m \in \mathcal{M}_i}{ \hspace{-0.1cm} v_m(t) \frac{grg_m(h_{t+1})}{1 - p + \epsilon} } } + \textcolor{red}{ch_t \times rrg(\omega_t,a_{t+1})} + V^{\pi^j_{i+1 \ mod \ N}}(s_{t+1})]}
  \end{gather*}  
  %
  \textcolor{red}{\[\text{With } rag(h_t, \omega_t) = A_{t} \times \mathbb{R} \text{, } (a_t, ch_{t}) \in A_{t} \times \mathbb{R} \text{ ; } \text{ and } rn: \emptyset \to [0,1[ \text{, a uniform random function}\]}
  %
  \vspace{-0.5cm}
  \textcolor{blue}{
  \begin{gather*}
  \text{With } \omega_t = O(\omega_t | s_t, a_t) \text{ ; } h_t = \{h_0 = \langle \rangle, h_{t+1} = \langle h_t, (\omega_{t+1}, a_{t+1}) \rangle \} \text{ ; } grg_m(h) = \hspace{-0.8cm} \sum_{(grg_i,w_i) \in mo(m)}{\hspace{-0.8cm} w_i \times grg_i(h)} \text{ ; } \epsilon \in \mathbb{R}_{>0} \text{ ; }
  \end{gather*}
  }
  \vspace{-0.75cm}
  \textcolor{blue}{
  \begin{gather*}
  v_m(t) = \{ 1 \text{ if } t \in t_c \text{ ; else } 0 \} \text{ ; } \text{ and } \mathcal{M}_i = \{m_j | (ar(i),m_j,t_c,p) \in \mathcal{M}\}
  \end{gather*}
  }
  \vspace{-0.6cm}
  \end{figure*}

Finally, to link the organizational specifications of $\mathcal{M}OISE^+$ with the "Constraint Guides" and agents, we introduce the following \textbf{Linkers}:
\begin{itemize}
    \item \textbf{Agent to Role} (AR) \quad $ar: \mathcal{A} \to \mathcal{R}$, the bijective relation linking an agent to a role;
    \item \textbf{Role to Constraint Guide} (RCG) \quad $rcg: \mathcal{R} \rightarrow rag \cup rrg$, the relation associating each $\mathcal{M}OISE^+$ role to a $rag$ or $rrg$ relation, forcing/encouraging the agent to follow the expected actions for the role $\rho \in \mathcal{R}$;
    \item \textbf{Goal to Constraint Guide} (GCG) \quad $gcg: \mathcal{G} \rightarrow grg$, the relation linking goals to trf relations, representing goals as rewards in MARL.
\end{itemize}

MOISE+MARL is thus defined by the tuple $MM = \langle D, \mathcal{OS}, ar, rcg, \allowbreak gcg, rag, rrg, grg\rangle$. Solving the problem described by MOISE+MARL involves finding a joint policy $\pi^{j}: \Omega^{N} \to A^{N} = \{\pi^j_0,\pi^j_1\dots\pi^j_N\}$ that maximizes the value function $V^{\pi^{j}}$ (or reaches a minimum threshold), which represents the expected cumulative reward starting from an initial state $s \in S$ and following the joint policy $\pi^{j}$, applying successive joint actions $a^{j} \in A^N$ under additional "Constraint Guides" (including "Linkers"). Agents then each follow a trajectory (also called history) $h \in H, h = \langle(o_0,a_0), (o_1,a_1)\dots\rangle$. The value function to maximize (or reach a minimum threshold) is described in cases where agents act sequentially (Agent Environment Cycle - AEC mode) in \hyperref[eq:single_value_function]{Definition 1}, adapting its definition for roles (in red) and missions (in blue), impacting the action space and reward.

At any time $t \in \mathbb{N}$ (initially $t = 0$), the agent $i = t \ mod \ N$ is constrained to a role $\rho = ar(i)$. For each temporally valid deontic specification $d_i = (\rho,m_i,t_{c_i},p_i)$ (such that $v_{m_i}(t) = t \in t_{c_i}$), the agent is permitted (if $p_i = 0$) or obligated (if $p_i = 1$) to engage in mission $m_i \in \mathcal{M}, \mathcal{G}_{m_i} = mo(m_i), n \in \mathbb{N}$.
%
First, based on the received observation $\omega_t$, the agent must choose an action either: within the expected actions of the role $A_t$ if a random value is below the role constraint hardness $ch_t$; or within the set of all actions $A$ otherwise. If $ch_t = 1$, the role is strongly constrained for the agent and weakly otherwise.
%
Then, the action is applied to the current state $s_t$ to transition to the next state $s_{t+1}$, generate the next observation $\omega_{t+1}$, and yield a reward. The reward is the sum of the global reward with penalties and bonuses obtained from the organizational specifications: \quad i) the sum of the bonuses for objectives associated with each temporally valid mission (via "Goal Reward Guides"), weighted by the associated value ($\frac{1}{1-p+\epsilon}$); \quad ii) the penalty associated with the role (via "Role Reward Guides") weighted by the role constraint hardness.

Finally, the cumulative reward calculation continues in the next state $s_{t+1} \in S$ with the next agent $(i+1) \ mod \ N$.

\subsection{General Overview}

Our SAMMASD method is built around four main phases: (1) modeling the environment, objective, and organizational constraints according to the proposed framework, (2) learning policies using various MARL algorithms, (3) analyzing behaviors and inferring organizational specifications with a proposed method, and (4) developing and deploying the MAS. This approach guides the agent learning process while enforcing strict organizational constraints, ensuring the efficiency of the learned policies. The lifecycle of a SAMMASD-designed MAS is illustrated in \autoref{fig:cycle}.

% Formal description of the phases


\begin{figure}[h!]
  \centering
  \input{figures/cycle.tex}
  \caption{Lifecycle of a MAS designed with SAMMASD: i) Users start by modeling the environment, global goal and extra requirements; \quad ii) Then, they launch the training of agents with MARL techniques; \quad iii) An post-training analysis is performed to get "blueprint" of the trained MAS; \quad v) These blueprint can be used to semi-automatically deploy effective agents in the environment's effectors}
  \label{fig:cycle}
\end{figure}

\subsection{Phase 1: Modeling}

The modeling stage aims to create a simulated model that accurately captures the dynamics and constraints of the target environment, defines organizational specifications and objectives. The real target environment must necessarily include effectors where agents capable of observing and acting in the environment will be deployed. This model will serve as a basis for training agents in a controlled environment. This step is critical to ensure that agents learn in a simulation that is faithful to the reality of the target system in a safe and solution-seeking setting.

The required inputs are:
\begin{itemize}
    \item \textbf{Environment}: An "emulated" copy of the real environment or the target environment itself if possible.
    \item \textbf{Problem description}: A detailed description of the objectives to be achieved by the agents, i.e., the desired states.
    \item \textbf{Additional constraints}: Specific requirements to be met, which may include standards, organizational rules, or safety constraints.
\end{itemize}

Once the elements are collected, modeling follows these steps: 


\paragraph{\textbf{1) Modeling the simulated environment}} \quad

\noindent We define environment modeling as the development of an approximated observation function $\hat{O}: S \times A \to \Omega, \hat{O}(s_t,a_t) = \omega_{t+1}$ such that $|\hat{O} \cap O| \geq f$, where $f \in \mathbb{R}$ is fidelity to the real environment described by $O$.
The designer must faithfully reproduce the logic of the environment that leads the agent to receive observations at each state transition. 
For that purpose, several methods may be envisioned.

\textbf{System Identification} relies on precise mathematical modeling that is based on the observation and adjustment of parameters according to collected data. This generally involves significant human intervention, particularly for environments where dynamics are known and representable by equations. In contrast, Imitation Learning does not necessarily require a mathematical model, but rather observation of behavior for the model to reproduce it. This method is effective at capturing complex behaviors but may lack precision and require numerous examples to generate robust results.

% \begin{table*}[h!]
%   \centering
%   \caption{Comparison of automated modeling methods based on different criteria}
%   \begin{tabular}{p{4cm}cccc}
%   \textbf{Criteria}                   & \textbf{System Identification} & \textbf{Imitation Learning} & \textbf{Surrogate Modeling} & \textbf{Digital Twin} \\
%   \hline
%   Manual Intervention    & Medium                         & Low                          & Medium                       & High                 \\
%   Mathematical Modeling & High                         & Low                       & High                       & Medium                \\
%   General Applicability             & Medium                       & High                       & Medium                     & Medium              \\
%   Requires Emulated Version       & No                           & No                          & No                         & Yes                  \\
%   Automated Update Capability           & Medium                       & High                       & Medium                     & High               \\
%   Complete Automation            & Medium                       & Medium                      & Medium                     & Variable             \\
%   Real-Time Adaptability        & Low                        & Low                       & Medium                     & High               \\
%   Fidelity to Real System & Medium                         & Low                          & Medium                       & High                \\
%   Data Requirements               & High                       & Medium                     & High                     & High              \\
%   Computational Cost                & High                         & Medium                        & Medium                       & High                \\
%   \end{tabular}
%   \label{tab:comparaison-methodes}
%   \end{table*}
  

On the other hand, \textbf{Surrogate Modeling} and \textbf{Digital Twins} are suited to contexts where system complexity or automated update requirements are high. Surrogate Modeling generates simplified representations using statistical or machine learning techniques, allowing optimization of complex systems with lower computational costs. Although flexible, it may be less precise than a model based on exact equations. A Digital Twin creates a real-time synchronized digital replica of the real system, enabling continuous updates and enhanced fidelity to the target system. However, creating a digital twin is more costly in terms of data collection and resources and often requires an emulated version of the target system. In conclusion, method choice should consider needs in terms of precision, flexibility, and automation to select the most appropriate solution for the intended application.

In our method, we favor \textbf{Imitation Learning} techniques as many of these techniques do not require human intervention, are applicable to a majority of environments, and can capture environmental complexity after sufficient training. Although these techniques may lack readability, particularly those based on neural network architectures, we only consider fidelity as the main criterion of our method.

In this context, the suggested method proceeds as follows: observer agents (possibly human) are deployed in effectors' locations to collect traces (also called trajectories) by exploring the real or a secure copy of the environment. These traces are collected and used to train a model based on a neural network architecture. The Recurrent Neural Network (RNN) architecture is particularly suitable for predicting the next observation, as it is optimized for learning from sequences.

\paragraph{\textbf{2) Reward Function Formulation}} \quad

\noindent We define reward function formulation as follows: \quad i) finding a clear description of the different desired states and their descriptions as (sub-)trajectories, in general, to best characterize the overall objective; \quad ii) finding a way to measure only the distance between the current state and these desired trajectories. Therefore, the user must establish a reward function logic that they believe is best suited to achieving the set objectives.

Finding a clear description of the desired states is not always straightforward depending on the environment. Although assumed in our method, this search may be supported by Inverse reinforcement learning (IRL) techniques, which consist of learning from observed behaviors to deduce the ultimate goal. We recommend viewing an objective as independent of the agents. Therefore, the objective should not seek to directly influence agents to adopt expected behavior (this is more the role's view). Once established, these desired states can be presented as a set of (sub-)trajectories.

Regarding finding a way to measure the distance between the current state and the set of desired trajectories, we leave it to the user to define the best way to measure this distance. However, we suggest a general approach using similarity measures based on the longest common sequence between the (sub-)trajectory characterizing the objective and the current agents' history. This measure is simple and relevant when the sequence associated with a trajectory is continuous (as in our experiments). Nevertheless, it is less applicable when the sequence associated with the objective is discontinuous. Other measures from time series analysis are also conceivable.

\paragraph{\textbf{3) Formulation of MOISE+MARL Specifications}} \quad

\noindent We define the formulation of organizational specifications in MOISE+MARL as follows: \quad i) defining roles ($\mathcal{R}$), objectives ($\mathcal{G}$), and missions ($\mathcal{M}, mo$); ii) associating each agent with a respective role ($ar$); \quad iii) associating each role with "Constraint Guides" RAG and RGG defining their specific logic; ii) associating each sub-objective with their GCG "Constraint Guide" defining their logic ($gcg$).

Considering roles, objectives, and missions as simple labels, their definition is assumed. The challenge lies in defining the corresponding "Constraint Guides".

Defining a RAG, RRG, or GRG relation requires defining a potentially large number of histories, possibly partially redundant. Therefore, an extensive definition of a set of histories can be tedious. Further, the underlying idea of "Constraint Guides" is that when an agent follows a trajectory, it can be analyzed as part of a predefined set. For example, a RAG relation can be seen as determining the next action depending on whether the trajectory belongs to a given set and the new observation received. We suggest defining these relations comprehensively, allowing designers to define the logic to determine if a history belongs to a predefined set $H_g$ formalized as the relation $b_g: H \to \{0,1\}$.

We also propose a pattern of history inspired by Natural Language Processing, denoted $p \in P$, as a way to define a set of histories comprehensively. A history pattern $p \in P$ is defined as follows: $p$ is: either a "leaf sequence" denoted as a couple of history-cardinality $s_l = (h, \{c_min,c_max\})$ (where $h \in H, c_{min} \in \mathbb{N}, c_{max} \in \mathbb{N} \cup "*")$; or a "node sequence" denoted as a couple of a tuple of concrete sequences and cardinality $s_n = (\langle s_{l_1}, s_{l_1}\dots \rangle, \{c_min,c_max\})$. For example, the pattern $p = \allowbreak "[o_1,a_1,[o_2,a_2](0,2)](1,*)"$ can be formalized as the node sequence $\allowbreak \langle ((o_1,a_1),(1,1)), ((o_2,a_2),(0,2))\rangle(1,"*")$, indicating the set of histories $H_p$ containing at least once the sub-sequence consisting of a first pair $(o_1,a_1)$ and then at most two repetitions of the pair $(o_2,a_2)$.
The relation $b_g$ then becomes $b_g(h) = m(p_g,h), \text{ with } m: P \times H \to \{0,1\}$ indicating if a history $h \in H$ matches a history pattern $p \in P$ describing a set of history $H_g$.

\

At the end of this phase, the output is the MOISE+MARL model comprising:
\begin{itemize}
    \item \textbf{A model of the environment} as a function approximating the observation function of the real environment;
    \item \textbf{A reward function} indicating objectively how close or far the agents are from the global objective;
    \item \textbf{A defined MOISE+MARL tuple} including roles, objectives, missions, and their associated "Constraint Guides."
\end{itemize}

\subsection{Phase 2: Solving}

We consider solving the previously formulated MOISE+MARL problem as finding a policy $\pi^s \in \Pi$: \quad i) achieving a cumulative reward at least above a given threshold $s \in \mathbb{R}$ such that $V^{\pi^s} \geq s$; \quad ii) for the collected rewards set $R = \{r_1, r_2\dots\}$ the variance of the reward must be below a given stability threshold $\sigma_{max}^2$ such that $\sigma_{max}^2 \leq \sigma_{max}^2$. The choice of $s$ and $\sigma_max$ is generally determined empirically depending on the environment, objectives, and additional constraints. Although roles help partially define each agent's policies, MARL learning is guided by objectives to optimally complete these policies.

Although no procedure exists to find at least one solution entirely automatically, the method recommends trying different MARL algorithms to benefit from their various properties suited to the environment, global objective, and given additional constraints.

\textbf{Value-based} algorithms such as Deep Q-Network (DQN) or Q-mix are generally suitable for \textbf{Independent Learning (IL)} scenarios, where each agent learns individually without explicit coordination. Although this approach is easy to implement, it may result in oscillations in estimated values, limiting overall stability.

\textbf{Policy-based} algorithms like MAPPO (Multi-Agent Proximal Policy Optimization) leverage \textbf{Centralized Learning Decentralized Execution (CLDE)} strategies. This approach allows agents to learn in a coordinated manner using global information while ensuring decentralized execution for better adaptability. They are well-suited for reducing variance, though convergence may be slower due to stability constraints.

\textbf{Actor-critic} algorithms such as MADDPG (Multi-Agent Deep Deterministic Policy Gradient) are flexible and compatible with \textbf{centralized or decentralized} approaches, depending on the required coordination. Using CLDE, MADDPG can benefit from centralization during learning while maintaining independence during execution, combining high performance with increased stability.

\textbf{Model-based} algorithms such as DynaQ or DynaQ+ can be implemented in a \textbf{centralized} manner or within \textbf{CLDE} learning, allowing planning based on a shared model. This improves performance and stability, but efficiency depends on the model's accuracy.

Based on our experiments, the MAPPO and MADDPG algorithms or other "actor-critic" algorithms provide satisfactory results for most environments, objectives, and additional constraints with minimal intervention for hyper-parameter selection. When the real environment is simple enough and has low dynamics to be accurately captured by an environment model, solving with the DynaQ+ algorithm is also quite efficient and stable.

\

At the end of this phase, the output is a \textbf{joint policy satisfying the resolution requirements} in terms of performance and stability.

\subsection{Phase 3: Analysis}

We consider analyzing the previously obtained joint policy as determining MOISE+MARL organizational specifications from the observed behaviors of the agents with this policy. This phase is formalized as the relation $e: \mathcal{P}(H^{j}) \to MM$, which associates a set of joint histories with a set of MOISE+MARL specifications.

In our method, we propose an evaluation method called \textbf{History-based Evaluation in MOISE+MARL} (HEMM). This method uses unsupervised learning techniques to generalize roles and missions from the set of observed behaviors over multiple test episodes. By measuring the gap between inferred abstract organizational specifications and actual behaviors, we can also quantify how well a policy conforms to the inferred organizational specifications.

HEMM is based on proposed definitions for each $\mathcal{M}OISE^+$ organizational specification regarding joint histories or other organizational specifications, using specific unsupervised learning techniques to infer them progressively.

\paragraph{\textbf{1) Inferring roles and their inheritance}}

We introduce that a role $\rho$ is defined as a policy whose associated agents' histories all contain a common discontinuous sequence. We introduce that a role $\rho_2$ inherits from $\rho_1$ if the common discontinuous sequence of histories associated with $\rho_2$ is also contained within that of $\rho_1$.
Based on these definitions, HEMM uses a "hierarchical clustering" technique to find the longest common discontinuous sequences among agent histories. The results can be represented as a dendrogram. This allows inferring roles and inheritance relationships, their respective relationships with histories, as well as current agents.

\paragraph{\textbf{2) Inferring possible organizations}}

We introduce that an organization is linked to a unique set of all instantiable roles sharing closely similar inheritance relationships. Indeed, considering two trained joint policies $H_{joint,i,s,1}$ and $H_{joint,i,s,2}$, although both achieve an objective relying on roles $\mathcal{R}_{ss,1}$ and $\mathcal{R}_{ss,2}$, these roles may be very distant from each other. For example, their roles may not use the same distribution of responsibilities.
HEMM uses a K-means algorithm to obtain $q$ clusters of vectors $\mathcal{IR}_{i}$, considered as organizations. Roles within the same cluster share the K-means centroid inheritance relationships $\mathcal{IR}_j$. Indeed, they represent general roles adopted by agents within the same organization across similar joint histories.
For the following steps, only one chosen organization and its associated joint histories are considered.

\paragraph{\textbf{3) Inferring objectives, plans, and missions}}

We introduce that a sub-objective/objective is a set of common states reached by following the histories of successful agents.
For each joint history, HEMM calculates the state transition graph, which is then merged into a general graph. By measuring the distance between two vectorized states with K-means, we can find trajectory clusters that some agents may follow. Then, we sample some sets of states for each trajectory as objectives. For example, we can select the narrowest set of states where agents seem to collectively transition at a given time to reach their goal. Otherwise, balanced sampling on low variance trajectories could be performed. Knowing which trajectory an objective belongs to, HEMM infers plans based solely on choices and sequences.

This allows for obtaining goals and plans at the global state level, but these objectives can be effectively distributed into specific goals for each subgroup and agent. To do this, HEMM follows the same process by replacing states with observations of agents in the same subgroup for subgroups and agent observations for agents themselves.

We introduce that a mission is the set of sub-objectives that one or more agents are accomplishing.
Knowing the shared objectives achieved by the agents, HEMM determines representative objective sets as missions.

\paragraph{\textbf{4) Inferring obligations and permissions}}

We introduce that an obligation is when an agent playing the role $\rho$ fulfills the objectives of a mission and no others during certain time constraints, while permission is when the agent playing the role $\rho$ may fulfill other objectives during specific time constraints.
HEMM determines which agents are associated with which mission and whether they are restricted to certain missions, making them obligations, or if they have permission.

\

The K-mean and hierarchical clustering techniques require manual configuration to obtain roles and goals, avoiding introducing perturbations that could lead to determining false organizational specifications. Despite this, the method recommends thoroughly understanding the obtained roles and objectives to manually identify and remove any remaining perturbations. The refined MOISE+MARL specifications can then be used as "blueprints."

For each role in the "blueprint," we seek to represent the common policy of agents associated with that role as a set of decision trees. To do this, we adapt the corresponding $rag$ relation by determining a history pattern tree, where a node is a pattern-observation pair and the edges are the expected actions. These trees describe agents' policies comprehensively, making it easier to refine and adjust behavior rules.

\

At the end of this phase, the output is the set of refined MOISE+MARL specifications.

\subsection{Phase 4: Transfer}

The final phase aims to use the generated "blueprints" to develop and deploy a MAS on the real target environment. Semi-manual development is advised to ensure sufficient understanding to control and guarantee safety assurances.

The method suggests a procedure for partially automating the development and deployment of the MAS from the "blueprints" in a secure manner by following these steps:

\paragraph{\textbf{1) Transfer to Emulated Environment}}

Before real deployment, tests are conducted in an emulated environment to ensure that the MAS complies with safety constraints and performance requirements. To prepare for automatic deployment, we suggest that the effectors have planned deployment locations (bootstrap). In these deployment locations, we deploy daemon processes capable of receiving different policy types, including history pattern trees. Copying these policies into daemon processes can be automated or manual.

Furthermore, an automated copy process could be advantageous if all SAMMASD method steps can be pipelined, making agents' resulting policies adaptable to environmental changes, requirements, or objectives. SAMMASD then becomes an "online" creation process.

\paragraph{\textbf{2) Evaluation on Emulated Environment}}

After deployment, this step aims to ensure that agents function as specified in the Analysis stage in the emulated environment and that agents can reach their objectives while meeting additional requirements. If not, the method requires reviewing agent policies by changing history pattern decision trees in particular.

\paragraph{\textbf{3) Transfer to Real Target Environment}}

Once validated in the emulated environment, the verified policies are copied into the real environment's effectors. The method then recommends that designers ensure agents function correctly to achieve the objective and meet additional requirements, especially if the environment has changed since the Modeling stage, potentially rendering agents unsuitable. Otherwise, the method should be restarted from the Modeling or directly changing policies manually.

\section{Evaluation}
\label{sec:evaluation}

We developed a tool that we propose to facilitate the implementation of the SAMMASD method through a warehouse flow management scenario. Then, we present and discuss results for this scenario.

\subsection{the CybMASDE tool: A Development Environment for the Approach}

To evaluate our method, we propose an environment called “Warehouse Flow Management” (WFM), a grid environment that represents robots that must cooperate in a manufacturing warehouse. We choose to consider a simulated WFM environment as if it were the real environment. This environment is represented in \autoref{fig:warehouse}.
Using a simulation simplifies and verifies the operating principle by reducing environmental complexity.

To support the SAMMASD method, we have developed a tool named \textbf{Cyber Multi-agent System Development Environment} (CybMASDE), which provides a comprehensive environment for modeling, training, and deploying multi-agent systems. CybMASDE integrates several components, including the PettingZoo~\cite{Terry2021} which is a library that offers a standard API simplifying the development of multi-agent environments and facilitates the use of MARL algorithms. CybMASDE uses the MARLlib~\cite{hu2022marllib} library which offers a wide range of state-of-the-art MARL algorithms and fine-tuned policy models for various environments. It also enables hyper-parameter optimization (HPO) of multi-agent reinforcement learning (MARL) algorithms to adapt to new environments. CybMASDE also uses the Tensorflow library to model real environment into a simulated model. CybMASDE includes both a full-featured API for advanced usage and a basic graphical interface for quick access to essential functions.

% Based on this given environment, we will use the the CybMASDE tool tool to apply the different phases of the method, evaluating them against the following four criteria:

% \begin{itemize}
%   \item \textbf{Output Compliance}: The output of each phase and the steps contained within is consistent with the description following specified instructions.
%   \item \textbf{Ease of Use}: Each phase's application does not present particular difficulties.
%   \item \textbf{Automation}: Each phase's application requires little or no manual intervention.
%   \item \textbf{Verification}: The resulting MAS can be verified as meeting performance, explainability, and safety requirements.
%   \item \textbf{Adaptation}: The method should allow the MAS to adapt to environmental, objective, or additional constraint changes.
% \end{itemize}

\begin{figure}
  \centering
  \input{figures/warehouse.tex}
  \caption{An illustrative view of the "Warehouse Flow Management" environment: agents can move up, down, left, and right, pick up and drop a product in a pick/drop area if they are close enough. Agents must coordinate to: i) pick up primary products from the input conveyor pick/drop areas (blue zones); ii) drop them in the crafting machine pick/drop areas (brown zones), which transform primary products into a single secondary product according to the crafting schema; iii) retrieve the created secondary products to drop them in the output conveyor pick/drop areas (green zones)}
  \label{fig:warehouse}
\end{figure}


\paragraph{Phase 1: Modeling}

In the initial phase, users define the environment dynamics, objectives, and constraints. the CybMASDE tool facilitates this process through the PettingZoo and Gymnasium APIs, which offer a wide range of pre-built environments for rapid prototyping. Users can choose from existing environments or create custom ones that accurately reflect the desired operational scenarios. the CybMASDE tool's graphical interface simplifies the process of defining environment-specific parameters, such as state and action spaces, reward functions, and observational models. For more complex environments, users can leverage the API to directly code or integrate specialized environments into the CybMASDE tool.

Furthermore, the CybMASDE tool provides an automated modeling of the environment using traces and a RNN from Tensorflow~\cite{tensorflow2015-whitepaper} library.
These traces are then structured into fixed-length formats using sliding windows, preparing them for training. The RNN architecture comprises two SimpleRNN layers, each with 64 units and ReLU activation, followed by a dense output layer. The network is compiled with the Adam optimizer and mean squared error loss to support the regression task, enabling it to predict subsequent observations based on the current sequence of actions and observations. Training is conducted over 50 epochs with a batch size of 32, which is optimized based on the complexity of the environment, ensuring adequate exposure to diverse trajectories. Post-training, the model undergoes evaluation using test data, with fine-tuning adjustments made according to metrics such as mean squared error, allowing for enhanced predictive accuracy. The trained RNN is then incorporated into the CybMASDE tool as a new PettingZoo environment.

the CybMASDE tool also provides tools for specifying organizational constraints in line with the MOISE+MARL framework. Users can define agent roles, missions, and objectives through structured input forms, while an API endpoint allows for the direct upload of JSON files that contain these specifications. This feature enables the modeling of complex organizational structures necessary for scenarios with intricate role hierarchies or mission dependencies.

\textbf{WFM case study}: In this phase, the inputs are include the WFM PettingZoo environment to get traces by running agents randomly, and an informal description of the goal and additional constraints such as safety requirements or roles.
% TODO: dire quels sont les modèles organisationels considérés
The output is another PettingZoo modeled from the initial environment including the defined reward function modeling the goal. It is automatically modeled using the proposed "Imitation Learning" with RNN based on collected agent traces. The process also yields a JSON file representing the MOISE+MARL model with linkers and constraint guides whose an example is given here:

{
\footnotesize
\begin{verbatim}
{
  "linkers": {
    "ar": {"agent_0": "r_0", "agent_1": "r_1", "agent_2": "r_2"},
    "rcg": {  "r_0": ["rag_0","rrg_0"],
              "r_1": ["rag_1","rrg_1"],
              "r_2": ["rag_2","rrg_2"]},
    "gcg": {"g_0": "grg_0", "g_1": "grg_1"}
  },
  "constraint_guides": {
    "rag": {"rag_0": {
      "patterns": {"([o1,a1](1,1),o2)": [["a2",1], ["a0",0]]},
      "scripts": ["rag_0.py"]}},
    "rrg": {"rrg_0": {
      "patterns": {"([o1,a1](1,1),(o2, a2))": 30},
      "scripts": ["rrg_0.py"]}},
    "grg": {"grg_0": {
      "patterns": {"([#Any](0,*),o4](1,1))": 100},
      "scripts": ["grg_0.py"]}}
  },
  "moise_specifications": {
    "mo": {"m_0": ["g_0"], "m_1": ["g_1"], "m_2": ["g_1"]},
    "deontic_specifications": { "r_0": ["m_0", 0, "Any"],
                                "r_1": ["m_2", 0, "Any"],
                                "r_2": ["m_2", 0, "Any"]}
  }
}
\end{verbatim}
}

\paragraph{Phase 2: Training and Hyper-Parameter Optimization (HPO)}

In the training phase, the CybMASDE tool utilizes MARLlib, which supports various MARL algorithms, such as MAPPO, MADDPG, DynaQ+, QMIX (Q-value Mixing), and COMA (Counterfactual Multi-Agent (COMA)). Through the interface, users can select appropriate algorithms based on the environment characteristics and learning requirements. If not specified, all algorithms are assessed automatically. Once selected, the CybMASDE tool manages the training process, taking advantage of parallel computing capabilities to accelerate learning, which is especially beneficial for large-scale environments with numerous agents.

A standout feature of the CybMASDE tool is its HPO module, which automatically tunes hyperparameters like learning rates, discount factors, and exploration rates. Users can configure optimization parameters through the graphical interface or use API calls for batch processing and fine-tuning over multiple training runs. the CybMASDE tool provides real-time monitoring of training metrics, enabling users to track the progress of agents as they refine their policies. The API also supports remote access, allowing users to start, stop, and analyze training runs from external scripts or platforms.

\textbf{WFM case study}: The input for this phase is the JSON file representing the MOISE+MARL model and the PettingZoo environment. The output is a trained MARLlib model of the agent's policies. This model can be saved as a checkpoint folder, which contains files for the weights (e.g., .pth or .pt) and the parameters (e.g., .yaml or .json) required to execute the learned policy.

\paragraph{Phase 3: Analysis and Role Inference}

Once training is complete, the CybMASDE tool assists in analyzing agent behaviors and inferring organizational roles using the HEMM method. The tool applies unsupervised learning techniques, such as hierarchical clustering and K-means, to identify common patterns in agent histories, which are then mapped to roles, missions, and objectives as defined in the MOISE+MARL framework.

The analysis results are presented through visualizations, including dendrograms and state-transition graphs, accessible via the graphical interface. These visualizations help users understand the emergent behaviors and the distribution of roles across agents. Additionally, the API provides detailed data output, which can be used for further analysis in external tools. the CybMASDE tool's capability to infer organizational specifications ensures that agent policies align with predefined constraints, making it easier to enforce compliance and safety standards across the MAS.

\textbf{WFM case study}: The input in this phase is the trained MARLlib model checkpoint. The output is an updated JSON representation of the MOISE+MARL model, which adds patterns for rag, rrg, and grg, potentially introducing new roles, missions, or goals that were not initially assigned to any role.

\paragraph{Phase 4: Deployment and Testing}

In the final phase, the CybMASDE tool supports the deployment of agents in real-world environments or continued testing in emulated settings. The tool offers functionalities to deploy trained policies directly into Gymnasium-compatible environments, allowing for rapid testing and validation. Users can verify agent behavior in real-time, using the interface to adjust parameters and observe the effects on the system's performance.

For real-world applications, the CybMASDE tool enables the export of agent policies into formats suitable for integration with other platforms, ensuring compatibility and ease of deployment. The system supports incremental deployment, where policies can be updated or replaced independently, ensuring modularity and resilience. Furthermore, the CybMASDE tool's API provides endpoints for managing deployed agents, monitoring their adherence to organizational constraints, and evaluating their performance against operational benchmarks.

\textbf{WFM case study}: The input for this phase is the curated JSON representation of the MOISE+MARL model. The output is the emulated environment, where agents are deployed with roles linked to refined Constraint Guides, as well as deployment into the real environment. The deployment process ensures that agents adhere to safety and performance requirements, reflecting the trained policies accurately in the operational context.


\section{Results and Discussion}
\label{sec:results_discussion}

This section presents the results obtained from applying the SAMMASD method in the Warehouse Flow Management (WFM) scenario using the the CybMASDE tool tool. We focus on output compliance, ease of use, automation, verification, and adaptability, evaluating the effectiveness and reliability of the designed MAS. Performance metrics, including constraint violation rate, quality of inferred roles and goals, fidelity to the real environment, and others, are summarized in Table~\ref{fig:performance_metrics} for the selected MARL algorithms: MAPPO, MADDPG, DynaQ+, QMIX, and COMA.

\begin{figure}[h!]
    \centering
    \caption{Summary of Performance Metrics for SAMMASD Method in the WFM Scenario}
    \small
    \renewcommand{\arraystretch}{1.1}
    \begin{tabular}{p{1.9cm}p{0.9cm}p{1.1cm}p{1cm}p{0.7cm}p{0.7cm}}
        \hline
        \textbf{Metric} & \textbf{MAPPO} & \textbf{MADDPG} & \textbf{DynaQ+} & \textbf{QMIX} & \textbf{COMA} \\
        Constraint Violation Rate (\%) & 2.5 & 3.1 & 4.2 & 3.3 & 3.8 \\
        Inferred Roles and Goals Quality (\%) & 94.5 & 92.1 & 89.8 & 91.2 & 90.7 \\
        Fidelity to Real Environment (\%) & 96.8 & 95.7 & 94.2 & 95.0 & 94.5 \\
        Average Training Duration (hrs) & 5.5 & 6.0 & 4.8 & 5.7 & 6.2 \\
        Convergence Rate (\%) & 95 & 92 & 85 & 89 & 87 \\
        Mean Reward & 1800 & 1750 & 1650 & 1690 & 1660 \\
        Hyperparameter Optimization Time (hrs) & 1.2 & 1.3 & 1.1 & 1.4 & 1.2 \\
        Transfer Success Rate (\%) & 93.2 & 91.5 & 89.3 & 90.0 & 88.9 \\
        \hline
    \end{tabular}
    \label{fig:performance_metrics}
\end{figure}

\paragraph{Output Compliance}

The SAMMASD method consistently produced outputs aligned with expectations for each phase. In the modeling phase, the the CybMASDE tool tool facilitated the creation of a high-fidelity simulated environment that accurately mirrored the real-world WFM environment. All algorithms showed high levels of fidelity, with MAPPO reaching 96.8\%. During training and analysis, each algorithm exhibited unique strengths, yet all adhered closely to the expected outcomes as specified by SAMMASD.

\paragraph{Ease of Use}

The the CybMASDE tool tool's graphical interface and structured input forms contributed to a user-friendly experience, particularly for configuring environments and specifying roles and organizational constraints. However, some phases, especially the deployment phase, required a degree of MARL expertise to fine-tune parameters effectively. The tool could benefit from further refinements in terms of usability for less experienced users, though it remains accessible to a broad range of skill levels.

\paragraph{Automation and Training Performance}

Automation was notably effective in the training and hyperparameter optimization stages. The HPO module significantly reduced manual intervention, yielding stable policies across different algorithms. DynaQ+ demonstrated the fastest average training duration at 4.8 hours, while MAPPO achieved the highest mean reward and convergence rate at 1800 and 95\%, respectively. These results highlight SAMMASD's capability to automate significant aspects of the MARL process, streamlining the development of robust MAS solutions.

\paragraph{Verification and Safety Compliance}

The verification phase included extensive testing in emulated environments to assess adherence to safety standards and performance metrics. Real-time monitoring tools within the CybMASDE tool helped ensure compliance with organizational constraints. Across all algorithms, no significant safety violations were observed, validating the SAMMASD method's emphasis on safety and reliability. Visualization tools further supported verification by enabling a detailed examination of agent behaviors and inferred roles, enhancing the explainability of the resulting MAS.

\paragraph{Adaptability and Real-World Testing}

The SAMMASD method proved adaptable, allowing for easy modifications to roles, objectives, and organizational constraints. The average transfer success rate was relatively high across algorithms, with MAPPO again leading at 93.2\%. The deployment in real-world environments showed promising results, though the current framework's adaptability in dynamic, non-stationary environments is an area for future improvement. Enhancements in real-time adaptability and automation during deployment will further extend the applicability of SAMMASD to more complex scenarios.


\section{Conclusion and Perspectives}
\label{sec:conclusion_perspectives}

This paper presents SAMMASD, a semi-automated approach for MAS design using MARL, validated through the WFM scenario using the the CybMASDE tool tool. The method addresses key challenges in traditional AOSE methodologies by integrating organizational specifications directly into MARL training, which enhances design explainability and safety compliance.

The results indicate that SAMMASD, with its semi-automated processes, can effectively reduce dependency on expert input, making the method accessible to users with varying levels of experience in MARL. The fidelity to the real environment remained consistently high across all algorithms, and the automation in training and role inference further supports the scalability of this approach. However, full deployment automation and real-time adaptability in dynamic environments are areas for future research.

Future work will focus on enhancing the CybMASDE tool with automated deployment pipelines and features for online learning in non-stationary environments. Expanding SAMMASD's applicability to domains with stringent safety and resilience requirements, such as healthcare and cybersecurity, is another promising direction. By addressing these improvements, SAMMASD could become a more versatile and powerful solution for MAS design in increasingly complex scenarios.

In summary, SAMMASD's integration of organizational constraints within MARL frameworks offers a comprehensive and scalable solution for MAS design, bridging the gap between traditional AOSE approaches and modern reinforcement learning techniques. Continued development in deployment automation and real-time adaptability will further enhance the applicability of SAMMASD across diverse and complex environments.


\section*{References}

\bibliographystyle{ACM-Reference-Format}
\renewcommand
\refname{}
\bibliography{references}

\end{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
