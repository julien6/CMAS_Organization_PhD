%Version 3.1 December 2024
% See section 11 of the User Manual for version history
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                                                 %%
%% Please do not use \input{...} to include other tex files.       %%
%% Submit your LaTeX manuscript as one .tex document.              %%
%%                                                                 %%
%% All additional figures and files should be attached             %%
%% separately and not embedded in the \TeX\ document itself.       %%
%%                                                                 %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%\documentclass[referee,sn-basic]{sn-jnl}% referee option is meant for double line spacing

%%=======================================================%%
%% to print line numbers in the margin use lineno option %%
%%=======================================================%%

%%\documentclass[lineno,pdflatex,sn-basic]{sn-jnl}% Basic Springer Nature Reference Style/Chemistry Reference Style

%%=========================================================================================%%
%% the documentclass is set to pdflatex as default. You can delete it if not appropriate.  %%
%%=========================================================================================%%

%%\documentclass[sn-basic]{sn-jnl}% Basic Springer Nature Reference Style/Chemistry Reference Style

%%Note: the following reference styles support Namedate and Numbered referencing. By default the style follows the most common style. To switch between the options you can add or remove �Numbered� in the optional parenthesis. 
%%The option is available for: sn-basic.bst, sn-chicago.bst%  
 
%%\documentclass[pdflatex,sn-nature]{sn-jnl}% Style for submissions to Nature Portfolio journals
%%\documentclass[pdflatex,sn-basic]{sn-jnl}% Basic Springer Nature Reference Style/Chemistry Reference Style
\documentclass[pdflatex,sn-mathphys-num]{sn-jnl}% Math and Physical Sciences Numbered Reference Style
%%\documentclass[pdflatex,sn-mathphys-ay]{sn-jnl}% Math and Physical Sciences Author Year Reference Style
%%\documentclass[pdflatex,sn-aps]{sn-jnl}% American Physical Society (APS) Reference Style
%%\documentclass[pdflatex,sn-vancouver-num]{sn-jnl}% Vancouver Numbered Reference Style
%%\documentclass[pdflatex,sn-vancouver-ay]{sn-jnl}% Vancouver Author Year Reference Style
%%\documentclass[pdflatex,sn-apa]{sn-jnl}% APA Reference Style
%%\documentclass[pdflatex,sn-chicago]{sn-jnl}% Chicago-based Humanities Reference Style

%%%% Standard Packages
%%<additional latex packages if required can be included here>

% \usepackage{graphicx}%
% \usepackage{multirow}%
\usepackage{amsmath,amssymb,amsfonts}%
\usepackage{amsthm}%
\usepackage{mathrsfs}%
% \usepackage[title]{appendix}%
% \usepackage{xcolor}%
% \usepackage{textcomp}%
% \usepackage{manyfoot}%
% \usepackage{booktabs}%
% \usepackage{algorithm}%
% \usepackage{algorithmicx}%
% \usepackage{algpseudocode}%
% \usepackage{listings}%

\usepackage{hyperref}

%%%% For camera-ready, use this
%\documentclass[sigconf]{aamas}

\usepackage{listings}
\usepackage{xcolor}

\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usepackage{amsmath}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}
 
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}
 
\lstset{style=mystyle}

% --- Tickz
\usepackage{physics}
\usepackage{tikz}
\usepackage{amsmath}
\usepackage{mathdots}
% \usepackage{yhmath}
\usepackage{cancel}
\usepackage{color}
\usepackage{siunitx}
\usepackage{array}
\usepackage{multirow}
% \usepackage{amssymb}
\usepackage{gensymb}
\usepackage{tabularx}
\usepackage{extarrows}
\usepackage{booktabs}
\usetikzlibrary{fadings}
\usetikzlibrary{patterns}
\usetikzlibrary{shadows.blur}
\usetikzlibrary{shapes}

% ---------

\usepackage{balance} % for balancing columns on the final page
\usepackage{csquotes}
% \usepackage{cite}
\newcommand{\probP}{\text{I\kern-0.15em P}}
\usepackage{etoolbox}
\patchcmd{\thebibliography}{\section*{\refname}}{}{}{}
% \usepackage{amsthm,amssymb,amsfonts}

\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{color}
% \renewcommand\UrlFont{\color{blue}\rmfamily}

\usepackage[inline, shortlabels]{enumitem}
\usepackage{tabularx}
\usepackage{caption}
\usepackage{listings}
\usepackage{stfloats}
\usepackage{titlesec}
\usepackage{ragged2e}
% \usepackage[hyphens]{url}
\usepackage{float}
\usepackage[english]{babel}
\addto\extrasenglish{  
    \def\figureautorefname{Figure}
    \def\tableautorefname{Table}
    \def\algorithmautorefname{Algorithm}
    \def\sectionautorefname{Section}
    \def\subsectionautorefname{Subsection}
    \def\proofoutlineautorefname{Proof Outline}
}

\usepackage[linesnumbered, ruled, vlined]{algorithm2e}
\SetKwComment{Comment}{$\triangleright$\ }{}
\SetAlgoNlRelativeSize{0}
\SetAlgoNlRelativeSize{-1}

\usepackage{amssymb}
\usepackage{pifont}
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%

\setlength{\textfloatsep}{1pt plus 1.0pt minus 2.0pt} % Espace entre figures/tables flottantes et le texte
\setlength{\floatsep}{1pt plus 1.0pt minus 2.0pt}      % Espace entre figures flottantes
\setlength{\intextsep}{1pt plus 1.0pt minus 2.0pt}     % Espace autour des figures non flottantes
\setlength{\abovecaptionskip}{1pt}                     % Avant la légende
\setlength{\belowcaptionskip}{1pt}                     % Après la légende

%%%%%=============================================================================%%%%
%%%%  Remarks: This template is provided to aid authors with the preparation
%%%%  of original research articles intended for submission to journals published 
%%%%  by Springer Nature. The guidance has been prepared in partnership with 
%%%%  production teams to conform to Springer Nature technical requirements. 
%%%%  Editorial and presentation requirements differ among journal portfolios and 
%%%%  research disciplines. You may find sections in this template are irrelevant 
%%%%  to your work and are empowered to omit any such section if allowed by the 
%%%%  journal you intend to submit to. The submission guidelines and policies 
%%%%  of the journal take precedence. A detailed User Manual is available in the 
%%%%  template package for technical guidance.
%%%%%=============================================================================%%%%

%% as per the requirement new theorem styles can be included as shown below
\theoremstyle{thmstyleone}%
\newtheorem{theorem}{Theorem}%  meant for continuous numbers
%%\newtheorem{theorem}{Theorem}[section]% meant for sectionwise numbers
%% optional argument [theorem] produces theorem numbering sequence instead of independent numbers for Proposition
\newtheorem{proposition}[theorem]{Proposition}% 
%%\newtheorem{proposition}{Proposition}% to get separate numbers for theorem and proposition etc.

\theoremstyle{thmstyletwo}%
\newtheorem{example}{Example}%
\newtheorem{remark}{Remark}%

\theoremstyle{thmstylethree}%
\newtheorem{definition}{Definition}%

\raggedbottom
%%\unnumbered% uncomment this for unnumbered level heads

\begin{document}



\title[Assisting Multi-Agent System Design with MOISE+ and MARL: The MAMAD Method]{Assisting Multi-Agent System Design with $\mathcal{M}OISE^+$ and MARL: The MAMAD Method}

%%=============================================================%%
%% GivenName	-> \fnm{Joergen W.}
%% Particle	-> \spfx{van der} -> surname prefix
%% FamilyName	-> \sur{Ploeg}
%% Suffix	-> \sfx{IV}
%% \author*[1,2]{\fnm{Joergen W.} \spfx{van der} \sur{Ploeg} 
%%  \sfx{IV}}\email{iauthor@gmail.com}
%%=============================================================%%

\author*[1]{\fnm{Julien} \sur{Soulé}}\email{julien.soule@lcis.grenoble-inp.fr}

\author[1]{\fnm{Jean-Paul} \sur{Jamont}}\email{jean-paul.jamont@lcis.grenoble-inp.fr}
% \equalcont{These authors contributed equally to this work.}

\author[1]{\fnm{Michel} \sur{Occello}}\email{michel.occello@lcis.grenoble-inp.fr}

\author[2]{\fnm{Louis-Marie} \sur{Traonouez}}\email{louis-marie.traonouez@thalesgroup.com}

\author[3]{\fnm{Paul} \sur{Théron}}\email{paul.theron@orange.fr}

\affil*[1]{\orgdiv{Laboratoire de Conception et d'Intégration des Systèmes (LCIS)}, \orgname{Université Grenoble Alpe}, \orgaddress{\street{50 Rue Barthélémy de Laffemas}, \city{Valence}, \postcode{26000}, \state{Auvergne-Rhône-Alpes}, \country{France}}}

\affil[2]{\orgdiv{Thales Land and Air Systems}, \orgname{BL IAS}, \orgaddress{\street{1 Rue Louis Braille}, \city{Saint-Jacques-de-la-Lande}, \postcode{35136}, \state{Ille-et-Vilaine}, \country{France}}}

\affil[3]{\orgname{AICA IWG}, \orgaddress{\street{22 Av. Gazan Prolongée, 06600 Antibes}, \city{Antibes}, \postcode{06600}, \state{Provence-Alpes-Côte d'Azur}, \country{France}}}

%%==================================%%
%% Sample for unstructured abstract %%
%%==================================%%

\abstract{
    Traditional Agent-Oriented Software Engineering (AOSE) methods rely on explicit and expert-driven design for MAS, but often lack automation. In contrast, Multi-Agent Reinforcement Learning (MARL) and related fields offer automated ways to model environments and learn suitable agent policies. However, integrating these techniques into AOSE remains underexplored partly due to the lack of control, explainability, and unifying frameworks.
    %
    We propose \textbf{MOISE+MARL Assisted MAS Design (MAMAD)}, a four-activity method framing MAS design as a constrained optimization problem: learning joint policies that maximize rewards while respecting $\mathcal{M}OISE^+$ roles and goals. The activities include:
    1) \textbf{Modeling} the environment,  
    2) \textbf{Training} under organizational constraints,  
    3) \textbf{Analyzing} emergent behaviors,  
    4) \textbf{Transferring} to real-world deployment.
    %
    We evaluate MAMAD on various environments, showing that the generated MAS exhibit expected performance, compliance with design requirements and are explainable, while reducing manual design overhead.
}

% Introduction
% Purpose
% Methods
% Results
% Conclusion

\keywords{Agent-oriented Software Engineering, Multi-Agent Reinforcement Learning, Assisted-Design, Organizational Model}

%%\pacs[JEL Classification]{D8, H51}

%%\pacs[MSC Classification]{35A01, 65L10, 65L12, 65L20, 65L70}

\maketitle

\section{Introduction}

\subsection{Context}

Designing \textbf{Multi-Agent Systems (MAS)} for real-world domains (such as cybersecurity, autonomous logistics or robotic swarms) requires designing agents that are both autonomous and coordinated, adaptable and structured. To address these demands, the field of \textbf{Agent-Oriented Software Engineering (AOSE)} has historically provided principled methodologies based on symbolic representations. Methods such as GAIA~\cite{gaia1998}, ADELFE~\cite{adelfe2002}, or DIAMOND~\cite{Jamont2007} provide well-defined processes for designing MAS, relying on explicit modeling with possibly explicit roles, missions, or interaction protocols~\cite{Pavon2003,Bernon1999}. They offer guarantees in terms of predictability, safety, and explainability by leveraging expert-driven design processes~\cite{Hindriks2014,Jamont2O15}. However, these methods are largely manual and require specialized knowledge to define agent behaviors, making scalability in complex or dynamic environments cumbersome.

To improve efficiency and scalability, several AOSE frameworks such as INGENIAS~\cite{Pavon2003}, KB-ORG~\cite{Sims2008} or AutoGenesisAgent~\cite{harper2024autogenesisagent} have also sought to automate key aspects of MAS design.
Yet, these AOSE works still faces major limitations towards a fully end-to-end automated design process. Crucially, they lack automated support for modeling complex environments as test environments, optimizing agent behaviors, or analyzing emergent dynamics.

In parallel, \textbf{Machine Learning (ML)} has led to diverse works and subfields that, although developed independently from AOSE in Multi-Agent Systems paradigm, provide capabilities likely relevant for MAS design offering the automation, adaptivity, and scalability that AOSE lacks. Two major subfields are:
\begin{enumerate*}[label={\roman*)}, itemjoin={; \quad}]
    \item \textbf{World Models}~\cite{Ha2018}, which learn high-fidelity environment simulations from agents trajectories
    \item \textbf{MARL}~\cite{Zhang2021, Papoudakis2021}, which enables decentralized policy optimization through exploration and trial-and-error
\end{enumerate*}

However, ML-based approaches also present key limitations. While MARL enables agents to learn without manual supervision, the resulting policies are often opaque, difficult to control, and poorly aligned with explicit design requirements~\cite{Nguyen2020, Anastassacos2020}, limiting their safe deployment.
Although World Models~\cite{Ha2018} show promise in single-agent settings, their extension to multi-agent systems remains challenging due to increased complexity in coordination and observability.
Most importantly, there is still no fully or partly automated MAS design framework that bridges real-world environments with a pipeline orchestrating potentially leveraged ML-based works.

This strong motivation to bridge the symbolic, model-driven rigor of AOSE with the learning-based automation of ML and particularly MARL, led to the \textbf{MOISE+MARL} framework~\cite{soule2025moisemarl}%
\footnote{This article introducing MOISE+MARL has been accepted at AAMAS 2025 and is freely available at \url{https://arxiv.org/abs/2503.23615}}.
%
MOISE+MARL integrates the $\mathcal{M}OISE^+$ organizational model~\cite{Hubner2002} into the MARL paradigm, using formal organizational specifications both to guide the agents during training and to interpret their learned behaviors in terms of roles and goals. While this represents a significant advancement toward integrating organizational reasoning into MARL, this framework is not yet conceived as part of a comprehensive MAS design methodology.

\subsection{Problem statement and research gaps}

Extending the MOISE+MARL framework, we adopt an optimization-based perspective to bridge AOSE and MARL paradigms in MAS design. We consider the problem of designing a MAS that must operate in a real-world environment, achieve a global goal, and possibly satisfy additional design requirements. Then, the core design task is framed as a \textbf{constrained optimization problem in a MARL context}, where:
\begin{enumerate*}[label={\roman*)}, itemjoin={; \quad}]
    \item The \textbf{optimization variable} is the agents' joint policy;
    \item The \textbf{goal function} seeks to maximize cumulative rewards;
    \item The \textbf{constraints} represent symbolic design requirements, such as roles or goals.
\end{enumerate*}

\noindent This formulation leverages the MOISE+MARL hybrid approach by solving the design problem through MARL, while ensuring that the resulting agent behaviors remain controllable and interpretable through AOSE symbolic principles. Based on this perspective, we aim to address the following research gaps:
%
\begin{enumerate*}[label={\roman*)}, itemjoin={; \quad}]
    \item \textbf{(G1) End-to-end automation.} ML techniques (particularly World Models and MARL) have the potential to automate the key activities of MAS design. However, no existing framework integrates these techniques into a unified, iterative design pipeline. Moreover, World Models remain underdeveloped in multi-agent settings, especially for realistic deployment scenarios.
    
    \item \textbf{(G2) Compliance with design requirements.} Most MARL approaches focus on performance optimization without enforcing structured constraints such as safety rules, roles, or missions. While some recent works have begun to address this limitation, MOISE+MARL~\cite{soule2025moisemarl} has demonstrated that incorporating organizational specifications during training is feasible. However, this framework has not yet been integrated into a fully or partly automated MAS design process.
    
    \item \textbf{(G3) Organizational-level explainability.} Policies learned via MARL are often opaque, making it difficult to understand how agent behaviors contribute to system-level goals. In contrast, AOSE methods benefit from explicit symbolic structures that facilitate interpretability. Among the few works addressing collective explainability in MARL, MOISE+MARL~\cite{soule2025moisemarl} introduced a trajectory-based analysis method to extract implicit organizational structures. Yet, this approach remains disconnected from a design-oriented pipeline and lacks mechanisms for reintegrating insights into the iterative design process.
\end{enumerate*}

\subsection{Contributions and paper organization}

We propose the \textbf{MAMAD method} which extends the \textbf{MOISE+MARL} framework~\cite{soule2025moisemarl} towards MAS design. MAMAD is driven by three main inputs: (i) the environment, (ii) the global goal, and (iii) user-defined design requirements (obtained through a \textit{Requirements Engineering} activity). MAMAD leverages these inputs to generate a MAS by means of a fully end-to-end, continuously refining both the organizational specifications and the policy through iterative interaction with the real environment.
The MAMAD method is structured in four activities:

\begin{enumerate}
    \item \textbf{Modeling activity:} This activity constructs a high-fidelity simulated environment by training a neural architecture inspired by World Models~\cite{Ha2018}, using agent trajectories collected from early deployments. It also encodes the global goal into a reward function and formalizes design requirements into organizational specifications (e.g., roles and missions).
          
    \item \textbf{Training activity:} Agents are trained using MARL in the simulated environment. The \textit{control mechanism} of MOISE+MARL is employed to constrain or guide the learning process based on symbolic specifications: \textit{roles} restrict the allowable action space, while \textit{goals} modulate rewards. This integration ensures that learned policies comply with user-defined design constraints.
          
    \item \textbf{Analysis activity:} The analysis method of MOISE+MARL is leveraged to extract implicit organizational structures from successful trajectories using unsupervised learning. Emergent roles and goals are inferred and can be compared with predefined specifications to assess organizational alignment or even suggest refinements.
          
    \item \textbf{Transfer activity:} The trained joint policy is deployed in the real environment. New agent trajectories are continuously collected and reintegrated into the modeling activity to reduce the simulation-to-reality gap and allow for ongoing design refinement. This activity closes the loop, enabling a continual update of both the simulated environment and the learned policies.
\end{enumerate}


\noindent MAMAD thus provides a general framework for automating MAS design while supporting constraint satisfaction and symbolic interpretability. It enables the designer to benefit from both the adaptability of ML techniques and the AOSE principles.



We evaluated \textbf{MAMAD} in gamified environments, used as controlled testbeds to assess its ability to generate high-fidelity simulations during the Modeling activity (bypassing the complexity of modeling physical environments). Results show strong alignment between the specifications applied during training and those inferred post hoc, validating both \textbf{(G3) Organizational-level explainability} and \textbf{(G2) Compliance with design requirements}. Compared to manual methods, \textbf{(G1) End-to-end automation.} improved significantly, requiring fewer interventions. Ablation studies revealed that omitting automated modeling reduced policy generalization, while removing organizational constraints led to erratic agent behavior.

\

\noindent The remainder of the paper is structured as follows. \autoref{sec:related_works} reviews prior work relevant to each of the identified gaps. \autoref{sec:background} introduces the background and notation, covering World Models, Markovian formulism for MARL, the $\mathcal{M}OISE^+$ organizational model, and the MOISE+MARL framework. \autoref{sec:mamad} details the MAMAD method, presenting the overall workflow and the contributions bridging the identified gaps. \autoref{sec:experimental_setup} describes the experimental setup, followed by \autoref{sec:results}, which presents and discusses the obtained results. Finally, \autoref{sec:conclusion} concludes the paper and outlines future research directions.

\section{Related works}\label{sec:related_works}

This section reviews distinct bodies of literature relative to the three research gaps.

\subsection{Automating end-to-end MAS design (G1)}

Several advanced AOSE approaches have attempted to improve automation in MAS design. For example, INGENIAS adopts a model-driven engineering paradigm, offering meta-models and tooling to automatically generate code, documentation, and tests from high-level specifications, thereby streamlining MAS development~\cite{Pavon2003}. Similarly, the KB-ORG framework employs a knowledge-based approach to organizational design, using predefined templates and domain-specific knowledge to automate the assignment of roles and responsibilities~\cite{Sims2008}. More recently, the field of Automated Design of Agentic Systems has emerged, focusing on the automatic generation and composition of agentic components into functional MAS with minimal human intervention~\cite{smith2024automated}. Taking this idea further, AutoGenesisAgent proposes a fully autonomous pipeline in which MAS can design and deploy new MAS tailored to specific tasks, covering the full lifecycle from initial concept to deployment~\cite{harper2024autogenesisagent}. Likewise, the BMW Agents framework illustrates how collaborative agent architectures can support scalable task automation through planning and execution in complex industrial environments~\cite{crawford2024bmw}.

Despite these advances, such approaches typically assume symbolic inputs and predefined environments. They do not support the dynamic modeling of complex or unknown environments, nor do they integrate learning-based policy optimization. Moreover, they lack closed-loop design mechanisms capable of refining agent specifications based on the analysis of emergent behaviors, an essential capability for scalable and adaptive MAS development.

\

\noindent In parallel, some of the most promising advances in MAS automation have emerged from the field of Machine Learning. A notable example is the \textit{Cyber Security Learning Environment}~\cite{hammar2023scalable}, an \textbf{online framework} for cybersecurity applications in which agents are trained using RL techniques in automatically generated, near-realistic simulations to dynamically acquire task-specific behaviors. This framework constitutes a significant step toward \textbf{end-to-end MAS design automation}, offering an almost fully or partly automated pipeline (from environment modeling to policy learning and deployment) while minimizing manual effort. It also provides visualization tools for monitoring agent behavior, though it does not incorporate organizational modeling.

More broadly, ML-based paradigms offer critical capabilities that could benefit MAS design. In particular, the World Models framework~\cite{Ha2018} proposes to first learn a compressed latent representation of the environment, which is then used as a high-fidelity simulation for policy training or planning. Although effective in single-agent contexts, World Models remain underexplored in multi-agent scenarios, particularly in settings involving partial observability, interaction complexity, and the need for coordination at scale.
%
To date, no existing framework provides a fully or partly automated, iterative MAS design pipeline that connects to a real-world deployment environment while orchestrating multiple ML techniques within a unified process.

\subsection{Integrating design constraints in MARL (G2)}

The MARL literature has primarily focused on optimizing coordination and cooperation among agents in complex and uncertain environments~\cite{Zhang2021, Papoudakis2021}. However, most approaches overlook the incorporation of symbolic or organizational constraints into the learning process. Agents typically learn through trial and error, without guarantees that their emergent behaviors will satisfy critical design requirements such as safety rules, role adherence, or structured team hierarchies. Several recent works have attempted to address this limitation by introducing constraint-aware reinforcement learning techniques.

Constraint-Guided Reinforcement Learning~\cite{spieker2021constraint} incorporates explicit constraint models into the agent-environment interaction, enabling agents to learn policies that remain within predefined behavioral bounds. Similarly, Deep Constrained Q-Learning~\cite{kalweit2020deep} introduces both single-step and approximate multi-step constraints into the Q-value update process to ensure compliance with safety and performance criteria. Constrained Policy Optimization (CPO)\cite{achiam2017constrained} provides theoretical guarantees for near-constraint satisfaction throughout policy search, making it particularly appealing for safety-critical applications. Beyond safety, MENTOR\cite{zhou2025mentor} integrates human feedback into hierarchical RL, guiding agents through dynamically constrained subgoal selection to promote more stable learning. Other approaches such as reward-free constrained learning~\cite{miryoosefi2022} circumvent the need for hand-crafted reward functions by directly optimizing constraint satisfaction.

While these approaches enhance safety and control at the policy level, they do not integrate with symbolic design models such as those used in AOSE. A notable exception is MOISE+MARL~\cite{soule2025moisemarl}, which bridges the gap by extending the $\mathcal{M}OISE^+$ organizational framework~\cite{Hubner2002} into MARL, allowing agents to learn while respecting organizational roles, missions, and behavioral constraints. However, MOISE+MARL remains focused on execution-time control and post-hoc analysis, lacking a full design pipeline or environment modeling capability. In particular, it assumes access to a manually specified environment modeled as a Dec-POMDP, whereas our approach is to operate within environments that are automatically modeled from agent interactions, following the World Models paradigm.

\subsection{Organizational-level explainability (G3)}

While the AOSE tradition ensures explainability through structured design artifacts (such as protocols, roles, missions, or goals) these symbolic elements are typically lost in standard MARL approaches. Learned policies are often represented as opaque neural networks, making it difficult to assess how well agent behaviors align with the original design intent or organizational principles. Although explainability in MARL has gained attention, most existing efforts focus on individual agent behavior or internal policy mechanisms, rather than on collective or organizational alignment.
A range of approaches have been proposed to enhance interpretability in MARL. Some integrate interpretability into model design: Zabounidis et al.~\cite{zabounidis2023concept} require agents to predict human-understandable concepts before acting, while Iturria-Rivera et al.~\cite{iturria2024explainable} use reward decomposition in factorized value functions. Liu et al.~\cite{liu2025} combine RNNs with decision trees for transparent policy learning. Others focus on post-hoc methods, such as relevance backpropagation~\cite{poupart2025perspectives} or Shapley-value approximations~\cite{li2025from}, to explain decisions without altering models.

However, these methods mainly offer local insights and rarely address collective or organizational dynamics. A few works (e.g.,~\cite{berenji2000learning,yusuf2020inferential,serrino2019finding}) explore role or goal inference, but lack abstractions aligned with symbolic organizational models. In contrast, the TEMM method~\cite{soule2025moisemarl}, part of the MOISE+MARL framework, targets organizational-level explainability by clustering trajectories to extract roles and goals from emergent behaviors. It assesses how well these inferred structures align with predefined organizational models. While promising, TEMM is not yet fully integrated into a design loop that refines specifications iteratively—an extension that would support more structured and context-aware MAS design.


% =======================


\section{Theoretical background}\label{sec:background}

This section recaps the notation and basics we used in our contributions for MARL and the $\mathcal{M}OISE^+$ organizational model.

\subsection{Markov framework for MARL}

To apply MARL techniques, we adopt the \textbf{Decentralized Partially Observable Markov Decision Process} (Dec-POMDP)\cite{Oliehoek2016}, a standard formalism for modeling multi-agent coordination under partial observability. Compared to \textbf{Partially Observable Stochastic Games} (POSGs), Dec-POMDPs assume a shared reward, naturally fostering collaboration\cite{Matignon2007}. While both formalisms typically rely on access to the true state, limiting their realism, Dec-POMDPs remain suitable for embedding organizational constraints.

A Dec-POMDP is defined as the 7-tuple $\left(S, {A_i}, T, R, {\Omega_i}, O, \gamma \right)$, where $S$ is the set of states; $A_i$ is the action set for agent $i$; $T(s, a, s')$ is the transition probability; $R$ is the reward function; $\Omega_i$ is the observation set for agent $i$; $O(s', a, o)$ is the observation probability; and $\gamma$ is the discount factor.

Let $m$ be the number of teams, each comprising agents in $\mathcal{A}$. For a team $i$ of $n$ agents, we use the following notation~\cite{Matignon2007,Yuan2023}:
%
\begin{enumerate*}[label={\roman*)}, itemjoin={; \quad}]
    \item $\Pi$: the set of policies, where $\pi: \Omega \rightarrow A$ maps observations to actions;
    \item $\Pi^j$: the set of joint-policies, $\pi^j: \Omega^n \rightarrow A^n = \Pi^n$, specifying actions for all agents;
    \item $H$: the set of histories, where $h = ((\omega_k, a_k)){k \leq z}$ is a trajectory of $z$ steps;
    \item $H^j$: the set of joint-histories, $h^j = {h_1, ..., h_n}$;
    \item $U^j_i(\langle \pi^j_i, \pi^j{-i} \rangle)$: the expected cumulative reward for team $i$ with given joint policies;
    \item $SR^j_i(\pi^j_i, s)$: the set of joint-policies yielding at least reward $s$.
\end{enumerate*}

\noindent Solving the Dec-POMDP thus amounts to finding a joint-policy $\pi^j_i \in SR^j_i(\pi^j_i, s)$ that achieves at least the expected cumulative reward threshold $s \leq U_i^*$.

\subsection{The $\mathcal{M}OISE^+$ organizational model}

The $\mathcal{M}OISE^+$ model~\cite{Hubner2002, Hubner2007} provides a formal framework for specifying multi-agent organizations. We focus on the core components relevant to our work: \textit{roles}, \textit{missions} (goals), and \textit{permissions/obligations}.
%
The $\mathcal{M}OISE^+$ model defines \textbf{structural}, \textbf{functional}, and \textbf{deontic} specifications. \textbf{Structural specifications} include a set of roles, denoted as $\mathcal{R}$ (with $\rho \in \mathcal{R}$), along with inheritance relations, group structures, inter-role links (e.g., communication, authority), compatibility constraints (specifying roles that may be held concurrently), and cardinality bounds for roles and groups. \textbf{Functional specifications} define goals, denoted as $\mathcal{G}$ (with $g \in \mathcal{G}$), missions denoted as $\mathcal{M}$ (with $m \in \mathcal{M}$), and a goal-to-mission mapping $mo: \mathcal{M} \rightarrow \mathbb{P}(\mathcal{G})$ (for example, for given $m_a \in \mathcal{M}, mo(m_a) = {g_i \in \mathcal{G}_a}, with \mathcal{G}_a \subseteq \mathcal{G}$); goals are hierarchically decomposed through plans, and missions may specify agent cardinality (fixed to 1 in our setting) and optional preference orders. \textbf{Deontic specifications} express normative rules through permissions, denoted as $\mathcal{PER}$ (with $per \in \mathcal{PER}, per = (\rho_a, m, tc)$) and obligations, denoted as $\mathcal{OBL}$ (with $obl \in \mathcal{OBL}, obl = (\rho_a, m, tc)$), indicating that an agent playing role $\rho_a$ is permitted or obligated to commit to mission $m$ during time constraint $tc \in \mathcal{TC}$ (e.g., $Any$ for always).

\noindent In our setting, only roles, missions, goal mappings, and permissions/obligations are needed to guide policy learning within Dec-POMDPs. Other structural and functional aspects (e.g., links, plans, or cardinalities) are either implicit or effectively captured through these core elements, also enabling to take them into account explicitly in future works. We therefore define the organizational specification used in MOISE+MARL as $\mathcal{OS} = \langle \mathcal{R}, \mathcal{M}, \mathcal{PER}, \mathcal{OBL}, mo \rangle$ where $\mathcal{R}$ is the set of all roles, $\mathcal{M}$ is the set of all missions, $\mathcal{PER}$ is the set of all permissions, $\mathcal{OBL}$ is the set of all obligations, and $mo$ is the goal-to-mission mapping.


\subsection{MOISE+MARL for linking $\mathcal{M}OISE^+$ with MARL}

\begin{figure}[h!]
    \centering
    \input{figures/mm_synthesis_single_column.tex}
    \caption{A minimal view of the MOISE+MARL framework:
        Users first define $\mathcal{M}OISE^+$ specifications, which include roles ($\mathcal{R}$) and missions ($\mathcal{M}$), both associated through $rds$. They then create MOISE+MARL specifications by first defining \textbf{Constraint guides} such as $rag$ and $rrg$ to specify role logic, and $grg$ for goal logic. 
        Next, \textbf{Linkers} are used to connect agents with roles through $ar$ and to link the logic of the constraint guides to the defined $\mathcal{M}OISE^+$ specifications. Once this is set up, roles can be assigned to agents, and the MARL framework updates accordingly during training.
    }
    \label{fig:mm_synthesis}
\end{figure}

\noindent MOISE+MARL introduces means to control or guide the agents' training in MARL. Its core contribution ar the \textbf{Constraint Guides}, which are three new relations introduced to describe the logics of roles and goals in the Dec-POMDP formalism:
%
\begin{enumerate*}[label={\roman*) },itemjoin={; \quad}]
    
    \item \textbf{Role Action Guide} \quad $rag: H \times \Omega \rightarrow \mathcal{P}(A \times \mathbb{R})$, the relation that models a role as a set of rules which, for each pair consisting of a history $h \in H$ and an observation received by the agent $\omega \in \Omega$, associates expected actions $A \in \mathcal{P}(A)$ each associated with a constraint hardness $ch \in [0,1]$ ($ch = 1$ by default). By restricting the choice of the next action among those authorized, the agent is forced to adhere to the role's expected behavior
    \item \textbf{Role Reward Guide} \quad $rrg: H \times \Omega \times A \to \mathbb{R} = \{r_m \text{ if } a \notin A_\omega \text{, } rag(h, \omega) \allowbreak = \allowbreak A_\omega \times \mathbb{R} \text{, } h \in H; \text{ else } 0\}$, the relation that models a role by adding a penalty $r_m$ to the global reward if the last action chosen by the agent $a \in A$ is not authorized. This is intended to encourage the agent to adhere to the expected behavior
    \item \textbf{Goal Reward Guide} \quad $grg: H \rightarrow \mathbb{R}$, the relation that models a goal as a soft constraint adding a reward bonus $r_b \in \mathbb{R}$ if the agent's history $h \in H$ contains a goal's characteristic sub-sequence $h_g \in H_g$, encouraging the agent to reach it.
\end{enumerate*}

\noindent Finally, we introduce the \textbf{Linkers} to link the $\mathcal{M}OISE^+$ organizational specifications with constraint guides and agents:
%
\begin{enumerate*}[label={\roman*) },itemjoin={; \quad}]
    
    \item \textbf{Agent to Role} \quad $ar: \mathcal{A} \to \mathcal{R}$, the bijective relation linking an agent to a role;
    \item \textbf{Role to Constraint Guide} \quad $rcg: \mathcal{R} \rightarrow rag \cup rrg$, the relation associating each $\mathcal{M}OISE^+$ role to a $rag$ or $rrg$ relation, forcing/encouraging the agent to follow the expected actions for the role $\rho \in \mathcal{R}$;
    \item \textbf{Goal to Constraint Guide} \quad $gcg: \mathcal{G} \rightarrow grg$, the relation linking goals to $grg$ relations, representing goals as rewards in MARL.
\end{enumerate*}

\paragraph{\textbf{Resolving the Dec-POMDP with MOISE+MARL}}

A MOISE+MARL model is defined as $\mathcal{MM} = \langle \mathcal{OS}, ar, rcg, gcg, rag, rrg, grg \rangle$.
Solving a Dec-POMDP with $mm \in \mathcal{MM}$ consists in finding a joint policy $\pi^j = {\pi^j_0, \pi^j_1, \dots, \pi^j_n}$ that maximizes the expected cumulative reward (or satisfies a minimal threshold), represented by the state-value function $V^{\pi^j}$. This value reflects the return from an initial state $s \in S$ when applying successive joint actions $a^j \in A^n$ under the additional organizational constraints.
%
The definition of $V^{\pi^j}$ follows the sequential and cyclic agent execution scheme (AEC mode), and is formalized in \hyperref[eq:single_value_function]{Definition 1}, incorporating role-based (in red) and mission-based (in blue) adaptations that influence both the action space and the reward.
\autoref{fig:mm_synthesis} illustrates how the MOISE+ specifications are integrated into the Dec-POMDP resolution via the MOISE+MARL framework.

\medskip

\begin{figure*}[h!]
    \label{eq:single_value_function}
    \raggedright
    \textbf{\textit{Definition 1} \quad State-Value function adapted to constraint guides in AEC:}
    
    \begin{scriptsize}
        \vspace{-0.6cm}
        \begin{gather*}
            V^{\pi^j}(s_t) = \hspace{-0.75cm}
            %
            \sum_{\textcolor{red}{ \substack{a_{t} \in A \text{ if } rn() < ch_{t}, \\
                        a_{t} \in A_{t} \text{ else}}
                }}{\hspace{-0.7cm} \pi_i(a_{t} | \omega_t)}
            %
            \sum_{s_{t+1} \in S}
            %
            {\hspace{-0.1cm} T(s_{t+1} | s_t, a_{t})
            \Bigl[R(s_t,a_{t},s_{t+1}) + \hspace{-0.1cm}
            \textcolor{blue}{ \sum_{m \in \mathcal{M}_i}{ \hspace{-0.1cm} v_m(t) \frac{grg_m(h_{t+1})}{1 - p + \epsilon} } }
            + } \\
            {\textcolor{red}{(1-ch_t) \times rrg(\omega_t,a_{t+1})} + V^{\pi^j_{i+1 \ mod \ n}}(s_{t+1})\Bigr]}
        \end{gather*}
        %
        \vspace{-0.5cm}
        \textcolor{red}{\[\text{ \hspace{-0.1cm} With } rag(h_t, \omega_t) = A_{t} \times \mathbb{R} \text{, } \langle a_t, ch_{t} \rangle \in A_{t} \times \mathbb{R} \text{ ; } rn: \emptyset \to [0,1[ \text{, a uniform random function}\]}
        %
        \vspace{-0.6cm}
        \textcolor{blue}{
            \begin{gather*}
                \hspace{-0.001cm}
                \text{With } \omega_t = O(\omega_t | s_t, a_t) \text{ ; } h_t = \{h_0 = \langle \rangle, h_{t+1} = \langle h_t, \langle \omega_{t+1}, a_{t+1} \rangle \rangle \} \text{ ; } \epsilon \in \mathbb{R}_{>0} \text{ ; } grg_m(h) = 
            \end{gather*}
        }
        \vspace{-0.95cm}
        \textcolor{blue}{
            \begin{gather*}
                \hspace{-0.5cm} \sum_{\hspace{0.3cm}(grg_i,w_i) \in mo(m)}{\hspace{-0.9cm} w_i \times grg_i(h)}
                \text{ ; } v_m(t) = \{ 1 \text{ if } t \in t_c \text{ ; else } 0 \} \text{ ; } \mathcal{M}_i = \{m_j | \langle ar(i),m_j,t_c,p \rangle \in \mathcal{M}\}
            \end{gather*}
        }
        \vspace{-0.6cm}
    \end{scriptsize}
    
\end{figure*}

\noindent At each time step $t \in \mathbb{N}$ (starting from $t=0$), agent $i = t \bmod n$ is assigned to role $\rho_i = ar(i)$. For each temporally valid deontic specification $d_i = rds(\rho_i) = \langle tc_i, y_i, m_i \rangle$, the agent is permitted ($y_i = 0$) or obligated ($y_i = 1$) to commit to mission $m_i \in \mathcal{M}$, with goal set $\mathcal{G}_{m_i} = mo(m_i)$ and $n \in \mathbb{N}$ agents.
%
Upon observing $\omega_t$, the agent selects an action from $A_t$ (the role-expected actions) with probability $ch_t$, or from $A$ otherwise. If $ch_t = 1$, the agent is strictly constrained by its role.
%
The selected action transitions the system from $s_t$ to $s_{t+1}$, yields observation $\omega_{t+1}$, and returns a reward composed of:
i) bonuses for achieved goals in valid missions (via Goal Reward Guides), weighted by $\frac{1}{1 - p + \epsilon}$;
ii) penalties from the Role Reward Guide, scaled by $ch_t$.
%
The process continues in state $s_{t+1}$ with agent $(i + 1) \bmod n$.

\subsection{The TEMM method}
\label{sec:TEMM_algorithm}

The TEMM method is part of the explanation component of the MOISE+MARL framework. It leverages unsupervised learning techniques to infer organizational specifications from observed agent trajectories. It allows computing the organizational fit between emergent behaviors and expected roles, goals, and missions.

\textbf{1) Roles and role inheritance.} \quad TEMM defines a role $\rho$ as a policy whose agents share a \textit{Common Longest Sequence} (CLS) in their histories. A role $\rho_2$ inherits from $\rho_1$ if $\text{CLS}(\rho_2) \subseteq \text{CLS}(\rho_1)$. Hierarchical clustering is used to extract these CLSs and role hierarchies from trajectories. The \textbf{structural organizational fit} is computed as the distance between actual agent behaviors and inferred role sequences.

\textbf{2) Goals, plans, and missions.} \quad Goals are identified as clusters of joint-observations commonly reached in successful trajectories, using K-means over trajectory embeddings. Plans are inferred as sub-sequences of transitions that consistently lead to goals. A \textbf{mission} groups goals pursued collectively by one or more agents. The \textbf{functional organizational fit} quantifies how well current behaviors match inferred goals and missions.

\textbf{3) Permissions and obligations.} \quad Permissions and obligations are derived by examining whether agents fulfilling a role consistently (or exclusively) achieve certain missions under time constraints. Obligations imply exclusivity, whereas permissions imply optionality. The global \textbf{organizational fit} is obtained by aggregating structural and functional scores.

While clustering hyperparameters may require manual tuning to ensure robust role and goal extraction, TEMM offers a principled way to analyze emergent organizational behaviors and refine specifications accordingly.



\subsection{Learning World Models}

In Reinforcement Learning (RL), particularly under partial observability, \textbf{World Models}~\cite{ha2018recurrent, hafner2020dream} aim to learn internal models approximating both the environment's transition and observation dynamics. Such models enable agents to perform planning, improve sample efficiency, and facilitate safe exploration. This modeling approach belongs to the \textit{model-based RL} (MBRL) paradigm~\cite{moerland2020model}, and is especially useful for automatically constructing high-fidelity simulation models even when explicit environment representations are unavailable.

Formally, at each time step $t$, let $\omega_t \in \Omega$ denote the current high-dimensional observation, $a_t \in A$ the action taken, and $\tilde{h}_{t-1} \in \mathcal{H}$ the recurrent hidden state summarizing the interaction history up to $t-1$. Since observations are typically high-dimensional (e.g., images, complex state vectors), an encoder $Enc: \Omega \rightarrow Z$ is first applied to project observations into a compact latent space $Z$ with $z_t = Enc(\omega_t)$, where $\dim(Z) \ll \dim(\Omega)$.

The core temporal structure is modeled using a \textbf{Recurrent Latent Dynamics Model (RLDM)}~\cite{hafner2020dream} $\mathcal{T}^{z} = f(g(h_{t-1},z_t, a_t))$, which predicts the next latent state $z_{t+1}$ by updating the recurrent hidden state with $f$ and applying latent dynamics with $g$:
$h_t = f(h_{t-1}, z_t, a_t), z_{t+1} = g(h_t)$
where $f(\cdot)$ typically corresponds to a recurrent neural network (e.g., LSTM~\cite{hochreiter1997long}) applied to the concatenation of $h_{t-1}$, $z_t$, and $a_t$, and $g(\cdot)$ maps the recurrent state to the next observation latent representation (often implmented as an MLP~\cite{hochreiter1997long}).

The predicted latent state is then decoded via $Dec: Z \rightarrow \Omega$ into the predicted observation $\hat{\omega}_{t+1} = Dec(z_{t+1})$. The entire model is jointly trained to minimize both the \emph{reconstruction loss} $\|\omega_{t+1} - \hat{\omega}_{t+1}\|$ in observation space, and optionally, a \emph{latent prediction loss} to stabilize latent dynamics learning.

The recurrent hidden state $\tilde{h}_t$ serves as a compact summary of the full interaction history up to time $t$, avoiding the need to explicitly store long observation-action trajectories.
For simplicity of notation, we define the full composition that directly maps current observation, action, and recurrent state to the next predicted observation as the \textbf{Observation Prediction Model}:
\[
    \mathcal{T}(h_{t-1}, \omega_t, a_t) := Dec(g(f(h_{t-1}, Enc(\omega_t), a_t))) = \hat{\omega}_{t+1}.
\]



\section{The MAMAD method}\label{sec:mamad}

\subsection{General overview of the method}

The MAMAD~\footnotemark[1] method is built around four main activities: (1) modeling the environment, goal, and organizational constraints, (2) learning policies using various MARL algorithms, (3) analyzing behaviors and inferring organizational specifications with a proposed method, and (4) maintaining consistency between the simulated and real environments by deploying trained policies and updating the simulation. This approach guides the agent learning process while enforcing strict organizational constraints, ensuring the efficiency of the learned policies. The lifecycle of a MAMAD-designed MAS is illustrated in \autoref{fig:cycle}.
%
\begin{figure}[h!]
    \centering
    \input{figures/cycle.tex}
    \caption{Lifecycle of a MAS designed with MAMAD: i) Users start by modeling the environment from a sufficient amount of real trajectories (obtained by initially transfered agents) or any available one, global goal and design requirements as roles and goals; \quad ii) Then, they launch the training of agents with MARL techniques; \quad iii) A post-training analysis is performed to get insights into the emergent agents' roles and goals, guiding the improvement of the applied organizational specifications ; \quad iv) Once validated, trained policies are launched to operate the environment's actuators, generating new traces for a better environment modeling}
    \label{fig:cycle}
\end{figure}
%
The MAMAD method frames MAS design as an iterative constrained optimization process. Given:
\begin{enumerate*}[label={\roman*)}, itemjoin={; \quad}]
    \item $\mathcal{E}_0$: the initial environment where agents can act;
    \item $\mathcal{G}_{\text{inf}}$: an informal description of the desired global goal;
    \item $\mathcal{C}_{\text{inf}}$: an informal specification of design constraints;
    \item $\gamma \in [0,1]$: the discount factor leading to long or short term solutions even though it often determined empirically (default is 1);
    \item $A, \Omega$: the action space and observation space respectively;
    \item $\texttt{org\_fit}_{min}$, $\overline{r}_{min}$, $\sigma_{min}$: respectively the minimum organizational fit level, average reward and standard deviation required to validate a trained joint policy. Usually, these values are typically determined empirically
\end{enumerate*}
%
\noindent As described in \autoref{alg:mamad}, the method's framework enables the continous design of a MAS as an iterative and asynchronous coordination of two distinct processes: the \textit{Transferring process}, which connects to the real environment and handles real-time execution and joint-history collection; and the \textit{MTA (Model-Train-Analyze) process}, which consumes stored data to iteratively improve the simulated environment model, the joint-policy, and the MAS organizational specifications.

\paragraph{Transferring process: policy deployment and data collection}

This process is always active while the MAS is operating in the real environment. Its role is twofold. First, it deploys the most recent joint policy $\pi^j_{\text{latest}}$ to the real agents, ensuring up-to-date behavior without interrupting execution. Second, it continuously monitors and collects agent trajectories in the form of joint histories $H^j$, buffering them in batches. Once a sufficient number of trajectories is collected, the batch is appended to the global trajectory store $\mathcal{D}_{H^j}$. If the update process is not already running, it triggers the launch of the \textit{MTA} process.

\paragraph{MTA process: policy optimization and organizational refinement}

This process models the current design problem and improves the MAS policy and associated organizational specifications. It first builds a JOPM $T^j$ using extended World Models with the collected trajectories. Design requirements are formalized as MOISE+MARL organizational specifications $\mathcal{MM}$ and the ultimate goal is formalized as an History-based Reward Function $R^j_H$ as well.
%
Then, a Markovian model is built out of previously modeled elements so that agents can be trained with organizational specifications using the MOISE+MARL framework. Once training is completed, the resulting joint policy $\pi^j$ is analyzed using TEMM to infer implicit organizational specifications $\mathcal{MM}_{\text{imp}}$ and compute the organizational fit score.

\paragraph{Refinement loop through organizational specifications}

If the learned policy shows low organizational fit, subpar performance, or high variance (relative to predefined thresholds), the inferred organizational structures from the analysis phase are leveraged to refine the initial specifications. This process may involve manually inspecting inferred roles and goals to identify key success factors and revise the organizational constraints accordingly.

This loop repeats up to $n_{refine}$ times, progressively steering the policy space toward more structured, effective behaviors. The final validated policy is stored as $\pi^j_{\text{latest}}$ for deployment.
%
Such iterative refinement is especially useful in complex or poorly understood environments, where manual design is costly or infeasible. Starting without any prior specification, the method gradually constructs relevant and agnostic constraints, narrowing the policy search space based on recurring organizational patterns detected across training cycles.

\medskip

\noindent This feedback loop creates a closed, autonomous MAS design cycle: the system learns from real execution, updates its simulation, retrains under refined constraints, and redeploys improved policies—requiring minimal human intervention. It seamlessly merges symbolic AOSE principles with learning-based automation, ensuring compliance, adaptability, and explainability at the organizational level.


\begin{algorithm}[H]
    \caption{MOISE+MARL Assisted MAS Desgin (MAMAD)}
    \label{alg:mamad}
    \footnotesize
    \DontPrintSemicolon
    
    \KwIn{Initial environment $\mathcal{E}$, goal $\mathcal{G}_{\text{inf}}$, design constraints $\mathcal{C}_{\text{inf}}$, $n_{refine}$ max number of refinement cycles}
    \KwOut{A MAS deployed satisfying design, performance and explainability requirements; and associated organizational specifications}
    
    Initialize: $\mathcal{D}_{H^j} \gets \emptyset$, $\pi^j_{\text{latest}} \gets \pi^j_{\text{init}}$, $running\_MTA \gets False$ \;
    
    \vspace{0.3em}
    
    \While{MAS is active in environment $\mathcal{E}$}{
        \tcp*[l]{Transferring: retrieve trajectories \& deploy policy}
        $\mathcal{D}_{H^j}, \texttt{need\_update} \gets \text{transfer}(\pi^i_{latest}, \mathcal{D}_{H^j})$ \tcp*[r]{asynchronous call}
        \If{\texttt{need\_update} and not \texttt{running\_MTA}}{
            \texttt{launch\_MTA()} \tcp*[r]{asynchronous call}
        }
    }
    
    \vspace{1em}
    \SetKwProg{MTA}{Process \normalfont(MTA)}{}{}
    \MTA{}{}{
    
    $\texttt{running\_MTA} \gets True$ \tcp*[l]{Global variable assignment}
    
    \tcp*[l]{Modeling: model the real environment into a simulated model}
    $\mathcal{T}^j, R^j_H, \mathcal{MM} \gets \texttt{model}(\mathcal{E}, \mathcal{D}_{H^j}, \mathcal{G}_{\text{inf}}, \mathcal{C}_{\text{inf}}, \gamma, A, \Omega)$ \;
    
    \While{$i < n_{refine}$}{
    
    \vspace{0.5em}
    \tcp*[l]{Training: train policy under org. constraints}
    $\pi^j, \overline{r}, \sigma \gets \texttt{train}(\mathcal{T}^j, \mathcal{MM})$ \;
    
    \vspace{0.5em}
    \tcp*[l]{Analyzing: analyze policy to infer new org. specs}
    $(\mathcal{MM}_{\text{imp}}, \texttt{org\_fit}) \gets \texttt{analyze}(\mathcal{T}^j, \mathcal{MM}, \pi^j)$ \;
    
    \vspace{0.5em}
    
    \tcp*[l]{If policy not satisfying, retrain with new org. spec.}
    \If{$\texttt{org\_fit} < \texttt{org\_fit}_{min} \ or \ \overline{r} < \overline{r}_{min} \ or \  \sigma > \sigma_{min}$}{
    $\mathcal{MM} \gets \mathcal{MM}_{\text{imp}}$ \;
    back to 'Analyzing' \;
    }
    
    $\pi^j_{\text{latest}} \gets \pi^j$
    \tcp*[r]{Update most recent policy}
    
    $i \gets i + 1$
    
    }
    
    $\texttt{running\_MTA} \gets False$ \tcp*[l]{Global variable assignment}
    
    
    }
\end{algorithm}

\

\noindent One can point out that we propose to leverage a modeled simulated environment as a Digital Twin for a later training whereas MBRL both integrates environment modeling and training at the same time. Indeed, we favour decoupling environment modeling from training for : i) the reusability of the modeled environment in new agent training optionally requiring small adjustments ; \quad ii) the need for simple agents that do not embedded costly environment model for planning ; \quad iii) the need to have a high-fidelity modeled environment focusing all efforts on a common one for any agent.

\

\noindent In the following subsections, we detail each activity within the overall MAMAD framework, identifying the specific challenges encountered in achieving this goal and describing the proposed contribution and its use that address these challenges.

\subsection{Modelling}\label{sec:modelling}

The \textbf{Modelling activity} addresses the following formal component:
\begin{displaymath}
    \texttt{model}(\mathcal{D}_{H^j}, \mathcal{G}_{\text{inf}}, \mathcal{C}_{\text{inf}}, \gamma, A, \Omega) = \mathcal{T}^j, R^j_H, \mathcal{MM}
\end{displaymath}

\noindent The \textit{Modelling activity} aims to model the design problem as a constrained optimization problem. To do that, it first generates a high-fidelity simulated environment using the Joint-observation Prediciton Model (JOPM) $\mathcal{T}^j: H^j \times \Omega^j \times A^j \rightarrow \mathcal{H} \times \hat{\Omega^j}$ derived from real-world interaction traces $\mathcal{D}_{H^j}$. At time step $t \in \mathbb{N}$, for any recurrent hidden state $\tilde{h}_{t-1} \in \mathcal{H}$ representing joint-history until $t-1$, the currently received joint-observation $\omega_t^j \in H^j$ and the joint-action to be applied $a_t^j \in A^j$, $\mathcal{T}^j$ gives the next recurrent hidden state $\tilde{h}_t \in \mathcal{H}$ and the predicted next joint-observation $\hat{\omega}^j \in \hat{\Omega}^j$. This way, MAMAD enables building the environment from scratch. In addition to the simulated environment, the optimization problem also formalizes the informal goal description $\mathcal{G}_{\text{inf}}$ into $R^j_H: H^j \times \Omega^j \rightarrow \mathbb{R}$ the History-based Reward Function. Finally, the \textit{Modelling activity} also aims to formalize constraint stemming from informal design requirements as MOISE+MARL organizational specifications $\mathcal{MM}$ from $\mathcal{C}_{\text{inf}}$.

We assume to leave the work of formalizing informal design requirements into MOISE+MARL organizational specifications and informal goal description into an History-based Reward Function.

Since we are not able to access the real environment's state, we have to rely on stored joint-histories hence the idea to use World Models for its capability to generalize from a large amount of histories to compute hidden state transitions and observations transitions.
A major gap we encountered when willing to implement this function with World Models is the absence of explicitely defined World Models for Multi-Agent settings. Below, we propose an extension of the World Model framework for Multi-Agent settings.

\subsubsection*{Extension to Multi-Agent World Models}

In multi-agent settings, joint-observations rapidly become high-dimensional as the number of agents increases. To address this, joint encoding functions are introduced for both observations and actions.

Specifically, joint-observations $\omega_t^{j} = (\omega_t^1, \dots, \omega_t^n) \in \Omega^{j}$ are mapped to compact latent representations using an joint-observation encoder $Enc_{\omega^j}: \Omega^{j} \rightarrow z$, yielding $z_t = Enc_{\omega^j}(\omega_t^{j})$. The joint-observation decoder $Dec_{z}: z \rightarrow \hat{\Omega}^{j}$ allows reconstruction of the joint-observation if needed.
%
MLPs or attention-based architectures are typically employed for these encoders to aggregate multi-agent information into fixed-size feature vectors while capturing relevant inter-agent dependencies.

Once joint encoding is performed, the multi-agent world model operates analogously to the single-agent case, using the encoded observation $z_t$ in histories to the RLDM $\mathcal{T}^{z}$. This design enables scalable modeling while preserving critical interaction patterns between agents. Within the MAMAD framework, such world models instantiate the simulation core of the \hyperref[sec:modelling]{Modeling activity}, effectively serving as high-fidelity digital twins of the target environment.

\

\begin{algorithm}[H]
    \caption{Modeling activity algorithm}
    \label{alg:modeling}
    \footnotesize
    \DontPrintSemicolon
    
    \KwIn{Joint histories $\mathcal{D}_{H^j}$, informal goal $\mathcal{G}_{\text{inf}}$, informal constraints $\mathcal{C}_{\text{inf}}$, discount factor $\gamma$, action space $A$, observation space $\Omega$}
    \KwOut{JOPM $\mathcal{T}^j$, History-based Reward Function $R^j_H$, MOISE+MARL specifications $\mathcal{MM}$}
    
    \vspace{0.5em}
    \tcp{1. Manual formalization of symbolic requirements}
    $R^j_H \gets \texttt{manual\_formalize}(\mathcal{G}_{\text{inf}})$ \;
    $\mathcal{MM} \gets \texttt{manual\_formalize}(\mathcal{C}_{\text{inf}})$ \;
    
    \vspace{0.5em}
    \tcp{2. Train encoders for joint-observations and actions}
    Extract datasets $\Omega^j = \{\omega^j_t\}$ from joint-histories $\mathcal{D}_{H^j}$ \;
    Train auto-encoder $(Enc_{\omega^j}, Dec_{\omega^j})$ on $\Omega^j$ minimizing reconstruction loss \;
    
    \vspace{0.5em}
    \tcp{3. Encode joint-observations in joint-history}
    For each joint-history $h^j = \{\omega_t^j, a_t^j\} \in \mathcal{D}_{H^j}$, encode each joint-observation ${z}_t = Enc_{\omega^j}(\omega^j_t)$ to build training set $\mathcal{B} = \{ \{(z_t,a^j_t, z_{t+1})\} = h_z^j, h_z^j \in \mathcal{D}_{H^j}\}$
    
    \vspace{0.5em}
    \tcp{4. Train the RLDM}
    Initialize the RLDM (f and g function) $\mathcal{T}^z = f(g)$
    
    \For{$h_z^j \in \mathcal{B}$}{
        \For{$(z_t,a^j_t, z_{t+1}) \in h^j$}{
            Train RLDM $\mathcal{T}^{z}$ minimizing the MSE of predicted joint-observation $\hat{z}_{t+1}$ from the real one $z_{t+1}$.
        }
    }
    
    \vspace{0.5em}
    \tcp{5. Save all initial joint-observations and form the JOPM}
    
    $\Omega^{\mathcal{T}^j}_0 \gets \{\omega^j_0\}$ from joint-histories $\mathcal{D}_{H^j}$
    
    $\mathcal{T}^j(h_{t-1}, \omega_t, a_t) = \langle f(h_{t-1}, Enc(\omega^j_t), a^j_t), Dec(\mathcal{T}^{z}(h_{t-1}, Enc(\omega^j_t), a^j_t)) \rangle$ \;
    
    \vspace{0.5em}
    \tcp{6. Return modelled elements}
    \Return{$\mathcal{T}^j, \Omega^{\mathcal{T}^j}_0, R^j_H, \mathcal{MM}$}
\end{algorithm}



\subsection{Training}\label{sec:training}

The \textbf{Training activity} addresses the following formal component:
%
\begin{displaymath}
    \pi^j_i \gets \texttt{train}(\mathcal{T}^j, \Omega^{\mathcal{T}^j}_0, \gamma, R^j_H, \Omega, A, \mathcal{MM})
\end{displaymath}

The \textit{Training activity} aims to solve the modeled design problem as a constrained optimization task under the MOISE+MARL framework. However, a key limitation arises from the fact that MOISE+MARL operates under the Dec-POMDP formalism, which assumes full access to the true underlying state of the environment. In contrast, our approach relies exclusively on observable data (namely, agents' joint histories) without assuming access to real environmental states. To bridge this gap, a new Markovian formalism is needed, one that operates over observable sequences via the JOPM, while remaining compatible with existing MARL algorithmic frameworks.


\subsubsection{Extension of MOISE+MARL to Multi-Agent World Models}

\noindent In realistic settings, we rely solely on histories stacking transitions of actions and received observations. To better reflect this setting, we introduce a new formalism called the \textbf{Observation-based Dec-POMDP} (ODec-POMDP).
%
An ODec-POMDP $d_\Omega \in OD_\Omega$ (with $OD_\Omega$ the set of all Observation-based Dec-POMDPs) is defined as a 5-tuple:
%
$d_\Omega = \left(\Omega, A, \mathcal{T}^j, R^j_H, \gamma \right)$
%
where:
\begin{enumerate*}[label={\roman*)}, itemjoin={; \quad}]
    \item $A$: the action space.
    \item $\Omega$: the observation space.
    \item $\Omega^{\mathcal{T}^j}_0$: the recorded initial joint observation.
    \item $\mathcal{T}^j(h, \omega, a) = \langle {\tilde{h}}', \mathbb{P}(\omega' \mid h, \omega, a) \rangle$ the JOPM estimating the next joint-observation $\omega'$ based on the previous joint-history $\tilde{h} \in \mathcal{H}$, the most recent joint-observation $\omega$, and the current joint-action to be applied $a$. The JOPM also outputs the updated recurrent hidden state $\tilde{h}'$.
    \item $R^j_H: H \times \Omega \times A \times \Omega \rightarrow \mathbb{R}$ is the History-based Reward Function, returning the reward based on the previous joint-history, the last observation used to select last joint-action and the resulting next joint-observation.
    \item $\gamma \in [0, 1]$ is the discount factor.
\end{enumerate*}
%
\noindent This formulation allows MARL agents to operate purely on observable data, making it compatible with learned simulated environments.

\paragraph{\textbf{Resolving the ODec-POMDP with MOISE+MARL}}

Solving a ODec-POMDP with $mm \in \mathcal{MM}$ consists in finding a joint policy $\pi^j = {\pi^j_0, \pi^j_1, \dots, \pi^j_n}$ that maximizes the expected cumulative reward (or satisfies a minimal threshold), represented by the observation-based value function $V_{\mathcal{T}^j}^{\pi^j}$. This value reflects the cumulated return from an initial joint-observation $\omega^j \in \Omega^{\mathcal{T}^j}_0$, joint-history $h^j$ and reccurent hidden state $\tilde{h}$ when applying successive joint actions $a^j \in A^n$ under the additional organizational constraints $\mathcal{MM}$ using the JOPM $\mathcal{T}^j$ instead of unknown observation and state transition functions.
%
The definition of $V_{\mathcal{T}^j}^{\pi^j}$ follows a parallel mode formalized in \hyperref[eq:single_value_function]{Definition 2}, incorporating role-based (in red) and mission-based (in blue) adaptations that influence both the joint-action space and the reward.
\autoref{fig:mm_synthesis} illustrates how the MOISE+ specifications are integrated into the ODec-POMDP resolution via the MOISE+MARL framework.

\medskip

\begin{figure*}[h!]
    \label{eq:single_value_function}
    \raggedright
    \textbf{\textit{Definition 2} \quad Observation-Value function adapted to constraint guides in parallel mode:}
    
    \begin{scriptsize}
        \vspace{-0.6cm}
        \begin{gather*}
            V^{\pi^j}(\tilde{h}_{t-1},h^j_{t-1},\hat{\omega}^j_t) = \hspace{-0.95cm}
            %
            \sum_{\textcolor{red}{ \substack{a^j_{t} \in A^j \text{ if } rn() < ch_{t}, \\
            a^j_{t} \in A^j_{t} \text{ else}}
            }}{\hspace{-0.9cm} \pi_i(a^j_{t} | \hat{\omega}^j_t)}
            %
            \hspace{-1.2cm}
            \sum_{\phantom{XXXX}(\tilde{h}_t,\hat{\omega}^j_{t+1}) \in \mathcal{H} \times \hat{\Omega}^j}
            %
            {\hspace{-1.2cm} \mathcal{T}^j(\langle \tilde{h}_t,\hat{\omega}^j_{t+1} \rangle | \tilde{h}_{t-1}, \hat{\omega}_t, a^j_{t})
            \Bigl[R^j_H(h^j_{t-1},\hat{\omega}^j_t,a^j_t,\hat{\omega}^j_{t+1}) \hspace{-0.1cm} }
        \end{gather*}
        %
        \vspace{-1.1cm}
        \begin{gather*}
            \hspace{4.5cm}
            {+ \  \textcolor{blue}{grg^j_m(h^j_t)}
            +
            \textcolor{red}{(1-ch_t) \times rrg^j(\hat{\omega}^j_t,a^j_{t+1})} + V^{\pi^j}(\tilde{h}_{t}, h^j_t, \hat{\omega}^j_{t+1})\Bigr]}
        \end{gather*}
        %
        \vspace{-0.15cm}
        %
        \[\hspace{-0.9cm}\text{With \ } \tilde{h}_{-1} = \mathbf{0} \text{ and } \tilde{\omega}^j_0 \in \Omega_0^{\mathcal{T}^j} \text{ ; } a^j_t = \langle a_{t,0}, a_{t,1} \dots a_{t,|\mathcal{A}|} \rangle \text{ ; } \omega^j_t = \langle \omega_{t,0}, \omega_{t,1} \dots \omega_{t,|\mathcal{A}|} \rangle \text{ ; }\]
        %
        \vspace{-0.25cm}
        \[\hspace{-5.85cm} h^j_t = \langle h_{t,0}, h_{t,1} \dots h_{t,|\mathcal{A}|} \rangle = \langle \langle h_{t-1,i}, \omega_{t,i}, a_{t,i} \rangle \rangle_{i \in \mathcal{A}}\]
        %
        \vspace{-0.2cm}
        \textcolor{red}{\[\hspace{-2.6cm}\text{ \hspace{-0.1cm} With } \langle rag_i, rrg_i \rangle = rcg(ar(i)) \text{ ; } rn: \emptyset \to [0,1[ \text{, a uniform random function}\]}
        %
        \vspace{-0.3cm}
        \textcolor{red}{\[A^j_t \times \mathbf{R}^{|\mathcal{A}|} = rag^j(h^j_t, \tilde{\omega}^j_t) = \langle rag_i(h_{t,i}, \omega_{t,i}) \rangle_{i \in \mathcal{A}} \text{ ; } rrg^j(h^j_t, \tilde{\omega}^j_t, a^j_t) = \sum_{i \in \mathcal{A}}{rrg_i(h_{t,i}, \omega_{t,i}, a_{t,i})}\]}
        %
        \vspace{-0.75cm}
        \textcolor{blue}{
            \begin{gather*}
                \hspace{-1.7cm} grg_m(h) = \hspace{-1cm} \sum_{\hspace{0.3cm}(grg_i,w_i) \in mo(m)}{\hspace{-1.1cm} w_i \times grg_i(h)}
                \text{ ; }
                grg^j_m(h^j_t) = \hspace{-0.1cm} \sum_{i \in \mathcal{A}}{\sum_{m \in \mathcal{M}_i}{ \hspace{-0.1cm} v_m(t) \frac{grg_m(h_{t,i})}{1 - p + \epsilon} }} \text{ ; } \epsilon \in \mathbb{R}_{>0} \text{ ; }
            \end{gather*}
        }
        \vspace{-0.9cm}
        \textcolor{blue}{
            \begin{gather*}
                \hspace{-4cm}
                v_m(t) = \{ 1 \text{ if } t \in t_c \text{ ; else } 0 \} \text{ ; } \mathcal{M}_i = \{m_j | \langle ar(i),m_j,t_c,p \rangle \in \mathcal{M}\}
            \end{gather*}
        }
    \end{scriptsize}
    
\end{figure*}

\noindent Considering a parallel mode, at each time step $t \in \mathbb{N}$ (starting from $t=0$), an agent $i \in \mathcal{A}$ is assigned to role $\rho_i = ar(i)$. For each temporally valid deontic specification $d_i = rds(\rho_i) = \langle tc_i, y_i, m_i \rangle$, the agent is permitted ($y_i = 0$) or obligated ($y_i = 1$) to commit to mission $m_i \in \mathcal{M}$, with goal set $\mathcal{G}_{m_i} = mo(m_i)$ and $n \in \mathbb{N}$ agents.
%
Upon observing $\tilde{\omega}^j_t$, each agent $i \in \mathcal{A}$ select an action from $A_{i,t} \text{ with } A^j_t = \langle A_{0,t}, A_{1,t}, A_{|\mathcal{A}|,t}\rangle$ (from Role Reward Guides) with probability $ch_t$, or from $A^t$ otherwise. If $ch_t = 1$, the agent is strictly constrained by its role.
%
The state and observation transitions due to the application of the selected actions are both approximated via the JOPM function $\mathcal{T}^j$ that takes the previous reccurrent hidden state up to $t-1$ $\tilde{h}_{t-1}$, the lastly received joint-observation $\hat{\omega}_t^j$ and lastly chosen joint-action $a_t^j$ to get the next predicted joint-observation $\hat{\omega}_{t+1}^j$ and the updated reccurrent hidden state up to $t$ $\tilde{h}_t$. The History-based Reward Function uses the last joint-history $h^j_{t-1}$, the last joint-observation $\hat{\omega}_t^j$, the chosen joint-action $a_t^j$ and the next joint-observation $\hat{\omega}^j_{t+1}$ to get the next reward. The reward is also updated by adding bonus/malus for:
i) achieving goals in valid missions (via Goal Reward Guides), weighted by $\frac{1}{1 - p + \epsilon}$;
ii) aligning with roles (via the Role Reward Guide), scaled by $(1-ch_t)$.

\begin{algorithm}[H]
    \caption{Training activity algorithm}
    \label{alg:training_mamad}
    \footnotesize
    
    \DontPrintSemicolon
    
    \KwIn{
        Joint-Observation Prediction Model (JOPM) $\mathcal{T}^j$,
        Initial joint observations $\Omega_0^{\mathcal{T}^j}$,
        History-based Reward Function $R_H^j$,
        Organizational specification $\mathcal{MM}$,
        Discount factor $\gamma$
    }
    \KwOut{$\pi^j$ : Trained joint policy}
    
    \vspace{0.3em}
    
    Initialize parameters of policy $\pi^j$ and replay buffer $\mathcal{B}$ \;
    
    \ForEach{episode $e = 1 \dots N$}{
    Sample $\omega_0^j \sim \Omega_0^{\mathcal{T}^j}$, set $\tilde{h}_{-1} \gets \mathbf{0}$ \;
    Initialize joint history $h_{-1}^j \gets \emptyset$ \;
    
    \ForEach{step $t = 0 \dots T$}{
    Compute $A_t^j = rag^j(h^j_t, \omega^j_t)$ via role reward guides from $\mathcal{MM}$ \;
    \If{$rn() < ch_t$}{
        Select $a_t^j \sim \pi^j(\cdot | \omega_t^j)$ within constrained $A_t^j$ \;
    }
    \Else{
        Select $a_t^j \sim \pi^j(\cdot | \omega_t^j)$ within $A_t$ \;
    }
    
    $(\tilde{h}_t, \omega_{t+1}^j) \gets \mathcal{T}^j(\tilde{h}_{t-1}, \omega_t^j, a_t^j)$ \tcp*{JOPM prediction}
    
    $r_t \gets \gamma^t \times R_H^j(h^j_{t-1}, \omega_t^j, a_t^j, \omega_{t+1}^j)$ \tcp*{Base reward}
    
    $r_t \gets r_t + grg^j(h^j_t)$ \tcp*{Goal Rew. Guides}
    
    
    $r_t \gets r_t + (1 - ch_t) \times rrg^j(h^j_t, \omega_t^j, a_t^j)$ \tcp*{Role Rew. Guides}
    
    Append $(\omega_t^j, a_t^j, r_t, \omega_{t+1}^j)$ to $\mathcal{B}$ \;
    Update $h^j_t \gets \langle \langle h_{t-1,i}, \omega_{t,i}, a_{t,i} \rangle \rangle_{i \in \mathcal{A}}$ \;
    
    Train $\pi^j$ with minibatches from $\mathcal{B}$ using any MARL method \;
    }
    }
    
    \Return{$\pi^j$}
\end{algorithm}


\subsection{Analyzing}\label{sec:analyzing}

\noindent The \textbf{Analyzing activity} addresses the following formal component:
\[
    (\mathcal{MM}_{i,\text{implicit}}, \text{OF}) \gets \texttt{analyze}(d_i, \mathcal{MM}_i, \pi^j_i, d_\Omega)
\]

The goal of this phase is twofold: (i) explain the learned joint policy through MOISE+MARL organizational constructs (roles, goals, missions), and (ii) compute the \textbf{organizational fit}, measuring the alignment between learned behaviors and a regular—explicit or implicit—organization.
%
To this end, we employ the \textbf{TEMM} method~\cite{soule2025moisemarl}, which assumes that successful agent behaviors follow a small set of latent strategies, despite superficial variability. By averaging a large number of successful joint histories, regularities can be uncovered while filtering out behavioral noise.

Observation trajectories are clustered using distance-based metrics (e.g., LCS, Smith-Waterman), yielding centroids composed of average observations per timestep. Each observation is assigned a \textbf{representativeness} score based on normalized inverted variance—high values reflect frequent, consistent patterns; low values signal sparse or divergent behavior. Using a minimal threshold, representative observations are grouped into \textbf{intermediate goals}. Higher thresholds yield compact and robust goal sets, while lower ones may introduce noise or overfitting. Final goal labeling is performed manually.

Similarly, agent transition trajectories $(\omega, a) \in \Omega \times A$ are clustered to infer implicit \textbf{roles}. Centroids and their per-step/global variances help extract representative transitions as behavioral \textbf{rules}. High representativeness ensures consistency; low thresholds may yield overly generic or noisy rules. Like goals, roles require manual semantic labeling.
%
We then compute the \textbf{organizational fit}, combining:
\begin{enumerate*}[label={\roman*)}, itemjoin={; \quad}]
    \item \textbf{Structural Organizational Fit (SOF)}: normalized inverse of global variance over role-clustered transitions;
    \item \textbf{Functional Organizational Fit (FOF)}: same, but over goal-clustered observations.
\end{enumerate*}
%
The final score is $\text{OF} = \frac{1}{2}(\text{SOF} + \text{FOF})$, indicating how well inferred structures align with actual behaviors. High values imply strong alignment, while low values reveal weak or inconsistent behavioral patterns.

A key limitation is the manual hyperparameter tuning in TEMM—e.g., \textbf{distance metric}, \textbf{cluster distance}, and \textbf{minimal representativeness}—which affects the precision and reliability of inferred specifications. Low thresholds risk overfitting; high ones may impair learning guidance. This reliance on manual calibration hinders full automation of the analysis process.



\subsubsection{Extended TEMM method with HPO}
To address this, we propose a hyperparameter optimization (HPO) process for the Analyzing phase based on a grid search. It includes:
(i) a joint grid search on distance metrics and minimal cluster distance for both observations and transitions, aiming to minimize the number of clusters while maximizing organizational fit (i.e., minimizing intra-cluster variance);
(ii) the tuning of structural and functional minimal representativeness thresholds used to sample trajectories. As shown in \autoref{fig:conv_time_repr}, lowering representativeness increases the coverage of inferred roles and goals but risks overfitting to noisy behaviors; higher thresholds yield robust but sparse specifications, offering little guidance and thus longer convergence times.

To balance this, we select the representativeness value at the \textit{elbow point}—the start of the convergence-time plateau—typically defined as the maximum representativeness yielding a normalized 3.5\% convergence to minimal cumulative reward. This tradeoff ensures meaningful, non-overfitted specifications that remain compact and interpretable, while preserving performance.
%
This HPO process enables producing concise and effective organizational specifications from agent trajectories.

\medskip

\begin{figure}[h!]
    \centering
    \includegraphics[trim=0cm 0cm 0cm -0.05cm, clip, width=0.85\linewidth]{figures/convergence_time_relative_to_representativeness.pdf}
    \caption{General normalized time to converge to a minimal cumulated reward over minimal representativeness}
    \label{fig:conv_time_repr}
\end{figure}

\begin{algorithm}[H]
    \caption{Analyzing activity algorithm}
    \label{alg:auto_temm}
    \footnotesize
    \DontPrintSemicolon
    
    \KwIn{
        Trained joint policy $\pi^j$;
        ODec-POMDP $d_\Omega$;
        Initial specification $\mathcal{MM}$;
        Target normalized convergence threshold (default: 3.5\%) $\eta$
    }
    
    \KwOut{
    Inferred organizational specification$\mathcal{MM}_{\text{implicit}}$;
    Organizational Fit score $\text{OF}$
    }
    
    \tcp*[l]{1. Collect trajectories}
    Generate individual histories $\mathcal{D}_{\text{trans}}$ from $d_\Omega$ under $\pi^j$ \;
    $\mathcal{D}_{\text{obs}} \gets$ individual observation trajectories from $\mathcal{D}_{\text{full}}$ \;
    
    \tcp*[l]{2. HPO on clustering distance and threshold}
    \For{$t \in \{obs,trans\}$}{
    \ForEach{distance metric $\delta_t \in \{0.0\dots\frac{i}{n}\dots1.0\}_{i \in \{0\dots n\}}$}{
    \ForEach{minimal cluster distance $\tau_t \in \{0.0\dots\frac{i}{n}\dots1.0\}_{i \in \{0\dots n\}}$}{
    Cluster $\mathcal{D}_{\text{obs}}$ and $\mathcal{D}_{\text{trans}}$ using $(\delta_t, \tau_t)$ \;
    Compute: $\sigma_{\text{obs}}, \sigma_{\text{trans}}, N_{\text{clusters}}$ \;
    Score $\gets \alpha (\sigma_{\text{obs}} + \sigma_{\text{trans}}) + \beta N_{\text{clusters}}$ \tcp*[l]{default: $\alpha=0.4,\beta=0.6$}
    Retain $(\delta_t^*, \tau_t^*)$ with minimal Score \;
    }
    }
    }
    
    \tcp*[l]{3. Apply clustering with optimal HPO parameters}
    Cluster $\mathcal{D}_{\text{obs}}$ into $C_{obs}$ clusters $(\delta_{trans}^*, \tau_{trans}^*)$ \;
    Cluster $\mathcal{D}_{\text{trans}}$ into $C_{trans}$ clusters using $(\delta_{trans}^*, \tau_{trans}^*)$ \;
    
    \tcp*[l]{4. HPO on repr. to evaluate convergence times as func. $ct_t$}
    \For{$t \in \{obs,trans\}$}{
    \ForEach{representativeness $\rho_t \in \{0.0\dots\frac{i}{n}\dots1.0\}_{i \in \{0\dots n\}}$}{
    Infer org. spec. $\mathcal{MM}_{\rho_t}$ from clusters using $\rho_t$ \;
    Initialize empty joint policy $\pi^j_{\rho_t}$ \;
    Train $\pi^j_{\rho_t}$ on $(d_\Omega, \mathcal{MM}_{\rho_t})$ until $R_{cum} \geq R_{\min}$ \;
    Record normalized conv. time $c_{\rho_t}$ so that $ct_t(\rho_t) = c_{\rho_t}$ \;
    }
    
    \tcp*[l]{5. Select repr. for threshold $\eta$ (by default $3.5\%$)}
    $\rho_t^* \gets max(\{\rho_t \in \{0.0\dots\frac{i}{n}\dots1.0\}_{i \in \{0\dots n\}} | ct_t(\rho_t) < \eta \})$ \tcp*[r]{Elbow point}
    
    }
    
    \tcp*[l]{6. Final role/goal inference using TEMM}
    Infer roles from $\mathcal{D}_{\text{trans}}, \delta^*, \tau^*, \rho^*$ \;
    Infer goals from $\mathcal{D}_{\text{obs}}, \delta^*, \tau^*, \rho^*$ \;
    
    \tcp*[l]{7. Compute Organizational Fit (OF)}
    Compute SOF (structural) and FOF (functional) from intra-cluster variances \;
    $\text{OF} \gets \frac{1}{2}(\text{SOF} + \text{FOF})$ \;
    
    \Return{$\mathcal{MM}_{\text{implicit}}, \text{OF}$}
\end{algorithm}


\subsection{Transferring}\label{sec:transferring}

\noindent The \textbf{Transferring activity} addresses the following formal component:
\[
    \mathcal{D}_{H^j}, \texttt{need\_update} \gets \texttt{transfer}(\pi^j_{\text{latest}}, \mathcal{E}, \mathcal{D}_{H^j})
\]

The \textit{Transferring activity} fulfills two main goals: (1) continuously deploy the most recent joint policy $\pi^j_{\text{latest}}$ into the real environment $\mathcal{E}$ to ensure agents act and interact effectively, and (2) collect fresh real-world joint trajectories $(\omega^j_t, a^j_t, \omega^j_{t+1})$ to enrich the trajectory dataset $\mathcal{D}_{H^j}$ used for updating the simulated environment and organizational specifications.

To the best of our knowledge, no framework exists that provides both asynchronous policy deployment and automatic, threshold-triggered joint-history collection in a closed-loop manner while enabling synchronization with the learning pipeline. The lack of such infrastructure hampers automation of MAS design cycles in real environments.

\paragraph{Transferring framework}
We propose a general theoretical framework that implements an asynchronous and event-driven control system responsible for policy execution and trace harvesting. It maintains a buffer of trajectories, triggers the retraining pipeline once a size threshold is met, and ensures the most recent policy is continuously applied. This control logic is structured around two mechanisms:
(i) a \texttt{Transfer Loop} that handles real-time deployment and data collection, and
(ii) an \texttt{Update Trigger} that launches the full design process once sufficient data is available. The system avoids launching multiple parallel updates and guarantees synchronization between transfer and modeling phases.

\vspace{-0.3em}
\begin{algorithm}[H]
    \caption{Transferring activity}
    \label{alg:transferring}
    \footnotesize
    \DontPrintSemicolon
    \KwIn{Current policy $\pi^j_{\text{latest}}$, real environment $\mathcal{E}$, trajectory store $\mathcal{D}_{H^j}$}
    \KwOut{Updated trajectory dataset $\mathcal{D}_{H^j}$, update signal $\texttt{need\_update}$}
    
    \vspace{0.3em}
    \SetKwProg{Transfer}{Procedure \normalfont TransferLoop}{}{}
    \Transfer{}{
    
    \While{MAS is active in environment $\mathcal{E}$}{
    \tcp*[l]{Execute latest policy in environment}
    $\omega^j_t \gets \texttt{observe}(\mathcal{E})$ \;
    $a^j_t \gets \pi^j_{\text{latest}}(\omega^j_t)$ \;
    $\omega^j_{t+1} \gets \texttt{apply}(\mathcal{E}, a^j_t)$ \;
    Append $(\omega^j_t, a^j_t, \omega^j_{t+1})$ to temporary buffer $\mathcal{B}$ \;
    
    \tcp*[l]{Check whether retraining is needed}
    \If{$|\mathcal{B}| \geq \texttt{batch\_size}$}{
        Add $\mathcal{B}$ to $\mathcal{D}_{H^j}$ and clear $\mathcal{B}$ \;
        $\texttt{need\_update} \gets \texttt{True}$ \;
        \If{$\texttt{running\_update} = \texttt{False}$}{
            \texttt{launch\_update()} \tcp*[r]{Asynchronous call}
        }
    }
    }
    }
\end{algorithm}

This mechanism ensures (i) continuity of execution, (ii) reactivity to data freshness, and (iii) automation of update cycles. It forms the bridge between the simulated world and the deployment context by continually adjusting the design loop to the evolving environment, a core requirement for long-term MAS autonomy.


\section{Experimental setup}
\label{sec:experimental_setup}

We developed a tool that we propose to facilitate the implementation of the MAMAD method through four environments following a proposed evaluation protocol.

\subsection{CybMASDE: A development environment to implement the MAMAD method}
\label{sec:cybmasde}

To support the implementation and execution of the MAMAD method in a reproducible and modular way, we developed the \textbf{Cyber Multi-Agent System Development Environment (CybMASDE)}~\footnotemark[1], a dedicated platform that orchestrates the modeling, training, analysis, and deployment of MAS driven by the MOISE+MARL framework.
%
\footnotetext[1]{Source code and documentation available at \url{https://github.com/julien6/CybMASDE}}
%
The modeling module builds a Joint Observation Prediction Model (JOPM) using LSTM dynamics trained on joint histories $\mathcal{D}_{H^j}$, with observations compressed via VAE encoders (latent size 16-64) and actions via MLPs. The LSTM (hidden size 64 or 128) is trained with Adam ($\text{lr}\in[1e^{-4}, 5e^{-4}]$). Rewards $R_H^j$ are manually derived from $\mathcal{G}_{\text{inf}}$, and inferred constraints $\mathcal{C}_{\text{inf}}$ are encoded into MOISE+MARL specs $\mathcal{MM}$ using the MMA API.

Training uses MARLlib~\cite{hu2022marllib} (e.g., MAPPO, MADDPG), with MOISE+MARL enforced via action masking and reward shaping. RLlib runs use $\text{lr}\in[1e^{-4}, 5e^{-4}]$, $\gamma\in[0.9, 0.99]$, PPO clip $\in[0.1, 0.3]$, batch sizes $\{64,128\}$, and MLPs of 64-256 units.
%
Auto-TEMM extends TEMM with full HPO via Optuna. Hierarchical clustering uses distance metrics (e.g., DTW, LCS), with cluster distance optimized to minimize intra-cluster variance and count. Representativeness sweeps (0-1.0) identify values minimizing convergence time (target threshold 3.5\%). Organizational fit averages SOF and FOF, derived from cluster variance.
%
Transferring deploys policies $\pi^j_{\text{latest}}$ via PettingZoo or environment-specific APIs. Once trajectory buffers reach a threshold (e.g., 512 steps), a new modeling--training--analysis cycle is triggered.



\subsection{Computing resources}

All experiments were conducted on an academic high-performance computing cluster, utilizing various configurations of GPU nodes. Specifically, we employed nodes with:
\begin{enumerate*}[label={\roman*)}, itemjoin={; \quad}]
    \item \textbf{GPUs:} NVIDIA A100, AMD MI210
    \item \textbf{Frameworks:} TensorFlow, PyTorch
    \item \textbf{Hyperparameter tuning:} \textbf{Optuna}~\cite{akiba2019optuna} for learning rate, exploration-exploitation balance, and network architecture.
\end{enumerate*}
%
Each algorithm-environment combination was executed on 5 parallel instances to ensure robust and consistent results.

\subsection{Test environments and organizational specifications}

To evaluate the MAMAD method, we employ four distinct multi-agent environments that serve as controlled testbeds. These environments span different problem domains, requiring coordination, strategic decision-making, and role-based interactions. Each environment is formally described below, including its state space, observation space, action space, reward structure, and overall goal. Additionally, we provide the corresponding organizational specifications, detailing roles, missions, and constraints used in the MAMAD framework.

\paragraph{Warehouse Management (WM)}
The \textbf{Warehouse Management}~\cite{warehouse_management} environment models a grid-based logistics warehouse where multiple robots must collaborate to transport goods efficiently. The environment is inspired by industrial warehouse automation scenarios and serves as an ideal testbed for evaluating task allocation, role specialization, and real-time coordination. This environment is illustrated in \autoref{fig:warehouse}.
%
% \begin{enumerate*}[label={\roman*)}, itemjoin={; \quad}]
%     \item \textbf{State Space:} A $N \times M$ grid where each cell contains a robot, a product, a crafting machine, or a drop-off location. The system tracks agent positions, inventory levels, and machine states
%     \item \textbf{Observation Space:} Each agent has a local $V \times V$ view, perceiving products, teammates, and nearby machines
%     \item \textbf{Action Space:} 
%     \begin{enumerate*}[label={\roman*)}, itemjoin={; \quad}]
%         \item Move: \texttt{Up, Down, Left, Right}
%         \item Interact: \texttt{Pick Product, Drop Product}.
%     \end{enumerate*}
%     \item \textbf{Reward Structure:}
%     \begin{enumerate*}[label={\roman*)}, itemjoin={; \quad}]
%         \item Successful product delivery: $+10$
%         \item Inefficient movement: $-1$ per unnecessary step
%         \item Product mishandling: $-5$ for incorrect drop-offs.
%     \end{enumerate*}
%     \item \textbf{goal:} Transport raw materials to processing machines and deliver finished products to drop-off locations.
% \end{enumerate*}
%
\textbf{Organizational Specifications:}
\begin{enumerate*}[label={\roman*)}, itemjoin={; \quad}]
    \item \textbf{Roles:} \texttt{Transporter, Inventory Manager}
    \item \textbf{Missions:} Transporters move products, while Inventory Managers oversee stock levels
    \item \textbf{Constraints:} Transporters must prioritize essential deliveries first.
\end{enumerate*}

\begin{figure}[h!]
    \centering
    \includegraphics[trim=0cm 3cm 0cm 3cm, clip, width=0.6\linewidth]{figures/wm.png}
    \caption{A screenshot of the Warehouse Management environment: agents can move up, down, left, and right, multiple agents operate within a warehouse grid, performing tasks to process and deliver products. Agents can move in four directions (up, down, left, right) and interact with pick/drop zones when adjacent. The workflow involves: (i) collecting primary products from input conveyor pick/drop areas (blue zones); (ii) transporting them to crafting machine pick/drop areas (brown zones), where the primary products are transformed into a single secondary product based on a predefined crafting schema; (iii) retrieving the resulting secondary products and delivering them to output conveyor pick/drop areas (pink zones). Successful operation requires agents to coordinate their movements and actions to optimize throughput and efficiency within the warehouse.}
    \label{fig:warehouse}
\end{figure}

\paragraph{Predator-Prey (PP)}
The \textbf{Predator-Prey} environment is a well-known MARL benchmark~\cite{lowe2017multi}, designed to evaluate coordination among cooperative pursuers (predators) attempting to capture an evasive agent (prey). This environment is illustrated in \autoref{fig:predator_prey}.
%
% \begin{enumerate*}[label={\roman*)}, itemjoin={; \quad}]
%     \item \textbf{State Space:} A continuous 2D space where agents (predators and prey) have $(x, y)$ positions and velocities
%     \item \textbf{Observation Space:} Agents sense nearby entities within a limited radius $r$
%     \item \textbf{Action Space:} 
%     \begin{enumerate*}[label={\roman*)}, itemjoin={; \quad}]
%         \item Move: \texttt{Up, Down, Left, Right, Stay}.
%     \end{enumerate*}
%     \item \textbf{Reward Structure:}
%     \begin{enumerate*}[label={\roman*)}, itemjoin={; \quad}]
%         \item Predators gain $+50$ for capturing the prey
%         \item The prey earns $+1$ per timestep survived ;.
%     \end{enumerate*}
%     \item \textbf{goal:} Predators must cooperate to trap the prey, while the prey attempts to escape as long as possible.
% \end{enumerate*}
%
\textbf{Organizational Specifications:}
\begin{enumerate*}[label={\roman*)}, itemjoin={; \quad}]
    \item \textbf{Roles:} \texttt{Predator, Prey}
    \item \textbf{Missions:} Predators coordinate to enclose the prey; prey seeks optimal escape routes
    \item \textbf{Constraints:} Predators must balance aggressive pursuit with blocking strategies.
\end{enumerate*}
%
\begin{figure}[h!]
    \centering
    \includegraphics[trim=0cm 4.5cm 0cm 1cm, clip,width=0.6\linewidth]{figures/predator_prey.png}
    \caption{A screenshot of the Predator-Prey environment: \textbf{green agents} (cooperative) and \textbf{red agents} (adversarial). The green agents aim to collect food items scattered across the environment while avoiding detection by the red agents. The environment includes \textbf{forest regions} that provide concealment; when a green agent enters a forest, it becomes partially or fully hidden from the red agents' observations. One red agent acts as a \textbf{leader} with enhanced observational capabilities and can communicate with other red agents to coordinate their pursuit.}
    \label{fig:predator_prey}
\end{figure}
%
\paragraph{Overcooked-AI (OA)}
The \textbf{Overcooked-AI} environment~\cite{Carroll2019} simulates a cooperative cooking scenario where agents must collaborate to prepare and serve meals in a structured kitchen. This environment is illustrated in \autoref{fig:overcooked}.
%
% \begin{enumerate*}[label={\roman*)}, itemjoin={; \quad}]
%     \item \textbf{State Space:} A discrete grid-based kitchen with workstations (chopping board, stove, serving counter), ingredients, and agents
%     \item \textbf{Observation Space:} Agents observe kitchen elements within a defined radius
%     \item \textbf{Action Space:} 
%     \begin{enumerate*}[label={\roman*)}, itemjoin={; \quad}]
%         \item Move: \texttt{Up, Down, Left, Right}
%         \item Interact: \texttt{Pick Ingredient, Chop, Cook, Serve}.
%     \end{enumerate*}
%     \item \textbf{Reward Structure:}
%     \begin{enumerate*}[label={\roman*)}, itemjoin={; \quad}]
%         \item Successful meal preparation: $+20$
%         \item Ingredient misplacement: $-5$
%         \item Idle behavior: $-1$ per step without meaningful action.
%     \end{enumerate*}
%     \item \textbf{goal:} Maximize completed meal orders within a fixed time limit.
% \end{enumerate*}
%
\textbf{Organizational Specifications:}
\begin{enumerate*}[label={\roman*)}, itemjoin={; \quad}]
    \item \textbf{Roles:} \texttt{Chef, Assistant, Server}
    \item \textbf{Missions:} The Chef prepares food, the Assistant supplies ingredients, and the Server delivers meals
    \item \textbf{Constraints:} Task execution must be synchronized to prevent bottlenecks.
\end{enumerate*}

\begin{figure}[h!]
    \centering
    \includegraphics[trim=0cm -0.5cm 0cm -0.5cm, clip, width=0.6\linewidth]{figures/overcooked.png}
    \caption{A screenshot of the Overcooked-AI environment: Two agents (chefs) must collaborate to prepare and serve onion soups efficiently. The process involves collecting three onions (one at a time) from the dispenser, placing them into a cooking pot, waiting for the soup to cook, retrieving a clean dish, plating the soup, and delivering it to the serving counter. The kitchen layout includes obstacles and narrow pathways, requiring agents to coordinate their movements to avoid collisions and optimize task completion.}
    \label{fig:overcooked}
\end{figure}

\paragraph{Cyber-Defense Simulation (CS)}
The \textbf{Cyber-Defense Simulation} is an ad hoc drom warm network on which defender agents must defend it from malicious intrusions in various cyberattack scenarios~\cite{Standen2021}. This environment is illustrated in \autoref{fig:cyborg}.
%
% \begin{enumerate*}[label={\roman*)}, itemjoin={; \quad}]
%     \item \textbf{State Space:} A dynamic network graph where nodes represent devices and edges denote active connections
%     \item \textbf{Observation Space:} Agents receive security alerts and network state updates
%     \item \textbf{Action Space:} 
%     \begin{enumerate*}[label={\roman*)}, itemjoin={; \quad}]
%         \item \texttt{Monitor}: Analyze node activity
%         \item \texttt{Block IP}: Restrict access from a suspicious source
%         \item \texttt{Deploy Patch}: Strengthen network defenses ;.
%     \end{enumerate*}
%     \item \textbf{Reward Structure:}
%     \begin{enumerate*}[label={\roman*)}, itemjoin={; \quad}]
%         \item Preventing an attack: $+30$
%         \item False positive block: $-10$
%         \item Allowing a breach: $-50$.
%     \end{enumerate*}
%     \item \textbf{goal:} Detect and mitigate cyber threats while avoiding false positives.
% \end{enumerate*}
%
\textbf{Organizational Specifications:}
\begin{enumerate*}[label={\roman*)}, itemjoin={; \quad}]
    \item \textbf{Roles:} \texttt{Threat Analyst, Firewall Manager, Security Operator}
    \item \textbf{Missions:} Detect threats, block unauthorized access, maintain network integrity
    \item \textbf{Constraints:} Minimizing false positives while ensuring security coverage.
\end{enumerate*}

\begin{figure}[h!]
    \centering
    \includegraphics[trim=0cm 1cm 0cm 1cm, clip, width=0.6\linewidth]{figures/cyborg.png}
    \caption{A screenshot of the CybORG environment: A swarm of 18 autonomous drones, initially controlled by blue (defensive) agents, forms an ad hoc network to facilitate communication between ground units. Each drone is susceptible to a hardware Trojan that can activate randomly, replacing the blue agent with a red (offensive) agent. Red agents aim to compromise the network by intercepting or blocking communications. The drones move according to a swarm algorithm, dynamically altering the network topology. Blue agents must detect and neutralize compromised drones while maintaining communication integrity.}
    \label{fig:cyborg}
\end{figure}

\medskip

\noindent These four environments provide diverse challenges, covering cooperative, competitive, hierarchical, and adversarial scenarios, enabling a representative evaluation.


\subsection{Evaluation metrics}

To assess whether the MAMAD method effectively bridges the identified research gaps, we define a set of quantitative metrics across four evaluation criteria: \textbf{automation}, \textbf{efficiency}, \textbf{compliance with design requirements}, and \textbf{explainability}.

\subsubsection{Automation metrics}
To measure MAMAD's automatation level in generating MAS, we evaluate:
\begin{enumerate*}[label={\roman*)}, itemjoin={; \quad}]
    \item \textbf{Performance relative to time required for manual MAS design} ($T_{design}$): Measures the MAS performance to reaching its goal relative to the approximate order of magnitude of total duration (roughly expressed as days) from environment modeling to final deployment;
    \item \textbf{Injected knowledge quantity} ($K_{design}$): Measures the number of lines required to define roles and goals as a way to quantify the human involvement into MAS design;
    \item \textbf{Iterations to convergence} ($N_{iter}$): Counts the number of training cycles needed for the system to stabilize at an optimal policy (considering zero for hand-crafted MAS).
\end{enumerate*}

\subsubsection{Efficiency metrics}
To determine the effectiveness of the MAS solutions generated by MAMAD, we use:
\begin{enumerate*}[label={\roman*)}, itemjoin={; \quad}]
    \item \textbf{Cumulative Reward} ($R_{cum}$): The total reward achieved by agents, reflecting their overall performance in achieving the system's goals
    \item \textbf{Policy Stability} ($\sigma_R$): Standard deviation of cumulative rewards across episodes, assessing consistency
    \item \textbf{Convergence Rate} ($CR$): Measures the speed at which learning stabilizes
    \item \textbf{Robustness Score} ($R_{robust}$): Evaluates the ability of the MAS to maintain performance under external perturbations.
\end{enumerate*}

\subsubsection{Metrics for compliance with design requirements metrics and explainability}
To validate whether MAMAD produces policies that conform to predefined specifications, we measure:
\begin{enumerate*}[label={\roman*)}, itemjoin={; \quad}]
    \item \textbf{Constraint Violation Rate} ($V_c$): The percentage of policy executions where agents fail to adhere to predefined organizational constraints
    \item \textbf{Organizational Fit Level} ($F_{org}$): The similarity between the inferred organizational structure (post-training) and the predefined design or implicit one
    \item \textbf{Consistency Score} ($S_{cons}$): Quantifies how closely the assigned roles and missions match those expected by human designers. A high consistency score shows the TEMM method to be efficiently able to find back the initially given roles and goals, demonstrating its capability to organizationally explain agents behaviors.
\end{enumerate*}

\subsection{Evaluation protocol}

We structure the experimental protocol into the following components:

\subsubsection{Comparison with classical MAS design methods}
To benchmark MAMAD's performance, we compare it against traditional manual MAS design approaches:
\begin{enumerate*}[label={\roman*)}, itemjoin={; \quad}]
    \item \textbf{Reference Baseline (RB)}: Agents trained using standard MARL algorithms (e.g., MADDPG, MAPPO) without any organizational constraints. This serves as an ablation baseline to assess the added value of organizational guidance
    \item \textbf{Organizational Baseline (OB)}: Agents trained with manually specified $\mathcal{M}OISE^+$ organizational constraints, developed by human experts
    \item \textbf{MAMAD-Based MAS (MB)}: Agents trained using MAMAD's automated workflow, including inferred organizational constraints.
\end{enumerate*}
%
All experiments are conducted in four test environments using the same training settings across baselines.

\subsubsection{Validation of explainability and organizational compliance}
To ensure that MAMAD produces meaningful and interpretable organizational specifications, we conduct:
\begin{enumerate*}[label={\roman*)}, itemjoin={; \quad}]
    \item \textbf{Comparative Role and Mission Analysis}: We compare predefined and inferred role structures, measuring consistency and stability
    \item \textbf{Similarity Analysis on Organizational Specifications}: We compute role similarity scores to assess the alignment between predefined and learned roles
    \item \textbf{Visualization of Goals and Transitions}: PCA of observations, actions, and transitions are used to assess the interpretability of goals and roles inferred via TEMM.
\end{enumerate*}
%
If inferred roles and missions remain stable across training runs and align with expectations, this validates MAMAD's ability to structure MAS designs.

% \subsubsection{Ablation studies and robustness evaluation}
% To evaluate the impact of MAMAD's automated components, we conduct ablation studies by selectively disabling key components~\label{sec:experimental_setup_ablations}:
% \begin{enumerate*}[label={\roman*)}, itemjoin={; \quad}]
%     \item \textbf{Without Automated Modeling (WAM)}: Manual environment models were used instead of neural world models
%     \item \textbf{Without Organizational Constraints (WOC)}: No MOISE+MARL specifications were provided during training
%     \item \textbf{Without Trajectory-Based Analysis (WTA)}: No trajectory-based organizational extraction; policies were directly deployed post-training.
% \end{enumerate*}

% Each ablation scenario is tested in at least two environments, with performance compared to the full MAMAD pipeline.


% \begin{table}[h!]
%     \centering
%     \renewcommand{\arraystretch}{1.3}
%     \begin{footnotesize}
%         \footnotesize
%         \begin{tabular}{p{1.5cm}p{2.4cm}p{2.2cm}p{4.5cm}}
%             \hline
%             \textbf{Criterion} & \textbf{Metric}       & \textbf{Validation Method}   & \textbf{Potential Bias or Limitation}            \\
%             \hline
%             \multirow{3}{*}{Automation}
%                                & $T_{design}$ (Performance per design time)   & Experiment logs + MAS performance & May depend on subjective time estimation granularity \\
%                                & $K_{design}$ (Injected knowledge quantity)   & Code lines of role/goal specs    & Approximate proxy for knowledge complexity \\
%                                & $N_{iter}$ (Iterations to convergence)       & Training curves                  & Influenced by initial HPO space \\
%             \hline
%             \multirow{4}{*}{Efficiency}
%                                & $R_{cum}$ (Cumulative reward)   & Score tracking     & Task-specific reward shaping may bias \\
%                                & $\sigma_R$ (Policy stability)   & Reward variance     & Sensitive to stochasticity \\
%                                & $CR$ (Convergence rate)         & Convergence analysis & Hyperparameter-sensitive \\
%                                & $R_{robust}$ (Robustness score) & Perturbation tests  & Depends on perturbation type \\
%             \hline
%             \multirow{3}{*}{\shortstack{Compliance \& \\ Explainability}}
%                                & $V_c$ (Constraint violation rate) & Rule checking      & May miss implicit violations \\
%                                & $F_{org}$ (Organizational fit)    & Role/goal alignment analysis & Relies on cluster assumptions \\
%                                & $S_{cons}$ (Consistency score)    & Role recovery analysis & Sensitive to clustering granularity \\
%             \hline
%         \end{tabular}
%         \caption{Validation strategy with evaluation criteria, metrics, methods, and limitations.}
%         \label{tab:validation_strategy}
%     \end{footnotesize}
% \end{table}


\section{Results and discussion} \label{sec:results}

This section presents the results obtained by applying MAMAD across the four test environments. The evaluation follows the defined metrics and protocol. A summary of results is provided in \autoref{tab:g1_g2_g3_g4}.

\medskip

\begin{table}[h!]
    \centering
    \caption{All metrics across methods and environments (G1--G3)}
    \footnotesize
    \begin{tabular}{p{0.5cm}|p{1cm}|p{0.4cm}p{0.3cm}p{0.3cm}p{0.85cm}|p{0.4cm}p{0.4cm}p{0.55cm}|p{0.8cm}p{0.8cm}p{0.8cm}}
        \hline
        \textbf{Env.} & \textbf{Method}

        
        
        
        
        
        
        
        
        
        
        
        
                      & $R_{cum}$       & $\sigma_R$   & $CR$       & $R_{robust}$

        
        
        
        
        
        
        
        
        
        
        
        
                      & $V_c$           & $F_{org}$    & $S_{cons}$

        
        
        
        
        
        
        
        
        
        
        
        
                      & $T_{design}$    & $K_{design}$ & $N_{iter}$                                                            \\
        \hline
        \multirow{3}{*}{OA}
                      & RB              & 85\%         & 8\%        & 220          & 75\% & 15\% & 70\% & 65\% & 1.0 & 20  & 1 \\
                      & OB              & 92\%         & 4\%        & 160          & 85\% & 3\%  & 92\% & 90\% & 2.5 & 300 & 2 \\
                      & MB              & 95\%         & 3\%        & 150          & 90\% & 2\%  & 95\% & 93\% & 1.5 & 120 & 2 \\
        \hline
        \multirow{3}{*}{PP}
                      & RB              & 80\%         & 10\%       & 250          & 70\% & 18\% & 65\% & 60\% & 1.2 & 25  & 1 \\
                      & OB              & 88\%         & 5\%        & 180          & 80\% & 5\%  & 88\% & 85\% & 3.0 & 320 & 3 \\
                      & MB              & 90\%         & 4\%        & 170          & 85\% & 4\%  & 90\% & 88\% & 1.8 & 140 & 3 \\
        \hline
        \multirow{3}{*}{WM}
                      & RB              & 83\%         & 9\%        & 230          & 72\% & 12\% & 68\% & 63\% & 1.5 & 30  & 1 \\
                      & OB              & 91\%         & 5\%        & 170          & 82\% & 4\%  & 91\% & 89\% & 3.5 & 340 & 2 \\
                      & MB              & 93\%         & 4\%        & 160          & 87\% & 3\%  & 94\% & 91\% & 2.0 & 150 & 2 \\
        \hline
        \multirow{3}{*}{CS}
                      & RB              & 78\%         & 12\%       & 280          & 65\% & 20\% & 60\% & 55\% & 2.0 & 40  & 1 \\
                      & OB              & 85\%         & 7\%        & 200          & 75\% & 6\%  & 85\% & 82\% & 4.0 & 400 & 3 \\
                      & MB              & 87\%         & 6\%        & 190          & 80\% & 5\%  & 87\% & 85\% & 2.5 & 180 & 3 \\
        \hline
    \end{tabular}
    \label{tab:g1_g2_g3_g4}
\end{table}

\subsection{G1 - Leveraging MARL within AOSE (Efficiency)}

We first evaluate the efficiency of learning across the three methods using:
%
\begin{enumerate*}[label={\roman*)}, itemjoin={; \quad}]
    \item \textbf{Cumulative reward} ($R_{cum}$)
    \item \textbf{Policy stability} ($\sigma_R$)
    \item \textbf{Convergence rate} ($CR$)
    \item \textbf{Robustness score} ($R_{robust}$).
\end{enumerate*}
%
MAMAD (MB) consistently outperforms both baselines across environments in reward maximization and stability. The organizational guidance accelerates convergence ($CR$) and improves robustness under perturbations ($R_{robust}$). The largest gain is observed in highly cooperative tasks like Overcooked-AI.

\subsection{G1 - Automation capability}

We evaluate design automation using:
%
\begin{enumerate*}[label={\roman*)}, itemjoin={; \quad}]
    \item \textbf{Performance relative to design time} ($T_{design}$)
    \item \textbf{Injected knowledge quantity} ($K_{design}$)
    \item \textbf{Iterations to convergence} ($N_{iter}$).
\end{enumerate*}
%
While RB has minimal knowledge injection, it lacks any organizational control. OB requires large expert effort ($K_{design}$) and longer design cycles. MAMAD significantly reduces manual specification while automatically discovering relevant organizational structures in only a few iterations.

\subsection{G2 \& G3 - Compliance and explainability}

Finally, we now assess policy compliance and explainability using:
%
\begin{enumerate*}[label={\roman*)}, itemjoin={; \quad}]
    \item \textbf{Constraint violation rate} ($V_c$)
    \item \textbf{Organizational fit level} ($F_{org}$)
    \item \textbf{Consistency score} ($S_{cons}$).
\end{enumerate*}
%
MAMAD achieves high organizational fit and consistency, closely approaching OB where constraints are manually engineered. The trajectory-based analysis allows MB to recover organizational structures with minimal violations ($V_c$), even outperforming OB in certain environments due to better alignment during learning.

\section{Conclusion and perspectives}\label{sec:conclusion}

This work introduced \textbf{MAMAD}, a method for automating the design of multi-agent systems by integrating organizational modeling with MARL. By structuring the design process into distinct phases—environment modeling, agent training, behavior analysis, and deployment—MAMAD reduces reliance on expert intervention while increasing automation throughout the MAS development pipeline. Quantitative evaluations show that MAMAD not only accelerates design iteration but also improves compliance with organizational constraints and enhances the explainability of agent roles and missions. These results underscore the potential of combining MARL with organizational frameworks to streamline and strengthen MAS development.

While MAMAD offers a structured and automated pipeline for MAS design, it still presents several limitations that warrant further research. Expert oversight remains necessary for defining reward functions and tuning hyperparameters; the method's scalability to complex, large-scale, or real-world settings remains limited; interpretability of learned behaviors, especially in adversarial contexts, could be enhanced; and the computational overhead of automated modeling may hinder real-time applications. To address these challenges and broaden its applicability, future work should focus on integrating advanced interpretability tools such as causal reasoning or attention-based visualizations, improving scalability to high-dimensional problems and large agent populations, combining automation with human-in-the-loop feedback to balance control and flexibility, and validating MAMAD in real-world domains like robotics, cybersecurity, or logistics.



\section*{Declarations}
%
\begin{enumerate*}
    \item \textbf{Funding:} This work was supported by Thales Land \& Air Systems.
    \item \textbf{Conflict of interest:} The authors declare no conflict of interest.
    \item \textbf{Ethics approval and consent to participate:} Not applicable.
    \item \textbf{Consent for publication:} Not applicable.
    \item \textbf{Data availability:} Not applicable.
    \item \textbf{Materials availability:} Not applicable.
    \item \textbf{Code availability:} The source code used in this study is publicly available at: \url{https://github.com/julien6/CybMASDE}
    \item \textbf{Author contributions:} Julien Soulé: conceptualization, methodology, implementation, writing—original draft.\\
    Jean-Paul Jamont, Michel Occello: supervision, writing—review.\\
    Louis-Marie Traonouez, Paul Théron: validation, writing—review.
\end{enumerate*}

\setlength{\bibsep}{4.5pt}
\bibliography{references}

\end{document}
