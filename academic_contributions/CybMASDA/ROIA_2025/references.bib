@inproceedings{soule2024paper-jfsma,
  author    = {Soule, Julien and Jamont, Jean-Paul and Occello, Michel and Traonouez, Louis-Marie and Théron, Paul},
  title     = {Une Approche bas{\'{e}}e sur l'Apprentissage par Renforcement pour l'Ing{\'{e}}nierie Organisationelle d'un {SMA}},
  booktitle = {32èmes
               journ{\'{e}}es francophones sur les syst{\`{e}}mes multi-agents},
  year      = {2024}
}

@inproceedings{maisonhaute2024,
  author    = {Tiziano Maisonhaute and
               Fabien Michel and
               Jean{-}Christophe Souli{\'{e}}},
  title     = {{\'{E}}tat de l'art sur les approches en apprentissage par renforcement
               multi-agent},
  booktitle = {32{\'{e}}me journ{\'{e}}es francophones sur les syst{\`{e}}mes multi-agents},
  year      = {2024}
}

@inproceedings{soule2024aomea,
  author    = {Soule, Julien and Jamont, Jean-Paul and Occello, Michel and Traonouez, Louis-Marie and Théron, Paul},
  title     = {A MARL-Based Approach for Easing {SMA} Organization Engineering},
  booktitle = {Proc. of the 20th Int. Conf. Artificial Intelligence Applications and Innovations},
  year      = {2024},
  doi       = {10.1007/978-3-031-63223-5\_24}
}

@article{soule2025moisemarl,
  title   = {An organizationally-Oriented Approach to Enhancing Explainability and Control in Multi-Agent Reinforcement Learning},
  author  = {Soule, Julien and Jamont, Jean-Paul and Occello, Michel and Traonouez, Louis-Marie and Théron, Paul},
  journal = {Proc. of the 24th Int. Conf. on Autonomous Agents and Multiagent Systems},
  year    = {2025},
  note    = {à paraitre}
}

@article{yu2021mappo,
  title   = {The Surprising Effectiveness of PPO in Cooperative Multi-Agent Games},
  author  = {{Chengjie Yu et al.}},
  journal = {NeurIPS},
  year    = {2021}
}

@inproceedings{akiba2019optuna,
  author    = {{Akiba Takuya et al.}},
  title     = {Optuna: A Next-generation Hyperparameter Optimization Framework},
  year      = {2019},
  isbn      = {9781450362016},
  doi       = {10.1145/3292500.3330701},
  booktitle = {Proc. of the 25th ACM SIGKDD Int. Conf. on Knowledge Discovery \& Data Mining}
}

@article{foerster2018counterfactual,
  title   = {Counterfactual multi-agent policy gradients},
  author  = {Foerster, Jakob and others},
  journal = {Int. Conf. on Machine Learning},
  year    = {2018}
}

@article{rashid2018qmix,
  title   = {QMIX: Monotonic value function factorisation for deep multi-agent reinforcement learning},
  author  = {{Tabish Rashid et al.}},
  journal = {Proc. of the 35th Int. Conf. on Machine Learning},
  year    = {2018}
}

@misc{Maxwell2021,
  title         = {CybORG: A Gym for the Development of Autonomous Cyber Agents},
  author        = {{Standen Maxwell et al.}},
  year          = {2021},
  eprint        = {2108.09118},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CR}
}

@article{overcookedai,
  title   = {Overcooked-AI: A Benchmark for Multi-Agent Learning under Partial Observability},
  author  = {{Micah Carroll et al.}},
  journal = {Proc. of the IEEE/RSJ Int. Conf. on Intelligent Robots and Systems},
  year    = {2020}
}

@article{lowe2017multi,
  title   = {Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments},
  author  = {{Ryan Lowe et al.}},
  journal = {NeurIPS},
  year    = {2017},
  volume  = {30}
}

@article{hu2021marlib,
  title   = {MarlLib: A comprehensive library for multi-agent reinforcement learning},
  author  = {{Qi Hu et al.}},
  journal = {arXiv preprint arXiv:2106.05912},
  year    = {2021}
}

@misc{kwiatkowski2024,
  title         = {Gymnasium: A Standard Interface for Reinforcement Learning Environments},
  author        = {{Ariel Kwiatkowski et al.}},
  year          = {2024},
  eprint        = {2407.17032},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG},
  url           = {https://arxiv.org/abs/2407.17032}
}

@article{terry2020pettingzoo,
  title   = {PettingZoo: Gym for multi-agent reinforcement learning},
  author  = {{Justin K Terry et al.}},
  journal = {Proc. of the NeurIPS 2020 Track on Datasets and Benchmarks},
  year    = {2020}
}

@inproceedings{ferber2003,
  title     = {{Agent/Group/Roles: Simulating with Organizations}},
  author    = {Ferber, Jacques and Gutknecht, Olivier and Michel, Fabien},
  url       = {https://hal-lirmm.ccsd.cnrs.fr/lirmm-00269714},
  booktitle = {{4th Int. Workshop on Agent-Based Simulation}},
  year      = {2003}
}

@book{Oliehoek2016,
  title  = {A Concise Introduction to Decentralized POMDPs},
  author = {Frans A. Oliehoek and Christopher Amato},
  year   = {2016},
  isbn   = {978-3-319-28929-8},
  url    = {https://link.springer.com/book/10.1007/978-3-319-28929-8}
}

@inproceedings{Beynier2013,
  title     = {A Decentralized Approach for Reinforcement Learning in Cooperative Multi-agent Systems},
  author    = {Aurélie Beynier and Alain Mouaddib},
  booktitle = {Proc. of the 23rd Int. Joint Conf. on Artificial Intelligence},
  year      = {2013}
}

@article{Albrecht2024,
  title   = {Survey on Recent Advances in Cooperative Multi-Agent Reinforcement Learning},
  author  = {Stefano V. Albrecht and Jacob Y. Foerster},
  journal = {Journal of Artificial Intelligence Research},
  year    = {2024}
}

@inproceedings{achiam2017cpo,
  title     = {Constrained Policy Optimization},
  author    = {{Joshua Achiam et al.}},
  booktitle = {Proc. of the 34th Int. Conf. on Machine Learning},
  year      = {2017}
}

@inproceedings{ray2019benchmarking,
  title     = {Benchmarking Safe Exploration in Deep Reinforcement Learning},
  author    = {{Alex Ray et al.}},
  booktitle = {arXiv:1910.01708},
  year      = {2019}
}

@article{garcia2015comprehensive,
  title   = {A comprehensive survey on safe reinforcement learning},
  author  = {Garcia, Javier and Fernandez, Fernando},
  journal = {Journal of Machine Learning Research},
  year    = {2015}
}

@article{alshiekh2018safe,
  title   = {Safe reinforcement learning via shielding},
  author  = {{Mohammed Alshiekh et al.}},
  journal = {Proc. of the 32nd AAAI Conf. on Artificial Intelligence},
  year    = {2018}
}

@inproceedings{ghavamzadeh2006hrl,
  title     = {Hierarchical reinforcement learning with cooperative agents},
  author    = {Ghavamzadeh, Mohammad and Mahadevan, Sridhar},
  booktitle = {Proc. of the 23rd Int. Conf. on Machine Learning},
  year      = {2006}
}

@article{foerster2018communication,
  title   = {Learning to Communicate with Deep Multi-Agent Reinforcement Learning},
  author  = {{Jakob Foerster et al.}},
  journal = {NeurIPS},
  year    = {2018}
}

@inproceedings{wilson2008learning,
  title     = {Learning and transferring roles in multi-agent MDPs},
  author    = {{Andrew Wilson et al.}},
  booktitle = {Proc. of AAAI},
  year      = {2008}
}

@article{berenji2000learning,
  title   = {Learning, cooperation, and coordination in multi-agent systems},
  author  = {Berenji, Hamid R and Vengerov, David},
  journal = {Inference Systems Corporation, Technical report},
  year    = {2000}
}

@article{yusuf2020inferential,
  title   = {Inferential Reasoning for Heterogeneous Multi-Agent Missions},
  author  = {Yusuf, Sagir M and Baber, Christopher},
  journal = {Int. Journal of Electrical and Computer Engineering},
  year    = {2020}
}

@inproceedings{serrino2019finding,
  title     = {Finding Friend and Foe in Multi-Agent Games},
  author    = {Serrino, Jack and Kleiman-Weiner, Max and others},
  booktitle = {NeurIPS},
  year      = {2019}
}

@article{Hubner2007,
  title   = {Developing organised multiagent systems using the MOISE+ model: programming issues at the system and agent levels},
  issn    = {1746-1383},
  url     = {http://dx.doi.org/10.1504/IJAOSE.2007.016266},
  doi     = {10.1504/ijaose.2007.016266},
  journal = {Int. Journal of Agent-Oriented Software Engineering},
  author  = {{Hubner,  Jomi F et. al.}},
  year    = {2007}
}

@phdthesis{SaoMai2024,
  author = {Sao Mai, N.},
  title  = {The intrinsic motivation of reinforcement and imitation learning for sequential tasks},
  school = {HAL Archive},
  year   = {2024},
  url    = {https://hal.science/tel-04853270}
}

@article{Matsuyama2025,
  author        = {{K. Matsuyama et al.}},
  title         = {CORD: Generalizable Cooperation via Role Diversity},
  journal       = {arXiv preprint},
  year          = {2025},
  archiveprefix = {arXiv},
  eprint        = {2501.02221},
  url           = {https://arxiv.org/abs/2501.02221}
}

@article{Qi2024,
  author  = {Qi, Y. and Cao, J. and Wu, B.},
  title   = {Bidirectional Q-learning for recycling path planning of used appliances under strong and weak constraints},
  journal = {Communications in Transportation Research},
  year    = {2024},
  url     = {https://www.sciencedirect.com/science/article/pii/S2772424724000362}
}

@article{Xie2024,
  author  = {{Z. Xie et al.}},
  title   = {Roco: Role-Oriented Communication for Efficient Multi-Agent Reinforcement Learning},
  journal = {SSRN Electronic Journal},
  year    = {2024},
  url     = {https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5060074}
}

@incollection{Isakov2024,
  author    = {{A. Isakov et al.}},
  title     = {Cooperative-Competitive Decision-Making in Resource Management: A Reinforcement Learning Perspective},
  booktitle = {Advances in Machine Learning and Automated Learning},
  year      = {2024},
  doi       = {10.1007/978-3-031-77731-8_34},
  url       = {https://link.springer.com/chapter/10.1007/978-3-031-77731-8_34}
}

@article{Wen2024,
  author        = {{W. Wen et al.}},
  title         = {Role Play: Learning Adaptive Role-Specific Strategies in Multi-Agent Interactions},
  journal       = {arXiv preprint},
  year          = {2024},
  archiveprefix = {arXiv},
  eprint        = {2411.01166},
  url           = {https://arxiv.org/abs/2411.01166}
}

@inproceedings{Foerster2016,
  author    = {Foerster, Jakob and Assael, Ioannis Alexandros and de Freitas, Nando and Whiteson, Shimon},
  booktitle = {Advances in Neural Information Processing Systems},
  editor    = {D. Lee and M. Sugiyama and U. Luxburg and I. Guyon and R. Garnett},
  pages     = {},
  publisher = {Curran Associates, Inc.},
  title     = {Learning to Communicate with Deep Multi-Agent Reinforcement Learning},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2016/file/c7635bfd99248a2cdef8249ef7bfbef4-Paper.pdf},
  volume    = {29},
  year      = {2016}
}

@book{Albrecht2024book,
  author    = {Stefano V. Albrecht and Filippos Christianos and Lukas Sch\"afer},
  title     = {Multi-Agent Reinforcement Learning: Foundations and Modern Approaches},
  publisher = {MIT Press},
  year      = {2024},
  url       = {https://www.marl-book.com}
}

@inproceedings{wang2020roma,
  title        = {ROMA: Multi-Agent Reinforcement Learning with Emergent Roles},
  author       = {Wang, Tonghan and Dong, Hao and Lesser, Victor and Zhang, Chongjie},
  booktitle    = {Proceedings of the 37th International Conference on Machine Learning (ICML)},
  year         = {2020},
  pages        = {9876--9886},
  organization = {PMLR}
}

@article{zeng2023sird,
  title        = {Effective and Stable Role-Based Multi-Agent Collaboration by Structural Information Principles},
  volume       = {37},
  url          = {https://ojs.aaai.org/index.php/AAAI/article/view/26390},
  doi          = {10.1609/aaai.v37i10.26390},
  abstractnote = {Role-based learning is a promising approach to improving the performance of Multi-Agent Reinforcement Learning (MARL). Nevertheless, without manual assistance, current role-based methods cannot guarantee stably discovering a set of roles to effectively decompose a complex task, as they assume either a predefined role structure or practical experience for selecting hyperparameters. In this article, we propose a mathematical Structural Information principles-based Role Discovery method, namely SIRD, and then present a SIRD optimizing MARL framework, namely SR-MARL, for multi-agent collaboration. The SIRD transforms role discovery into a hierarchical action space clustering. Specifically, the SIRD consists of structuralization, sparsification, and optimization modules, where an optimal encoding tree is generated to perform abstracting to discover roles. The SIRD is agnostic to specific MARL algorithms and flexibly integrated with various value function factorization approaches. Empirical evaluations on the StarCraft II micromanagement benchmark demonstrate that, compared with state-of-the-art MARL algorithms, the SR-MARL framework improves the average test win rate by 0.17%, 6.08%, and 3.24%, and reduces the deviation by 16.67%, 30.80%, and 66.30%, under easy, hard, and super hard scenarios.},
  number       = {10},
  journal      = {Proceedings of the AAAI Conference on Artificial Intelligence},
  author       = {Zeng, Xianghua and Peng, Hao and Li, Angsheng},
  year         = {2023},
  month        = {Jun.},
  pages        = {11772-11780}
}

@article{ahmed2022survey,
  title   = {Deep Reinforcement Learning for Multi-Agent Interaction: A Survey},
  author  = {Ahmed, Fawaz and Dinh, Thang and Thai, My T.},
  journal = {ACM Computing Surveys},
  year    = {2022},
  volume  = {55},
  number  = {8},
  pages   = {1--37},
  doi     = {10.1145/3543510},
  url     = {https://dl.acm.org/doi/10.1145/3543510}
}

@article{zhang2025clustering,
  author   = {Peng Zhang and Mengyu Jin and Ming Wang and Jie Zhang and Junjie He and Peng Zheng},
  title    = {A clustering-aided multi-agent deep reinforcement learning for multi-objective parallel batch processing machines scheduling in semiconductor manufacturing},
  journal  = {Measurement and Control},
  volume   = {58},
  number   = {5},
  pages    = {614-631},
  year     = {2025},
  doi      = {10.1177/00202940241269643},
  url      = { 
              
              https://doi.org/10.1177/00202940241269643
              
              
              
              },
  eprint   = { 
              
              https://doi.org/10.1177/00202940241269643
              
              
              
              },
  abstract = { Batch processing machines are often the bottleneck in semiconductor manufacturing and their scheduling plays a key role in production management. Pioneer researches on multi-objective batch machines scheduling mainly focus on evolutionary algorithms, failing to meet the online scheduling demand. To deal with the challenges confronted by incompatible job families, dynamic job arrivals, capacitated machines and multiple objectives, we propose a clustering-aided multi-agent deep reinforcement learning approach (CA-MADRL) for the scheduling problem. Specifically, to achieve diverse nondominated solutions, an offline multi-objective scheduling algorithm named Multi-Subpopulation fast elitist Non-Dominated Sorting Genetic Algorithm (MS-NSGA-II) is firstly developed to obtain the Pareto Fronts, and a clustering algorithm based on cosine distance is employed to analyze the distribution of Pareto frontier solution, which would be used to guide reward functions design in multi-agent deep reinforcement learning. To realize multi-objective optimization, several reinforcement learning base models are trained for different optimization directions, each of which composed of batch forming agent and batch scheduling agent. To alleviate time complexity of model training, a parameter sharing strategy is introduced between different reinforcement learning base model. By validating the proposed approach with 16 instances designed based on actual production data from a semiconductor manufacturing company, it has been demonstrated that the approach not only meets the high-frequency scheduling requirements of manufacturing systems for parallel batch processing machines but also effectively reduces the total job tardiness and machine energy consumption. }
}

@article{nguyen2022transfer,
  title   = {Learning to Transfer Role Assignment Across Team Sizes},
  author  = {Nguyen, Thanh and Nguyen, Van-Hieu and La, Hung},
  journal = {arXiv preprint arXiv:2204.12937},
  year    = {2022},
  url     = {https://arxiv.org/abs/2204.12937}
}

@inproceedings{Hammar2023,
  author    = {Hammar, Kim and Stadler, Rolf},
  booktitle = {NOMS 2023-2023 IEEE/IFIP Network Operations and Management Symposium},
  doi       = {10.1109/NOMS56928.2023.10154288},
  keywords  = {Automation;Emulation;Reinforcement learning;Markov processes;Data models;Digital twins;Security;Digital twin;cybersecurity;network security;automation;reinforcement learning;bMDP;POMDP},
  number    = {},
  pages     = {1-6},
  title     = {Digital Twins for Security Automation},
  volume    = {},
  year      = {2023}
}

@article{Wang2025,
  author   = {Wang, Xiaoxiao and Yi, Peng and Hong, Yiguang},
  journal  = {IEEE Transactions on Artificial Intelligence},
  title    = {A Hierarchical Deep Reinforcement Learning Strategy for Collective Pursuit-Evasion Game with Partial Observations},
  year     = {2025},
  volume   = {},
  number   = {},
  pages    = {1-15},
  keywords = {Games;Heuristic algorithms;Training;Artificial intelligence;Biological system modeling;Collaboration;Resource management;Multi-agent systems;Differential games;Analytical models;Collective Pursuit-Evasion Games;Deep Reinforcement Learning;Hierarchical Strategies;Multi-agent Systems;Partially Observable Markov Decision Process},
  doi      = {10.1109/TAI.2025.3566069}
}


@inproceedings{Chahoud2025,
  author    = {Chahoud, Mario and Sami, Hani and Mizouni, Rabeb and Otrok, Hadi and Bentahar, Jamal and Mourad, Azzam and Talhi, Chamseddine},
  booktitle = {2025 International Wireless Communications and Mobile Computing (IWCMC)},
  title     = {Multi-Agent Deep Reinforcement Learning for Resource Management in On-Demand Environments},
  year      = {2025},
  volume    = {},
  number    = {},
  pages     = {1264-1269},
  keywords  = {Training;Wireless communication;Decision making;Dynamic scheduling;Deep reinforcement learning;Real-time systems;Resource management;Mobile computing;Convergence;Overfitting;Multi-Agent Reinforcement Learning;Deep Q-Network;On-Demand Service Deployment;Dec-POMDP;Resource Management},
  doi       = {10.1109/IWCMC65282.2025.11059489}
}
