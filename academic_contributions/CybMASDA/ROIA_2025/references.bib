@article{soulej2025jaamas,
  author   = {Julien Soul{\'{e}} and Jean{-}Paul Jamont and Michel Occello and Louis{-}Marie Traonouez and Paul Th{\'{e}}ron},
  title    = {Assisting Multi-Agent System Design with $\mathcal{M}OISE^+$ and {MARL}: The {MAMAD} Method},
  journal  = {Journal of Autonomous Agents and Multi-Agent Systems (JAAMAS)},
  note     = {\textit{Under revision}},
  year     = {2025},
  keywords = {journal}
}

@article{soule2025roia,
  title    = {{MOISE+MARL : un cadre organisationnel pour l'explicabilité et le contrôle en apprentissage par renforcement multi-agent}},
  author   = {Soulé, Julien and Jamont, Jean-Paul and Occello, Michel and Traonouez, Louis-Marie and Théron, Paul},
  journal  = {{Revue Ouverte d'Intelligence Artificielle (ROIA)}},
  keywords = {journal},
  year     = {2025},
  note     = {\textit{Under revision}}
}

@inproceedings{soulej2025cloud,
  author    = {Soulé, Julien and Jamont, Jean-Paul and Occello, Michel and Traonouez, Louis-Marie and Théron, Paul},
  booktitle = {2025 IEEE 18th International Conference on Cloud Computing (CLOUD)},
  title     = {Streamlining Resilient Kubernetes Autoscaling with Multi-Agent Systems via an Automated Online Design Framework},
  year      = {2025},
  volume    = {},
  number    = {},
  pages     = {43-53},
  keywords  = {Training;Cloud computing;Scalability;Reinforcement learning;Manuals;Aerodynamics;Robustness;Resilience;Multi-agent systems;Periodic structures;Adversarial;Horizontal Pod Autoscaling;Multi-Agent Reinforcement Learning;Multi-Agent System Design component;formatting;style;styling;insert},
  doi       = {10.1109/CLOUD67622.2025.00015},
  keywords  = {international}
}


@inproceedings{soule2024moise_marl,
  author    = {Soul\'{e}, Julien and Jamont, Jean-Paul and Occello, Michel and Traonouez, Louis-Marie and Th\'{e}ron, Paul},
  title     = {An Organizationally-Oriented Approach to Enhancing Explainability and Control in Multi-Agent Reinforcement Learning},
  year      = {2025},
  isbn      = {9798400714269},
  publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
  address   = {Richland, SC},
  abstract  = {Multi-Agent Reinforcement Learning can lead to the development of collaborative agent behaviors that show similarities with organizational concepts. Pushing forward this perspective, we introduce a novel framework that explicitly incorporates organizational roles and goals from the MOISE+ model into the MARL process, guiding agents to satisfy corresponding organizational constraints. By structuring training with roles and goals, we aim to enhance both the explainability and control of agent behaviors at the organizational level, whereas much of the literature primarily focuses on individual agents. Additionally, our framework includes a post-training analysis method to infer implicit roles and goals, offering insights into emergent agent behaviors. This framework has been applied across various MARL environments and algorithms, demonstrating coherence between predefined organizational specifications and those inferred from trained agents.},
  booktitle = {Proceedings of the 24th International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1968–1976},
  numpages  = {9},
  keywords  = {multi-agent reinforcement learning, organizational control, organizational explainability},
  location  = {Detroit, MI, USA},
  series    = {AAMAS '25},
  keywords  = {international}
}

@inproceedings{soule2024marl,
  author    = {Soul{\'e}, Julien
               and Jamont, Jean-Paul
               and Occello, Michel
               and Traonouez, Louis-Marie
               and Th{\'e}ron, Paul},
  editor    = {Maglogiannis, Ilias
               and Iliadis, Lazaros
               and Macintyre, John
               and Avlonitis, Markos
               and Papaleonidas, Antonios},
  title     = {A MARL-Based Approach for Easing MAS Organization Engineering},
  booktitle = {Artificial Intelligence Applications and Innovations},
  year      = {2024},
  publisher = {Springer Nature Switzerland},
  address   = {Cham},
  pages     = {321--334},
  abstract  = {Multi-Agent Systems (MAS) have been successfully applied in industry for their ability to address complex, distributed problems, especially in IoT-based systems. Their efficiency in achieving given objectives and meeting design requirements is strongly dependent on the MAS organization during the engineering process of an application-specific MAS. To design a MAS that can achieve given goals, available methods rely on the designer's knowledge of the deployment environment. However, high complexity and low readability in some deployment environments make the application of these methods to be costly or raise safety concerns. In order to ease the MAS organization design regarding those concerns, we introduce an original Assisted MAS Organization Engineering Approach (AOMEA). AOMEA relies on combining a Multi-Agent Reinforcement Learning (MARL) process with an organizational model to suggest relevant organizational specifications to help in MAS engineering.},
  isbn      = {978-3-031-63223-5},
  keywords  = {international}
}


@inproceedings{soulej2023sim,
  author    = {Soulé, Julien and Jamont, Jean-Paul and Occello, Michel and Théron, Paul and Traonouez, Louis-Marie},
  booktitle = {2023 IEEE International Conference on Systems, Man, and Cybernetics (SMC)},
  title     = {Towards a Multi-Agent Simulation of Cyber-attackers and Cyber-defenders Battles},
  year      = {2023},
  volume    = {},
  number    = {},
  pages     = {3594-3599},
  keywords  = {Organizations;Behavioral sciences;Cyberattack;Cybernetics},
  doi       = {10.1109/SMC53992.2023.10394564},
  keywords  = {international}
}


@inproceedings{soule2023rjciathese,
  title       = {De l'organisation des syst{\`e}mes multi-agents de cyber-d{\'e}fense},
  author      = {Soul{\'e}, Julien and Jamont, Jean-Paul and Occello, Michel and Th{\'e}ron, Paul and Traonouez, Louis-Marie},
  booktitle   = {Rencontres Jeunes Chercheurs en Intelligence Artificielle (RJCIA 2023)},
  address     = {Strasbourg, France},
  year        = {2023},
  month       = jul,
  url         = {https://hal.science/hal-04565426},
  pdf         = {https://pfia23.icube.unistra.fr/conferences/rjcia/Actes/RJCIA2023_paper_15.pdf},
  hal_id      = {hal-04565426},
  hal_version = {v1},
  keywords    = {national}
}

@inproceedings{soule2023ressithese,
  title     = {De l'organisation d'un syst{\`e}me multi-agent de cyberd{\'e}fense},
  author    = {Soul{\'e}, Julien and Jamont, Jean-Paul and Occello, Michel and Th{\'e}ron, Paul and Traonouez, Louis-Marie},
  booktitle = {Rendez-Vous de la Recherche et de l'Enseignement de la S{\'e}curit{\'e} des Syst{\`e}mes d'Information (RESSI 2023)},
  year      = {2023},
  month     = may,
  url       = {https://ressi2023.sciencesconf.org/450961/document},
  keywords  = {national}
}

@inproceedings{soule2023jfsmathese,
  title       = {De l'organisation d'un syst{\`e}me multi-agent de cyberd{\'e}fense},
  author      = {Soul{\'e}, Julien and Jamont, Jean-Paul and Occello, Michel and Th{\'e}ron, Paul and Traonouez, Louis-Marie},
  booktitle   = {31{\`e}mes Journ{\'e}es Francophones sur les Syst{\`e}mes Multi-Agents (JFSMA 2023)},
  publisher   = {C{\'e}padu{\`e}s},
  series      = {JFSMA},
  pages       = {54--54},
  year        = {2023},
  url         = {https://hal.science/hal-04165020},
  hal_id      = {hal-04165020},
  hal_version = {v1},
  keywords    = {national}
}

@inproceedings{soule2024outil,
  title        = {Un outil pour la conception de SMA par apprentissage par renforcement et mod{\'e}lisation organisationnelle},
  author       = {Soul{\'e}, Julien and Jamont, Jean-Paul and Occello, Michel and Traonouez, Louis-Marie and Th{\'e}ron, Paul},
  booktitle    = {32{\`e}mes Journ{\'e}es Francophones sur les Syst{\`e}mes Multi-Agents (JFSMA 2024)},
  address      = {Carg{\`e}se, France},
  organization = {S{\'e}bastien Picault},
  publisher    = {C{\'e}padu{\`e}s},
  year         = {2024},
  month        = nov,
  url          = {https://hal.science/hal-04840721},
  hal_id       = {hal-04840721},
  hal_version  = {v1},
  keywords     = {national}
}

@inproceedings{soule2024approche,
  title        = {Une approche bas{\'e}e sur l'apprentissage par renforcement pour l'ing{\'e}nierie organisationnelle d'un SMA},
  author       = {Soul{\'e}, Julien and Jamont, Jean-Paul and Occello, Michel and Traonouez, Louis-Marie and Th{\'e}ron, Paul},
  booktitle    = {32{\`e}mes Journ{\'e}es Francophones sur les Syst{\`e}mes Multi-Agents (JFSMA 2024)},
  address      = {Carg{\`e}se, France},
  organization = {S{\'e}bastien Picault},
  publisher    = {C{\'e}padu{\`e}s},
  year         = {2024},
  month        = nov,
  url          = {https://hal.science/hal-04840696},
  hal_id       = {hal-04840696},
  hal_version  = {v1},
  keywords     = {national}
}

@inproceedings{soule2025jfsma,
  title       = {Une approche organisationnelle pour am{\'e}liorer l'explicabilit{\'e} et le contr{\^o}le dans l'apprentissage par renforcement multi-agent},
  author      = {Soul{\'e}, Julien and Jamont, Jean-Paul and Occello, Michel and Th{\'e}ron, Paul and Traonouez, Louis-Marie},
  booktitle   = {33{\`e}mes Journ{\'e}es Francophones sur les Syst{\`e}mes Multi-Agents (JFSMA 2025)},
  address     = {Dijon, France},
  publisher   = {Association Fran{\c c}aise pour l'Intelligence Artificielle},
  year        = {2025},
  month       = jul,
  url         = {https://hal.science/hal-05151654},
  pdf         = {https://hal.science/hal-05151654v1/file/Actes_JFSMA_PFIA2025.pdf},
  hal_id      = {hal-05151654},
  hal_version = {v1},
  keywords    = {national},
  note        = {Prix du meilleur article}
}


@inproceedings{ma2022elign,
  author    = {Ma, Zixian and Wang, Rose and Li, Fei-Fei and Bernstein, Michael and Krishna, Ranjay},
  booktitle = {Advances in Neural Information Processing Systems},
  editor    = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
  pages     = {8304--8317},
  publisher = {Curran Associates, Inc.},
  title     = {ELIGN: Expectation Alignment as a Multi-Agent Intrinsic Reward},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2022/file/3753163b089e405ef10302698cd9a7fc-Paper-Conference.pdf},
  volume    = {35},
  year      = {2022}
}

@misc{xu2024subgoalhrl,
  title         = {Subgoal-based Hierarchical Reinforcement Learning for Multi-Agent Collaboration},
  author        = {Cheng Xu and Changtian Zhang and Yuchen Shi and Ran Wang and Shihong Duan and Yadong Wan and Xiaotong Zhang},
  year          = {2024},
  eprint        = {2408.11416},
  archiveprefix = {arXiv},
  primaryclass  = {cs.MA},
  url           = {https://arxiv.org/abs/2408.11416}
}

@misc{zeng2025valuealignment,
  title         = {Multi-level Value Alignment in Agentic AI Systems: Survey and Perspectives},
  author        = {Wei Zeng and Hengshu Zhu and Chuan Qin and Han Wu and Yihang Cheng and Sirui Zhang and Xiaowei Jin and Yinuo Shen and Zhenxing Wang and Feimin Zhong and Hui Xiong},
  year          = {2025},
  eprint        = {2506.09656},
  archiveprefix = {arXiv},
  primaryclass  = {cs.AI},
  url           = {https://arxiv.org/abs/2506.09656}
}

@article{Selmonaj2025,
  author     = {Ardian Selmonaj and
                Alessandro Antonucci and
                Adrian Schneider and
                Michael R{\"{u}}egsegger and
                Matthias Sommer},
  title      = {Explaining Strategic Decisions in Multi-Agent Reinforcement Learning
                for Aerial Combat Tactics},
  journal    = {CoRR},
  volume     = {abs/2505.11311},
  year       = {2025},
  url        = {https://doi.org/10.48550/arXiv.2505.11311},
  doi        = {10.48550/ARXIV.2505.11311},
  eprinttype = {arXiv},
  eprint     = {2505.11311},
  timestamp  = {Mon, 23 Jun 2025 11:00:52 +0200},
  biburl     = {https://dblp.org/rec/journals/corr/abs-2505-11311.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{maisonhaute2024,
  title       = {{{\'E}tat de l'art sur les approches en apprentissage par renforcement multi-agent.}},
  author      = {Maisonhaute, Tiziano and Michel, Fabien and Souli{\'e}, Jean-Christophe},
  url         = {https://hal.science/hal-04932606},
  booktitle   = {{JFSMA 2024 - 32es Journ{\'e}es Francophones sur les Syst{\`e}mes Multi-Agents}},
  address     = {Carg{\`e}se (Corse), France},
  editor      = {S{\'e}bastien Picault},
  publisher   = {{C{\'e}padu{\`e}s}},
  pages       = {99-108},
  year        = {2024},
  month       = Nov,
  keywords    = {Multi-agent reinforcement learning ; MARL ; Multi-agent systems ; MAS ; Reinforcement Learning ; Apprentissage par renforcement multi-agent ; MARL ; Syst{\`e}mes multi-agents ; SMA ; Apprentissage par renforcement},
  pdf         = {https://hal.science/hal-04932606v1/file/JFMSA_MARL_2024.pdf},
  hal_id      = {hal-04932606},
  hal_version = {v1}
}

@inproceedings{yu2021mappo,
  author    = {Yu, Chao and Velu, Akash and Vinitsky, Eugene and Gao, Jiaxuan and Wang, Yu and Bayen, Alexandre and WU, YI},
  booktitle = {Advances in Neural Information Processing Systems},
  editor    = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
  pages     = {24611--24624},
  publisher = {Curran Associates, Inc.},
  title     = {The Surprising Effectiveness of PPO in Cooperative Multi-Agent Games},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2022/file/9c1535a02f0ce079433344e14d910597-Paper-Datasets_and_Benchmarks.pdf},
  volume    = {35},
  year      = {2022}
}

@inproceedings{akiba2019optuna,
  author    = {Akiba, Takuya and Sano, Shotaro and Yanase, Toshihiko and Ohta, Takeru and Koyama, Masanori},
  title     = {Optuna: A Next-generation Hyperparameter Optimization Framework},
  year      = {2019},
  isbn      = {9781450362016},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3292500.3330701},
  doi       = {10.1145/3292500.3330701},
  abstract  = {The purpose of this study is to introduce new design-criteria for next-generation hyperparameter optimization software. The criteria we propose include (1) define-by-run API that allows users to construct the parameter search space dynamically, (2) efficient implementation of both searching and pruning strategies, and (3) easy-to-setup, versatile architecture that can be deployed for various purposes, ranging from scalable distributed computing to light-weight experiment conducted via interactive interface. In order to prove our point, we will introduce Optuna, an optimization software which is a culmination of our effort in the development of a next generation optimization software. As an optimization software designed with define-by-run principle, Optuna is particularly the first of its kind. We will present the design-techniques that became necessary in the development of the software that meets the above criteria, and demonstrate the power of our new design through experimental results and real world applications. Our software is available under the MIT license (https://github.com/pfnet/optuna/).},
  booktitle = {Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
  pages     = {2623–2631},
  numpages  = {9},
  keywords  = {Bayesian optimization, black-box optimization, hyperparameter optimization, machine learning system},
  location  = {Anchorage, AK, USA},
  series    = {KDD '19}
}


@article{foerster2018counterfactual,
  title        = {Counterfactual Multi-Agent Policy Gradients},
  volume       = {32},
  url          = {https://ojs.aaai.org/index.php/AAAI/article/view/11794},
  doi          = {10.1609/aaai.v32i1.11794},
  abstractnote = { &lt;p&gt; Many real-world problems, such as network packet routing and the coordination of autonomous vehicles, are naturally modelled as cooperative multi-agent systems. There is a great need for new reinforcement learning methods that can efficiently learn decentralised policies for such systems. To this end, we propose a new multi-agent actor-critic method called counterfactual multi-agent (COMA) policy gradients. COMA uses a centralised critic to estimate the Q-function and decentralised actors to optimise the agents’ policies. In addition, to address the challenges of multi-agent credit assignment, it uses a counterfactual baseline that marginalises out a single agent’s action, while keeping the other agents’ actions fixed. COMA also uses a critic representation that allows the counterfactual baseline to be computed efficiently in a single forward pass. We evaluate COMA in the testbed of StarCraft unit micromanagement, using a decentralised variant with significant partial observability. COMA significantly improves average performance over other multi-agent actor-critic methods in this setting, and the best performing agents are competitive with state-of-the-art centralised controllers that get access to the full state. &lt;/p&gt; },
  number       = {1},
  journal      = {Proceedings of the AAAI Conference on Artificial Intelligence},
  author       = {Foerster, Jakob and Farquhar, Gregory and Afouras, Triantafyllos and Nardelli, Nantas and Whiteson, Shimon},
  year         = {2018},
  month        = {Apr.}
}

@article{rashid2018qmix,
  author  = {Tabish Rashid and Mikayel Samvelyan and Christian Schroeder de Witt and Gregory Farquhar and Jakob Foerster and Shimon Whiteson},
  title   = {Monotonic Value Function Factorisation for Deep Multi-Agent Reinforcement Learning},
  journal = {Journal of Machine Learning Research},
  year    = {2020},
  volume  = {21},
  number  = {178},
  pages   = {1--51},
  url     = {http://jmlr.org/papers/v21/20-081.html}
}

@inproceedings{Maxwell2021,
  author    = {Maxwell Standen and Martin Lucas and David Bowman and Toby J\. Richer and Junae Kim and Damian Marriott},
  title     = {CybORG: A Gym for the Development of Autonomous Cyber Agents},
  booktitle = {IJCAI-21 1st International Workshop on Adaptive Cyber Defense.},
  publisher = {arXiv},
  year      = {2021}
}

@inproceedings{overcookedai,
  author    = {Carroll, Micah and Shah, Rohin and Ho, Mark K and Griffiths, Tom and Seshia, Sanjit and Abbeel, Pieter and Dragan, Anca},
  booktitle = {Advances in Neural Information Processing Systems},
  editor    = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
  pages     = {},
  publisher = {Curran Associates, Inc.},
  title     = {On the Utility of Learning about Humans for Human-AI Coordination},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2019/file/f5b1b89d98b7286673128a5fb112cb9a-Paper.pdf},
  volume    = {32},
  year      = {2019}
}

@inproceedings{lowe2017multi,
  author    = {Lowe, Ryan and WU, YI and Tamar, Aviv and Harb, Jean and Pieter Abbeel, OpenAI and Mordatch, Igor},
  booktitle = {Advances in Neural Information Processing Systems},
  editor    = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
  pages     = {},
  publisher = {Curran Associates, Inc.},
  title     = {Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2017/file/68a9750337a418a86fe06c1991a1d64c-Paper.pdf},
  volume    = {30},
  year      = {2017}
}

@article{hu2021marlib,
  author  = {Siyi Hu and Yifan Zhong and Minquan Gao and Weixun Wang and Hao Dong and Xiaodan Liang and Zhihui Li and Xiaojun Chang and Yaodong Yang},
  title   = {MARLlib: A Scalable and Efficient Multi-agent Reinforcement Learning Library},
  journal = {Journal of Machine Learning Research},
  year    = {2023},
  volume  = {24},
  number  = {315},
  pages   = {1--23},
  url     = {http://jmlr.org/papers/v24/23-0378.html}
}

@misc{kwiatkowski2024,
  title         = {Gymnasium: A Standard Interface for Reinforcement Learning Environments},
  author        = {Mark Towers and Ariel Kwiatkowski and Jordan Terry and John U. Balis and Gianluca De Cola and Tristan Deleu and Manuel Goulão and Andreas Kallinteris and Markus Krimmel and Arjun KG and Rodrigo Perez-Vicente and Andrea Pierré and Sander Schulhoff and Jun Jet Tai and Hannah Tan and Omar G. Younis},
  year          = {2025},
  eprint        = {2407.17032},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG},
  url           = {https://arxiv.org/abs/2407.17032}
}

@inproceedings{terry2020pettingzoo,
  author    = {Terry, J and Black, Benjamin and Grammel, Nathaniel and Jayakumar, Mario  and Hari, Ananth  and Sullivan, Ryan and Santos, Luis S and Dieffendahl, Clemens and Horsch, Caroline and Perez-Vicente, Rodrigo and Williams, Niall  and Lokesh, Yashas  and Ravi , Praveen },
  booktitle = {Advances in Neural Information Processing Systems},
  editor    = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
  pages     = {15032--15043},
  publisher = {Curran Associates, Inc.},
  title     = {PettingZoo: Gym for Multi-Agent Reinforcement Learning},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2021/file/7ed2d3454c5eea71148b11d0c25104ff-Paper.pdf},
  volume    = {34},
  year      = {2021}
}

@inproceedings{ferber2003,
  title       = {{Agent/Group/Roles: Simulating with Organizations}},
  author      = {Ferber, Jacques and Gutknecht, Olivier and Michel, Fabien},
  url         = {https://hal-lirmm.ccsd.cnrs.fr/lirmm-00269714},
  booktitle   = {{ABS 2003 - 4th International Workshop on Agent-Based Simulation}},
  address     = {Montpellier, France},
  editor      = {Muller, J.P.},
  year        = {2003},
  month       = Apr,
  keywords    = {agent based simulation ; multi-agent design ; multi-agent methodology ; organizational structures ; organizations ; Multi-agent systems},
  pdf         = {https://hal-lirmm.ccsd.cnrs.fr/lirmm-00269714v1/file/ArticleABS03.AGR-Simulating_with_org.pdf},
  hal_id      = {lirmm-00269714},
  hal_version = {v1}
}

@book{Oliehoek2016,
  title     = {A concise introduction to decentralized POMDPs},
  author    = {Oliehoek, Frans A and Amato, Christopher and others},
  volume    = {1},
  year      = {2016},
  publisher = {Springer}
}

@inproceedings{Beynier2013,
  title     = {A decentralized approach for reinforcement learning in cooperative multi-agent systems},
  author    = {Beynier, Aur{\'e}lie and Mouaddib, Alain},
  booktitle = {Proceedings of the 23rd International Joint Conference on Artificial Intelligence (IJCAI)},
  pages     = {163--168},
  year      = {2013}
}

@book{Albrecht2024,
  author    = {Stefano V. Albrecht and Filippos Christianos and Lukas Sch\"afer},
  title     = {Multi-Agent Reinforcement Learning: Foundations and Modern Approaches},
  publisher = {MIT Press},
  year      = {2024},
  url       = {https://www.marl-book.com}
}

@inproceedings{achiam2017cpo,
  title     = {Constrained Policy Optimization},
  author    = {Joshua Achiam and David Held and Aviv Tamar and Pieter Abbeel},
  booktitle = {Proceedings of the 34th International Conference on Machine Learning},
  pages     = {22--31},
  year      = {2017},
  editor    = {Precup, Doina and Teh, Yee Whye},
  volume    = {70},
  series    = {Proceedings of Machine Learning Research},
  month     = {06--11 Aug},
  publisher = {PMLR},
  pdf       = {http://proceedings.mlr.press/v70/achiam17a/achiam17a.pdf},
  url       = {https://proceedings.mlr.press/v70/achiam17a.html},
  abstract  = {For many applications of reinforcement learning it can be more convenient to specify both a reward function and constraints, rather than trying to design behavior through the reward function. For example, systems that physically interact with or around humans should satisfy safety constraints. Recent advances in policy search algorithms (Mnih et al., 2016, Schulman et al., 2015, Lillicrap et al., 2016, Levine et al., 2016) have enabled new capabilities in high-dimensional control, but do not consider the constrained setting. We propose Constrained Policy Optimization (CPO), the first general-purpose policy search algorithm for constrained reinforcement learning with guarantees for near-constraint satisfaction at each iteration. Our method allows us to train neural network policies for high-dimensional control while making guarantees about policy behavior all throughout training. Our guarantees are based on a new theoretical result, which is of independent interest: we prove a bound relating the expected returns of two policies to an average divergence between them. We demonstrate the effectiveness of our approach on simulated robot locomotion tasks where the agent must satisfy constraints motivated by safety.}
}


@article{ray2019benchmarking,
  title   = {Benchmarking safe exploration in deep reinforcement learning},
  author  = {Ray, Alex and Achiam, Joshua and Amodei, Dario},
  journal = {arXiv preprint arXiv:1910.01708},
  volume  = {7},
  number  = {1},
  pages   = {2},
  year    = {2019}
}

@article{garcia2015comprehensive,
  author  = {Javier GarcÃ­a and Fernando FernÃ¡ndez},
  title   = {A Comprehensive Survey on Safe Reinforcement Learning},
  journal = {Journal of Machine Learning Research},
  year    = {2015},
  volume  = {16},
  number  = {42},
  pages   = {1437--1480},
  url     = {http://jmlr.org/papers/v16/garcia15a.html}
}

@article{alshiekh2018safe,
  title        = {Safe Reinforcement Learning via Shielding},
  volume       = {32},
  url          = {https://ojs.aaai.org/index.php/AAAI/article/view/11797},
  doi          = {10.1609/aaai.v32i1.11797},
  abstractnote = { &lt;p&gt; Reinforcement learning algorithms discover policies that maximize reward, but do not necessarily guarantee safety during learning or execution phases. We introduce a new approach to learn optimal policies while enforcing properties expressed in temporal logic. To this end, given the temporal logic specification that is to be obeyed by the learning system, we propose to synthesize a reactive system called a shield. The shield monitors the actions from the learner and corrects them only if the chosen action causes a violation of the specification. We discuss which requirements a shield must meet to preserve the convergence guarantees of the learner. Finally, we demonstrate the versatility of our approach on several challenging reinforcement learning scenarios. &lt;/p&gt; },
  number       = {1},
  journal      = {Proceedings of the AAAI Conference on Artificial Intelligence},
  author       = {Alshiekh, Mohammed and Bloem, Roderick and Ehlers, Rüdiger and Könighofer, Bettina and Niekum, Scott and Topcu, Ufuk},
  year         = {2018},
  month        = {Apr.}
}

@inproceedings{ghavamzadeh2006hrl,
  title     = {Hierarchical reinforcement learning with cooperative agents},
  author    = {Ghavamzadeh, Mohammad and Mahadevan, Sridhar},
  booktitle = {Proceedings of the 23rd International Conference on Machine Learning},
  pages     = {119--126},
  year      = {2006}
}

@inproceedings{wilson2008learning,
  title     = {Learning and transferring roles in multi-agent mdps},
  author    = {Wilson, Aaron and Fern, Alan and Ray, Soumya and Tadepalli, Prasad},
  booktitle = {Proceedings of AAAI},
  year      = {2008}
}

@article{berenji2000learning,
  author = {Hamid, Dr and Berenji, R. and Vengerov, David},
  year   = {2000},
  month  = {11},
  pages  = {},
  title  = {Learning, Cooperation, and Coordination in Multi-Agent Systems}
}

@article{yusuf2020inferential,
  title   = {Inferential Reasoning for Heterogeneous Multi-Agent Missions},
  author  = {Yusuf, Sagir M and Baber, Christopher},
  journal = {International Journal of Electrical and Computer Engineering},
  year    = {2020}
}

@inproceedings{serrino2019finding,
  author    = {Serrino, Jack and Kleiman-Weiner, Max and Parkes, David C and Tenenbaum, Josh},
  booktitle = {Advances in Neural Information Processing Systems},
  editor    = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
  pages     = {},
  publisher = {Curran Associates, Inc.},
  title     = {Finding Friend and Foe in Multi-Agent Games},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2019/file/912d2b1c7b2826caf99687388d2e8f7c-Paper.pdf},
  volume    = {32},
  year      = {2019}
}

@article{Hubner2007,
  author   = {Hubner, Jomi F. and Sichman, Jaime S. and Boissier, Olivier},
  title    = {Developing organised multiagent systems using the MOISE+ model: programming issues at the system and agent levels},
  journal  = {International Journal of Agent-Oriented Software Engineering},
  volume   = {1},
  number   = {3-4},
  pages    = {370-395},
  year     = {2007},
  doi      = {10.1504/IJAOSE.2007.016266},
  url      = {https://www.inderscienceonline.com/doi/abs/10.1504/IJAOSE.2007.016266},
  eprint   = {https://www.inderscienceonline.com/doi/pdf/10.1504/IJAOSE.2007.016266},
  abstract = { Multiagent Systems (MASs) have evolved towards the specification of global constraints that heterogeneous and autonomous agents are supposed to follow when concerning open systems. A subset of these constraints is known as the MAS organisation. This article describes a set of computational tools that supports the development and the programming of such systems. At the system level, a middleware is provided which ensures that all agents will follow the organisational constraints. At the agent level, the AgentSpeak language is extended, using Jason features, so that the agents can perceive and act upon the organisation to which they belong. }
}

@phdthesis{SaoMai2024,
  author = {Sao Mai, N.},
  title  = {The intrinsic motivation of reinforcement and imitation learning for sequential tasks},
  school = {HAL Archive},
  year   = {2024},
  url    = {https://hal.science/tel-04853270}
}

@article{Matsuyama2025,
  author     = {Kanefumi Matsuyama and
                Kefan Su and
                Jiangxing Wang and
                Deheng Ye and
                Zongqing Lu},
  title      = {{CORD:} Generalizable Cooperation via Role Diversity},
  journal    = {CoRR},
  volume     = {abs/2501.02221},
  year       = {2025},
  url        = {https://doi.org/10.48550/arXiv.2501.02221},
  doi        = {10.48550/ARXIV.2501.02221},
  eprinttype = {arXiv},
  eprint     = {2501.02221},
  timestamp  = {Wed, 09 Apr 2025 15:47:32 +0200},
  biburl     = {https://dblp.org/rec/journals/corr/abs-2501-02221.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}

@article{Qi2024,
  title    = {Bidirectional Q-learning for recycling path planning of used appliances under strong and weak constraints},
  journal  = {Communications in Transportation Research},
  volume   = {4},
  pages    = {100153},
  year     = {2024},
  issn     = {2772-4247},
  doi      = {https://doi.org/10.1016/j.commtr.2024.100153},
  url      = {https://www.sciencedirect.com/science/article/pii/S2772424724000362},
  author   = {Yang Qi and Jinxin Cao and Baijing Wu},
  keywords = {Path planning, -learning, Waste electrical recovery, Reinforcement learning, Reward function},
  abstract = {With the continuous innovation in household appliance technology and the improvement of living standards, the production of discarded household appliances has rapidly increased, making their recycling increasingly significant. Traditional path planning algorithms encounter difficulties in balancing efficiency and constraints in addressing the multi-objective, multi-constraint challenge posed by discarded household appliance recycling routes. To tackle this issue, this study introduces a bi-directional Q-learning-based path planning algorithm. By developing a bi-directional Q-learning mechanism and enhancing the initialization method of Q-learning, the algorithm aims to achieve efficient and effective optimization of discarded household appliance recycling routes. It implements bidirectional updates of the state-action value function from both the starting point and the target point. Additionally, a hierarchical reinforcement learning strategy and guided rewards are introduced to minimize blind exploration and expedite convergence. By decomposing complex recycling tasks into multiple sub-tasks and seeking paths with superior performance at each sub-task level, the initial exploratory blindness is reduced. To validate the efficacy of the proposed algorithm, gridbased modeling of real-world environments is utilized. Comparative experiments reveal significant improvements in iteration counts and path lengths, thereby validating its practical applicability in path planning for recycling initiatives.}
}

@article{Xie2024,
  title    = {ROCO: Role-oriented communication for efficient multi-agent reinforcement learning},
  journal  = {Expert Systems with Applications},
  volume   = {297},
  pages    = {129421},
  year     = {2026},
  issn     = {0957-4174},
  doi      = {https://doi.org/10.1016/j.eswa.2025.129421},
  url      = {https://www.sciencedirect.com/science/article/pii/S0957417425030374},
  author   = {Zaipeng Xie and Sitong Shen and Yaowu Wang and Chentai Qiao and Bin Tang and Wenzhan Song},
  keywords = {Multi-agent reinforcement learning, Role-based communication, Information-guided communication, Attention mechanisms, Communication efficiency, Multi-agent coordination, Cooperative learning},
  abstract = {Effective communication and coordination among agents are pivotal for addressing complex tasks in multi-agent systems. Conventional methods often encounter limitations in adapting to dynamic environments and efficiently managing group communication. To overcome these challenges, we propose Role-Oriented multi-agent Communication (ROCO), a framework that adaptively modulates communication strategies through role-based learning. ROCO incorporates a dual-layer communication architecture: an inter-role layer and an intra-role layer. The inter-role layer facilitates cross-functional information exchange via role-specific protocols and attention mechanisms, thereby enhancing inter-role coordination. The intra-role layer streamlines communication within role groups by leveraging mutual information-based message prediction and filtration, effectively reducing redundancy and improving decision-making processes. Empirical evaluations on established benchmarks, including the StarCraft Multi-Agent Challenge and Google Research Football, demonstrate that ROCO achieves faster convergence and higher win rates than state-of-the-art methods. Ablation studies further highlight the efficacy of its key components, underscoring ROCO’s robustness and adaptability in cooperative multi-agent learning scenarios. These results indicate that ROCO offers a principled approach for improving coordination and communication in complex multi-agent environments, establishing a solid foundation for future applications in structured and dynamic settings.}
}

@inproceedings{Isakov2024,
  author    = {Isakov, Artem
               and Peregorodiev, Danil
               and Brunko, Pavel
               and Tomilov, Ivan
               and Gusarova, Natalia
               and Vatian, Alexandra},
  editor    = {Julian, Vicente
               and Camacho, David
               and Yin, Hujun
               and Alberola, Juan M.
               and Nogueira, Vitor Beires
               and Novais, Paulo
               and Tall{\'o}n-Ballesteros, Antonio},
  title     = {Cooperative-Competitive Decision-Making in Resource Management: A Reinforcement Learning Perspective},
  booktitle = {Intelligent Data Engineering and Automated Learning -- IDEAL 2024},
  year      = {2025},
  publisher = {Springer Nature Switzerland},
  address   = {Cham},
  pages     = {375--386},
  abstract  = {This study presents a multi-agent approach to modeling deci-sion-making processes for cooperative-competitive game scenarios generated by long-tailed distributions. We introduce a novel simulation environment that aims to replicate real-world resource allocation challenges, particularly in healthcare settings. The environment incorporates dynamic changes in agents' perceptions and utilizes heavy-tailed distributions to model unpredictable events. We adapt the Multi-Agent Deep Deterministic Policy Gradient algorithm to this environment, demonstrating its effectiveness in learning and strategy development. Our approach includes a role-based system and a two-component reward function that guides agents through the learning process. The results, evaluated using metrics such as cumulative reward, average episodic reward, and approximate Kullback-Leibler divergence, indicate successful learning and sufficent agent strategies. This research contributes to the field of multi-agent systems and resource management by providing a framework that can generalize learned policies to varying numbers of agents and resources, making it applicable to real-world scenarios with changing conditions.},
  isbn      = {978-3-031-77731-8}
}


@misc{Wen2024,
  title         = {Role Play: Learning Adaptive Role-Specific Strategies in Multi-Agent Interactions},
  author        = {Weifan Long and Wen Wen and Peng Zhai and Lihua Zhang},
  year          = {2024},
  eprint        = {2411.01166},
  archiveprefix = {arXiv},
  primaryclass  = {cs.MA},
  url           = {https://arxiv.org/abs/2411.01166}
}

@inproceedings{Foerster2016,
  author    = {Foerster, Jakob and Assael, Ioannis Alexandros and de Freitas, Nando and Whiteson, Shimon},
  booktitle = {Advances in Neural Information Processing Systems},
  editor    = {D. Lee and M. Sugiyama and U. Luxburg and I. Guyon and R. Garnett},
  pages     = {},
  publisher = {Curran Associates, Inc.},
  title     = {Learning to Communicate with Deep Multi-Agent Reinforcement Learning},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2016/file/c7635bfd99248a2cdef8249ef7bfbef4-Paper.pdf},
  volume    = {29},
  year      = {2016}
}

@book{Albrecht2024book,
  author    = {Stefano V. Albrecht and Filippos Christianos and Lukas Sch\"afer},
  title     = {Multi-Agent Reinforcement Learning: Foundations and Modern Approaches},
  publisher = {MIT Press},
  year      = {2024},
  url       = {https://www.marl-book.com}
}

@inproceedings{wang2020roma,
  title     = {{ROMA}: Multi-Agent Reinforcement Learning with Emergent Roles},
  author    = {Wang, Tonghan and Dong, Heng and Lesser, Victor and Zhang, Chongjie},
  booktitle = {Proceedings of the 37th International Conference on Machine Learning},
  pages     = {9876--9886},
  year      = {2020},
  editor    = {III, Hal Daumé and Singh, Aarti},
  volume    = {119},
  series    = {Proceedings of Machine Learning Research},
  month     = {13--18 Jul},
  publisher = {PMLR},
  pdf       = {http://proceedings.mlr.press/v119/wang20f/wang20f.pdf},
  url       = {https://proceedings.mlr.press/v119/wang20f.html},
  abstract  = {The role concept provides a useful tool to design and understand complex multi-agent systems, which allows agents with a similar role to share similar behaviors. However, existing role-based methods use prior domain knowledge and predefine role structures and behaviors. In contrast, multi-agent reinforcement learning (MARL) provides flexibility and adaptability, but less efficiency in complex tasks. In this paper, we synergize these two paradigms and propose a role-oriented MARL framework (ROMA). In this framework, roles are emergent, and agents with similar roles tend to share their learning and to be specialized on certain sub-tasks. To this end, we construct a stochastic role embedding space by introducing two novel regularizers and conditioning individual policies on roles. Experiments show that our method can learn specialized, dynamic, and identifiable roles, which help our method push forward the state of the art on the StarCraft II micromanagement benchmark. Demonstrative videos are available at https://sites.google.com/view/romarl/.}
}

@article{zeng2023sird,
  title        = {Effective and Stable Role-Based Multi-Agent Collaboration by Structural Information Principles},
  volume       = {37},
  url          = {https://ojs.aaai.org/index.php/AAAI/article/view/26390},
  doi          = {10.1609/aaai.v37i10.26390},
  abstractnote = {Role-based learning is a promising approach to improving the performance of Multi-Agent Reinforcement Learning (MARL). Nevertheless, without manual assistance, current role-based methods cannot guarantee stably discovering a set of roles to effectively decompose a complex task, as they assume either a predefined role structure or practical experience for selecting hyperparameters. In this article, we propose a mathematical Structural Information principles-based Role Discovery method, namely SIRD, and then present a SIRD optimizing MARL framework, namely SR-MARL, for multi-agent collaboration. The SIRD transforms role discovery into a hierarchical action space clustering. Specifically, the SIRD consists of structuralization, sparsification, and optimization modules, where an optimal encoding tree is generated to perform abstracting to discover roles. The SIRD is agnostic to specific MARL algorithms and flexibly integrated with various value function factorization approaches. Empirical evaluations on the StarCraft II micromanagement benchmark demonstrate that, compared with state-of-the-art MARL algorithms, the SR-MARL framework improves the average test win rate by 0.17%, 6.08%, and 3.24%, and reduces the deviation by 16.67%, 30.80%, and 66.30%, under easy, hard, and super hard scenarios.},
  number       = {10},
  journal      = {Proceedings of the AAAI Conference on Artificial Intelligence},
  author       = {Zeng, Xianghua and Peng, Hao and Li, Angsheng},
  year         = {2023},
  month        = {Jun.},
  pages        = {11772-11780}
}

@article{ahmed2022survey,
  title   = {Deep Reinforcement Learning for Multi-Agent Interaction: A Survey},
  author  = {Ahmed, Fawaz and Dinh, Thang and Thai, My T.},
  journal = {ACM Computing Surveys},
  year    = {2022},
  volume  = {55},
  number  = {8},
  pages   = {1--37},
  doi     = {10.1145/3543510},
  url     = {https://dl.acm.org/doi/10.1145/3543510}
}

@article{zhang2025clustering,
  author   = {Peng Zhang and Mengyu Jin and Ming Wang and Jie Zhang and Junjie He and Peng Zheng},
  title    = {A clustering-aided multi-agent deep reinforcement learning for multi-objective parallel batch processing machines scheduling in semiconductor manufacturing},
  journal  = {Measurement and Control},
  volume   = {58},
  number   = {5},
  pages    = {614-631},
  year     = {2025},
  doi      = {10.1177/00202940241269643},
  url      = {https://doi.org/10.1177/00202940241269643},
  eprint   = {https://doi.org/10.1177/00202940241269643},
  abstract = { Batch processing machines are often the bottleneck in semiconductor manufacturing and their scheduling plays a key role in production management. Pioneer researches on multi-objective batch machines scheduling mainly focus on evolutionary algorithms, failing to meet the online scheduling demand. To deal with the challenges confronted by incompatible job families, dynamic job arrivals, capacitated machines and multiple objectives, we propose a clustering-aided multi-agent deep reinforcement learning approach (CA-MADRL) for the scheduling problem. Specifically, to achieve diverse nondominated solutions, an offline multi-objective scheduling algorithm named Multi-Subpopulation fast elitist Non-Dominated Sorting Genetic Algorithm (MS-NSGA-II) is firstly developed to obtain the Pareto Fronts, and a clustering algorithm based on cosine distance is employed to analyze the distribution of Pareto frontier solution, which would be used to guide reward functions design in multi-agent deep reinforcement learning. To realize multi-objective optimization, several reinforcement learning base models are trained for different optimization directions, each of which composed of batch forming agent and batch scheduling agent. To alleviate time complexity of model training, a parameter sharing strategy is introduced between different reinforcement learning base model. By validating the proposed approach with 16 instances designed based on actual production data from a semiconductor manufacturing company, it has been demonstrated that the approach not only meets the high-frequency scheduling requirements of manufacturing systems for parallel batch processing machines but also effectively reduces the total job tardiness and machine energy consumption. }
}

@inproceedings{nguyen2022transfer,
  author    = {Dung Nguyen and
               Phuoc Nguyen and
               Svetha Venkatesh and
               Truyen Tran},
  editor    = {Piotr Faliszewski and
               Viviana Mascardi and
               Catherine Pelachaud and
               Matthew E. Taylor},
  title     = {Learning to Transfer Role Assignment Across Team Sizes},
  booktitle = {21st International Conference on Autonomous Agents and Multiagent
               Systems, {AAMAS} 2022, Auckland, New Zealand, May 9-13, 2022},
  pages     = {963--971},
  publisher = {International Foundation for Autonomous Agents and Multiagent Systems
               {(IFAAMAS)}},
  year      = {2022},
  url       = {https://www.ifaamas.org/Proceedings/aamas2022/pdfs/p963.pdf},
  doi       = {10.5555/3535850.3535958},
  timestamp = {Wed, 03 May 2023 14:37:18 +0200},
  biburl    = {https://dblp.org/rec/conf/atal/NguyenNV022.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Hammar2023,
  author    = {Hammar, Kim and Stadler, Rolf},
  booktitle = {NOMS 2023-2023 IEEE/IFIP Network Operations and Management Symposium},
  title     = {Digital Twins for Security Automation},
  year      = {2023},
  volume    = {},
  number    = {},
  pages     = {1-6},
  keywords  = {Automation;Emulation;Reinforcement learning;Markov processes;Data models;Digital twins;Security;Digital twin;cybersecurity;network security;automation;reinforcement learning;bMDP;POMDP},
  doi       = {10.1109/NOMS56928.2023.10154288}
}


@article{Wang2025,
  author   = {Wang, Xiaoxiao and Yi, Peng and Hong, Yiguang},
  journal  = {IEEE Transactions on Artificial Intelligence},
  title    = {A Hierarchical Deep Reinforcement Learning Strategy for Collective Pursuit-Evasion Game with Partial Observations},
  year     = {2025},
  volume   = {},
  number   = {},
  pages    = {1-15},
  keywords = {Games;Heuristic algorithms;Training;Artificial intelligence;Biological system modeling;Collaboration;Resource management;Multi-agent systems;Differential games;Analytical models;Collective Pursuit-Evasion Games;Deep Reinforcement Learning;Hierarchical Strategies;Multi-agent Systems;Partially Observable Markov Decision Process},
  doi      = {10.1109/TAI.2025.3566069}
}



@inproceedings{Chahoud2025,
  author    = {Chahoud, Mario and Sami, Hani and Mizouni, Rabeb and Otrok, Hadi and Bentahar, Jamal and Mourad, Azzam and Talhi, Chamseddine},
  booktitle = {2025 International Wireless Communications and Mobile Computing (IWCMC)},
  title     = {Multi-Agent Deep Reinforcement Learning for Resource Management in On-Demand Environments},
  year      = {2025},
  volume    = {},
  number    = {},
  pages     = {1264-1269},
  keywords  = {Training;Wireless communication;Decision making;Dynamic scheduling;Deep reinforcement learning;Real-time systems;Resource management;Mobile computing;Convergence;Overfitting;Multi-Agent Reinforcement Learning;Deep Q-Network;On-Demand Service Deployment;Dec-POMDP;Resource Management},
  doi       = {10.1109/IWCMC65282.2025.11059489}
}
