%% Ceci est un exemple et un gabarit pour les articles publi{\'e}s par la 
%% Revue Ouverte d'Intelligence Artificielle (ROIA)

%% �La classe cedram.cls et son option cedram-ROIA.clo doivent se
%% trouver dans le r{\'e}pertoire de pr{\'e}paration de l'article, ou dans le
%% chemin de recherche de LaTeX.  Elle est bas{\'e}e sur la classe 
%% amsart.cls (version 2) et impl{\'e}mente la mise en page de la revue,
%% ainsi que quelques commandes sp{\'e}cifiques en vue de la mise en ligne.  
%% Les options disponibles : 
%% francais pour les articles {\'e}crits en fran{\c c}ais 
%% (anglais n'est pas n{\'e}cessaire car c'est la langue par d{\'e}faut). 
%% Unicode pour disposer des caract{\`e}res accentu{\'e}s sans avoir recours {\`a}
%% des macros (ceci impose un codage utf8). 
%% L'option manuscript est obligatoire pour les manuscrits pr{\'e}par{\'e}s
%% par les auteurs.
\documentclass[francais,ROIA,Unicode,manuscript]{cedram}

%% On peut charger ici des extensions standard si les fonctions
%% fournies sont n{\'e}cessaires {\`a} la compilation de l'article. 
%\usepackage[matrix,arrow,tips,curve]{xy}
% \usepackage{gensymb}
% \usepackage{booktabs}
% \usepackage{algorithmic}
% \usepackage[french,onelanguage,linesnumbered,algoruled]{algorithm2e}
% ...
\usepackage{subcaption}
\usepackage{xcolor}
% --- Tickz
\usepackage{physics}
\usepackage{tikz}
\usepackage{amsmath}
\usepackage{mathdots}
% \usepackage{yhmath}
\usepackage{cancel}
\usepackage{color}
\usepackage{siunitx}
\usepackage{array}
\usepackage{multirow}
% \usepackage{amssymb}
\usepackage{gensymb}
\usepackage{tabularx}
\usepackage{extarrows}
\usepackage{booktabs}
\usetikzlibrary{fadings}
\usetikzlibrary{patterns}
\usetikzlibrary{shadows.blur}
\usetikzlibrary{shapes}
% ---------

\usepackage[inline, shortlabels]{enumitem}
\usepackage{arydshln}
\usepackage{amssymb}
\usepackage{csquotes}
\usepackage[export]{adjustbox}

\usepackage{tabularray}\UseTblrLibrary{varwidth}
\usepackage{xcolor}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\usepackage{amsmath}
\newcommand{\probP}{\text{I\kern-0.15em P}}

\newcommand{\myCustomSize}[1]{%
  \fontsize{9}{10.8}\selectfont
  #1
}

\DeclareMathAlphabet\mathbfcal{OMS}{cmsy}{b}{n}

\usepackage{amssymb}
\usepackage{pifont}

\newcommand{\cmark}{\ding{51}}
\newcommand{\xmark}{\ding{55}}

\addto\extrasfrench{%
\renewcommand{\figureautorefname}{Figure}
% \renewcommand{\lstlistingautorefname}{Listing}
\renewcommand{\tableautorefname}{Table}
\renewcommand{\partautorefname}{Partie}
\renewcommand{\appendixautorefname}{Appendice}
\renewcommand{\chapterautorefname}{Chapitre}
\renewcommand{\sectionautorefname}{Section}
\renewcommand{\subsectionautorefname}{Sous-section}
}

%% D{\'e}finitions utilisateur et macros pratiques...  De telles
%% d{\'e}finitions sont interdites dans les titres, les r{\'e}sum{\'e}s ou la
%% bibliographie.
%\newcommand{\la}{\longrightarrow}
% ...

%% Un ensemble de th{\'e}or{\`e}mes sont pr{\'e}d{\'e}finis. La r{\`e}gle mn{\'e}motechnique
%% est que le nom de l'environnement est form{\'e} des quatre premi{\`e}res
%% lettres (sans accents) de l'{\'e}tiquette utilis{\'e}e (theo, exam, rema,
%% coro, conj, etc.) ; les versions ast{\'e}risqu{\'e}es (non num{\'e}rot{\'e}es)
%% existent de m{\^e}me (theo*, etc.). Pour ne pas modifier vos habitudes
%% de saisie, il est possible de d{\'e}clarer, par exemple :
%\equalenv{remark}{rema}
%% qui d{\'e}finit un environnement � remark � identique {\`a} � rema �.

%% Le titre de l'article: syntaxe d'amsart.
\title
%% L'argument optionnel donne la version courte pour les ent{\^e}tes.
%% Inutile si le titre est suffisamment court 
[MOISE+MARL]
%% L'argument obligatoire est imprim{\'e} sur la premi{\`e}re page, dans les
%% ent{\^e}tes si la version courte n'est pas sp{\'e}cifi{\'e}e.
{MOISE+MARL : un cadre organisationnel pour l’explicabilité et le contrôle en apprentissage par renforcement multi-agent \textsuperscript{(1)}}

%% The English title of the article.
\alttitle{MOISE+MARL: an organizational framework for explainability and control in multi-agent reinforcement learning}

%% Les noms des auteurs, selon la syntaxe d'amsart, avec en outre la
%% distinction pr{\'e}nom/nom
\author{\firstname{Julien} \lastname{Soulé}}
\address{Université Grenoble Alpes, Grenoble INP, LCIS, 26000 Valence, France}
\author{\firstname{Jean-Paul} \lastname{Jamont}} 
\address[2]{Université Grenoble Alpes, Grenoble INP, LCIS, 26000 Valence, France}
\author{\firstname{Michel} \lastname{Occello}} 
\address[2]{Grenoble Alpes University, LCIS, 26000 Valence, France}
\author{\firstname{Louis-Marie} \lastname{Traonouez}} 
\address[3]{Thales Land and Air Systems, BU IAS, 35000 Rennes, France}
\author{\firstname{Paul} \lastname{Théron}} 
\address[4]{AICA IWG, 26000 Valence, France}

%Mickaël Bettinelli$^{1}$, Michel Occello$^{1}$ and Damien Genthial$^{1}$

%% N'inclure aucune autre information dans l'argument de la macro
%% \author ! Toutes les autres donn{\'e}es ont des commandes sp{\'e}cifiques :



%% Addresse de messagerie {\'e}lectronique
\email{julien.soule@lcis.grenoble-inp.fr}


%% Les cr{\'e}dits ou remerciements ne se mettent pas en note dans
%% \author, mais {\`a} part :
%\thanks{L'auteur a b{\'e}n{\'e}fici{\'e} d'un soutien important de la communaut{\'e}.}

%% En pr{\'e}sence de coauteurs, chacun est saisi ici de la m{\^e}me fa{\c c}on,
%% dans l'ordre alphab{\'e}tique.
%\author{\firstname{Alan} \middlename{W.} \lastname{Turing}}
%\address{Big Apple University\\
% Myself Institute \\
%  Wilmslow, UK}
%\email{turing@turing.edu.uk}

%% Mots et expressions cl{\'e}s :
\keywords{Apprentissage par renforcement multi-agent, Système Multi-Agent, Explicabilité, Contrôle, Organisation}
  
%% Mots et expressions cl{\'e}s en anglais :
\altkeywords{Multi-Agent Reinforcement Learning, Multi-Agent System, Explainability, Control, Organization}

% %% Mots et expressions cl{\'e}s en espagnol :
% \keywordsES{Muy similarez,  igual-fatal , Maquina M{\'\i} a}


%% R{\'e}sum{\'e}
\begin{abstract}
Des agents en apprentissage par renforcement multi-agent peuvent développer des comportements interprétables comme des rôles ou objectifs implicites, suggérant une hypothétique organisation émergente. Afin d’exploiter pleinement cette analogie, nous introduisons MOISE+MARL, un cadre organisationnel qui couple le modèle organisationnel $\mathcal{M}OISE^+$ à un modèle Markovien afin d’améliorer le contrôle et l’explicabilité des politiques apprises.
MOISE+MARL guide ou contraint les agents selon des rôles et des objectifs en ajustant dynamiquement leurs actions et récompenses, et intègre une méthode d’analyse permettant d’inférer a posteriori des spécifications organisationnelles implicites à partir des trajectoires des agents.
MOISE+MARL a été évalué sur quatre environnements et plusieurs algorithmes d'apprentissage par renforcement multi-agent. Les résultats montrent que les politiques des agents convergent plus rapidement, sont plus stables et deviennent plus explicables. La méthode d'analyse est confirmé par la cohérence entre les spécifications organisationnelles définies et celles inférées.
\end{abstract}

%% R{\'e}sum{\'e} anglaisss
\begin{altabstract}
Multi-agent reinforcement learning agents can develop interpretable behaviors such as implicit roles or objectives, suggesting a hypothetical emergent organization. To fully exploit this analogy, we introduce \textsc{MOISE+MARL}, an organizational framework that couples the organizational model $\mathcal{M}OISE^+$ with a Markovian model in order to improve both the control and the explainability of the learned policies.  
\textsc{MOISE+MARL} guides or constrains agents through roles and objectives by dynamically adjusting their actions and rewards, and integrates an analysis method that allows inferring implicit organizational specifications \textit{a posteriori} from the agents’ trajectories.  
\textsc{MOISE+MARL} was evaluated on four environments and several multi-agent reinforcement learning algorithms. The results show that agents’ policies converge faster, are more stable, and become more explainable. The analysis method is validated by the consistency between the predefined organizational specifications and the inferred ones.
\end{altabstract}

%%�R{\'e}sum{\'e} espagnol
% \begin{abstractES}
%  Eso es el Resumen.
% \end{abstractES}

%% Toutes ces informations doivent se trouver avant \maketitle, et
%% encore mieux : avant \begin{document}

\begin{document}

\maketitle


\section{Introduction}

% Contexte
\noindent
L'apprentissage par renforcement multi-agent~\cite{maisonhaute2024} (\textit{Multi-Agent Reinforcement Learning -- MARL}) constitue aujourd'hui un paradigme central pour l'entraînement d'agents constituant un Système Multi-Agent (SMA). Il vise à déterminer une \emph{politique conjointe} qui régit simultanément les actions individuelles des agents ainsi que leurs interactions, de manière à atteindre un objectif collectif sans qu'il soit nécessaire de spécifier explicitement les mécanismes de coordination~\cite{Albrecht2024}.
%
Cette axe de recherche a démontré son intérêt dans de nombreux domaines tels que la robotique collective~\cite{Wang2025}, la gestion de ressources distribuées~\cite{Chahoud2025} ou la cybersécurité~\cite{Hammar2023}.

\noindent
Dans des environnements nécessitant une forte composante d'interactions sociales, il n’est pas rare que les agents convergent vers des comportements collectifs réguliers. Ces comportements peuvent être interprétés comme la manifestation implicite de \textbf{rôles} (c’est-à-dire des fonctions assumées spontanément par certains agents) et de la poursuite coordonnée d’\textbf{objectifs}, ce qui peut donner l’impression de l’émergence d’une organisation~\cite{Foerster2016}.
%
Cette dynamique rapproche partiellement ces systèmes d’une véritable \textbf{organisation multi-agent}, structurée et fonctionnelle, similaire à celles décrites par des cadres normatifs comme AGR~\cite{ferber2003} (\textit{Agent Group Roles}) ou $\mathcal{M}OISE^+$~\cite{Hubner2007}. Dans ces modèles, les spécifications organisationnelles, comme les rôles ou objectifs, constituent les briques fondamentales pour décrire le comportement attendu d’un SMA. La comparaison entre comportements émergents du MARL et organisation explicite suggère ainsi un rapprochement entre apprentissage et modèle organisationnel.

% Problématique
\noindent
Toutefois, l’identification fiable de ces rôles et objectifs émergents demeure une tâche complexe. En effet, en émettant l'hypothèse que tout agent MARL suit une politique retranscrite par un rôle et des objectifs implicites possiblement inconnus, les comportements d'agents entraînés peuvent fréquemment apparaître comme \emph{bruités}, \emph{irréguliers} ou dépendants de dynamiques locales difficiles à interpréter. Cette variabilité rend délicate toute tentative de caractériser, de manière systématique, les régularités collectives comme de véritables \emph{rôles} ou \emph{objectifs}.
%
Pour capturer cette difficulté, nous introduisons la notion d’\textbf{adéquation organisationnelle}. Ce concept est théorisé comme la proximité entre les comportements appris par les agents par rapport aux comportements produits par des agents suivant une organisation explicite et normative. En cherchant à déterminer et à maîtriser dans quelle mesure les politiques apprises respectent ou s’écartent des spécifications organisationnelles définies, l'adéquation organisationnelle ouvre alors la voie à une double problématique encore peu explorée dans le domaine du MARL : le \textbf{contrôle} et l’\textbf{explicabilité}.
%
\begin{itemize}
    \item \textbf{Évaluation de l’adéquation organisationnelle.}
          Il s’agit de mesurer l’alignement d’une politique conjointe avec une organisation explicitement définie, c’est-à-dire un ensemble de comportements réguliers attendus. Or, la littérature existante reste principalement centrée sur l’inférence ou l’assignation de rôles~\cite{Isakov2024, Wen2024, Xie2024} au sein et pour le bénéfice de l'apprentissage seulement.

    \item \textbf{Contrôle de l’adéquation organisationnelle.}
          Il consiste à orienter l’apprentissage des agents vers des politiques conformes à une organisation cible, au moyen de \emph{contraintes} ou \emph{incitations} explicites. Un tel contrôle permet non seulement de réduire l’espace de recherche des politiques, mais aussi d’améliorer la convergence des algorithmes et d’assurer le respect de contraintes de sûreté ou de sécurité.
\end{itemize}


% Contribution
\noindent Nous présentons le cadre \textbf{MOISE+MARL} qui articule une double contribution :
%
\begin{itemize}
    \item \textbf{Unification MARL et modèle organisationnel : }
          MOISE+MARL combine la structure du modèle Markovien Dec-POMDP (\textit{Decentralized Partially Observable Markov Decision Process}) avec le modèle organisationnel $\mathcal{M}OISE^+$, afin de spécifier la logique des rôles et objectifs. Le cadre permet alors d'appliquer des rôles et objectifs aux agents, en ajustant leurs actions et leurs récompenses pour favoriser / imposer l’alignement organisationnel.
    \item \textbf{Méthode d'analyse organisationnelle : }
          MOISE+MARL comprend la méthode TEMM (\textit{Trajectory-based Evaluation in MOISE+MARL}) qui propose un mécanisme d’analyse \emph{a posteriori} permettant d’inférer automatiquement des rôles et objectifs à partir des trajectoires d’agents. En mobilisant des techniques d’apprentissage non supervisé, cette méthode fournit une mesure quantitative de l’\emph{adéquation organisationnelle}.
\end{itemize}

\noindent Contrairement au \textit{Hierarchical Reinforcement Learning} (HRL)~\cite{Qi2024,Matsuyama2025}, qui repose sur une décomposition interne des tâches, MOISE+MARL applique le guidage/contrainte organisationnelle avec un ensemble limité de spécifications organisationnelles, de façon \emph{externe} et indépendante de l'apprentissage, favorisant l’\emph{utilisabilité} et la \emph{scalabilité}.

% Évaluation et résultats
\noindent
Nous avons évalué MOISE+MARL selon trois axes principaux :
%
\begin{itemize}
    \item \textbf{Diversité des environnements} : Quatre environnements MARL ont été sélectionnés, chacun nécessitant des formes d’organisation implicite différentes, afin de tester la généralisabilité du cadre à des contextes variés ;
    \item \textbf{Multiplicité des algorithmes} : Quatre algorithmes MARL issus des familles \textit{policy-based}, \textit{actor-critic} et \textit{value-based} ont été comparées avec les spécifications organisationnelles durant l’apprentissage et l’analyse post-entraînement ;
    \item \textbf{Spécifications organisationnelles dédiées} : Pour chaque environnement, un ensemble de rôles, objectifs et permissions/obligations a été défini, permettant une évaluation manuelle ou via TEMM de l’impact organisationnel sur les politiques apprises.
\end{itemize}
%
Bien que l’établissement des spécifications organisationnelles nécessite un investissement initial, leur application permet généralement d’accélérer la convergence et de stabiliser les politiques apprises. Cela atténue le problème de non-stationnarité et contribue également à résoudre partiellement le problème d’attribution du mérite, qui sont des difficultés inhérentes au MARL~\cite{Albrecht2024}.
L’analyse manuelle montre que les agents dotés de rôles et d'objectifs adoptent des comportements conformes aux spécifications organisationnelles. Les rôles et objectifs inférés par TEMM correspondent aux spécifications prédéfinies, validant la cohérence interne du cadre.  Enfin, il est à noter que les algorithmes basés sur la politique et acteur-critique semblent produire des politiques plus stables que les algorithmes basés sur la valeur.

% Structure de l'article  
\noindent La \autoref{sec:related_works} examine les travaux liés à l'adéquation organisationnelle, la \autoref{sec:moise_marl_framework} détaille le cadre MOISE+MARL, la \autoref{sec:TEMM_algorithm} présente la méthode TEMM, la \autoref{sec:experimental_setup} décrit le protocole expérimental, la \autoref{sec:results} analyse les résultats obtenus, et enfin la \autoref{sec:discussion_conclusion_future_work} propose une conclusion et des perspectives.

\section{Travaux connexes}
\label{sec:related_works}

Cette section explore des travaux liant l'organisation avec le MARL.

\subsection{Évaluation de l'adéquation organisationnelle}

L'évaluation de l'adéquation organisationnelle consiste à vérifier si les comportements acquis par les agents correspondent à une organisation explicitement définie ou émergente. Les principaux critères considérés sont :
(E1) l’identification des rôles implicites,
(E2) la détection des objectifs et missions atteints,
(E3) la cohérence structurelle,
(E4) la mesure quantitative de l’adéquation organisationnelle,
et (E5) l’explicabilité des comportements collectifs.
%
Nous présentons ici les travaux saillants liés au sujet de l'inférence de rôles ou d'objectifs afin d'évaluer l'adéquation organisationnelle ou des notions proches.

Des mécanisme de découverte et de transfert de rôles à travers différents environnements ont été proposées pour identifier des rôles implicite~\cite{wilson2008learning}. Toutefois, ces rôles sont définis comme des abstractions statistiques internes à l’apprentissage, et ne permettent pas de satisfaire les critères de \emph{cohérence structurelle} ou de \emph{mesure explicite de l’adéquation organisationnelle}. L’explicabilité reste limitée à l’interprétation des structures de rôles transférées

Yusuf et Baber~\cite{yusuf2020inferential} contribue indirectement à la \emph{cohérence structurelle} en gérant l’incertitude et les conflits entre agents par inférence bayésienne. Toutefois, elle ne vise ni l’\emph{identification de rôles implicites}, ni la \emph{détection explicite d’objectifs ou missions}. L’absence de métriques quantitatives d’adéquation organisationnelle et de représentations interprétables limite sa portée au regard des critères recherchés.

Dans le cadre de missions \textit{Unmanned Aerial Vehicle}, une approche a été proposée pour montrer que les agents apprennent à coordonner leurs actions et ressources pour coopérer efficacement~\cite{berenji2000learning}. Cependant, l’approche ne formalise pas la notion de \emph{rôle implicite} généralisable et n’évalue pas la \emph{cohérence structurelle} au sein d’une organisation. La mesure d’adéquation est absente, et l’explicabilité reste limitée à des scénarios de coopération spécifiques.

Serrino et al.~\cite{serrino2019finding} propose d'aborder l’\emph{identification de rôles implicites}, mais uniquement sous la forme de relations sociales locales (allié/adversaire). Cela contribue à l’explicabilité immédiate des interactions, mais ne satisfait pas les critères de \emph{cohérence structurelle}, de \emph{mesure d’adéquation organisationnelle} ou de \emph{détection de missions globales}. L’évaluation reste contextuelle et ne s’appuie pas sur une organisation normative.

\medskip
\noindent
En résumé, les travaux identifiés couvrent partiellement certains critères en particulier l’identification de rôles~\cite{serrino2019finding,wilson2008learning}, la détection d’objectifs dans des missions spécifiques~\cite{berenji2000learning} mais aucun ne répond simultanément à l’ensemble des critères recherchés, laissant un vide que nous souhaitons combler comme illustré dans la \autoref{tab:evaluation_criteria_summary}.

\begin{table*}[h!]
    \centering
    \caption{Travaux liés à l’évaluation de l’adéquation organisationnelle}
    \label{tab:evaluation_criteria_summary}
    \renewcommand{\arraystretch}{1.3}
    \scriptsize
    \begin{tabular}{
        p{3cm}
        >{\centering\arraybackslash}p{1.4cm}
        >{\centering\arraybackslash}p{1.4cm}
        >{\centering\arraybackslash}p{1.4cm}
        >{\centering\arraybackslash}p{1.4cm}
        >{\centering\arraybackslash}p{1.4cm}
        }
        \hline
        \textbf{Travaux}                                      & \textbf{(E1) Identification de rôles} & \textbf{(E2) Détection objectifs / missions} & \textbf{(E3) Cohérence structurelle} & \textbf{(E4) Mesure quantitative} & \textbf{(E5) Explicabilité organisationnelle} \\ \hline
        Wilson et al. (2008)~\cite{wilson2008learning}        & \checkmark                            & $\times$                                     & $\times$                             & $\times$                          & $\sim$                                        \\
        Berenji \& Vengerov (2000)~\cite{berenji2000learning} & $\times$                              & \checkmark                                   & $\times$                             & $\times$                          & $\sim$                                        \\
        Yusuf \& Baber (2020)~\cite{yusuf2020inferential}     & $\times$                              & $\times$                                     & $\sim$                               & $\times$                          & $\times$                                      \\
        Serrino et al. (2019)~\cite{serrino2019finding}       & \checkmark                            & $\times$                                     & $\times$                             & $\times$                          & $\sim$                                        \\
        Wang et al. (2020)~\cite{wang2020roma}                & \checkmark                            & $\times$                                     & $\sim$                               & $\times$                          & $\sim$                                        \\
        Nguyen et al. (2022)~\cite{nguyen2022transfer}        & \checkmark                            & $\sim$                                       & \checkmark                           & $\times$                          & $\sim$                                        \\
        Zeng et al. (2023)~\cite{zeng2023sird}                & \checkmark                            & $\times$                                     & \checkmark                           & $\times$                          & $\sim$                                        \\
        Zhang et al. (2025)~\cite{zhang2025clustering}        & $\sim$                                & $\times$                                     & $\sim$                               & $\times$                          & $\sim$                                        \\
        Ahmed et al. (2022)~\cite{ahmed2022survey}            & $\sim$                                & $\sim$                                       & $\times$                             & $\times$                          & $\sim$                                        \\
        Selmonaj et al. (2025)~\cite{Selmonaj2025}            & $\times$                              & $\sim$                                       & $\times$                             & $\sim$                            & $\sim$                                        \\ \hline
    \end{tabular}
\end{table*}

\subsection{Contrôle de l'adéquation organisationnelle}

Le contrôle de l'adéquation organisationnelle vise à aligner les politiques des agents sur une organisation cible via contraintes ou incitations. Les travaux récents abordent principalement : (C1) l’expressivité organisationnelle, (C2) le guidage par récompenses, (C3) les contraintes sur les actions, et (C4) l’indépendance vis-à-vis de l’algorithme d’apprentissage.
%
Nous présentons ici les travaux identifiés comme s’inscrivant le mieux dans cette perspective, avec des degrés de contrôle différents.

Le \emph{Constrained Policy Optimization} (CPO) incorpore des contraintes convexes de sûreté dans l’optimisation de politique~\cite{achiam2017cpo}. L’approche se concentre sur la sécurité globale plutôt que sur une structuration organisationnelle fine. L’orientation de l’apprentissage se fait directement par ces contraintes, sans modulation de la récompense. Les actions des agents peuvent être restreintes si elles enfreignent les bornes de sûreté définies, tandis que l’algorithme conserve une relative généralité puisqu’il reste une extension de PPO (\textit{Proximal Policy Optimization}).

Ray et al.~\cite{ray2019benchmarking} introduisent une suite de benchmarks où les contraintes sont injectées via des multiplicateurs de Lagrange. La logique est de transformer les violations en pénalités intégrées à la fonction de retour, ce qui agit comme un guidage implicite par la récompense. Les contraintes ne s’expriment pas en obligations explicites sur les actions mais en découragement progressif de comportements indésirables. Cette approche reste centrée sur l’évaluation de différents algorithmes plutôt que sur une mise en forme collective des comportements.

Le \emph{shielding formel} est fondé sur des spécifications logiques de sûreté. Cette formalisation permet de définir des propriétés complexes, mais toujours centrées sur la sécurité~\cite{alshiekh2018safe}. Le mécanisme agit exclusivement sur les actions, en interceptant celles qui violent la logique spécifiée, sans modifier la fonction de récompense. L’agent conserve ainsi son algorithme d’apprentissage intact, tout en étant encadré par un module externe.

Garcia et al. propose une revue dans laquelle ils distinguent deux familles de méthodes de sûreté : l’évitement par pénalité, et le \emph{shielding}, par correction externe des actions~\cite{garcia2015comprehensive}. Dans le premier cas, l’apprentissage est influencé par un retour négatif ; dans le second, les actions interdites sont directement bloquées. Ces techniques garantissent un contrôle robuste mais se limitent à la prévention des risques, sans intégrer de description organisationnelle ni de rôles.

Les approches de \emph{Hierarchical Reinforcement Learning} (HRL) reposent sur une décomposition des tâches en sous-tâches~\cite{ghavamzadeh2006hrl}. Cette structuration interne guide l’agent en accélérant l’exploration et l’apprentissage. Les actions disponibles sont organisées en macro-actions, mais sans lien avec des prescriptions normatives externes. La logique hiérarchique impose une organisation fonctionnelle, qui reste interne à la tâche et dépendante de la façon dont elle est modélisée.

La coordination par communication différentiable illustre une autre forme de contrôle, fondée sur le partage d’informations~\cite{Foerster2016}. Les agents construisent une organisation émergente par échange de messages, sans qu’aucune contrainte normative ne soit imposée. L’apprentissage repose sur les récompenses globales, tandis que la liberté d’action est préservée. Ce type d’approche conserve une forte indépendance vis-à-vis des algorithmes employés, mais l’adéquation organisationnelle y reste implicite.

\medskip
\noindent
En somme, ces contributions mettent en évidence différentes façons de canaliser les comportements : par contraintes explicites sur les actions, par modulation des récompenses, par hiérarchisation des tâches ou par échanges d’information. Cependant, elles se limitent à des objectifs de sûreté ou de coordination implicite, sans s’appuyer sur un modèle organisationnel explicite définissant rôles, missions et obligations. Nous cherchons précisément à introduire une telle dimension organisationnelle, capable de guider les agents de manière formalisée tout en restant compatible avec divers algorithmes d’apprentissage comme cela est résumé dans la \autoref{tab:control_criteria_summary}.

\begin{table}[h]
    \centering
    \caption{Comparaison implicite des approches de contrôle : structuration, rôle du retour, contraintes d’action et indépendance vis-à-vis de l’algorithme.}
    \label{tab:control_criteria_summary}
    \scriptsize
    \renewcommand{\arraystretch}{1.3}
    \begin{tabular}{
        p{3.35cm}
        >{\centering\arraybackslash}p{1.7cm}
        >{\centering\arraybackslash}p{1.7cm}
        >{\centering\arraybackslash}p{1.7cm}
        >{\centering\arraybackslash}p{1.7cm}
        }
        \hline
        \textbf{Travaux}                                    & \textbf{Structuration (C1)} & \textbf{Rôle du retour (C2)} & \textbf{Contrôle des actions (C3)} & \textbf{Indépendance algorithmique (C4)} \\
        \hline
        Achiam et al. (2017)~\cite{achiam2017cpo}           & $\times$                    & $\times$                     & \checkmark                         & $\sim$                                   \\
        Ray et al. (2019)~\cite{ray2019benchmarking}        & $\times$                    & \checkmark                   & $\sim$                             & \checkmark                               \\
        Garcia et al. (2015)~\cite{garcia2015comprehensive} & $\times$                    & \checkmark                   & \checkmark                         & \checkmark                               \\
        Alshiekh et al. (2018)~\cite{alshiekh2018safe}      & $\sim$                      & $\times$                     & \checkmark                         & \checkmark                               \\
        Ghavamzadeh et al. (2006)~\cite{ghavamzadeh2006hrl} & $\sim$                      & $\sim$                       & \checkmark                         & $\times$                                 \\
        Foerster et al. (2016)~\cite{Foerster2016}          & $\sim$                      & \checkmark                   & $\times$                           & \checkmark                               \\
        Ma et al. (2022)~\cite{ma2022elign}                 & $\sim$                      & \checkmark                   & $\times$                           & \checkmark                               \\
        Xu et al. (2024)~\cite{xu2024subgoalhrl}            & $\sim$                      & $\sim$                       & \checkmark                         & $\times$                                 \\
        Zeng et al. (2025)~\cite{zeng2025valuealignment}    & \checkmark                  & $\sim$                       & $\sim$                             & \checkmark                               \\
        \hline
    \end{tabular}
\end{table}




\section{Le cadre MOISE+MARL}
\label{sec:moise_marl_framework}

Cette section présente le formalisme utilisé pour décrire le cadre MOISE+MARL.

\subsection{Cadre Markoviens pour le MARL}

Pour appliquer des techniques MARL, il est nécéssaire de s'appuyer sur un cadre Markovien pour formaliser les observations, actions, récompense, etc. Nous nous basons sur le cadre du Dec-POMDP~\cite{Oliehoek2016}. Les Dec-POMDP permettent de modéliser la coordination décentralisée entre agents dans des contextes à observabilité partielle, ce qui les rend particulièrement adaptés à l'intégration de contraintes organisationnelles. Contrairement aux \textit{Partially Observable Stochastic Game}, le Dec-POMDP utilise une fonction de récompense commune, favorisant ainsi la collaboration~\cite{Beynier2013}.
%s
Un Dec-POMDP $d \in D$ (avec $D$ l'ensemble des Dec-POMDP) est défini par un 7-uplet $d = (S,\{A_i\},T,R,\{\Omega_i\},O,\gamma)$ où~:
\begin{itemize}
    \item $S = \{s_1, ..., s_{|S|}\}$~: l'ensemble des états possibles.
    \item $A_i = \{a_1^i, ..., a_{|A_i|}^i\}$~: l'ensemble des actions possibles pour l'agent $i$.
    \item $T$ tel que $T(s,a,s') = \probP(s'|s,a)$~: la probabilité de transition conditionnelle entre états.
    \item $R: S \times A \times S \rightarrow \mathbb{R}$~: la fonction de récompense.
    \item $\Omega_i = \{o_1^i, ..., o_{|\Omega_i|}^i\}$~: l'ensemble des observations possibles pour l'agent $ag_i$.
    \item $O$ tel que $O(s',a,o) = \probP(o|s',a)$~: la probabilité conditionnelle d'observer $o$ depuis $s'$ après avoir effectué $a$.
    \item $\gamma \in [0,1]$~: le facteur d'actualisation qui décrit l'importance des récompenses futures par rapport aux récompenses immédiates (i.e spectre entre un comportement glouton et un comportement prévenant).
\end{itemize}

En considérant $m$ \textbf{équipes} (ou \textbf{groupes}) contenant chacune plusieurs agents parmi $\mathcal{A}$, nous reprenons le formalisme minimal nécessaire à la résolution d'un Dec-POMDP pour une équipe donnée $i, 0 \leq i \leq m$, composée de $n$ agents~\cite{Beynier2013,Albrecht2024}~:

\begin{itemize}
    \item $\Pi$~: l'ensemble des politiques. Une \textbf{politique} $\pi \in \Pi, \pi~: \Omega \rightarrow A$ est une fonction déterministe qui associe à chaque observation une action. Elle représente la logique interne de l'agent.
    \item $\Pi_{joint}$~: l'ensemble des politiques conjointes. Une \textbf{politique conjointe} $\pi_{joint} \in \Pi_{joint}, \pi_{joint}~: \Omega^n \rightarrow A^n = \Pi^n$ associe une action à chaque agent en fonction de son observation, et peut être vue comme l'ensemble des politiques utilisées par les agents.
    \item $H$~: l'ensemble des historiques. Un \textbf{historique} sur $z \in \mathbb{N}$ étapes est un $z$-uplet $h = ((\omega_k, a_k) | k \leq z, \omega \in \Omega, a \in A)$.
    \item $H_{joint}$~: l'ensemble des historiques conjoints. Un \textbf{historique conjoint} sur $z$ étapes $h_{joint} \in H_{joint}, h_{joint} = \{h_1, h_2, ..., h_n\}$ est l'ensemble des historiques des agents.
    \item $U_{joint,i}(\langle \pi_{joint,i}, \pi_{joint,-i} \rangle): \Pi_{joint} \rightarrow \mathbb{R}$~: la \textbf{récompense cumulée espérée} pour l'équipe $i$ sur un horizon fini, avec $\pi_{joint,i}$ la politique conjointe de l'équipe $i$ et $\pi_{joint,-i}$ les politiques conjointes des autres équipes (considérées comme fixes).
    \item $SR_{joint,i}(\pi_{joint,i}, s) = \{\pi_{joint,i} \mid U(\langle \pi_{joint,i}, \pi_{joint,-i} \rangle) \geq s\}$~: la \textbf{réponse suffisante}, c'est-à-dire l'ensemble des politiques conjointes atteignant au moins une récompense cumulée attendue $s \in \mathbb{R}, s \leq U^*_i$.
\end{itemize}

On appelle \textbf{résolution du Dec-POMDP} la recherche d'une politique conjointe $\pi^j \in \Pi^j$ telle que $U_{joint,i}(\pi^j) \geq s$, atteignant une récompense cumulée espérée au moins égale à un seuil $s \in \mathbb{R}$.

\subsection{Le modèle organisationnel $\mathcal{M}OISE^+$}

\begin{figure}[h!]
    \centering
    \input{figures/moise_model}
    \caption{Vue synthétique du modèle $\mathcal{M}OISE^+$}
    \label{fig:moise_model}
\end{figure}

Le modèle $\mathcal{M}OISE^+$~\cite{Hubner2007} fournit une description formelle avancée d'une organisation, notamment pour la description formelle des politiques des agents (via les plans). Il prend explicitement en compte les aspects sociaux entre agents, là où AGR se concentre sur l'intégration de normes orientées conception. De plus, il propose une vision suffisamment détaillée de l'organisation pour être comprise selon différents points de vue. Une représentation visuelle des éléments formels de ce modèle est donné en \autoref{fig:moise_model}
En nous basant sur le formalisme de $\mathcal{M}OISE^+$~\cite{Hubner2007}, nous ne détaillons ici que les éléments minimaux utilisés dans notre approche en donnant des exemples issus du contexte d'une équipe de football comme proposé dans \cite{Hubner2007}.

\

\noindent \textbf{Spécifications organisationnelles}~:~\quad $\mathcal{OS} = \langle \mathcal{SS}, \mathcal{FS}, \mathcal{DS} \rangle$, l'ensemble des spécifications organisationnelles, où $\mathcal{SS}$ sont les \textbf{spécifications structurelles}, $\mathcal{FS}$ les \textbf{spécifications fonctionnelles}, et $\mathcal{DS}$ les \textbf{spécifications déontiques}.

\

\noindent \textbf{Spécifications Structurelles}~:~\quad $\mathcal{SS} = \langle \mathcal{R}, \mathcal{IR}, \mathcal{G} \rangle$, où~:

\begin{itemize}
    \item $\mathcal{R}_{ss}$~: l'ensemble des rôles (notés $\rho \in \mathcal{R}$) (par exemple : $\rho_{\text{attaquant}} \in \mathcal{R}$)~;
    \item $\mathcal{IR}: \mathcal{R} \rightarrow \mathcal{R}$~: la relation d'héritage entre rôles ($\mathcal{IR}(\rho_1) = \rho_2$ signifie que $\rho_1$ hérite de $\rho_2$, noté aussi $\rho_1 \sqsubset \rho_2$). Par exemple, le rôle  $\rho_{\text{attaquant}}$ hérite de $\rho_{\text{player}}$, $\rho_{\text{attaquant}} \sqsubset \rho_{\text{player}}$~;
    \item $RG \subseteq GR$~: l'ensemble des groupes racines, $GR = \langle \mathcal{R}, \mathcal{SG}, \mathcal{L}^{intra}, \mathcal{L}^{inter}, \allowbreak \mathcal{C}^{intra}, \mathcal{C}^{inter}, np, ng \rangle$, l'ensemble des groupes, où~:
          \begin{itemize}
              \item $\mathcal{R} \subseteq \mathcal{R}_{ss}$~: l'ensemble des rôles non-abstraits (par exemple $\rho_{\text{attaquant}}$ ou $\rho_{\text{gardien}}$)~;
              \item $\mathcal{SG} \subseteq \mathcal{GR}$~: l'ensemble des sous-groupes~;
              \item $\mathcal{L} = \mathcal{R} \times \mathcal{R} \times \mathcal{TL}$~: l'ensemble des liens. Un lien est un triplet $(\rho_s,\rho_d,t) \in \mathcal{L}$ (aussi noté $link(\rho_s,\rho_d,t)$), où $\rho_s$ est le rôle source, $\rho_d$ le rôle destination, et $t \in \mathcal{TL}, \mathcal{TL} = \{acq, com, aut\}$ le type de lien~:
                    \begin{itemize}
                        \item $t = acq$ (acquaintance)~: les agents jouant $\rho_s$ peuvent identifier les agents jouant $\rho_d$~;
                        \item $t = com$ (communication)~: les agents jouant $\rho_s$ peuvent communiquer avec ceux jouant $\rho_d$~;
                        \item $t = aut$ (authority)~: les agents jouant $\rho_s$ peuvent exercer une autorité sur ceux jouant $\rho_d$. Ce lien nécessite les liens d'acquaintance et de communication.
                    \end{itemize}
              \item $\mathcal{L}^{intra} \subseteq \mathcal{L}$~: ensemble des liens intra-groupe. Par exemple, $link(\rho_{\text{gardien}}, \allowbreak \rho_{\text{arrière}}, aut)$ signifie qu'un joueur jouant le rôle de gardien peut exercer une autorité sur un joueur jouant le rôle d'arrière~;
              \item $\mathcal{L}^{inter} \subseteq \mathcal{L}$~: ensemble des liens inter-groupe. Par exemple, $link(\rho_{\text{coach}}, \allowbreak \rho_{\text{joueur}}, aut)$ signifie qu'un joueur jouant le rôle de gardien peut exercer une autorité sur un joueur peu importe le groupe~;
              \item $\mathcal{C} = \mathcal{R} \times \mathcal{R}$~: l'ensemble des compatibilités. Une compatibilité est un couple $(\rho_a, \rho_b) \in \mathcal{C}$ (noté aussi $\rho_a \bowtie \rho_b$), signifiant qu'un agent jouant $\rho_a$ peut aussi jouer $\rho_b$~;
              \item $\mathcal{C}^{intra} \subseteq \mathcal{C}$~: ensemble des compatibilités intra-groupe. Par exemple, $\rho_{\text{leader}} \bowtie \rho_{\text{arrière}}$ signifie qu'un agent jouant le rôle de leader peut aussi jouer le rôle d'arrière au sein du même groupe~;
              \item $\mathcal{C}^{inter} \subseteq \mathcal{C}$~: ensemble des compatibilités inter-groupe. Par exemple, $\rho_{\text{leader}} \bowtie \rho_{\text{gardien}}$ signifie qu'un agent jouant le rôle de leader dans un groupe peut aussi jouer le rôle de gardien mais dans un autre groupe~;
              \item $np: \mathcal{R} \rightarrow \mathbb{N} \times \mathbb{N}$~: relation donnant la cardinalité du nombre d'agents par rôle. Par exemple, $np = \{\rho_{\text{gardien}} \mapsto (1,1), \rho_{\text{leader}} \mapsto (0,1), \rho_{\text{arrière}} \mapsto (3,3)\}$, signifie que dans un groupe donné, il doit y avoir un seul gardien, trois à l'arrière et potentiellement aucun ou un leader~;
              \item $ng: \mathcal{SG} \rightarrow \mathbb{N} \times \mathbb{N}$~: relation donnant la cardinalité de chaque sous-groupe. Par exemple, $ng = \{\text{def} \mapsto (1,1), \text{att} \mapsto (1,1)\}$, signifie qu'il ne doit y avoir qu'un groupe attaquant et un groupe défenseur.
          \end{itemize}
\end{itemize}

\medskip

\noindent \textbf{Spécifications Fonctionnelles}~:~\quad $\mathcal{FS} = \langle \mathcal{SCH}, \mathcal{PO} \rangle$, où~:

\begin{itemize}
    \item $\mathcal{SCH} = \langle\mathcal{G}, \mathcal{M}, \mathcal{P}, mo, nm \rangle$~: l'ensemble des \textbf{schémas sociaux}, où~:
          \begin{itemize}
              \item $\mathcal{G}$~: l'ensemble des objectifs globaux; Par exemple, $\mathcal{G} = \{g_0, g_1, g_2 \dots\}$ avec $g_0 = "\text{marquer un but}", g_1 = "\text{balle au milieu}", g_2 = " \text{un milieu a } \allowbreak \text{la balle}"$\dots~;
              \item $\mathcal{M}$~: l'ensemble des missions. Par exemple, $m_{\text{attaque}} \in \mathcal{M}$~;
              \item $\mathcal{P} = \langle \mathcal{G}, \{\mathcal{G}\}^s, OP, [0,1] \rangle, s \in \mathbb{N}^*$~: les plans définissant l'arbre des objectifs.
                    Un plan $p \in \mathcal{P}$ est un 4-uplet $p = (g_f, \{g_i\}_{0 \leq i \leq s}, op, p)$, où $g_f \in \mathcal{G}$ est un objectif, les $g_i \in \mathcal{G}$ sont des sous-objectifs, $op \in OP = \{sequence, choice, parallel\}$ est un opérateur, et $p \in [0,1]$ est une probabilité de succès~:
                    \begin{itemize}
                        \item $op = sequence$~: les $g_i$ doivent être atteints dans un ordre précis aussi notés (\textquote{$,$})~;
                        \item $op = choice$~: un seul $g_i$ doit être atteint (aussi noté (\textquote{$|$}))~;
                        \item $op = parallel$~: les $g_i$ peuvent être atteints en parallèle ou séquentiellement (aussi noté (\textquote{$||$})).
                    \end{itemize}
                    Par exemple, \textquote{$g_2 =_{0.85}g_{6},(g_{7}|g_{8})$} signifie que l'objectif $g_2$ peut être atteint avec une probabilité de 0.85 si l'objectif $g_6$ est atteint, suivi soit de l'objectif $g_7$ soit de l'objectif $g_8$. De même, \textquote{$g_{10} = g_{13} || g_{14}$}, signifie que l'objectif $g_{10}$ peut être atteint si les objectifs $g_{13}$ et $g_{14}$ sont atteints, possiblement parallèlement et peu importe l'ordre~;
              \item $mo: \mathcal{M} \rightarrow \mathbb{P}(\mathcal{G})$~: relation liant une mission à un ensemble d'objectifs. Par exemple, $mo = \{m_1 \mapsto \{g_2,g_6,g_7,g_8,g_{13} \}, m_2 \mapsto \{g_{13},g_{16},g_{11}, \allowbreak g_{24}\}, \dots\}$~;
              \item $nm: \mathcal{M} \rightarrow \mathbb{N} \times \mathbb{N}$~: cardinalité du nombre d'agents affectés à une mission. Par exemple, $nm = \{m_{\text{attaque}} \mapsto (2,4), m_{\text{défense}} \mapsto (3,5)\}$ signifie que la mission d'attaque doit être réalisée par au moins 2 agents et au plus 4 agents, tandis que la mission de défense doit être réalisée par au moins 3 agents et au plus 5 agents.
          \end{itemize}
    \item $\mathcal{PO}: \mathcal{M} \times \mathcal{M}$~: ensemble des \textbf{ordres de préférence}. Un ordre de préférence est un couple $(m_1, m_2)$ (noté aussi $m_1 \prec m_2$) signifiant que si un agent peut s'engager à la fois sur $m_1$ et $m_2$, il aura une préférence sociale pour $m_1$.
\end{itemize}

\medskip

\noindent \textbf{Spécifications Déontiques}~:~\quad $\mathcal{DS} = \langle \mathcal{OBL}, \mathcal{PER} \rangle$, l'ensemble des spécifications déontiques, où~:

\begin{itemize}
    \item $\mathcal{TC}$~: ensemble des \textbf{contraintes temporelles}. Une contrainte $tc \in \mathcal{TC}$ indique les périodes pendant lesquelles une permission ou obligation est valide. Par exemple, $Any \in \mathcal{TC}$ signifie tout le temps et \textquote{$[t_1, t_2]$} signifie la période entre $t_1$ et $t_2$~;
    \item $\mathcal{OBL}: \mathcal{R} \times \mathcal{M} \times \mathcal{TC}$~: ensemble des \textbf{obligations}. Une obligation est un triplet $(\rho_a, m, tc)$ (aussi noté $obl(\rho_a, m, tc)$), signifiant qu'un agent jouant le rôle $\rho_a$ est obligé de s'engager dans la mission $m$ pendant la période spécifiée $tc$. Par exemple $obl(\rho_{arriere}, m_2, [t_1, t_2])$ signifie qu'un agent jouant le rôle d'arrière est obligé de s'engager dans la mission $m_2$ pendant la période entre $t_1$ et $t_2$~;
    \item $\mathcal{PER}$~: ensemble des \textbf{permissions}. Une permission est un triplet $(\rho_a, m, tc)$ (aussi noté $per(\rho_a, m, tc)$), signifiant qu'un agent jouant le rôle $\rho_a$ est autorisé à s'engager dans la mission $m$ pendant $tc$. Par exemple, $per(\rho_{attaquant}, m_1, \allowbreak Any)$ signifie qu'un agent jouant le rôle d'attaquant est autorisé à s'engager dans la mission $m_1$ tout le temps~;
\end{itemize}

\

\noindent Les spécifications organisationnelles appliquées aux agents sont les rôles et les objectifs (en tant que missions) à travers les permissions ou obligations. En effet, les autres spécifications structurelles comme les compatibilités ou les liens sont inhérentes aux rôles. De même, nous considérons que les objectifs, missions et leur association ($mo$) permettent de relier les autres spécifications fonctionnelles comme les plans, les cardinalités ou les préférences.
Par conséquent, nous considérons qu'il est suffisant de prendre en compte les rôles, les missions (objectifs et correspondance) et les permissions/obligations pour décrire l'essentiel de l'organisation d'un SMA.


\subsection{Liaison de \(\mathcal{M}OISE^+\) avec le MARL}

\begin{figure}[h!]

    \adjustbox{lap=-0.38cm}{\input{figures/mm_synthesis_single_column.tex}}
    \caption{\textbf{Une vue minimale du cadre MOISE+MARL} :
        Les utilisateurs définissent d'abord les spécifications de \(\mathcal{M}OISE^+\), qui incluent des rôles (\(\mathcal{R}\)) et des missions (\(\mathcal{M}\)) associées via \(rds\). Ensuite, ils créent des spécifications MOISE+MARL en définissant d'abord des \textbf{Guides de Contraintes} tels que \(rag\) et \(rrg\) pour spécifier la logique des rôles, et \(grg\) pour la logique des objectifs.
        Ensuite, des \textbf{Lieurs} sont utilisés pour connecter les agents aux rôles via \(ar\) et pour relier la logique des \textbf{Guides de Contrainte} aux spécifications définies de \(\mathcal{M}OISE^+\). Une fois cette configuration réalisée, des rôles peuvent être attribués aux agents, et le cadre MARL s'adapte en conséquence pendant l'entraînement.
    }
    \label{fig:mm_synthesis}
\end{figure}

Alors qu'\textit{AGR}~\cite{ferber2003} est un cadre informel qui introduit des rôles par groupes, \(\mathcal{M}OISE^+\) offre une description plus détaillée et flexible des structures et fonctions d'un système multi-agent (MAS), facilitant ainsi la formalisation des politiques en MARL.

\medskip
\noindent \textbf{Guides de Contraintes} : Trois relations décrivent la logique des rôles et objectifs dans le cadre Dec-POMDP :

\begin{itemize}
    \item \textbf{Guide d'action de rôle} \(rag: H \times \Omega \to \mathcal{P}(A \times \mathbb{R})\) : Pour chaque couple \((h,\omega)\) (\(h\in H\), \(\omega\in \Omega\)), il associe un ensemble d'actions \(A_\omega \subseteq A\) avec une rigidité \(ch \in [0,1]\) (par défaut \(ch=1\)), restreignant ainsi le choix de la prochaine action ;
    \item \textbf{Guide de récompense de rôle} \(rrg: H \times \Omega \times A \to \mathbb{R}\) : Défini par \(rrg(h,\omega,a)=r_m\) si \(a \notin A_\omega\) (avec \(rag(h,\omega)=A_\omega \times \mathbb{R}\)), et 0 sinon, afin d'encourager le respect du rôle ;
    \item \textbf{Guide de récompense d'objectif} \(grg: H \to \mathbb{R}\) : Attribue un bonus \(r_b\) à la récompense globale si \(h\) contient une sous-séquence caractéristique \(h_g \in H_g\) correspondant à un objectif.
\end{itemize}


\medskip
\noindent \textbf{Lieurs} : Relient les spécifications de \(\mathcal{M}OISE^+\) aux Guides de Contraintes et aux agents :

\begin{itemize}
    \item \textbf{Agent vers Rôle} \(ar: \mathcal{A} \to \mathcal{R}\) (relation bijective) ;
    \item \textbf{Rôle vers Guide de Contrainte} \(rcg: \mathcal{R} \to rag \cup rrg\) : Associe à chaque rôle une relation \(rag\) ou \(rrg\) ;
    \item \textbf{Objectif vers Guide de Contrainte} \(gcg: \mathcal{G} \to grg\) : Relie chaque objectif à son guide \(grg\).
\end{itemize}

\begin{figure*}[h!]
    \label{eq:single_value_function}
    \raggedright
    \textbf{\textit{Definition 1} \quad Fonction État-Valeur adaptée aux guides de contraintes en AEC :}

    \begin{small}
        \vspace{0.cm}
        \begin{gather*}
            V^{\pi^j}(s_t) = \hspace{-0.75cm}
            %
            \sum_{\textcolor{red}{ \substack{a_{t} \in A \text{ if } rn() < ch_{t}, \\
                        a_{t} \in A_{t} \text{ else}}
                }}{\hspace{-0.7cm} \pi_i(a_{t} | \omega_t)}
            %
            \sum_{s_{t+1} \in S}
            %
            {\hspace{-0.1cm} T(s_{t+1} | s_t, a_{t})
            \Bigl[R(s_t,a_{t},s_{t+1}) + \hspace{-0.1cm}
            \textcolor{blue}{ \sum_{m \in \mathcal{M}_i}{ \hspace{-0.1cm} v_m(t) \frac{grg_m(h_{t+1})}{1 - p + \epsilon} } }
            + } \\
            {\hspace{5.5cm}\textcolor{red}{(1-ch_t) \times rrg(\omega_t,a_{t+1})} + V^{\pi^j_{i+1 \ mod \ n}}(s_{t+1})\Bigr]}
        \end{gather*}
        %
        \vspace{0.cm}
        \textcolor{red}{\[\text{ \hspace{-0.3cm} Avec } rag(h_t, \omega_t) \hspace{-0.05cm} = \hspace{-0.08cm} A_{t} \times \mathbb{R} \text{, } \langle a_t, ch_{t} \rangle \hspace{-0.08cm} \in \hspace{-0.08cm} A_{t} \times \mathbb{R} \text{ ; } rn: \emptyset \hspace{-0.05cm} \to \hspace{-0.1cm} [0,1[ \text{, une fonction aléatoire uniforme}\]}
        %
        \vspace{-0.2cm}
        \textcolor{blue}{
            \begin{gather*}
                \hspace{-0.65cm}
                \text{Avec } \omega_t = O(\omega_t | s_t, a_t) \text{ ; } h_t = \{h_0 = \langle \rangle, h_{t+1} = \langle h_t, \langle \omega_{t+1}, a_{t+1} \rangle \rangle \} \text{ ; } \epsilon \in \mathbb{R}_{>0} \text{ ; } grg_m(h) =
            \end{gather*}
        }
        \vspace{-0.4cm}
        \textcolor{blue}{
            \begin{gather*}
                \hspace{-0.5cm} \sum_{\hspace{0.3cm}(grg_i,w_i) \in mo(m)}{\hspace{-0.9cm} w_i \times grg_i(h)}
                \text{ ; } v_m(t) = \{ 1 \text{ if } t \in t_c \text{ ; else } 0 \} \text{ ; } \mathcal{M}_i = \{m_j | \langle ar(i),m_j,t_c,p \rangle \in \mathcal{M}\}
            \end{gather*}
        }
        \vspace{0.cm}
    \end{small}

\end{figure*}


\textbf{La résolution du problème MOISE+MARL} consiste à trouver une politique conjointe
$
    \pi^j = \{\pi^j_0, \pi^j_1, \dots, \pi^j_n\}
$
maximisant la fonction de valeur d'état \(V^{\pi^j}\) (ou atteignant un seuil minimal), qui représente la récompense cumulative attendue à partir d'un état initial \(s \in S\) en suivant les actions conjointes \(a^j \in A^n\) sous l'effet de \textbf{Guides de Contraintes}. Cette fonction est définie pour des agents agissant de manière séquentielle et cyclique (mode \textit{Agent Environment Cycle} -- AEC) (voir la \hyperref[eq:single_value_function]{Définition 1}). La \autoref{fig:mm_synthesis} illustre les liens entre \(\mathcal{M}OISE^+\) et le Dec-POMDP via MOISE+MARL.

À chaque instant \(t \in \mathbb{N}\) (initialement \(t=0\)), l'agent \(i = t \mod n\) doit assumer le rôle \(\rho_i = ar(i)\). Pour chaque spécification déontique valide
$
    d_i = rds(\rho_i) = \langle tc_i, y_i, m_i \rangle,
$
l'agent est permis (si \(y_i=0\)) ou obligé (si \(y_i=1\)) d'exécuter la mission \(m_i \in \mathcal{M}\) (avec \(\mathcal{G}_{m_i} = mo(m_i)\)). L'agent choisit d'abord une action parmi celles attendues \(A_t\) si une valeur aléatoire est inférieure à la rigidité \(ch_t\), sinon parmi l'ensemble \(A\); ainsi, un \(ch_t = 1\) impose une contrainte forte.

L'action appliquée à \(s_t\) conduit à l'état suivant \(s_{t+1}\), génère la prochaine observation \(\omega_{t+1}\) et une récompense. Celle-ci est la somme de la récompense globale et des ajustements organisationnels :
i) un bonus (via les Guides de Récompense d'Objectif), pondéré par \(\frac{1}{1-p+\epsilon}\) pour ajuster son impact, et
ii) une pénalité (via les Guides de Récompense de Rôle), pondérée par la rigidité \(ch_t\) pour ajuster son impact.
Le calcul de la récompense cumulative se poursuit dans \(s_{t+1}\) avec l'agent suivant \((i+1) \mod n\).

\subsection{Faciliter l'implémentation des \textbf{Guides de Contrainte}}

Puisque les rôles, objectifs et missions sont de simples étiquettes, leur définition est implicite. Cependant, implémenter une relation \(rag\), \(rrg\) ou \(grg\) nécessite de définir de nombreux historiques, souvent redondants, rendant une définition extensionnelle fastidieuse et peu évolutive. De plus, la logique de chaque \textbf{Guide de Contrainte} repose sur l’analyse de trajectoires d’agents : pour chaque historique observé, il faut décider s’il appartient à un ensemble d’historiques attendus et quelles conséquences en tirer (masquage d’actions, ajout de pénalités ou de bonus de récompense). Par exemple, \(rag\) restreint les actions disponibles selon l’appartenance de la trajectoire courante à un ensemble \(H_g\) et la nouvelle observation.

Une première approche consiste à laisser l’utilisateur définir ces guides par une logique procédurale (scripts Python ou règles spécifiques). Dans ce cas, la relation \(b_g: H \to \{0,1\}\) formalise la décision d’appartenance d’un historique à un ensemble \(H_g\). Cette solution est flexible, car elle peut exploiter la totalité du contexte disponible (positions spatiales, états internes, séquences passées). Toutefois, elle reste coûteuse à écrire et à maintenir, et conduit souvent à des définitions très verbeuses.

Pour dépasser cette limite, nous introduisons les \textit{Trajectory-based Patterns} (TP), inspirés du Traitement Automatique du Langage. L’idée est de fournir un formalisme déclaratif compact permettant d’exprimer des comportements attendus comme des motifs temporels. Un TP \(p \in P\) correspond ainsi à un patron qui capture un ensemble d’historiques de manière intentionnelle. Chaque observation ou action est associée à une étiquette \(l \in L\) (via \(l: \Omega \cup A \to L\)), ce qui rend la manipulation pratique et indépendante des détails bas-niveau de l’environnement.
%
Un TP \(p\) peut être :
\begin{itemize}
    \item une \textbf{séquence feuille} \(s_l = \langle h, \{c_{min}, c_{max}\} \rangle\), où \(h \in H\) désigne une sous-séquence observation/action et \(\{c_{min}, c_{max}\}\) est la cardinalité ;
    \item une \textbf{séquence nœud} \(s_n = \langle \langle s_{l_1}, s_{l_2}, \dots \rangle, \{c_{min}, c_{max}\} \rangle\), combinant plusieurs séquences en un motif hiérarchique.
\end{itemize}

\noindent
Par exemple, le pattern
%
$p = ``[o_1,a_1,[o_2,a_2]\langle0,2\rangle]\langle1,*\rangle"$
%
se lit comme suit : un historique valide doit contenir au moins une occurrence de la paire \(\langle o_1,a_1\rangle\), suivie de zéro à deux occurrences de \(\langle o_2,a_2\rangle\). Ce TP capture donc une famille entière de comportements, sans avoir à lister tous les historiquess.
%
% La relation de correspondance est alors définie par :
% \[
%     b_g(h) = m(p_g,h), \quad \text{avec } m: P \times H \to \{0,1\},
% \]
% où \(m(p_g,h)\) indique si un historique \(h \in H\) satisfait le pattern \(p_g \in P\).
%
L’intérêt des TP est double :
\begin{enumerate*}[label={\roman*) },itemjoin={; \quad}]
    \item Ils permettent de coder de manière \textbf{compacte} des comportements étendus dans le temps, difficiles à exprimer autrement.
    \item Ils facilitent la \textbf{réutilisation}, puisque les mêmes motifs peuvent être partagés entre plusieurs rôles ou objectifs.
\end{enumerate*}

\paragraph{Exemple concret.} Considérons l’environnement \textit{Overcooked-AI}, où des agents cuisiniers doivent collaborer en se déplaçant dans un monde en grille et interagir avec les ingrédient (oignons) et les instruments (pots, bols) pour faire de la soupe qui est expédiée en interagissant avec la zone d'expedition. Ici, l’on souhaite reconnaître le comportement suivant : \textquote{un agent qui détient un oignon, observe un pot et interagit avec lui pour le remplir}. Ce comportement peut être exprimé par le TP suivant :
\[
    p = [[\#any](*) , \; has\_onion , \; [\#any](*) , \; see\_pot , \; interact \;](1,1)
\]

\noindent
Ce TP peut être exploité dans les guides de contrainte comme montré dans la \autoref{tab:tp_guides_example} pour inciter l'agent à intéragir à nouveau pour récupérer la soupe par exemple.

\begin{table}[h]
    \centering
    \caption{Exemple de guides appliqués au TP ``remplir un pot avec un oignon''.}
    \label{tab:tp_guides_example}
    \scriptsize
    \renewcommand{\arraystretch}{1.3}
    % \setlength{\tabcolsep}{10pt}
    \begin{tabular}{p{2cm}@{\hspace{20pt}}p{6cm}}
        \hline
        \textbf{Guide}                            & \textbf{Exemple de règle}                                         \\
        \hline
        \textbf{RAG} (\textit{Role Action Guide}) & Si le TP est satisfait, restreindre les actions possibles~:
        \[
            rag(h,\omega) = \{\texttt{interact} \mapsto 1.0, \;\texttt{nothing} \mapsto 0.2\}
        \]
        L’action \texttt{interact} est fortement favorisée.                                                           \\
        \hdashline
        \textbf{RRG} (\textit{Role Reward Guide}) & Ajouter un bonus de rôle lorsque l’action attendue est réalisée~:
        \[
            rrg(h,\omega,a) =
            \begin{cases}
                +3 & \text{si } a = \texttt{interact} \\
                0  & \text{sinon}
            \end{cases}
        \]                                                                                                            \\
        \hdashline
        \textbf{GRG} (\textit{Goal Reward Guide}) & Accorder une récompense si le TP est détecté / atteint~:
        \[
            grg(h) =
            \begin{cases}
                +5 & \text{si le pot est rempli (TP reconnu)} \\
                0  & \text{sinon}
            \end{cases}
        \]                                                                                                            \\
        \hline
    \end{tabular}
\end{table}

Ainsi, au lieu de définir extensionnellement de vastes ensembles \(H_g\), l’utilisateur décrit quelques motifs symboliques qui capturent l’essence des comportements recherchés. MOISE+MARL applique alors automatiquement ces motifs aux guides (\(rag, rrg, grg\)), rendant leur mise en œuvre plus modulaire, scalable et interprétable.


\section{La méthode TEMM}
\label{sec:TEMM_algorithm}

Comme indiqué en \autoref{sec:related_works}, aucun travaux ne répond pleinement à nos exigences pour déterminer les rôles et objectifs implicites ainsi que l'adéquation organisationnelle. Nous proposons donc la méthode TEMM, qui vise à extraire automatiquement des spécifications organisationnelles \(\mathcal{M}OISE^+\) implicites à partir des trajectoires d’agents entraînés.

On suppose qu'une politique d'agent donnée peut être décrite au moins partiellement par un rôle et un ensemble d'objectifs (missions) qu'il cherche à atteindre. Ainsi, bien que les trajectoires d'un agent entraîné puissent sembler différentes, notre hypothèse est qu'elles peuvent être regroupées par similarité à un nombre limité de trajectoires typiques issues de l’exécution du rôle et des objectifs implicites à la politique de l'agent. Elle repose sur l’hypothèse que les comportements des agents, malgré une variabilité apparente, présentent des régularités lorsqu'ils atteignent des récompenses cumulées comparables. En d'autres mots, des comportements différents peuvent être interprétés comme des variantes bruitées d'un nombre limité de stratégies. Une analyse moyennant de nombreuses trajectoires d'agents permet de reveler des trajectoires typiques supposées comme les plus proches des trajectoires issues de l'exécution d'un rôle et d'objectifs implicites.
%
TEMM~\hyperref[fn:github]{\footnotemark[1]} repose sur l’apprentissage non supervisée pour généraliser ces spécifications à partir d'un nombre représentatif de trajectoires collectées comme illustrée en \autoref{fig:temm_overview}. En quantifiant l’écart entre les spécifications inférées et les comportements observés, TEMM propose une mesure quantitative de l'adéquation organisationnelle.

\begin{figure*}[h!]
    \centering
    \resizebox{\textwidth}{!}{%
        \input{figures/temm_overview.tex}
    }
    \caption{\textbf{Vue d'ensemble de la méthode TEMM} : (1) collecte des trajectoires d'agents de même performance ; (2) prétraitement des trajectoires en vecteurs ; (3) clustering pour extraire des trajectoires typiques, révélant rôles et objectifs implicites ; (4) inférence de règles organisationnelles et évaluation de l'adéquation organisationnelle à partir des variances intra-cluster.}
    \label{fig:temm_overview}
\end{figure*}

\footnotetext[1]{\label{fn:github}L'implémentation  \textbf{MMA} \textit{MOISE+MARL API}, les hyperparamètres et spécifications utilisés sont disponibles à \url{https://github.com/julien6/MOISE-MARL}. Une vidéo de démonstration est accessible à \url{https://www.youtube.com/watch?v=b3wqFpfXZi0}.}


\noindent
\textbf{1) Inférence des rôles et hiérarchie de rôles} \quad
Un rôle \(\rho\) est défini comme un ensemble de règles de transition (observation, action).
\begin{itemize}
    \item \textbf{Préparation.} Les trajectoires des agents sont construites en concaténant observations et actions (souvent encodées en one-hot si les actions sont catégorielles), puis représentées sous forme de vecteurs globaux.
    \item \textbf{Extraction.} Un \textit{clustering hiérarchique} est appliqué à ces trajectoires, en utilisant différentes distances (euclidienne, Smith-Waterman, cosinus, etc.). Les centroïdes de clusters révèlent des motifs typiques de transitions observation $\rightarrow$ action, interprétés comme des règles comportementales associées à un rôle abstrait (i.e. le RAG du rôle implicite).
    \item \textbf{Représentation.} Les résultats peuvent être visualisés sous forme de dendrogrammes, où les rôles apparaissent aux nœuds, hiérarchisés en fonction de leur inclusion séquentielle. Les vecteurs des transitions ainsi que les centroides peuvent visualiés dans un espace à deux dimensions via une Analyse Par Composant Principale (ACP).
    \item \textbf{Evaluation.} L'\textbf{adéquation structurelle} (\textit{structural organizational fit} -- SOF) est calculé comme l’inverse normalisé de la variance globale dans les clusters de transitions~: une faible variance indique une forte cohérence structurelle.
\end{itemize}


\noindent
\textbf{2) Inférence des objectifs et missions} \quad
Les objectifs sont définis comme des ensembles d’observations typiques conjointement atteints par les agents performants.
\begin{itemize}
    \item \textbf{Préparation.} Les trajectoires sont prétraitées en ne retenant que les observations perçues par l'agent au cours de l’épisode.
    \item \textbf{Extraction.} Ces trajectoires sont clusterisées afin d’identifier des centroïdes d’observations. Les observations fréquentes (faible variance locale) sont retenues comme des objectifs implicites intermédiaires, jalons vers l’objectif global. L'ensemble des objectifs intermédiaires associés à un même cluster de trajectoires est interprété comme une mission implicite.
    \item \textbf{Représentation.} Les résultats peuvent être illustrés par des dendrogrammes (hiérarchie des objectifs) ou via des réductions dimensionnelles (ACP) permettant de visualiser la diversité des trajectoires dans un espace à deux dimensions.
    \item \textbf{Evaluation.} évalue la cohérence fonctionnelle des agents dans l’atteinte de ces objectifs intermédiaires, calculé comme l’inverse normalisé de la variance dans les clusters d’observations.
\end{itemize}

\noindent
\textbf{3) Inférence des obligations et permissions} \quad
La dimension déontique est déduite de l’association entre rôles et missions : une obligation est attribuée à un agent occupant un rôle \(\rho\) lorsqu’il réalise exclusivement les objectifs d’une mission donnée dans un intervalle temporel spécifié, tandis qu’une permission est identifiée lorsqu’un rôle peut atteindre plusieurs objectifs alternatifs.

L’\textbf{adéquation organisationnelle globale} est obtenue moyenne des deux composantes :
%
$\text{OF} = \tfrac{1}{2}\left( \text{SOF} + \text{FOF} \right)$.
%
Un score élevé indique que les rôles, objectifs et missions inférés sont représentatifs des comportements effectivement appris. À l’inverse, un score faible reflète des comportements peu structurés ou fortement bruités. TEMM constitue ainsi une méthode explicable et reproductible pour analyser les dynamiques organisationnelles émergentes et ajuster les spécifications de \(\mathcal{M}OISE^+\).

\medskip

Cependant, TEMM présente aussi certaines limites : les algorithmes de clustering (K-means, hiérarchique) requièrent une paramétrisation manuelle (nombre de clusters, seuils de similarité), introduisant un biais potentiel ; les rôles et objectifs inférés nécessitent souvent une validation experte pour éliminer les artefacts liés au bruit des trajectoires ; la méthode reste sensible à la qualité et diversité des trajectoires, et son efficacité peut être limitée dans des environnements à faible variance comportementale.


\section{Cadre expérimental}
\label{sec:experimental_setup}

\noindent Cette section détaille le cadre expérimental que nous avons établi pour évaluer le cadre MOISE+MARL.


\subsection{Implémentation de MOISE+MARL}

Nous avons développé une API Python~\hyperref[fn:github]{\footnotemark[1]}, pour implémenter MOISE+MARL. Cette API structure le modèle \(\mathcal{M}OISE^+\) en classes de données imbriquées afin de définir les spécifications organisationnelles (rôles, objectifs, permissions\dots).

Nous utilisons la bibliothèque \textit{PettingZoo}~\cite{terry2020pettingzoo} (similaire à Gymnasium~\cite{kwiatkowski2024}) pour la gestion des environnements en y intégrant un dictionnaire personnalisable pour le mappage des étiquettes d'observation/action (\(l\)), ainsi que le support des TP pour définir et faire correspondre les motifs.

Chaque \textbf{Guide de Contrainte} (\(rag\), \(rrg\) et \(grg\)) est implémenté comme une classe distincte. Les utilisateurs peuvent les définir via des fonctions personnalisées ou des règles JSON (par exemple, \(rag\) associe un couple \(\langle\)TP, dernière observation\(\rangle\) à des actions attendues, et \(grg\) applique des bonus selon des TP spécifiques). La classe globale \textbf{MMA} intègre ces guides et relie les agents aux rôles via des relations telles que \(ar\), intégrant ainsi les spécifications de \(\mathcal{M}OISE^+\).

Une fois configuré, MMA encapsule l'environnement avec un wrapper \textit{PettingZoo} qui applique des masques d'actions et ajuste les récompenses pour garantir le respect des spécifications durant l'entraînement. Il intègre également \textit{MARLlib}~\cite{hu2021marlib} pour accéder aux algorithmes MARL de pointe sur un cluster haute performance.

Enfin, la méthode TEMM, avec des hyperparamètres optimisés manuellement, est utilisée après l'entraînement pour inférer les rôles et objectifs implicites via clustering hiérarchique et K-means. Cette analyse génère des sorties visuelles (dendrogrammes, graphes de transition) et permet d'exporter les trajectoires JSON des comportements organisationnels inférés.

\subsection{Environnements utilisés}

Nous testons MOISE+MARL dans quatre environnements MARL, modélisés comme des scénarios Dec-POMDP et chacune présentant des défis distincts en termes d'organisations requises pour atteindre au mieux l'objectif global :

i) \textbf{Predator-Prey} : Plusieurs prédateurs coopèrent pour capturer une proie, testant la coordination pour atteindre un objectif collectif~\cite{lowe2017multi}
; \quad
ii) \textbf{Overcooked-AI} : Jeu de cuisine en équipe où les agents préparent et servent des plats dans des cuisines de complexité croissante~\cite{overcookedai}. Cet environnement évalue la coordination et l'allocation des tâches avec des rôles clairs (chef, assistant, serveur)
; \quad
iii) \textbf{Warehouse Management} : Les agents gèrent un entrepôt en coordonnant les livraisons vers des points de demande, influençant leur spécialisation (transport, gestion des stocks)
; \quad
iv) \textbf{Cyber-Defense Simulation} : Simulation de défense d'un réseau contre des cyberattaques. Les agents identifient et contrent les menaces tout en respectant des règles de sécurité strictes, testant ainsi leur sûreté~\cite{Maxwell2021}.

Ces environnements, encapsulables via l'API PettingZoo, s'intègrent avec notre implémentation de MOISE+MARL et facilitent l'application des spécifications organisationnelles.

\subsection{Algorithmes MARL utilisés}

Nous avons évalué notre cadre avec plusieurs algorithmes MARL :
i) \textbf{MADDPG (\textit{Multi-Agent Deep Deterministic Policy Gradient})}~\cite{lowe2017multi} : Un algorithme d'apprentissage centralisé avec exécution décentralisée, permettant à chaque agent d'avoir une politique déterministe tout en utilisant l'information globale lors de l'entraînement
; \quad
ii) \textbf{MAPPO (\textit{Multi-Agent Proximal Policy Optimization})}~\cite{yu2021mappo} : Une version adaptée de PPO pour les systèmes multi-agent, optimisée pour une convergence stable de la politique conjointe dans des scénarios complexes
; \quad
iii) \textbf{Q-Mix}~\cite{rashid2018qmix} : Un algorithme basé sur les valeurs Q qui apprend à combiner les Q-valeurs individuelles des agents en une valeur conjointe afin d'optimiser la coopération
; \quad
iv) \textbf{COMA (\textit{Counterfactual Multi-Agent})}~\cite{foerster2018counterfactual} : Un algorithme acteur-critique capable d'estimer l'impact des actions d'un agent individuel sur la récompense globale de l'équipe.

\subsection{Spécifications organisationnelles}

Pour chaque environnement, nous avons défini un ensemble de spécifications organisationnelles. Ces spécifications comprennent les rôles, les missions, ainsi que les permissions et obligations. Voici une description informelle de ces spécifications~\hyperref[fn:github]{\footnotemark[1]} :
%
i) \textbf{Predator-Prey} : Des rôles de prédateur et de proie sont définis, chaque prédateur ayant des objectifs spécifiques tels que \textquote{capturer la proie} ou \textquote{bloquer les voies d'évasion}
; \quad
ii) \textbf{Overcooked-AI} : Les agents adoptent trois rôles principaux : chef, assistant et serveur. Le chef est responsable de la cuisson et de l'assemblage des plats, l'assistant s'occupe de la découpe et de l'approvisionnement en ingrédients, et le serveur se charge de la livraison des plats aux clients. Les missions consistent principalement à préparer et à servir un nombre déterminé de plats dans un délai imparti. Cet environnement est illustré dans \autoref{fig:overcooked}.
; \quad
iii) \textbf{Warehouse Management} : Les agents adoptent des rôles tels que \textquote{transporteur} et \textquote{gestionnaire d'inventaire}, avec des missions liées à la gestion des flux logistiques et à l'optimisation des livraisons
; \quad
iv) \textbf{Cyber-Defense Simulation} : Les agents occupent des rôles de défenseurs de réseau, avec des obligations telles que la détection d'intrusions, la levée d'alertes aux autres agents pour proteger l'essaim de drones.

\begin{figure}[h!]
    \centering
    \includegraphics[trim=0cm -0.5cm 0cm -0.5cm, clip, width=0.9\linewidth]{figures/overcooked.png}
    \caption[Capture d'écran de l'environnement Overcooked-AI]{Capture d'écran de l'environnement Overcooked-AI : deux agents (chefs cuisiniers) doivent collaborer pour préparer et servir efficacement des soupes à l'oignon. Le processus consiste à prélever trois oignons (un à la fois) dans le distributeur, à les placer dans une marmite, à attendre que la soupe cuise, à récupérer un plat propre, à dresser la soupe et à la livrer au comptoir de service. La disposition de la cuisine comprend des obstacles et des passages étroits, ce qui oblige les agents à coordonner leurs mouvements pour éviter les collisions et optimiser l'accomplissement des tâches.}
    \label{fig:overcooked}
\end{figure}


\subsection{Configuration matérielle}

Toutes les expériences ont été menées sur un cluster académique haute performance, avec des nœuds GPU (NVIDIA A100, V100 et AMD MI210). Chaque configuration algorithme-environnement a été exécutée 5 fois en parallèle pour assurer des résultats fiables.
Les hyperparamètres~\hyperref[fn:github]{\footnotemark[1]} (taux d'apprentissage, facteurs d'actualisation, taux d'exploration) proviennent soit des banques de MARLlib, soit d’une recherche sur grille réalisée via \textit{Optuna}~\cite{akiba2019optuna}.

\subsection{Métriques d'évaluation et protocole}

L'évaluation prend en compte l'efficacité des politiques et l'impact des spécifications organisationnelles en reposant sur les métriques suivantes :
%
i) \textbf{Récompense Cumulative} : Mesure l'efficacité de la politique dans l'atteinte des objectifs de l'environnement
; \quad
ii) \textbf{Écart-type de la Récompense} : Reflète la stabilité des politiques apprises au cours des épisodes
; \quad
iii) \textbf{Taux de Convergence} : Indique la rapidité avec laquelle les politiques atteignent une performance stable
; \quad
iv) \textbf{Taux de Violation des Contraintes} : Évalue le respect des contraintes organisationnelles par la politique, ce qui est crucial pour la sécurité
; \quad
v) \textbf{Score de Cohérence} : Mesure l'alignement entre les comportements appris et les spécifications organisationnelles
; \quad
vi) \textbf{Score de Robustesse} : Évalue la capacité des agents à maintenir leur performance face à une série de scénarios difficiles
; \quad
vii) \textbf{Niveau d'Adéquation Organisationnelle} : Quantifie l'adéquation organisationnelle avec TEMM.

Notre protocole compare le \textit{Baseline de Référence} (RB) sans contraintes organisationnelles au \textit{Baseline Organisé} (OB) utilisant MOISE+MARL.
Pour le RB, nous utilisons MMA pour entraîner les agents dans chaque environnement (jusqu'à convergence ou limite d'épisodes) sans appliquer de spécifications organisationnelles, puis nous sélectionnons l'algorithme obtenant la Récompense Cumulative maximale.
Pour l'OB, nous réinitialisons environnements et agents, appliquons via MMA des spécifications prédéfinies (chaque agent se voit attribuer un rôle) et ré-entraînons ces agents avec l'algorithme le plus performant du RB. Les métriques permettent alors des comparaisons.
%
Nous évaluons l'impact de MOISE+MARL en vérifiant si les comportements des agents s'alignent avec les rôles définis (à l'aide de l'Écart-type de Récompense, du Taux de Convergence et du Score de Robustesse). Une différence significative du Niveau d'Adéquation Organisationnelle entre RB et OB, et une corrélation entre les rôles et ce niveau, confirmera l'efficacité du cadre.
Enfin, nous comparons MOISE+MARL à AGR+MARL (qui ne considère que les rôles) pour évaluer l'importance des missions.

\section{Résultats}
\label{sec:results}

Cette section discute des résultats obtenus sur les quatre environnements.

\begin{table*}[h!]
    \centering
    \caption{Résultats détaillés pour chaque environnement et algorithme favorisé, pour le RB et l'OB.}
    \label{tab:detailed_results}
    \footnotesize
    \renewcommand{\arraystretch}{1.2}
    \begin{tabular}{p{1.8cm}p{1.1cm}p{.5cm}p{0.7cm}p{0.7cm}p{0.7cm}p{0.7cm}p{0.7cm}p{0.7cm}p{0.7cm}}
        \hline
        \textbf{Env.} & \textbf{Alg.} & \textbf{Spec. Org.} & \textbf{Récom. Cum.} & \textbf{Écart-Type} & \textbf{Taux Conv.} & \textbf{Taux Viol.} & \textbf{Score Cohé.} & \textbf{Score Rob.} & \textbf{Niv. Adéq.} \\ \hline
        Predator-Prey & MADDPG        &                     & 200.1                & 21.5                & 0.65                & 12.3\%              & -                    & 0.65                & 0.43                \\
        Predator-Prey & MADDPG        & Oui                 & 245.8                & 15.2                & 0.85                & 0.0\%               & 0.81                 & 0.83                & 0.87                \\
        Overcooked-AI & MAPPO         &                     & 348.2                & 15.6                & 0.75                & 7.1\%               & -                    & 0.71                & 0.48                \\
        Overcooked-AI & MAPPO         & Oui                 & 391.2                & 10.4                & 0.92                & 0.0\%               & 0.89                 & 0.89                & 0.91                \\
        Warehouse M.  & Q-Mix         &                     & 257.4                & 18.9                & 0.74                & 7.8\%               & -                    & 0.68                & 0.50                \\
        Warehouse M.  & Q-Mix         & Oui                 & 307.1                & 13.8                & 0.88                & 0.0\%               & 0.88                 & 0.86                & 0.90                \\
        Cyber-Defense & COMA          &                     & 162.4                & 17.3                & 0.70                & 12.2\%              & -                    & 0.67                & 0.45                \\
        Cyber-Defense & COMA          & Oui                 & 188.9                & 11.2                & 0.86                & 0.0\%               & 0.76                 & 0.80                & 0.83                \\ \hline
    \end{tabular}
\end{table*}

\subsection{Adéquation organisationnelle quantitative et cohérence}

\noindent
Comme l'illustre le \autoref{tab:detailed_results}, l'adéquation organisationnelle est systématiquement plus élevée dans l'OB, confirmant que MOISE+MARL aligne efficacement le comportement des agents sur les spécifications organisationnelles.
Par exemple, dans \textbf{Predator-Prey} avec \textbf{MADDPG}, l'OB atteint un niveau d'adéquation de 0.87 (soit +49\% par rapport aux 0.43 du RB), tandis que dans \textbf{Overcooked-AI} avec \textbf{MAPPO}, on observe 0.91 (+89\%). Même constat pour \textbf{Warehouse Management} avec \textbf{Q-Mix}, où l'adéquation passe de 0.50 (RB) à 0.90 (OB).

\medskip
\noindent
De façon générale, contraindre les agents par des spécifications organisationnelles diminue la déviation de récompense et accélère la convergence, indiquant un impact notable sur leur comportement. Nous avons observé manuellement, notamment dans \textbf{Predator-Prey}, que les politiques entraînées correspondent bien à une organisation structurelle et fonctionnelle implicite.

\medskip
\noindent
Le \textbf{score de cohérence} demeure également élevé (jusqu’à 0.76 dans le contexte bruité de \textbf{Cyber-Defense}), montrant que, malgré les perturbations, les spécifications organisationnelles inférées sont proches de celles appliquées.

\subsection{Element d'explicabilités : exemple de Overcooked-AI}

Nous avons appliqué MMA ainsi que la méthode TEMM pour générer une quinzaine de trajectoires d'agents entraînés avec \textbf{MAPPO} dans \textbf{Overcooked-AI}, selon les spécifications organisationnelles suivantes pour les deux agents cuisiniers :
%
\begin{itemize}
    \item Rôle "Polyvalent" : "si l'agent a un bol et voit un pot plein dans une case adjacente, il doit interagir avec le pot pour récupérer la soupe" et "si l'agent a une soupe et voit le comptoir de service dans une case adjacente, il doit interagir avec le comptoir pour livrer la soupe"
    \item Objectif "Tenir bol de soupe" : "tient un bol de soupe"
\end{itemize}

\noindent
Après application de TEMM, nous avons obtenu un score d'adéquation organisationnel de 0.87, indiquant des comportements d'agents entrainés assez réguliers même en dehors des comportements contraints. TEMM permet d'inferrer de nouvelles règles et observations sous forme vectorielle. Après analyse, ces règles peuvent être retranscrites en langage naturel et confirment que les agents ont complété les règles initiales par d'autres qui semblent les amener à atteindre l'objectif fixé. Par exemple~:
%
\begin{itemize}
    \item Règles RAG : "si l'agent n'a pas de bol et voit un bol vide dans une case adjacente, il doit interagir avec le bol pour le ramasser" et "si l'agent n'a pas d'oignon et voit un oignon dans une case adjacente, il doit interagir avec l'oignon pour le ramasser"
    \item Observations GRG : "Cuisson en cours" : "voit un pot en train de cuire"
\end{itemize}

Pour obtenir une meilleure représentation des trajectoires et des centroïdes, notre implémentation de TEMM génère également des figures telles que des dendrogrammes ou des visualisations en deux dimensions via une ACP. Dans ces deux visualisations, on remarque la similarité des comportements entre les deux agents, ce qui est cohérent avec le fait qu'ils partagent le même rôle et objectif. Par ailleurs, bien que l'entraînement aurait pu conduire à des comportements divergents, on observe tout de même une conservation du comportement attendu, probablement due à la symétrie spatiale de l'environnement utilisé. Cela se traduit par deux clusters (un pour chaque agent) qui, bien que distincts, sont regroupés dans le même macro-cluster (\autoref{fig:overcooked_dendrogram}) représentant le rôle "Polyvalent" enrichi des règles post-entraînement. Ce phénomène peut également être observé dans la visualisation en deux dimensions (\autoref{fig:overcooked_pca}), où les trajectoires des deux agents sont en fait le symétrique l'une de l'autre, ce qui est cohérent avec la nature symétrique de l'environnement spatial d'Overcooked-AI.

\begin{figure}[h!]
    \centering
    \includegraphics[trim=0cm 1cm 0cm 0.9cm, clip, width=0.8\linewidth]{figures/overcooked_figures/full_dendrogram.pdf}
    \caption{Dendrogramme des trajectoires de transition dans Overcooked-AI}
    \label{fig:overcooked_dendrogram}
\end{figure}

\begin{figure}[h!]
    \centering
    \includegraphics[trim=0cm 1cm 0cm 0.9cm, clip, width=0.8\linewidth]{figures/overcooked_figures/transition_pca.pdf}
    \caption{PCA des trajectoires de transition dans Overcooked-AI}
    \label{fig:overcooked_pca}
\end{figure}


\subsection{Performance et stabilité selon les algorithmes}

Les résultats indiquent que les algorithmes \textit{policy-based} et les algorithmes \textit{actor-critic}, tels que \textbf{MADDPG} et \textbf{MAPPO}, bénéficient considérablement du cadre MOISE+MARL, notamment en termes de cohérence et de stabilité. Par exemple, dans l'environnement \textbf{Overcooked-AI}, \textbf{MAPPO} a vu son écart-type de récompense passer de 15.6 (RB) à 10.4 (OB), reflétant une politique plus stable avec moins de fluctuations comportementales. De même, \textbf{MADDPG} dans \textbf{Predator-Prey} a montré une diminution similaire, passant de 21.5 en RB à 15.2 en OB, indiquant une fiabilité accrue.

En revanche, les algorithmes basés sur la valeur, comme \textbf{Q-Mix}, ont maintenu une haute performance en récompense cumulative, mais ont affiché une variabilité plus importante en termes de cohérence. Par exemple, dans l'environnement \textbf{Warehouse Management}, \textbf{Q-Mix} a atteint un écart-type de récompense de 13.8 en OB, soit une amélioration notable par rapport aux 18.9 en RB, mais toujours supérieur à la stabilité observée dans les algorithmes basés sur la politique. Cela suggère que, bien que \textbf{Q-Mix} soit efficace pour atteindre les objectifs de la tâche, il pourrait nécessiter un ajustement supplémentaire pour les rôles avec MOISE+MARL afin d'améliorer la cohérence.

\subsection{Impact des contraintes organisationnelles sur la convergence, la robustesse et le taux de violation des politiques}

L'application des contraintes organisationnelles a permis d'accélérer les taux de convergence dans tous les environnements. Dans l'environnement \textbf{Cyber-Defense}, \textbf{COMA} avec MOISE+MARL a convergé à un taux de 0.86, contre 0.70 en RB. Des tendances similaires ont été observées dans l'environnement \textbf{Warehouse Management} avec \textbf{Q-Mix}, qui est passé de 0.74 en RB à 0.88 en OB. Cette convergence accélérée est attribuée à un espace de recherche plus contraint par les rôles et objectifs.

En outre, nous avons observé que les taux de violation des contraintes étaient systématiquement plus élevés lorsque les contraintes organisationnelles étaient définies avec une rigidité de contrainte plus faible. Dans l'environnement \textbf{Overcooked-AI}, \textbf{MAPPO} a enregistré un taux de violation nul avec une rigidité de contrainte de 1, contre 7.1\% avec une rigidité de 0. De même, dans \textbf{Warehouse Management}, \textbf{Q-Mix} a effectivement vu le taux de violation passer de 7.8\% à zéro quand la rigidité est à 1.

De plus, nous avons observé une amélioration constante de la robustesse lorsque les spécifications organisationnelles étaient appliquées aux agents. Par exemple, \textbf{MADDPG} dans \textbf{Predator-Prey} et \textbf{MAPPO} dans \textbf{Overcooked-AI} ont obtenu des scores de cohérence élevés, respectivement 0.81 et 0.89, indiquant que les agents suivaient de près les rôles inférés. La robustesse s'est également améliorée, avec \textbf{MAPPO} dans \textbf{Overcooked-AI} atteignant un score de robustesse de 0.89, contre 0.71 en RB, soulignant une meilleure résilience face aux perturbations.

Cependant, un biais potentiel peut être souligné : les spécifications organisationnelles ont été conçues pour englober toutes les observations, évitant ainsi les situations nouvelles non gérées.

\subsection{Comparaison entre MOISE+MARL et AGR+MARL}

\begin{table}[h!]
    \centering
    \caption{Comparaison de la performance entre MOISE+MARL et AGR+MARL.}
    \label{tab:ablation_study}
    \footnotesize
    \renewcommand{\arraystretch}{1.1}
    \begin{tabular}{p{2.1cm}p{0.5cm}p{0.7cm}p{0.7cm}p{0.6cm}p{0.9cm}}
        \hline
        \textbf{Framework} & \textbf{Env.} & \textbf{Taux Conv.} & \textbf{Score Rob.} & \textbf{Niv. Adéq.} & \textbf{Récom. Cum.} \\ \hline
        MOISE+MARL         & PP            & 0.85                & 0.83                & 0.87                & 245.8                \\
        AGR+MARL           & PP            & 0.75                & 0.69                & 0.56                & 208.4                \\
        MOISE+MARL         & OA            & 0.92                & 0.89                & 0.91                & 391.2                \\
        AGR+MARL           & OA            & 0.82                & 0.75                & 0.58                & 348.9                \\
        MOISE+MARL         & WM            & 0.88                & 0.86                & 0.90                & 307.1                \\
        AGR+MARL           & WM            & 0.76                & 0.72                & 0.61                & 278.6                \\ \hline
    \end{tabular}
\end{table}

\paragraph{Impact des objectifs intermédiaires}
Le \autoref{tab:ablation_study} met en lumière l’effet de ces objectifs dans MOISE+MARL. Dans \textbf{Overcooked-AI}, \textbf{MAPPO} obtient une récompense cumulative de 391.2 et une adéquation organisationnelle de 0.91, soit 33\% de plus qu’AGR+MARL (0.58). Dans \textbf{Warehouse Management}, \textbf{Q-Mix} sous MOISE+MARL atteint 307.1 de récompense (contre 278.6 pour AGR+MARL) et un score de robustesse supérieur (0.86 vs 0.72).

Ces résultats soulignent l’importance des objectifs intermédiaires pour des comportements plus stables et mieux orientés vers l’objectif. MOISE+MARL surpasse ainsi AGR+MARL en récompense, robustesse et adéquation organisationnelle dans \textbf{Predator-Prey}, \textbf{Warehouse Management} et \textbf{Overcooked-AI}.
%
Enfin, l’augmentation du nombre de contraintes organisationnelles accroît quasi linéairement la durée d’entraînement, d’après nos premiers résultats~\hyperref[fn:github]{\footnotemark[1]}.

\section{Conclusion et travaux futurs}
\label{sec:discussion_conclusion_future_work}

Nous avons introduit le cadre MOISE+MARL, qui couple un modèle organisationnel explicite au MARL afin d'améliorer le contrôle et l'explicabilité des politiques multi-agent. Les expérimentations démontrent une convergence plus rapide, une stabilité accrue et une cohérence renforcée entre les comportements appris et les spécifications organisationnelles, qu'elles soient définies ou inférées.

Deux axes de recherche se dessinent pour la suite : d'une part, l'exploration de nouvelles approches visant à automatiser davantage la méthode TEMM, notamment par l'intégration de modèles de langage de grande taille (\textit{Large Language Models}), et d'autre part, l'amélioration de la scalabilité de TEMM ainsi que le développement de solutions pour réduire le surcoût computationnel. Ces perspectives devraient permettre une meilleure intégration de l'organisation dans le MARL, renforçant la robustesse, la sûreté et l'explicabilité des agents dans des systèmes réels.

\newpage
%\nocite{*}
\bibliography{references}

\bigskip

% Pour afficher les r{\'e}sum{\'e}s anglais et fran{\c c}ais
\setotherabstracts

\end{document}




