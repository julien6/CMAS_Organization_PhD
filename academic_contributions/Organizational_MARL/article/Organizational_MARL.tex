\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage[inline, shortlabels]{enumitem}
\usepackage{tabularx}
\usepackage{caption}
\usepackage{titlesec}
\usepackage[T2A,T1]{fontenc}
\usepackage[english]{babel}
\captionsetup{font=it}
\usepackage{ragged2e}
\usepackage{hyperref}
\usepackage{pifont}
\usepackage{footmisc}
\usepackage{multirow}

% --- Tickz
\usepackage{physics}
\usepackage{amsmath}
\usepackage{tikz}
\usepackage{mathdots}
\usepackage{yhmath}
\usepackage{cancel}
\usepackage{color}
\usepackage{siunitx}
\usepackage{array}
\usepackage{multirow}
\usepackage{amssymb}
\usepackage{gensymb}
\usepackage{tabularx}
\usepackage{extarrows}
\usepackage{booktabs}
\usetikzlibrary{fadings}
\usetikzlibrary{patterns}
\usetikzlibrary{shadows.blur}
\usetikzlibrary{shapes}

% ---------

\usepackage{pdfpages}
\usepackage{booktabs}
\usepackage{csquotes}
\usepackage{lipsum}  
\usepackage{arydshln}
\usepackage{smartdiagram}
\usepackage[inkscapeformat=png]{svg}
\usepackage{textcomp}
\usepackage{tabularray}\UseTblrLibrary{varwidth}
\usepackage{xcolor}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\usepackage{cite}
\usepackage{amsmath}
\newcommand{\probP}{\text{I\kern-0.15em P}}
\usepackage{etoolbox}
\patchcmd{\thebibliography}{\section*{\refname}}{}{}{}

\setlength{\extrarowheight}{2.5pt}

% \renewcommand{\arraystretch}{1.7}

% \setlength{\extrarowheight}{2.5pt}
% \renewcommand{\arraystretch}{0.2}
% \renewcommand{\arraystretch}{1.7}

% --------------
\titleclass{\subsubsubsection}{straight}[\subsection]

\newcounter{subsubsubsection}[subsubsection]
\renewcommand\thesubsubsubsection{\thesubsubsection.\arabic{subsubsubsection}}
\renewcommand\theparagraph{\thesubsubsubsection.\arabic{paragraph}} % optional; useful if paragraphs are to be numbered

\titleformat{\subsubsubsection}
  {\normalfont\normalsize\bfseries}{\thesubsubsubsection}{1em}{}
\titlespacing*{\subsubsubsection}
{0pt}{3.25ex plus 1ex minus .2ex}{1.5ex plus .2ex}

\makeatletter
\renewcommand\paragraph{\@startsection{paragraph}{5}{\z@}%
  {3.25ex \@plus1ex \@minus.2ex}%
  {-1em}%
  {\normalfont\normalsize\bfseries}}
\renewcommand\subparagraph{\@startsection{subparagraph}{6}{\parindent}%
  {3.25ex \@plus1ex \@minus .2ex}%
  {-1em}%
  {\normalfont\normalsize\bfseries}}
\def\toclevel@subsubsubsection{4}
\def\toclevel@paragraph{5}
\def\toclevel@paragraph{6}
\def\l@subsubsubsection{\@dottedtocline{4}{7em}{4em}}
\def\l@paragraph{\@dottedtocline{5}{10em}{5em}}
\def\l@subparagraph{\@dottedtocline{6}{14em}{6em}}
\makeatother

\setcounter{secnumdepth}{4}
\setcounter{tocdepth}{4}
% --------------


\newcommand{\before}[1]{\textcolor{red}{#1}}
\newcommand{\after}[1]{\textcolor{green}{#1}}

\newcommand{\old}[1]{\textcolor{orange}{#1}}
\newcommand{\rem}[1]{\textcolor{red}{#1}}
\newcommand{\todo}[1]{\textcolor{orange}{\newline \textit{\textbf{TODO:} #1}} \newline \newline }

\makeatletter
\newcommand{\linebreakand}{%
  \end{@IEEEauthorhalign}
  \hfill\mbox{}\par
  \mbox{}\hfill\begin{@IEEEauthorhalign}
}
\makeatother




% ---------------------------


\begin{document}

\title{A Step Towards Automated Design of Multi-Agent System Organization using Reinfocement Learning\\
    % {\footnotesize \textsuperscript{Note}}
    % \thanks{Identify applicable funding agency here. If none, delete this.}
}

% \IEEEaftertitletext{\vspace{-1\baselineskip}}

\author{

    \IEEEauthorblockN{Julien Soulé}
    \IEEEauthorblockA{\textit{Thales Land and Air Systems, BU IAS}}
    %Rennes, France \\
    \IEEEauthorblockA{\textit{Univ. Grenoble Alpes,} \\
        \textit{Grenoble INP, LCIS, 26000,}\\
        Valence, France \\
        julien.soule@lcis.grenoble-inp.fr}

    \and

    \IEEEauthorblockN{Jean-Paul Jamont\IEEEauthorrefmark{1}, Michel Occello\IEEEauthorrefmark{2}}
    \IEEEauthorblockA{\textit{Univ. Grenoble Alpes,} \\
        \textit{Grenoble INP, LCIS, 26000,}\\
        Valence, France \\
        \{\IEEEauthorrefmark{1}jean-paul.jamont,\IEEEauthorrefmark{2}michel.occello\}@lcis.grenoble-inp.fr
    }

    % \and

    % \IEEEauthorblockN{Michel Occello}
    % \IEEEauthorblockA{\textit{Univ. Grenoble Alpes,} \\
    % \textit{Grenoble INP, LCIS, 26000,}\\
    % Valence, France \\
    % michel.occello@lcis.grenoble-inp.fr}

    % \and

    \linebreakand

    \hspace{-0.5cm}
    \IEEEauthorblockN{Paul Théron}
    \IEEEauthorblockA{
        \hspace{-0.5cm}
        \textit{AICA IWG} \\
        \hspace{-0.5cm}
        La Guillermie, France \\
        \hspace{-0.5cm}
        %lieu-dit Le Bourg, France \\
        paul.theron@orange.fr}

    \and

    \hspace{0.5cm}
    \IEEEauthorblockN{Louis-Marie Traonouez}
    \IEEEauthorblockA{
        \hspace{0.5cm}
        \textit{Thales Land and Air Systems, BU IAS} \\
        \hspace{0.5cm}
        Rennes, France \\
        \hspace{0.5cm}
        louis-marie.traonouez@thalesgroup.com}}


\maketitle


\begin{abstract}

    % context
    Multi-Agent Reinforcement Learning has been successfully applied in various contexts to generate policies allowing agents to collaboratively achieve a goal in a non-fully observable environment. These policies are approximated functions that map observation to action allowing agents are to maximize the cumulative reward over an episode.
    % problem
    Yet, from a designer point of view, those trained policies raise explainability and safety issues because they do not give human-understandable or exploitable specifications of their organizational aspects such as individual, social or collective levels.
    % This particularly concerns black box models in deep-learning or random forest.
    % hypothesis / contribution
    This paper aims to define the organizational explainability problem formally. Then, we expose our approach to extract the organization specifications out of trained agents policies.
    % results
    We applied our approach in three manageable cooperative Atari games likely to have emergent organization among agents. Resulting organization specifications are indeed consistent with human expectations.

\end{abstract}

\begin{IEEEkeywords}
    multi-agent systems, reinforcement learning, organization, design
\end{IEEEkeywords}

\section{Introduction}

% Context
In Multi-Agent Systems (MASs), organization is a fundamental concept that impact how agents are coordinating their activities to collaboratively achieve a common goal\cite{Hubner2002}. In essence, we assume the entity of the organization (we simply call \textbf{organization}) always exists through the running agents interactions even though it may be implicit.
An \textbf{organizational model} specifies (at least partially) the organization whether it is used as medium to describe an explicit known organization in a top-down way, or describing an implicit organization in a bottom-up way. Examples of organizational models are the Agent/Group/Role (AGR) model\cite{Ferber2004} or more complex ones such as $\mathcal{M}oise^{+}$\cite{Hubner2002}. Organizational models can take into account aspects such as structural coordination, dynamic interactions, and the achievement of common objectives\cite{Ferber2004, Abbas2015}. We call the \textbf{specifications} of an organization, the set of components used in an instance of an organizational model to specify the organization.

We assume an organization in a MAS can be understood regarding the Agent Centered Point of View (ACPV) vs. Organization Centered Point of View (OCPV) and agent's organization awareness vs. unawareness\cite{Picard2009}.
Typical examples are emergent MAS (ACPV and organization unawareness), coalition based MAS (ACPV and organization awareness), organization based MAS (OCPV and organization awareness), and Agent oriented engineering (OCPV and organization unawareness)\cite{Picard2009}.
We assume an \textbf{architecture} (also called organizational paradigm) is an abstract organization gathering a range of organizations sharing common characteristics\cite{Horling2004}. Finally, MAS designing/development methods, have been proposed jointly with organizational models to help designers finding suited specifications of an organization so a MAS can reach a goal efficiently in a environment such as GAIA\cite{Wooldridge2000}, ADELFE\cite{Bernon2003} or DIAMOND\cite{Jamont2005}.

In most \textbf{self/re-organization} mechanisms agents' policies are defined and fixed by the designer from ACPV/OCPV so that an optional emerging/chosen organization allows reaching a global goal\cite{Picard2009}. We can envision Multi-Agent Reinforcement Learning (MARL) as a particular ACPV mechanism that aims to replace the designer by simultaneously making emerge agents' policies (micro-level) and consequently the emerging organization (macro-level) relying on quantitative feedbacks. In literature, that mechanism is mostly considered to satisfy the need that agents reach efficiently a specific goal with few other considerations. Typical examples include agents' policies modeled as neural networks that are updated using various algorithms such as Deep Q-Network or REINFORCE. In such examples, an emerging implicit organization among agents can converge.

In some environments, such as computers network with highly complex and non-visual interactions, the lack of intuitive comprehension of the environment can make MAS methods difficult to apply to develop a MAS whose organization optimally reach a goal. In such cases, the use of MARL could allow to have sufficiently and non over-fitted trained agents optionally respecting additional arbitrary designer's constraints (coming from a architecture for instance). We think an observer/designer could understand, interpret, and produce the specifications of valuable organizations by translating them into organizational models. For instance determining the individual, social, collective levels described in $\mathcal{M}oise^{+}$\cite{Hubner2002}. At least it may give relevant insights for guiding the design process.

% Problem
The idea of benefitting of the particularly adaptive and general MARL mechanism to get an approximated suited MAS organization and producing associated specifications, requires to link the MARL training of a set of policies in a bidirectional way with a MAS organizational model. For instance, a hierarchy described in a MAS model would constrain the possible policies to get ultimately trained in MARL. Reversely, a set of trained policies could be described in an organizational model, thus indicating resemblance with known MAS organization architectures.

% Contribution/Hypothesis
This paper first aims to formalize the aforementioned idea through a formal model. That formal model aims to unify the concepts and links between agents' policies, their training with MARL, architecture with the $\mathcal{M}oise^{+}$ organizational model. Relying on that formal model, we expose our approach to generate an efficient organization and associated OCPV specifications based on the environment, the initial agents' policies models, the design constraints, and the global goal.

% Results
We applied our approach to three simple Atari games involving several agents that must converge to a specific organization to achieve a goal efficiently. Obtained organization specifications are exploitable and coherent with human expectations.

The remainder of the article is built as following.
In section II, we give an overview of related works to our intended contribution as for MARL/RL, specifications, MAS organizational models and mechanisms. It shows the originality of our idea.
In section III, we introduce a formal model to properly formalize our idea by unifying MAS and MARL related concepts.
In section IV, we present our approach to help designers in designing a suited organization regarding environment, goal, agents' policy model and design constraints.
In section V, we discuss results obtained after applying our approach in three simple cooperative Atari games.
In section VI, we conclude on the viability and relevance of our model and approach and we highlight limitations to overcome and future works as well.

\section{Related works}

\begin{itemize}
    \item Rule extraction from trained neural networks: involves obtaining human-interpretable rules that approximate the policy of the neural network. Various algorithms have been developed for this task, including decompositional, pedagogical, and eclectics approaches. These algorithms aim to provide comprehensible descriptions of the network's hypothesis that closely approximate its policy. For example, NN2Rules is a decompositional approach that extracts a set of decision rules from the parameters of the trained neural network model, making the decision rules more interpretable. Rule extraction algorithms enable neural networks to justify their classification responses using explainable classification rules, enhancing the transparency and interpretability of the models\cite{Hailesilassie2016}\cite{Sato2001}\cite{Lal2022}.
    \item Specification-Guided Reinforcement Learning (SGRL): addresses the problem of generating an optimal policy in reinforcement learning (RL) with respect to a given task in an unknown environment. Traditionally, the task is encoded in the form of a reward function, which can be cumbersome for long-horizon goals. An alternative approach is to use logical specifications, such as Linear Temporal Logic (LTL) and SpectRL, to define the task, opening the direction of RL from logical specifications. SGRL aims to synthesize control policies for robotic systems and other autonomous agents by leveraging formal logical constructs to express the task or objective. This approach has led to the development of highly performant algorithms that enable RL from logical specifications, enhancing the transparency and trustworthiness of RL systems\cite{Bansal2022}\cite{Jothimurugan2023}.
    \item Compositional Reinforcement Learning: involves the development of a compositional learning approach, called DiRL, that leverages the specification to decompose the task into high-level planning and reinforcement learning\cite{Bansal2022}\cite{Jothimurugan2021}.
    \item Learning from Logical Specifications: covers the broader area of learning from logical specifications, including the development of reinforcement learning algorithms that leverage the compositional structure of the specification to learn control policies for complex tasks\cite{Jothimurugan2021}.
    \item Reward Generation and Reinforcement Learning: focuses on reward generation and reinforcement learning from logical specifications is also related, focusing on the generation of reward machines from temporal specifications and the application of reinforcement learning algorithms for language-based specifications\cite{Jothimurugan2021}.
\end{itemize}


\section{A formal model for unifying MAS and MARL concepts}

The proposed model aims to integrate MAS organization designing process by translating the $\mathcal{M}oise^{+}$ organizational model within the formalism used for MARL. Then, we formally describe how agents' policies and training process can be linked to the $\mathcal{M}oise^{+}$ organizational model.

The chosen formalism is based on the Decentralized Partially Observable Markov Decision Process (Dec-POMDP)~\cite{Oliehoek2016} because it considers multiple agents in a similar MAS fashion. It relies on stochastic processes to model uncertainty of the environment for the changes induced by actions, in received observations, in communication\dots Additionally, unlike Partially Observable Stochastic Games (POSG), the reward function can be common to agents which fosters training for collaborative oriented actions~\cite{Beynier2013}.
A Dec-POMDP is a 7-tuple $(S,\{A_i\},T,R,\{\Omega_i\},O,\gamma)$, where:
\begin{itemize}
    \item $S = \{s_1, ..s_{|S|}\}$: The set of the possible states.
    \item $A_{i} = \{a_{1}^{i},..,a_{|A_{i}|}^{i}\}$: The set of the possible actions for agent $i$.
    \item $T$ so that $T(s,a,s') = \probP{(s'|s,a)}$ : The set of conditional transition probabilities between states
    \item $R: S \times A \times S \rightarrow \mathbb{R}$: The reward function
    \item $\Omega_{i} = \{o_{1}^{i},..,o_{|\Omega_{i}|}^{i}\}$: The set of observations for agent $ag_i$
    \item $O$ so that $O(s',a,o) = \probP{(o|s',a)}$ : The set of conditional observation probabilities.
    \item $\gamma \in [0,1]$, the discount factor
\end{itemize}
Considering $n$ agents, solving the Dec-POMDP consists in finding a joint policy $\pi = \{\pi_1,..,\pi_n\}$ that maximize the cumulative reward over a finite horizon ($\gamma < 1$), where $\pi_i: O_i \times \rightarrow A_i$ (with $\pi_i \in \Pi$, the set policies) is the policy (or policy) of an agent $i$ that deterministically maps an observation to an action\cite{Beynier2013}.

The $\mathcal{M}oise^{+}$ organizational model comprises the structural, functional and deontic specifications.

\begin{itemize}
    \item 
\end{itemize}

The $\mathcal{M}oise^{+}$ structural specification is built in three levels~\cite{Hubner2007}:
\begin{itemize}
    \item The \textbf{individual level} referring to the policies that an agent may have according to its role
    \item The \textbf{social level} referring to the acquaintance, communication and authority links between roles
    \item The \textbf{collective level} referring to the aggregation of roles in groups.
\end{itemize}

The functional specifications

\ 

We envision to integrate the $\mathcal{M}oise^{+}$ organizational model within the Dec-POMDP model as described below:

\begin{itemize}
    \item $Roles = {role_1,..,role_{|Roles|}}$: the set of roles with $role_i = {\pi_1,..,\pi_{|role_i|}}$ (envision organization in ACPV and unawareness)
    \item $SocialLinks$: the set of social links that combine at least two policy into another one (envision organization in OCPV with awareness)
    \item $CollectiveLinks$: the set of collective 
\end{itemize}

% TODO

Consequently, the idea of producing a suited MAS and associated organization specifications based on the environment, agents' policy models, goal and design constraints can be formally expressed as follow:

% TODO

$D: T \cross R \cross A \cross S_{parametrized} \rightarrow S \cross M_{trained} \cross S_{tuned}$


\subsection{Two levels of MAS engineering to compute from MARL trained MAS}

The engineering of a Multi-Agent System must take into account two levels:

\begin{itemize}
    \item Questions at the Multi-Agent System level (system-centric approach)
          \begin{itemize}
              \item Number of agents, what heterogeneity?
              \item What is the common medium (Environment) shared by the agents?
              \item What communication mechanisms are available to agents?
              \item What are the communication languages, ontologies, interaction protocols used by the agents?
              \item What is the organization within which the agents operate? How is it established?
              \item How do the agents coordinate their actions? How to ensure coherent operation?
          \end{itemize}

    \item Agent level questions (agent-centered approach)
          \begin{itemize}
              \item What does an agent represent? What actions should be encapsulated in an agent?
              \item How do agents represent the environment and organization in which they operate?
              \item How do agents handle interactions with other agents?
              \item What is the internal structure of the agents?
          \end{itemize}
\end{itemize}

\section{Approach for assisted/automated design of MAS organization}

\section{Application and discussion for cooperative Atari games}


\section{Conclusion}


\section*{References}

% \bibliographystyle{abbrv}
\bibliographystyle{IEEEtran}

\bibliography{references}

\end{document}
