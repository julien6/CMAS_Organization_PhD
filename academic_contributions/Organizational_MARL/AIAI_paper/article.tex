\documentclass[runningheads]{llncs}

\usepackage[T1]{fontenc}
\usepackage{graphicx}
%\usepackage{color}
%\renewcommand\UrlFont{\color{blue}\rmfamily}

\usepackage{amsmath,amssymb,amsfonts}
\usepackage[inline, shortlabels]{enumitem}
\usepackage{tabularx}
\usepackage{caption}
\usepackage{listings}
% \usepackage{titlesec}
\usepackage[english]{babel}
\captionsetup{font=it}
\usepackage{ragged2e}
\usepackage[hyphens]{url}
\usepackage{hyperref}
\usepackage{xurl}
\usepackage{pifont}
\usepackage{footmisc}
\usepackage{multirow}
\usepackage{enumitem}
\usepackage{algorithm2e}
\usepackage{float}
\usepackage{listings}
\usepackage{xcolor}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}
 
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}
 
\lstset{style=mystyle}

% --- Tickz
\usepackage{physics}
\usepackage{amsmath}
\usepackage{tikz}
\usepackage{mathdots}
\usepackage{yhmath}
\usepackage{cancel}
\usepackage{color}
\usepackage{siunitx}
\usepackage{array}
\usepackage{multirow}
\usepackage{amssymb}
\usepackage{gensymb}
\usepackage{tabularx}
\usepackage{extarrows}
\usepackage{booktabs}
\usetikzlibrary{fadings}
\usetikzlibrary{patterns}
\usetikzlibrary{shadows.blur}
\usetikzlibrary{shapes}

% ---------
% \usepackage{titlesec}
\usepackage{pdfpages}
\usepackage{booktabs}
\usepackage{csquotes}
\usepackage{lipsum}  
\usepackage{arydshln}
\usepackage{smartdiagram}
\usepackage[inkscapeformat=png]{svg}
\usepackage{textcomp}
\usepackage{tabularray}\UseTblrLibrary{varwidth}
\usepackage{xcolor}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\usepackage{cite}
\usepackage{amsmath}
\newcommand{\probP}{\text{I\kern-0.15em P}}
\usepackage{etoolbox}
\patchcmd{\thebibliography}{\section*{\refname}}{}{}{}

\setlength\tabcolsep{0.5pt}

% \renewcommand{\arraystretch}{1.7}

% \setlength{\extrarowheight}{2.5pt}
% \renewcommand{\arraystretch}{0.2}
% \renewcommand{\arraystretch}{1.7}

% --------------
% \titleclass{\subsubsubsection}{straight}[\subsection]

% \newcounter{subsubsubsection}[subsubsection]
% \renewcommand\thesubsubsubsection{\thesubsubsection.\arabic{subsubsubsection}}
% \renewcommand\theparagraph{\thesubsubsubsection.\arabic{paragraph}} % optional; useful if paragraphs are to be numbered

% \titleformat{\subsubsubsection}
%   {\normalfont\normalsize\bfseries}{\thesubsubsubsection}{1em}{}
% \titlespacing*{\subsubsubsection}
% {0pt}{3.25ex plus 1ex minus .2ex}{1.5ex plus .2ex}

% \makeatletter
% \renewcommand\paragraph{\@startsection{paragraph}{5}{\z@}%
%   {3.25ex \@plus1ex \@minus.2ex}%
%   {-1em}%
%   {\normalfont\normalsize\bfseries}}
% \renewcommand\subparagraph{\@startsection{subparagraph}{6}{\parindent}%
%   {3.25ex \@plus1ex \@minus .2ex}%
%   {-1em}%
%   {\normalfont\normalsize\bfseries}}
% \def\toclevel@subsubsubsection{4}
% \def\toclevel@paragraph{5}
% \def\toclevel@paragraph{6}
% \def\l@subsubsubsection{\@dottedtocline{4}{7em}{4em}}
% \def\l@paragraph{\@dottedtocline{5}{10em}{5em}}
% \def\l@subparagraph{\@dottedtocline{6}{14em}{6em}}
% \makeatother

% \setcounter{secnumdepth}{4}
% \setcounter{tocdepth}{4}
% --------------

\newcommand{\before}[1]{\textcolor{red}{#1}}
\newcommand{\after}[1]{\textcolor{green}{#1}}

\newcommand{\old}[1]{\textcolor{orange}{#1}}
\newcommand{\rem}[1]{\textcolor{red}{#1}}
\newcommand{\todo}[1]{\textcolor{orange}{\newline \textit{\textbf{TODO:} #1}} \newline \newline }



\newcounter{relation}
\setcounter{relation}{0}
\renewcommand{\therelation}{\arabic{relation}}
\newcommand{\relationautorefname}{Relation}

\newenvironment{relation}[1][]{%
    \refstepcounter{relation}%
    \noindent \raggedright \textit{\textbf{Relation. \therelation}} \hfill$}
{%
$ \hfill \phantom{x}

}

\newcounter{proof}
\setcounter{proof}{0}
\renewcommand{\theproof}{\arabic{proof}}
\newcommand{\proofautorefname}{Proof}

\renewenvironment{proof}[1][]{
    \refstepcounter{proof}
    \noindent \raggedright \textit{\textbf{Proof. \theproof}}

    \setlength{\leftskip}{1em}

}
{

\
\setlength{\leftskip}{0pt}
}

% --------------------------------
%             DOCUMENT
% --------------------------------

\begin{document}
\title{A MARL-based Approach for Easing MAS Organization Engineering}
%
%\titlerunning{Abbreviated paper title}
% If the paper title is too long for the running head, you can set
% an abbreviated paper title here
%
% \author{Julien Soulé\inst{1}\orcidID{0000-1111-2222-3333} \and
% Jean-Paul Jamont\inst{1}\orcidID{1111-2222-3333-4444} \and
% Michel Occello\inst{1}\orcidID{2222--3333-4444-5555} \and
% Louis-Marie Traonouez\inst{2}\orcidID{2222--3333-4444-5555} \and
% Paul Théron\inst{3}\orcidID{2222--3333-4444-5555}}
\author{Julien Soulé\inst{1} \and
Jean-Paul Jamont\inst{1} \and
Michel Occello\inst{1} \and
Louis-Marie Traonouez\inst{2} \and
Paul Théron\inst{3}}
%
\authorrunning{J. Soulé et al.}
% First names are abbreviated in the running head.
% If there are more than two authors, 'et al.' is used.
%
\institute{Univ. Grenoble Alpes, Grenoble INP, LCIS, 26000, Valence, France
    \email{\{julien.soule, jean-paul.jamont, michel.occello\}@lcis.grenoble-inp.fr}
    \and
    Thales Land and Air Systems, BU IAS, Rennes, France
    \email{louis-marie.traonouez@thalesgroup.com}
    \and
    AICA IWG, La Guillermie, France \\
    \email{paul.theron@orange.fr}
}


\maketitle              % typeset the header of the contribution

% MAS have been succefully
% For many MAS, the organization has become a critical success factor. 
% Several related methods exist to design MAS. 
% However, these methods are ...
% To enhance the quality and effectiveness of ... this paper presents an assisted approach for MAS Organization Engineering (AMOEA). 
% AMOEA guides the designer of...
% 1 phrase par contrib/key point

\begin{abstract}

    % context
    Multi-Agent Systems (MAS) have been successfully applied in industry for their ability to address complex, distributed problems, especially in IoT-based systems.
    Their efficiency in achieving given objectives and meeting design requirements is strongly dependent on the MAS organization during the engineering process of an application-specific MAS. To design a MAS that can achieve given goals, available methods rely on the designer's knowledge of the deployment environment.
    % Yet, in some cases, the deployment environment is not easily readable or handleable due to the complexity and may lead to unexpected emergent phenomena raising safety concerns.
    However, high complexity and low readability in some deployment environments make the application of these methods to be costly or raise safety concerns.
    % That stresses out the need for methodological works for assisted MAS design that could be addressed with collective AI techniques.
    % hypothesis / contribution
    In order to ease the MAS organization design regarding those concerns, we introduce an original Assisted MAS Organization Engineering Approach (AOMEA). AOMEA relies on combining a Multi-Agent Reinforcement Learning (MARL) process with an organizational model to suggest relevant organizational specifications to help in MAS engineering.
    % We introduce , an novel design approach to assist the MAS design whose underlying idea is to use Multi-Agent Reinforcement Learning with organizational specifications for both understanding and constraining the training process regarding design constraints.
    % results
    % We applied our approach in cooperative Atari games and a Cyberdefense drone swarm scenario of the 3rd CAGE Challenge. Obtained specifications are indeed consistent with design constraints and provide insights of relevant collective strategies that led to develop an explainable MAS with scores close to finalists' ones.

    \keywords{Multi-Agent Systems \and Design \and Assisted engineering}
\end{abstract}

\section{Introduction}

% Context:

MAS have drawn significant interest in the industrial field due to their ability to address complex, distributed problems~\cite{Raileanu2023}.
That paradigm enables decomposing a complex task into missions that are delegated to autonomous agents that achieve them through cooperation mechanisms. Most notably, they provide models and approaches to handle conflicting goals, parallel computation, system robustness, and scalability.
%The applications of MAS are diverse, including collective robotics, Vehicular Ad Hoc Network (VANET)\cite{Oliveira1999, Gembarski2020}.
In MAS, the organization is a fundamental concept that has an impact on how agents coordinate their activities to collaboratively achieve a common goal~\cite{Hubner2007}.
Organizational aspects address the challenge of MAS design in dynamic and uncertain environments, where runtime behavior needs to be flexible~\cite{Kathleen2020}. Organization in MAS design is a central concept in methodologies and frameworks enabling the engineering of application-specific MAS~\cite{Bakliwal2018}.

MAS designing/development methods have often been proposed jointly with organizational models to help designers find suitable organizational specifications enabling a MAS to reach a goal efficiently. Methods such as GAIA~\cite{Wooldridge2000,Cernuzzi2014}, ADELFE~\cite{Mefteh2015}, or DIAMOND~\cite{Jamont2015}, KB-ORG~\cite{Sims2008} provide protocols that rely on the designer's experience to hand-craft the agent's rules (also called \textbf{policies}) leveraging \textbf{self/re-organization} mechanisms to adapt the MAS on the deployment environment.
These aforementioned methods are commonly applied through simulations for they enable a safe monitoring framework for the design process and assessment. A MAS developed in simulated environments with high fidelity to the target system is expected to be transferred to the target system to perform adequately~\cite{Schon2021}.

The designer defines the agents' policies in various ways ranging from the agent's individual point of view to the global organization point of view. A properly designed MAS is expected to show emerging or chosen organizations enabling reaching a goal~\cite{Picard2009}. That design approach often takes place as an iterative process proceeding by trial and error. Yet, it shows the following limitations:
\begin{enumerate*}[label=\roman*),itemjoin={;\quad}]
    \item It requires sufficiently experienced designers
    \item It may be costly to converge towards a sufficiently estimated successful MAS
    \item It gets difficult to apply for complex and highly dimensional target deployment environments.
\end{enumerate*}
For instance, research in Autonomous Intelligent Cyberdefense Agents~\cite{Kott2023} (AICA) aims to develop cooperative Cyberdefense agents deployed in highly complex computer networks. The development of an AICA faces the lack of visual and intuitive comprehension of the networked environments such as company networks.

% Problem:

Even though some methods may automate some parts of the MAS organization design such as KB-ORG~\cite{Sims2008}, they still require some knowledge and manual interactions to guide the designing process. Indeed, there is a need for
\begin{enumerate*}[label=\roman*),itemjoin={; and \ }]
    \item Finding automatically suited agents' policies satisfying design constraints
    \item Making explicit the organizational mechanisms that emerge from trained agents for the design process.
\end{enumerate*}

% Contribution

To address these issues, we introduce AMOEA, a MAS design approach whose underlying idea is to link a given MARL process with an organizational model that links the on-training agents' policies with explicit organizational specifications. It can be viewed as a tool for engineering to automatically generate relevant exploitable organizational specifications only regarding the performance in achieving the given goal and the design constraints. For the designer, the obtained organizational specifications are insights into the organizational mechanisms to set up for developing a MAS that meets performance requirements.

% Results

% We applied AOMEA in three spatial Atari games with various required degree of cooperation among agents so they achieve a goal the most efficiently; and additionally respecting organizational specifications as design constraints. Obtained organizational specifications are indeed exploitable, coherent with expectations, and respect design constraints.
%We also applied our approach, in a Cyberdefense drone swarm environment whose resulting organizational specifications led to develop a MAS with scores comparable to the leading ones.

Section II starts by introducing the theoretical background of AOMEA and focuses on the fundamental concepts we used for the organizational models and MARL.
% and the motivation for integrating a MAS organizational model into a MARL process in order to enhance the MAS design process.
In section III, we present AOMEA from the approach to the implemented tool. We assessed AOMEA in four simulated environments and discussed the obtained results in section IV. Finally, section V concludes on the AOMEA's viability and highlights limitations to overcome and future works as well.

% ====================================================================================================

\section{Theoretical background}

% // Mettre en avant les briques du raisonnement en expliquant le titre pour préparer l'introduction de la contribution avec AOMEA sans les justifier (sans faire de comparaison avec l'existant, pas d'édt, dire juste les points forts)

% Organization
%   -> moise (justifier parmi les existants)

% MAS methodologies (ALAADIN, GAIAI mais pas de moyens pour trouver une organisation automatiquemenet)

% MARL (basiques) // DECPOMDP (basiques)

In this section, we present the basics of the $\mathcal{M}OISE^+$ organizational model and the MARL basics on which our contribution is built.

\subsection{Multi-Agent Systems context}

% definition, modèle AEIO, focus Moise, Dec-POMDP

An agent is an entity immersed in an environment that perceives observation and makes a decision to act autonomously in the environment to achieve the objectives assigned to it.
Agent types include event-driven reactive to deal with uncertainties in an environment or cognitive proactive agents that leverage interactions with other agents. A MAS is a set of agents in a shared environment where each agent has only a local perception. These agents are to be endowed with self/re-organizing capabilities that allow them to adaptively modify their organizational structure according to their environment.

A MAS is strongly linked to the organization entity (we simply call \textbf{organization}) we consider it to always exist through the running agents' interactions even though it may be implicit.
%
% These methods are essential for ensuring that MAS can effectively coordinate, communicate, and execute tasks in a distributed and often dynamic environment.
% Most notable methods include
% \emph{Tropos} which is an agent-oriented software development methodology that emphasizes early requirements analysis and the continuous refinement of these requirements through the design and implementation phases~\cite{Bresciani2004};
% \emph{Gaia}, a methodology for the analysis and design of MAS, focusing on the organizational structure of the system~\cite{Zambonelli2003}; \emph{DIAMOND} which relies on a four phases iterative approach to enhance the development of multi-agent physical system; and {ADELFE} that use skills and attitudes during the design to create self-organizing systems and meet the final requirements.
% As part of methodological works, it is also worth noting the AEIO (Voyelles) model that emphasizes structuring entities within multi-agent systems by incorporating agents, environment, interactions, and organization as key components.
%
An \textbf{organizational model} specifies (at least partially) the organization whether it is used as a medium to describe an explicit known organization in a top-down way, or describing an implicit organization in a bottom-up way. An example of organizational model is the \emph{Agent/Group/Role} (AGR) model~\cite{Ferber2004}. We refer to the \textbf{organizational specifications}, the components used in an organizational model to characterize the organization. $\mathcal{M}OISE^+$ is an organizational model with which it is possible to link agents' policies to organizational specifications. It takes into account the social aspects between agents explicitly whereas \emph{AGR} focuses on the integration of standards oriented towards design. $\mathcal{M}OISE^+$~\cite{Hubner2007} considers three types of specifications:

The \textbf{structural specifications} describe the means agents can leverage to achieve a goal. It comprises the set of \emph{roles}, sub-groups, intra-group and inter-group \emph{links}, intra-group and inter-group \emph{compatibilities}, and the role and sub-group \emph{cardinalities}.
A \emph{link} indicates whether two roles are related because of acquaintance, communication, or authority ties. A \emph{compatibility} indicates whether two roles can be adopted by the same agent. Role and sub-group \emph{cardinalities} respectively refer to the minimal and maximal number of roles and sub-groups.

The \textbf{functional specifications} describe the way to achieve a goal. It comprises \emph{social schemes} and \emph{preference order}. A \emph{social scheme} is described by global goals, mission labels with plans, and the cardinality of agents committed to a mission. A \emph{preference order} means an agent a has social preference to commit to a specific mission among several possible ones.

The \textbf{deontic specifications} enable linking functional and structural specifications through a set of \emph{permissions} and obligations. A \emph{permission} means an agent playing role $\rho_a$ is permitted to commit to mission $m$ for a given time constraint $tc$. Similarly, an \emph{obligation} means an agent playing role $\rho_a$ has to commit to mission $m$ for a given time constraint $tc$. A time constraint $tc $ specifies a set of periods determining whether a permission or an obligation is valid.

% Yet, these methodological works significantly rely on human designers' experience while none of them enable automating the assistance of the MAS design process by guaranteeing sufficient efficiency while taking into account organizational aspects in a multi-agent context.

\subsection{MARL basics}

Reinforcement learning is a machine learning paradigm where agents learn to make decisions by interacting with an environment. The goal is for the agent to maximize a cumulative reward signal over time through a trial-and-error process.
MARL extends this concept to multiple agents that simultaneously learn to adapt their strategies while considering the actions and influences of other agents. That pushes agents to rely on cooperation, competition, and coordination mechanisms.

MARL enables automatically converging towards agents’ policies that enable reaching the given goal. Yet, unlike human-based design, the trained agents' logic is explicitly specified from a collective point of view. Few works attempt to address that issue and few are oriented for methodological purposes.
Kazhdan et. al.~\cite{Kazhdan2020} proposed means to extract symbolic models from MARL systems that improve the interpretability of MARL systems.
Wang et. al.~\cite{Wang2020} introduced a role-oriented MARL approach where roles are emergent, and agents with similar roles tend to share their learning and specialize in certain sub-tasks.
Tosic et. al~\cite{Tosic2010} proposed a framework for addressing coordination in collaborative MAS relying on the communication capabilities of multi-agent systems.
Zheng et. al.~\cite{Zheng2018} presented a platform for MARL that aims to facilitate research on artificial collective intelligence by providing a comprehensive set of baselines and evaluation metrics to benchmark the performance of MARL algorithms.

Markovian models are required to model the environment and apply MARL techniques. As a commonly used, Decentralized Dec-POMDP~\cite{Oliehoek2016} considers multiple agents in a similar MAS fashion. It relies on stochastic processes to model the uncertainty of the environment for the changes induced by the actions, the received observations, and the communications as well. Its reward function is common to agents which fosters training for collaborative oriented actions~\cite{Beynier2013}. Formally, a Dec-POMDP is a 7-tuple $(S,\{A_i\},T,R,\{\Omega_i\},O,\gamma)$ , where: $S = \{s_1, ..s_{|S|}\}$: The set of the possible states; $A_{i} = \{a_{1}^{i},..,a_{|A_{i}|}^{i}\}$: The set of the possible actions for agent $i$; $T$ so that $T(s,a,s') = \probP{(s'|s,a)}$ : The set of conditional transition probabilities between states; $R: S \times A \times S \rightarrow \mathbb{R}$: The reward function; $\Omega_{i} = \{o_{1}^{i},..,o_{|\Omega_{i}|}^{i}\}$: The set of observations for agent $ag_i$; $O$ so that $O(s',a,o) = \probP{(o|s',a)}$ : The set of conditional observation probabilities; $\gamma \in [0,1]$, the discount factor.

We refer to \textbf{solving} the Dec-POMDP for the team $t$ as finding a joint policy $\pi_{joint,i} \in \Pi_{joint}$ that maximizes the expected cumulative reward over a finite horizon.
We refer to \textbf{sub-optimally solving} the Dec-POMDP at $s$ expectancy as finding the joint policies $\pi_{joint,i} \in \Pi_{joint}$ that gets the expected cumulative reward over a finite horizon at least at $s \in \mathbb{R}$.

% ====
% \paragraph{\textbf{Method Fragments and Model Transformations}}
% A set of method fragments for developing MAS, which is grounded on the development process of two multi-agent systems, emphasizes the importance of model transformations in the engineering process~\cite{Garcia2011}. These fragments can be seen as modular methodologies that can be adapted and reused across different MAS projects, providing a flexible approach to system development.

% \paragraph{\textbf{Meta-Models for Analysis and Design}}
% The use of meta-models is reported to improve analysis and design activities in MAS engineering~\cite{Gomez2004}. Meta-models provide a high-level abstraction that can help in understanding and designing the complex interactions and behaviors of agents within the system, facilitating a more structured approach to MAS development.

% \paragraph{\textbf{Agent-Oriented Software Engineering Paradigms}}

% Agent technology represents a new software engineering paradigm that offers fresh prospects for analyzing, designing, and building software systems~\cite{Li2016}. This paradigm shift towards agent-oriented software engineering (AOSE) encourages developers to think in terms of autonomous agents and their interactions, leading to more robust and adaptable MAS.

% \paragraph{\textbf{Comprehensive Methodologies}}

% \paragraph{\textbf{Characterization of emergent collective strategies}}

% % Collective explainable AI: Explaining cooperative strategies and agent contribution in multiagent reinforcement learning with shapley values
% Heuillet et. al.~\cite{Heuillet2022} proposes a novel approach to explain cooperative strategies in multiagent reinforcement learning (RL) using Shapley values, a game theory concept used in eXplainable AI (XAI). The study aims to make deep RL more comprehensible and address the need for methods that provide better understanding and interpretability. The experimental results on Multiagent Particle and Sequential Social Dilemmas demonstrate the effectiveness of Shapley values in explaining the rationale behind decisions taken by agents. However, the article also highlights that Shapley values can only provide general explanations about a model and cannot explain specific actions taken by agents. The authors suggest that future work should focus on addressing these limitations. The study's implications extend to areas such as non-discriminatory decision making, ethical and responsible AI-derived decisions, and policy making under fairness constraints.

% % Social Influence as Intrinsic Motivation for Multi-Agent Deep Reinforcement Learning
% Jaques et. al.~\cite{Jaques2019} proposes a mechanism for achieving coordination and communication in MARL by rewarding agents for having causal influence over other agents' actions. This causal influence is assessed using counterfactual reasoning, where agents simulate alternate actions to compute their effect on the behavior of other agents. The paper demonstrates that this approach leads to enhanced coordination and communication, as well as more meaningful learned communication protocols. The proposed method is shown to significantly increase the learning curves of the deep reinforcement learning agents, leading to more diversified team behavior and more successful performance of the population as a whole. The paper also highlights that the influence rewards for all agents can be computed in a decentralized way, opening up new opportunities for research in this area.

% \paragraph{\textbf{Adaptation of MARL to meet requirements}}

% % Efficient MARL through automated supervision
% Chongjie et. al.~\cite{Chongjie2008} proposes a unified mechanism for achieving coordination and communication in MARL. The approach involves training multiple agents to independently maximize their own individual reward without sharing weights. The paper introduces a method for automated supervision, which enables the agents to learn to coordinate and communicate effectively. This automated supervision mechanism leads to enhanced coordination, communication, and more meaningful learned communication protocols, ultimately improving the learning curves of the deep reinforcement learning agents and the overall performance of the agent population
% %
% % Self-Organized Group for Cooperative MARL
% Shao et. al.~\cite{Shao2022} introduces a method called Self-Organized Group (SOG) for cooperative MARL. In this approach, a certain number of agents are randomly elected to be conductors, and the corresponding groups are constructed with conductor-follower consensus, allowing the groups to be re-organized at regular intervals. The organized group under the unified command of a conductor is found to embed the multi-agent system with stronger zero-shot generalization ability compared to traditional methods. The SOG method provides strong adaptability to scenarios with varying numbers of agents and varying agent sight. The paper presents this approach as a mechanism to enhance cooperative multi-agent tasks with dynamic characteristics, aiming to improve the adaptability and generalization of MARL systems
%
% % A MARL model of common-pool resource appropriation
% Perolat et. al.~\cite{Perolat2017} introduces a model that focuses on common-pool resource appropriation, a multi-agent social dilemma that includes issues such as sustainable use of fresh water, common fisheries, grazing pastures, and irrigation systems. The model emphasizes the importance of trial-and-error learning in addressing the challenges of common-pool resource sustainability and inequality. It explores the emergent behavior of groups of independently learning agents in a partially observed Markov game, shedding light on the relationship between exclusion, cooperation, and sustainability in the context of resource appropriation. The research highlights the potential of deep reinforcement learning in understanding and addressing complex societal and environmental challenges related to common-pool resource management. The paper provides valuable insights into the application of MARL in the context of real-world social dilemmas and resource management
%
% Promoting Coordination through Policy Regularization in Multi-Agent Deep Reinforcement Learning
% Roy et. al.~\cite{Roy2020} addresses the challenge of inducing coordination between agents in MARL. The research investigates the use of policy regularization to promote inter-agent coordination and discusses two approaches based on inter-agent modeling and synchronized sub-policy selection. The proposed methods are designed to improve cooperative behaviors without relying on explicit communication channels, allowing agents to exhibit coordinated behaviors during testing when acting in a decentralized fashion. The paper presents two policy regularization methods, TeamReg and CoachReg, and evaluates their performance on challenging cooperative multi-agent problems, showing improved results. The research contributes to the advancement of coordination-driven multi-agent approaches in reinforcement learning and provides valuable insights into promoting inter-agent coordination through policy regularization.

% Eventually, among considered works, none are specifically using an organizational model as a general way for both expressing the MARL resulting through OCPV; and constraining the MARL itself according to organizational specifications. It appears that augmenting MARL with an organizational model so it can be core of a aided-engineering MAS design approach; is not explicitly covered as far as we know.

% ====================================================================================================

\section{AOMEA approach}

% Mettre d'abord en avant le shcéma général de l'approche
% Donner la philosophie de l'approche

% 	Global overview (Fig 1)
% 	theoreetical core
% 	Engineering
%   Implémentation (vers une implémentation de type PoC)

\subsection{Overview}

\begin{figure}[h!]
    \centering
    \include{figures/approach}
    \caption{A summary view of our approach to MAS design}
    \label{fig:design_approach}
\end{figure}

We introduce AOMEA as an approach for MAS design that automates the preliminary design of a MAS according to some design constraints. Organizational specifications obtained after training allow the development of a curated MAS.
The underlying idea of our approach is to consider that a joint-policy or joint-history can be described in terms of organizational specifications, at least partially.
We refer to that broad approach as \textquote{Organization oriented MARL} (OMARL).
%
% As a high-level description, AOMEA considers the environment with agents that are to achieve some goals. It automatically enables finding relevant insights under the form of organizational specifications. They are to transparently express how agents could individually act and collaborate to reach the goals. Then, the MAS design process can be assisted in light of these indications. An illustrative representation of our approach is presented in \autoref{fig:design_approach}.
%
AOMEA consists of 4 sequential phases: modeling, solving, analyzing, and developing (respectively $1.x$, $2.x$, $3.x$, $4.x$ in arrow labels in \autoref{fig:design_approach}).

\textbf{Phase 1: Modeling} \quad In that phase, the designer has to manually develop a simulation of the target environment ($1.1$ in \autoref{fig:design_approach}) where agents must cooperate to achieve the designer's goal efficiently ($1.2$ in \autoref{fig:design_approach}) with the help of quantitative feedback. When developing the simulated environment, the designer can link parts of an agent's policy (as observations-actions couples) with known organizational specifications of any chosen organizational model.
For instance, in \textquote{leader-follower} organizations, the actions that send orders to other follower agents, are characteristics of leader agents.
Optionally, the designer may also want to restrict the set of possible policies agents can explore regarding given organizational specifications as constraints to meet design requirements or to help agents converge as well ($1.3$ in \autoref{fig:design_approach}).

\textbf{Phase 2: Solving} \quad In that phase, relying on the established relations between observation-action couples and organizational specifications, a MARL algorithm is used jointly with the chosen MAS organizational model through an OMARL process. It automatically enables finding optimal policies satisfying the given design organizational specifications ($2.1$ in \autoref{fig:design_approach}) that lead to the best expected cumulative reward; and getting the associated organizational specifications ($2.2$ in \autoref{fig:design_approach}). For instance, when training agents regarding the \textquote{leader-follower} organization, some agents may be forbidden to send orders while some other may be forced to. After training, the OMARL process characterizes emergent roles, links between roles, or sub-goals organized in plans to reach the goal.

\textbf{Phase 3: Analyzing} \quad In that phase, the designer observes the trained agents' policies ($3.2$ in \autoref{fig:design_approach}) and takes into account the inferred associated organizational specifications ($3.1$ in \autoref{fig:design_approach}) to understand how these agents can reach the goal. In light of these raw results, the designer can extract valuable design patterns from noisy or useless agents' decisions. The interest is to provide at least some indications of the organizational specifications capable of achieving the goal and to satisfy the design constraints. We refer to these valuable indications as curated organizational specifications ($3.3$ in \autoref{fig:design_approach}). For instance, after having trained several agents in a \textquote{predator-prey} environment, it is possible to analyze that a \textquote{leader} predator with \textquote{follower} predators, appears to be more efficient for catching prey.

\textbf{Phase 4: Developing} \quad In that phase, the designer takes into account the curated organizational specifications as a blueprint for implementing a MAS. From that point, a regular MAS development with one of the available methods that is used jointly with the chosen organizational model can be applied. Unlike the trained agents which may cause unexpected behavior, manually implemented agents enable giving safety guarantees required for sensitive environments. Finally, implemented agents are launched in simulations to assess whether the implemented MAS can effectively achieve the goal.

\subsection{Theoretical core}

To implement an OMARL process, we propose the \emph{Partial Relations with Agent History and Organization Model} algorithm (PRAHOM) to link agents' policies and their training to an organizational model.
It is a synthesis of two processes that fall into the OMARL purposes. The first process gets the specifications from the agents' policies, and the second process gets the joint-policies satisfying the given design specifications. An illustrative view of \emph{PRAHOM} is given in \autoref{fig:prahom_process}.
Here we just present the underlying idea at a high-level description for these two processes to avoid unnecessary formalism. More information on the use and implementation of \emph{PRAHOM} can be found in \autoref{PettingZoo-wrapper}.
% Since \emph{PRAHOM} relies on joint-histories instead of joint-policies, it is agnostic of the approximation function used for implementing the agents' policies such as neural networks based ones

\begin{figure}[h!]
    \centering
    \include{figures/prahom_process}
    \caption{A summary view of the PRAHOM process}
    \label{fig:prahom_process}
\end{figure}

% \RestyleAlgo{ruled}
% \SetKwComment{Comment}{// }{}

% \begin{algorithm}[hbt!]
%     \caption{Partial Relations with Agent History and Organization Model (PRAHOM)}\label{alg:prahom}

%     \KwData{$d$, the Dec-POMDP to solve}
%     \KwData{$ep_{max} \in \mathbb{N}$, the maximum number of episodes}
%     \KwData{$step_{max} \in \mathbb{N}$, the maximum number of steps per episodes}
%     \KwData{$s \in \mathbb{R}$, the cumulative reward expectancy}
%     \KwData{$\pi_{joint}$, the joint-policies to be trained}
%     \KwData{$os_{init}$, the design specifications to respect}
%     \KwData{$Solv$, a MARL algorithm updating policies to solve a Dec-POMDP}

%     \KwResult{$(s\pi_{joint,s,i} \in S\Pi_{joint,s,i}, os_{s,i} \in OS_{s,i})$, the sub-optimal policies associated organization specifications}

%     $s\pi_{joint,s,i} = PRAHOM-sop-training(s,i)$

%     $os_{s,i} = PRAHOM-pos(s\pi_{joint,s,i})$

% \end{algorithm}

\paragraph{\textbf{Inferring Organizational Specifications}}

Rather than using joint-policies directly, we use the joint-histories since they may be built with observed resulting actions when observations are received during a series of test episodes. Indeed, for a given policy $\pi \in \Pi$, the associated history is by definition $h \in h_{joint} = \langle(\omega_k,a_k) | k \in \mathbb{N}\rangle$ and the $(\omega_k,a_k) \in \pi$.
Then, due to the difficulty of inferring information related to organizational specifications, it is possible to associate each observation or action with organization specifications as a \textquote{many to many} relation. It sets up a first frame for identifying organizational specifications in histories. We address that problem in the remainder of this section.

One can define some relations between $\mathcal{M}OISE^+$ specifications and joint-histories. Their premises come from noticing some specifications in the $\mathcal{M}OISE^+$ organizational model can be mapped to subsets of actions from a single suboptimal joint-policy.
From these relations, it is possible to use empirical or statistical approaches to infer organizational specifications out of joint-histories. Below we informally describe key points for understanding that process.

As we have only one group, we do not consider the inter-links and inter-compatibilities. Additionally, as a simplification, we consider only one social scheme.
First, we look at the individual level by trying to figure out the roles, links, sub-groups, individual goals, missions, and plans played by agents by sampling history subsequences $h \in H$ and comparing with known history subsequences whose we know the associated role via the established relations.

After analyzing several joint-policies, we try to reinforce a global view of the goals, missions, plans, and the knowledge of the mission to the goal; with the partially inferred information at the individual level.
In the end, our process tries to synthesize the knowledge inferred until having a better view of the agent cardinality per sub-group, the agent cardinality for each mission, the role cardinality, the compatibilities between roles, the permissions, and obligations.

\paragraph{\textbf{Constraining Policies Space}}

We consider a given MARL algorithm that iteratively converges towards a joint-policy so that each agent's policy is updated at each step until a finite horizon.
We favored the Proximal Policy Optimization for its proven effectiveness in cooperative multi-agent environments without the need for domain-specific algorithmic modifications or architectures~\cite{Yu2022}.
To constrain the possible joint-policies to the ones satisfying the design organizational specifications $os_{init}$, we propose to constrain the action and observation sets for each agent according to $os_{init}$  at each step. For instance, we may constrain an agent to converge to a given role by forbidding actions related to other roles. We used that idea, to set up our process to guide the training according to some design organizational constraints.

First, we use the previously established relations between organizational specifications and action-observation couples, to determine the authorized or forbidden actions playable by agents at each step.
Then, it first computes the authorized actions set $A_{step}$ according to the current history $h_{joint,i}$. Then, an action is chosen among authorized actions. That action $a_{step} \in A_{step}$ is added in history to be used for updating the agent's policy in the next step. Then, the MARL algorithm updates the joint-policy hence the agents' policies with the current action and observation.
Finally, an analysis of the current suboptimal joint-policy $\pi_{joint}$ satisfying $os_{init}$ is triggered periodically. It enables iteratively improving the efficiency of joint-policies and the accuracy of the inferred organizational specifications.
We can note the restriction implied by $os_{init}$ in the possible joint-policies might prevent the MARL algorithm from finding a joint-policy that satisfies the minimal expected cumulative reward defined by the designer.

\subsection{Engineering tool}

PettingZoo is a library that offers a standardized API that simplifies the development of environments with agents and facilitates the application of MARL algorithms.
We developed \emph{PRAHOM PettingZoo Wrapper}\label{PettingZoo-wrapper}, a tool to help automate the setting up of \emph{PRAHOM} for a given PettingZoo environment.
It is a Proof of Concept that enables linking actions with $\mathcal{M}OISE^+$ specifications and defining the organizational specifications. It provides functions to extract the resulting suboptimal raw organizational specifications. During training, actions are masked to guide an agent to learn to act according to the organizational specifications.

\begin{lstlisting}[language=Python, caption=PRAHOM PettingZoo Wrapper basic use, label={lst:wrapper_basic_use}]
from omarl_experiments import PrahomWrapper
env = PettingZoo_env.parallel_env(render_mode="human")
...
action_to_specs = {"agent_name": {
        "14": "role='leader';link='(agent_0,agent_1,auth);..."
        ...}    
    ...}
training_specs = {
    "leadadversary_0": {
        "must": ["(23,41)"],
        "must_not": ["(14,74)"]}...}
env = PrahomWrapper(env, action_to_specs, training_specs, unknown_specs_inference=True, pca_output=True)
# ...TRAINING...
env.prahom_render_pca()
trained_specs, agent_to_specs = env.prahom_specs()
\end{lstlisting}

In \autoref{lst:wrapper_basic_use}, we detailed a basic use of the wrapper to augment a PettingZoo environment (l. 12) with known relations between actions and organizational specifications (l. 4) and the design constraints agents are to satisfy (l. 8). After training, we can print a Principal Component Analysis (PCA) of each agent history over an episode (l.14) which is an additional analysis tool to manually detect emerging roles. It also provides the inferred $\mathcal{M}OISE^+$ specifications considering the last episodes and how agents are instantiated in the last episode.

These results are constructed via empirical or statistical approaches using provided known relations between observations, actions, and organizational specifications. When no relation is available, it tries to detect subsequences of joint-histories likely to match an organizational specification (l. 12). Due to these limitations, the results may not fully describe the underlying organization or may contain noisy inferred organizational specifications. Since the results are compliant with $\mathcal{M}OISE^+$, it is then possible to use them with available MAS design methods to benefit from the identified emerging organizational specifications during the design process.

% We start defining actions with known specifications Focusing of the "lead adversary", we have:

% "observations": [self_vel, self_pos, landmark_rel_positions, other_agent_rel_positions, other_agent_velocities, leader_comm]
% "actions": [say_0, say_1, say_2, say_3] X [no_action, move_left, move_right, move_down, move_up]

% =============

% \subsection{Premises for AOMEA's theoretical core}

% - Organizational oriented MARL: a general research study focusing in integrating the organization in MARL at several aspects
% - DMO (Dec-POMDP MOISE+ OMARL): a class of algorithm falling into OMARL purposes that use MOISE+ as organizational model and Dec-POMDP as a MARL model
% - PRAHOM (Partial Relations with Agent History and Organization Model): a DMO algorithm that allows both getting the organizational specifications out of trained agents; and driving their training to satisfy extra design organizational specifications as constraints.

% \subsection{AOMEA for engineering}

% - We proposed PRAHOM (Partial Relations with Agent History and Organization Model): an algorithm that allows both getting the organizational specifications out of trained agents; and driving their training to satisfy extra design organizational specifications as constraints.
% - [Algo PRAHOM...]

% - We developped PRAHOM as a PettingZoo-wrapper to help in applying the proposed MAS design approach...
% - [Lien vers dépôt GitHub]
% - Description des fonctionalités

% - The general approach workflow follows:
% 1) Reproducing target environment in simulation
% 2) Defining the reward function so agents aim to achieve the goals
% 3) Adding agents to be trained to solve the previously defined problem
% 4) Launching the training in simulation with PRAHOM
% 5) Getting the raw organizational specifications with PRAHOM
% 6) Curating these to produce a safe MAS model
% 7) Using that model to implement a proper MAS
% 8) Check viability and requirements respect in simulation
% 9) Deploy that MAS in the target system

% \begin{itemize}
%     \item Questions at the Multi-Agent System level (system-centric approach)
%           \begin{itemize}
%               \item Number of agents, what heterogeneity?
%               \item What is the common medium (Environment) shared by the agents?
%               \item What communication mechanisms are available to agents?
%               \item What are the communication languages, ontologies, interaction protocols used by the agents?
%               \item What is the organization within which the agents operate? How is it established?
%               \item How do the agents coordinate their actions? How to ensure coherent operation?
%           \end{itemize}

%     \item Agent level questions (agent-centered approach)
%           \begin{itemize}
%               \item What does an agent represent? What actions should be encapsulated in an agent?
%               \item How do agents represent the environment and organization in which they operate?
%               \item How do agents handle interactions with other agents?
%               \item What is the internal structure of the agents?
%           \end{itemize}
% \end{itemize}

% ====================================================================================================

\section{Evaluation in cooperative game environments}

% Evaluation
%     In order to verify and demonstrate the approach, is applied on the following case study.
% 	Case study


In order to assess AOMEA, we considered using \emph{PRAHOM} in available simulated environments made up of agents that have to achieve a goal with the best performance through various collective strategies whose some can be easily understood (presented in \autoref{fig:simulated_environments}).
We selected three Atari-like environments for their visual rendering is a convenient way to assess the results with manual observations\footnotemark[1].
We also considered a Cyberdefense environment as a first attempt to apply \emph{PRAHOM} in a non-visual Cyberdefense environment:

\footnotetext[1]{Additional explanation and the examples discussed using \emph{PRAHOM PettingZoo wrapper} are available at \url{https://github.com/julien6/omarl_experiments?tab=readme-ov-file\#tutorial-predator-prey-with-communication}}

\vspace{0.2cm}

% Left column for text
\hspace{-0.6cm}
\begin{minipage}[t]{0.61\textwidth}

    \begin{itemize}
        \item \textquote{Drone swarm - 3rd CAGE Challenge}~\cite{cage_challenge_3_announcement} (CYB) consists of cyberdender agents deployed on networked drones trying to fight against maliciously deployed malware programs. We may expect agents to \allowbreak collectively isolate compromised drones;
        \item \textquote{Pistonball} (PBL)~\cite{Terry2021} consists of a series of pistons to bring a ball from right to left side hence requiring neighbors' representation;
        \item \textquote{Predator-prey with communication}~\cite{Lowe2017} (PPY) consists of predators monitored by a leader to better catch faster prey hence requiring collective hunting strategies;
        \item \textquote{Knights Archers Zombies}~\cite{Terry2021} (KAZ) consists in a few knights and archers learning how to kill as many zombies hence requiring efficient agent spatial positioning.
    \end{itemize}

\end{minipage}%
\hfill % Ajoute une marge horizontale
% Right column for a figure
\begin{minipage}[t]{0.35\textwidth}
    \vspace{-1cm}
    \begin{figure}[H] % Use the 'H' specifier from the 'float' package to force the figure placement
        \centering
        \includegraphics[width=0.6\linewidth]{figures/envs.png} % Replace "example-image" with your figure file name
        \caption{Overview of the selected environments: CYB, PBL, PPY, and KAZ}
        \label{fig:simulated_environments}
    \end{figure}
\end{minipage}

\

\noindent We applied AOMEA in three cases:
\begin{itemize}
    \item No organizational specifications (NTS): agents have to learn the most efficient collective strategies without any constraints or indications.
    \item Partially constraining organizational specifications (PTS): some constraints or indications are given to help converge faster or meet requirements.
    \item Fully constraining organizational specifications (FTS): manually crafted joint-policies are given for they are a reference regarding learned joint-policies.
\end{itemize}

Here, we do not present the details of the constraints that were given in NTS and FTS (available in Git repository\footnotemark[1]).

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/prahom_learning_curve.png}
    \caption{Average reward for each iteration in the PBL environment for the NTS, PTS, and FTS cases}
    \label{fig:prahom_learning_curve}
\end{figure}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/prahom_pca_analysis.png}
    \caption{PCA of the trained agents' histories in the PBL environment}
    \label{fig:prahom_pca_analysis}
\end{figure}

We evaluate the impact of \emph{PRAHOM} on the following criteria: convergence time ratios between PTS, NTS, and FTS for reaching a threshold cumulative reward. Performance stability shows how the trained agents can achieve the goal generally by assessing several environments generated with different parameters. Results are presented in Table~\ref{tab:training_AOMEA_results}.
%
\input{tables/training_OMARL_results.tex}
%
As a general observation, we can notice convergence time is longer for NTS than for PTS which is also longer than for FTS. As expected, the search space is decreasing, hence a shorter convergence time. For instance, we noticed a faster convergence to a sub-optimal solution in the PBL environment by providing organizational specifications as presented in \autoref{fig:prahom_learning_curve}. Although PTS converges faster than NTS to a comparable cumulative reward, NTS may outperform PTS because trained agents' policies are hand-tailored to solve the problem much more finely than the designer's organizational specifications can do. Low-performance stability in the more complex CYB environment indicates that the trained agents have difficulty finding general strategies compared to the agents in the other environments.
We also took into account criteria after training: roles, links, and compatibilities. A qualitative analysis is presented in Table~\ref{tab:trained_AOMEA_results}

\input{tables/trained_OMARL_results.tex}

% //TODO: Moise+ schemes and comparison with expected ones

For the PBL environment, we can notice roles being equivalent for agents are expected to act the same. Indeed, trained agents' histories are close hence showing a common emerging role. We generate the PCA presented in \autoref{fig:prahom_pca_analysis} by expressing agents' histories as vectors containing the observation-action couples. We can notice most agents’ histories are in the left bottom zone (circled in red). It shows most pistons seem to act similarly as expected. We observe no organizational specifications except roles have been generated because agents cannot communicate. For the KAZ environment, we can notice two distinct roles: archers tend to move away from zombies, while knights tend to approach them. For the PPY environment, we can observe the output specifications indicate authority links between the leader predator and the simple predators to enable collective strategies for circling prey. Finally, the CYB environment shows communications between blue agents are indeed understood as communication links that enable isolating infiltrated drones or trying to fix and alert recently suspected drones.

For the CYB environment, we developed our custom MAS via a simple hand-crafted decision tree as preconized in AOMEA in light of the organizational specifications we curated by removing noisy results. Our approach did not suggest general roles but relevant strategy patterns have been identified. For instance, regarding links between agents' roles, we noticed that the agents sending messages frequently seem to be spotted as suspected by their neighbors. In addition, a cyber-defender agent in the communication radius of a suspected drone tends to switch off its communication and reactivate afterward. Even though these insights are few, the mean score we got with our curated MAS is about -2000 which is indeed close to the top 5 scores. This shows AOMEA to be indeed applicable to the Cyberdefense context additionally bringing safety guarantees.

\section{Conclusion}

% =======> demande toi avant de rédiger la conclusion "Qu'est ce qui change dans l'état de l'art avec ce papier ?"... si tu avais lu ce papier et qu'il n'était pas de toi qu'est ce que tu te serai dis 

%  - Need for easing the MAS design by assisting human designer
%  - We proposed a novel general MAS design approach that consists in getting relevant organizational specifications out of agents in training satisfying design constraints. These insights allow implementing a suited safe MAS to be deployed on target system
%  - We validated that approach for various different environments including in a Cyberdefense context.
%  - Main perspective is to improve the reliability of PRAHOM between organizational specifications and agents' training satisfying to design constraints.
%  - Another future work is to reduce the manual parts of our approach with implementation of a full integrated development environment.

% 1er paragraphe: résumé + qu'est-ce qui est changé par rapport à l'edt
% 2ème paragraphe: perspectives et future works

% In this article we presented AOMEA, a novel general MAS design approach that aims to ease the MAS designing when it can not be easily conducted by designers in highly complex deployment environments.
MAS methodological works rely on the designer's knowledge to design a suited MAS organization but do not provide automatic or assisted ways to determine relevant organizational mechanisms.
%solely from the design requirements and the global goal.
MARL techniques have been successfully applied to train agents automatically to reach the given goal without explicit characterization of emergent collective strategies.
AOMEA's originality is to augment a MARL process with an explicit organizational model towards a methodological purpose to address these issues. We first exposed how AOMEA is intended to be used in MAS engineering as an additional tool to assist in the design process.
Then, we explained the AOMEA's theoretical core with links between Dec-POMDP and the $\mathcal{M}OISE^+$ through the \emph{PRAHOM} process.
Furthermore, we implemented the \emph{PRAHOM PettingZoo wrapper} as a Proof of Concept for practically applying AOMEA and we showed it enables getting some organizational specifications that satisfy the design constraints and allow achieving the given goal.
Finally, we applied our approach in four PettingZoo environments to assess the impact on and after training. The obtained performance results show to be comparable to known ones showing our approach to be viable.

Even though \emph{PRAHOM} is agnostic of the MARL algorithm because it uses agents' histories to infer organizational specifications, reconstructing agents' collective behaviors a posteriori may be difficult. Indeed, a major perspective for improving \emph{PRAHOM} is to go further with supervised and non-supervised learning techniques in addition to empirical statistical approaches for identifying valuable organizational specifications from joint-histories. Moreover, it is worth investigating recent works in MARL techniques such as hierarchical learning because they already seek to characterize emergent strategies throughout learning.
%Eventually, we also aim to improve AOMEA applicability by developing dedicated interfaces built around \emph{PRAHOM} making it more accessible for industrial and research contexts.


% ---- Bibliography ----
%
% BibTeX users should specify bibliography style 'splncs04'.
% References will then be sorted and formatted in the correct style.
%
% \bibliographystyle{splncs04}
% \bibliography{mybibliography}
%
\section*{References}

% \bibliographystyle{abbrv}
\bibliographystyle{splncs04}

\bibliography{references}

\end{document}