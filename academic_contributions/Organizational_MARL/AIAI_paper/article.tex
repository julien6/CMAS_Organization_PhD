\documentclass[runningheads]{llncs}

\usepackage[T1]{fontenc}
\usepackage{graphicx}
%\usepackage{color}
%\renewcommand\UrlFont{\color{blue}\rmfamily}

\usepackage{amsmath,amssymb,amsfonts}
\usepackage[inline, shortlabels]{enumitem}
\usepackage{tabularx}
\usepackage{caption}
% \usepackage{titlesec}
\usepackage[english]{babel}
\captionsetup{font=it}
\usepackage{ragged2e}
\usepackage{hyperref}
\usepackage{pifont}
\usepackage{footmisc}
\usepackage{multirow}
\usepackage{algorithm2e}

% --- Tickz
\usepackage{physics}
\usepackage{amsmath}
\usepackage{tikz}
\usepackage{mathdots}
\usepackage{yhmath}
\usepackage{cancel}
\usepackage{color}
\usepackage{siunitx}
\usepackage{array}
\usepackage{multirow}
\usepackage{amssymb}
\usepackage{gensymb}
\usepackage{tabularx}
\usepackage{extarrows}
\usepackage{booktabs}
\usetikzlibrary{fadings}
\usetikzlibrary{patterns}
\usetikzlibrary{shadows.blur}
\usetikzlibrary{shapes}

% ---------
% \usepackage{titlesec}
\usepackage{pdfpages}
\usepackage{booktabs}
\usepackage{csquotes}
\usepackage{lipsum}  
\usepackage{arydshln}
\usepackage{smartdiagram}
\usepackage[inkscapeformat=png]{svg}
\usepackage{textcomp}
\usepackage{tabularray}\UseTblrLibrary{varwidth}
\usepackage{xcolor}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\usepackage{cite}
\usepackage{amsmath}
\newcommand{\probP}{\text{I\kern-0.15em P}}
\usepackage{etoolbox}
\patchcmd{\thebibliography}{\section*{\refname}}{}{}{}

\setlength{\extrarowheight}{2.5pt}

\renewcommand{\arraystretch}{1.7}

\setlength{\extrarowheight}{2.5pt}
\renewcommand{\arraystretch}{0.2}
\renewcommand{\arraystretch}{1.7}

% --------------
% \titleclass{\subsubsubsection}{straight}[\subsection]

% \newcounter{subsubsubsection}[subsubsection]
% \renewcommand\thesubsubsubsection{\thesubsubsection.\arabic{subsubsubsection}}
% \renewcommand\theparagraph{\thesubsubsubsection.\arabic{paragraph}} % optional; useful if paragraphs are to be numbered

% \titleformat{\subsubsubsection}
%   {\normalfont\normalsize\bfseries}{\thesubsubsubsection}{1em}{}
% \titlespacing*{\subsubsubsection}
% {0pt}{3.25ex plus 1ex minus .2ex}{1.5ex plus .2ex}

% \makeatletter
% \renewcommand\paragraph{\@startsection{paragraph}{5}{\z@}%
%   {3.25ex \@plus1ex \@minus.2ex}%
%   {-1em}%
%   {\normalfont\normalsize\bfseries}}
% \renewcommand\subparagraph{\@startsection{subparagraph}{6}{\parindent}%
%   {3.25ex \@plus1ex \@minus .2ex}%
%   {-1em}%
%   {\normalfont\normalsize\bfseries}}
% \def\toclevel@subsubsubsection{4}
% \def\toclevel@paragraph{5}
% \def\toclevel@paragraph{6}
% \def\l@subsubsubsection{\@dottedtocline{4}{7em}{4em}}
% \def\l@paragraph{\@dottedtocline{5}{10em}{5em}}
% \def\l@subparagraph{\@dottedtocline{6}{14em}{6em}}
% \makeatother

% \setcounter{secnumdepth}{4}
% \setcounter{tocdepth}{4}
% --------------

\newcommand{\before}[1]{\textcolor{red}{#1}}
\newcommand{\after}[1]{\textcolor{green}{#1}}

\newcommand{\old}[1]{\textcolor{orange}{#1}}
\newcommand{\rem}[1]{\textcolor{red}{#1}}
\newcommand{\todo}[1]{\textcolor{orange}{\newline \textit{\textbf{TODO:} #1}} \newline \newline }



\newcounter{relation}
\setcounter{relation}{0}
\renewcommand{\therelation}{\arabic{relation}}
\newcommand{\relationautorefname}{Relation}

\newenvironment{relation}[1][]{%
    \refstepcounter{relation}%
    \noindent \raggedright \textit{\textbf{Relation. \therelation}} \hfill$}
{%
$ \hfill \phantom{x}

}

\newcounter{proof}
\setcounter{proof}{0}
\renewcommand{\theproof}{\arabic{proof}}
\newcommand{\proofautorefname}{Proof}

\renewenvironment{proof}[1][]{
    \refstepcounter{proof}
    \noindent \raggedright \textit{\textbf{Proof. \theproof}}

    \setlength{\leftskip}{1em}

}
{

\
\setlength{\leftskip}{0pt}
}

% --------------------------------
%             DOCUMENT
% --------------------------------

\begin{document}
%
\title{A Design Approach for Cyberdefense Multi-Agent System with Reinforcement Learning}
%
%\titlerunning{Abbreviated paper title}
% If the paper title is too long for the running head, you can set
% an abbreviated paper title here
%
\author{Julien Soulé\inst{1}\orcidID{0000-1111-2222-3333} \and
  Jean-Paul Jamont\inst{1}\orcidID{1111-2222-3333-4444} \and
  Michel Occelo\inst{1}\orcidID{2222--3333-4444-5555} \and
  Louis-Marie Traonouez\inst{2}\orcidID{2222--3333-4444-5555} \and
  Paul Théron\inst{3}\orcidID{2222--3333-4444-5555}}
%
\authorrunning{J. Soulé et al.}
% First names are abbreviated in the running head.
% If there are more than two authors, 'et al.' is used.
%
\institute{Univ. Grenoble Alpes, Grenoble INP, LCIS, 26000, Valence, France
  \email{\{julien.soule, jean-paul.jamont, michel.occello\}@lcis.grenoble-inp.fr}
  \and
  Thales Land and Air Systems, BU IAS, Rennes, France
  \email{louis-marie.traonouez@thalesgroup.com}
  \and
  AICA IWG, La Guillermie, France \\
  \email{paul.theron@orange.fr}
}


\maketitle              % typeset the header of the contribution

% \tableofcontents


\begin{abstract}

  % context
  Multi-Agent Systems (MAS) have proved to be suited in engineering due to their ability to address complex, distributed problems especially in IoT based systems. Their efficiency regarding respective objectives and requirements is strongly dependant on the organization for engineering an application-specific MAS. Futhermore, designing an MAS organization requires to have general knowledge of the deployment environment, the problem to express as goals, and having ideas of organizational mechanisms so that agents can achieve goals and subgoals.
  % problem
  Yet, in some cases, the deployment environment is not easily readable or handleable due to the complexity and may lead to unexpected emergent phenomena raising safety concerns. The design process may be hard and costly to converge towards a sufficiently estimated successful MAS. That stresses out the need for methodological works towards assisted MAS design that could be addressed with collective AI techniques.
  % hypothesis / contribution
  This paper introduces an novel design approach to assist the MAS design whose underlying idea is to use Multi-Agent Reinforcement Learning (MARL) with organizational specifications for both understanding and constraining the training process regarding design constraints.
  % results
  We applied our approach in cooperative Atari games and a Cyberdefense drone swarm scenario of the 3rd CAGE Challenge. Obtained specifications are indeed consistent with design constraints and provide insights of relevant collective strategies that led to develop an explainable MAS with scores close to finalists' ones.

  \keywords{Multi-Agent Systems \and Reinforcement Learning \and Organization \and Design \and Cyberdefense}
\end{abstract}


\section{Introduction}

% Context:

Multi-agent systems (MAS) have gained significant interest in engineering and industry due to their ability to address complex, distributed problems. They provide ways to handle conflicting goals, parallel computation, system robustness, and scalability. The applications of MAS are diverse, including autonomous driving, multi-robot factories, automated trading~\cite{Oliveira1999}.
In MAS, organization is a fundamental concept that impact how agents are coordinating their activities to collaboratively achieve a common goal~\cite{Hubner2002}. The design and implementation of a MAS are significantly impacted by the organization. Organizational aspects are crucial for addressing the challenge of MAS design in dynamic and uncertain environments, where runtime behavior needs to be flexible~\cite{Kathleen2020}. The impact organization on MAS design is core in methodologies and frameworks that enable the engineering of application-specific MAS~\cite{Noel2010}.

% In essence, we assume the entity of the organization (we simply call \textbf{organization}) always exists through the running agents interactions even though it may be implicit. An \textbf{organizational model} specifies (at least partially) the organization whether it is used as medium to describe an explicit known organization in a top-down way, or describing an implicit organization in a bottom-up way. Examples of organizational models are the Agent/Group/Role (AGR) model~\cite{Ferber2004} or more complex ones such as $\mathcal{M}OISE^{+}$~\cite{Hubner2002}. Organizational models can take into account aspects such as structural coordination, dynamic interactions, and the achievement of common objectives~\cite{Ferber2004, Abbas2015}. We call the \textbf{specifications} of an organization, the set of components used in an instance of an organizational model to specify the organization.

MAS designing/development methods, have been proposed jointly with organizational models to help designers finding suited specifications of an organization so that a MAS can reach a goal efficiently in a environment. Methods such as GAIA~\cite{Wooldridge2000}, ADELFE~\cite{Bernon2003} or DIAMOND~\cite{Jamont2005} lead to develop \textbf{self/re-organization} mechanisms from agents' rules (we call \textbf{policies}) enabling the MAS to adapt on the deployment environment.
These agents' policies are defined and fixed by the designer from Agent Centered Point of View (ACPV)/Organization Centered Point of View (OCPV) so that an optional emerging/chosen organization allows reaching a global goal~\cite{Picard2009}. It often takes place as an iterative process where designers are proceeding by trial and error. Despite the designer skills, that process may be hard and costly to converge towards a sufficiently estimated successful MAS. That process gets more difficult when the target deployment environment is not easily readable or handleable due to the complexity and internal safety policies such as for company infrastructure networks. Additionally, unexpected emergent phenomena may appear without giving guarantees for safety. As a notable environment, computers network with highly complex and non-visual interactions and the lack of intuitive comprehension of the environment can make MAS methods difficult to apply to develop a MAS that aims to ensure Cyberdefense by tackling the impact of malware programs.

% We assume an organization in a MAS can be understood regarding the Agent Centered Point of View (ACPV) vs. Organization Centered Point of View (OCPV) and agent's organization awareness vs. unawareness~\cite{Picard2009}.
% Typical examples are emergent MAS (ACPV and organization unawareness), coalition based MAS (ACPV and organization awareness), organization based MAS (OCPV and organization awareness), and Agent oriented engineering (OCPV and organization unawareness)~\cite{Picard2009}.
% We assume an \textbf{architecture} (also called organizational paradigm) is an abstract organization gathering a range of organizations sharing common characteristics~\cite{Horling2004}.

% Problem:

Instead of adopting a risky direct empiric approach, a common approach is to rely on a intermediary step by simulating the target deployment system, analogous to \textquote{digital twins}. Indeed, simulation can provide a monitoring framework that leaves room for a safe design process, while having an assessment of the resulting MAS designs. If the simulation is close enough to the target system in terms of fidelity, then we can expect the designed MAS to be transferred to the target system for indeed reaching the goals.
In that respect, creating a MAS that aims to reach a goal in any given environment, firstly focuses on finding a suited MAS design only in the associated simulation. We also, hypothesized the simulation to enable various collective AI techniques and to give, at least, raw description of the current environment and agents states.

Considering that context, there is no systematic available methodological approach to be used both at research and engineering levels that enables benefitting from the observation of emergent collective strategies in simulation for MAS designing. We must particularly address the need for automating the design process as much as possible while keeping the designer in the loop to ensure the resulting MAS is understandable and safe.

% Contribution

We propose a novel MAS design approach whose core uses a combination of Multi-Agent Reinforcement Learning (MARL) through Decentralized Partially Observable Markov Decision Process (Dec-POMDP) and the $\mathcal{M}OISE^+$ organizational model to generate suited agent's policies with respect to given specifications; and automatically generating the associated understandable organizational specifications. After having generated relevant OCPV specifications based on the environment, the initial design constraints, and goals; the approach consists in assisting the development of a safely built MAS in light of these specifications to better matches requirements and goals.

% Results

We applied our design approach to three spatial Atari games with various degree of cooperation among agents to achieve a goal the best while respecting initial organizational specifications as extra constraints. Obtained organization specifications are exploitable, coherent with expectations, and initial organizational specifications. We also applied our approach, in a Cyberdefense drone swarm environment whose resulting organizational specifications led to develop a MAS with scores comparable to the leading ones.

The remainder of the article is built as following.
In section II, while introducing the premises for the core of our approach, we expose related works we identified to those.
In section IV, we present our design approach that relies on Dec-POMDP and $\mathcal{M}OISE^+$ to help designers in designing a suited organization in their own MAS regarding environment, design constraints, and goals.
In section V, we discuss results obtained after applying our approach in the cooperative Atari games and the Cyberdefense environment.
In section VI, we conclude on the viability and relevance of our approach as we highlight limitations to overcome and future works as well.


\section{Related works and positioning}

% - Works related to individual agents' policy explainability with few links with social or collective interaction...
% - neural networks, bayesian networks...
% - Works related to the extraction of organizational specifications out of collective phenomena emergence through MARL or MAS algorithms on various environments...
% - For MARL: PPO in gfootball, MADDPG in prey-predator...
% - For MAS: Coalition-based MAS, Market-based MAS...

% - Yet, none of these explicitly address the issue of organization as implicit entity to be described in a bottom-up way or to be impacting agents' policies in a top-down way
% - Moreover, there is no systematic available methodological approach to be used both at research and engineering levels that enables benefitting from the observation of emergent collective strategies for MAS designing

% =====> On ne devrait pas parle d'explicabilité ici. Les travaux liés à ton probleme c'est les basics de l'organization, comment sont construites les actuellement les organisations, et des preconisations (la tu peux parler de tes outils MARL... 


We identified few works related to automated approaches that aim to assist the MAS design process to get sufficient efficiency while taking into account organizational aspects in a multiagent context. Therefore, we also extended our review to works dealing with to the links between organizational specifications and the emergence of collective phenomena through MARL or MAS algorithms on various environments.

% Collective explainable AI: Explaining cooperative strategies and agent contribution in multiagent reinforcement learning with shapley values
\cite{Heuillet2022} proposes a novel approach to explain cooperative strategies in multiagent reinforcement learning (RL) using Shapley values, a game theory concept used in eXplainable AI (XAI). The study aims to make deep RL more comprehensible and address the need for methods that provide better understanding and interpretability. The experimental results on Multiagent Particle and Sequential Social Dilemmas demonstrate the effectiveness of Shapley values in explaining the rationale behind decisions taken by agents. However, the article also highlights that Shapley values can only provide general explanations about a model and cannot explain specific actions taken by agents. The authors suggest that future work should focus on addressing these limitations. The study's implications extend to areas such as non-discriminatory decision making, ethical and responsible AI-derived decisions, and policy making under fairness constraints.

% Magent: A many-agent reinforcement learning platform for artificial collective intelligence
\cite{Zheng2018} presents MAgent, a platform for many-agent reinforcement learning (MARL) that aims to facilitate research on artificial collective intelligence. MAgent provides a flexible and efficient environment for training and evaluating MARL algorithms, enabling the study of complex multi-agent behaviors. The platform supports various scenarios, including cooperative, competitive, and mixed environments, and offers a high degree of scalability to accommodate a large number of agents. MAgent also includes a comprehensive set of baselines and evaluation metrics to benchmark the performance of MARL algorithms. The research contributes to the advancement of collective intelligence and the development of robust multi-agent systems.

% Self-Organized Group for Cooperative Multi-agent Reinforcement Learning
\cite{Shao2022} introduces a method called Self-Organized Group (SOG) for cooperative multi-agent reinforcement learning. In this approach, a certain number of agents are randomly elected to be conductors, and the corresponding groups are constructed with conductor-follower consensus, allowing the groups to be re-organized at regular intervals. The organized group under the unified command of a conductor is found to embed the multi-agent system with stronger zero-shot generalization ability compared to traditional methods. The SOG method provides strong adaptability to scenarios with varying numbers of agents and varying agent sight. The paper presents this approach as a mechanism to enhance cooperative multi-agent tasks with dynamic characteristics, aiming to improve the adaptability and generalization of multi-agent reinforcement learning systems

% A multi-agent reinforcement learning model of common-pool resource appropriation
\cite{Perolat2017} introduces a model that focuses on common-pool resource appropriation, a multi-agent social dilemma that includes issues such as sustainable use of fresh water, common fisheries, grazing pastures, and irrigation systems. The model emphasizes the importance of trial-and-error learning in addressing the challenges of common-pool resource sustainability and inequality. It explores the emergent behavior of groups of independently learning agents in a partially observed Markov game, shedding light on the relationship between exclusion, cooperation, and sustainability in the context of resource appropriation. The research highlights the potential of deep reinforcement learning in understanding and addressing complex societal and environmental challenges related to common-pool resource management. The paper provides valuable insights into the application of multi-agent reinforcement learning in the context of real-world social dilemmas and resource management

% MARLeME: A Multi-Agent Reinforcement Learning Model Extraction Library
\cite{Kazhdan2020} introduces MARLeME, a library designed to enhance the explainability of Multi-Agent Reinforcement Learning (MARL) systems by approximating them with symbolic models. The library aims to improve the interpretability of MARL systems, which is crucial for understanding the behavior of multiple agents interacting in a shared environment. By providing a means to extract symbolic models from MARL systems, MARLeME contributes to the advancement of explainable AI in the context of multi-agent systems.

% ROMA: Multi-Agent Reinforcement Learning with Emergent Roles
\cite{Wang2020} introduces a role-oriented MARL (Multi-Agent Reinforcement Learning) approach where roles are emergent, and agents with similar roles tend to share their learning and specialize in certain sub-tasks. The framework aims to combine the flexibility and adaptability of MARL with the concept of roles, allowing agents with similar roles to exhibit similar behaviors. The approach is designed to learn specialized, dynamic, and identifiable roles without relying on predefined role structures and behaviors. The research provides a novel perspective on the application of role-oriented MARL in complex multi-agent systems, offering potential advancements in the field of reinforcement learning.

% Promoting Coordination through Policy Regularization in Multi-Agent Deep Reinforcement Learning
\cite{Roy2020} addresses the challenge of inducing coordination between agents in multi-agent reinforcement learning. The research investigates the use of policy regularization to promote inter-agent coordination and discusses two approaches based on inter-agent modeling and synchronized sub-policy selection. The proposed methods are designed to improve cooperative behaviors without relying on explicit communication channels, allowing agents to exhibit coordinated behaviors during testing when acting in a decentralized fashion. The paper presents two policy regularization methods, TeamReg and CoachReg, and evaluates their performance on challenging cooperative multi-agent problems, showing improved results. The research contributes to the advancement of coordination-driven multi-agent approaches in reinforcement learning and provides valuable insights into promoting inter-agent coordination through policy regularization.

% Social Influence as Intrinsic Motivation for Multi-Agent Deep Reinforcement Learning
\cite{Jaques2019} proposes a mechanism for achieving coordination and communication in Multi-Agent Reinforcement Learning (MARL) by rewarding agents for having causal influence over other agents' actions. This causal influence is assessed using counterfactual reasoning, where agents simulate alternate actions to compute their effect on the behavior of other agents. The paper demonstrates that this approach leads to enhanced coordination and communication, as well as more meaningful learned communication protocols. The proposed method is shown to significantly increase the learning curves of the deep reinforcement learning agents, leading to more diversified team behavior and more successful performance of the population as a whole. The paper also highlights that the influence rewards for all agents can be computed in a decentralized way, opening up new opportunities for research in this area.

% Efficient multi-agent reinforcement learning through automated supervision
\cite{Chongjie2008} proposes a unified mechanism for achieving coordination and communication in Multi-Agent Reinforcement Learning (MARL). The approach involves training multiple agents to independently maximize their own individual reward without sharing weights. The paper introduces a method for automated supervision, which enables the agents to learn to coordinate and communicate effectively. This automated supervision mechanism leads to enhanced coordination, communication, and more meaningful learned communication protocols, ultimately improving the learning curves of the deep reinforcement learning agents and the overall performance of the agent population

% A unified framework for reinforcement learning, co-learning and meta-learning how to coordinate in collaborative multi-agent systems
\cite{Tosic2010} presents a comprehensive framework for addressing coordination in collaborative multi-agent systems. The framework integrates reinforcement learning, co-learning, and meta-learning to enable agents to learn how to coordinate effectively. By leveraging this unified approach, the paper aims to enhance the coordination and communication capabilities of multi-agent systems, ultimately improving their overall performance. 

Beside, these works explainability in RL is also widely considered in individual agents but are thought to be useful to understand the global organization. Among those works, most notable ones are:

Rule extraction from trained neural networks: involves obtaining human-interpretable rules that approximate the policy of the neural network. Various algorithms have been developed for this task, including decompositional, pedagogical, and eclectics approaches. These algorithms aim to provide comprehensible descriptions of the network's hypothesis that closely approximate its policy. For example, NN2Rules is a decompositional approach that extracts a set of decision rules from the parameters of the trained neural network model, making the decision rules more interpretable. Rule extraction algorithms enable neural networks to justify their classification responses using explainable classification rules, enhancing the transparency and interpretability of the models~\cite{Hailesilassie2016}~\cite{Sato2001}~\cite{Lal2022}.

Specification-Guided Reinforcement Learning (SGRL): addresses the problem of generating an optimal policy in reinforcement learning (RL) with respect to a given task in an unknown environment. Traditionally, the task is encoded in the form of a reward function, which can be cumbersome for long-horizon goals. An alternative approach is to use logical specifications, such as Linear Temporal Logic (LTL) and SpectRL, to define the task, opening the direction of RL from logical specifications. SGRL aims to synthesize control policies for robotic systems and other autonomous agents by leveraging formal logical constructs to express the task or objective. This approach has led to the development of highly performant algorithms that enable RL from logical specifications, enhancing the transparency and trustworthiness of RL systems~\cite{Bansal2022}~\cite{Jothimurugan2023}.

Learning from Logical Specifications: covers the broader area of learning from logical specifications, including the development of reinforcement learning algorithms that leverage the compositional structure of the specification to learn control policies for complex tasks~\cite{Jothimurugan2021}.

\section{An approach for addressing organization within MARL in MAS}

% - Organizational oriented MARL: a general research study focusing in integrating the organization in MARL at several aspects
% - DMO (Dec-POMDP MOISE+ OMARL): a class of algorithm falling into OMARL purposes that use MOISE+ as organizational model and Dec-POMDP as a MARL model
% - PAMID (Partially Action-based MOISE+ Identification DMO): a DMO algorithm that allows both getting the organizational specifications out of trained agents; and driving their training to satisfy extra design organizational specifications as constraints.


We propose the DMO process to integrate MAS organization designing process by translating the $\mathcal{M}OISE^{+}$ organizational model within the formalism used for MARL. Then, we formally describe how agents' policies and training process can be linked to the $\mathcal{M}OISE^{+}$ organizational model.

\subsection{MARL model}

The chosen MARL model is based on the Decentralized Partially Observable Markov Decision Process (Dec-POMDP)~\cite{Oliehoek2016} because it considers multiple agents in a similar MAS fashion. It relies on stochastic processes to model uncertainty of the environment for the changes induced by actions, in received observations, in communication\dots Additionally, unlike Partially Observable Stochastic Games (POSG), the reward function can be common to agents which fosters training for collaborative oriented actions~\cite{Beynier2013}.
A Dec-POMDP $d \in D$ (with $D$ the set of Dec-POMDP) is a 7-tuple $d = (S,\{A_i\},T,R,\{\Omega_i\},O,\gamma)$ , where:
\begin{itemize}
    \item $S = \{s_1, ..s_{|S|}\}$: The set of the possible states.
    \item $A_{i} = \{a_{1}^{i},..,a_{|A_{i}|}^{i}\}$: The set of the possible actions for agent $i$.
    \item $T$ so that $T(s,a,s') = \probP{(s'|s,a)}$ : The set of conditional transition probabilities between states
    \item $R: S \times A \times S \rightarrow \mathbb{R}$: The reward function
    \item $\Omega_{i} = \{o_{1}^{i},..,o_{|\Omega_{i}|}^{i}\}$: The set of observations for agent $ag_i$
    \item $O$ so that $O(s',a,o) = \probP{(o|s',a)}$ : The set of conditional observation probabilities.
    \item $\gamma \in [0,1]$, the discount factor
\end{itemize}

Considering $m$ \textbf{teams} (also referred as \textbf{groups}) each containing several agents among $\mathcal{A}$, we also detail the minimal formalism notation we re-used for solving the Dec-POMDP for a given team $i, 0 \leq t \leq m$ containing $n$ agents~\cite{Beynier2013,Albrecht2024}:

\begin{itemize}

    \item $\Pi$: the set of policies. A \textbf{policy} $\pi \in \Pi, \pi: \Omega \rightarrow A$ deterministically maps an observation to an action and is used in an agent;
    \item $\Pi_{joint}$: the set of joint-policies. A \textbf{joint-policy} $\pi_{joint} \in \Pi_{joint}, \pi_{joint} = \{\pi_1, \pi_2...\pi_n\}$ is a set of the policies used in agents;
    \item $H$: the set of histories. A \textbf{history} over $z \in \mathbb{N}$ steps (where $z$ is generally the maximum number of steps for an episode) is the $z$-tuple $h = ((\omega_{k}, a_{k}) | k \leq z, \omega \in \Omega, a \in A)$
    \item $H_{joint}$: the set of joint-histories. A \textbf{joint-history} over $z \in \mathbb{N}$ steps $h_{joint} \in H_{joint}, h_{joint} = \{h_1,h_2..h_n\}$ is the set of the agents' histories.
    \item $U_{joint,i}(<\pi_{joint,i}, \pi_{joint,-i}>): \Pi_{joint} \rightarrow \mathbb{R}$: gives the \textbf{expected cumulative reward} over a finite horizon (if $\gamma < 1$ or the number of steps in an episode is finite), with $\pi_{joint,i}$ the joint policy for team $i$ and $\pi_{joint,-i}$ all of the other concatenated joint-policies (considered as fixed);
    \item $BR_{joint,i}(\pi_{joint,i}) = argmax_{p_{joint,i}}(U(<\pi_{joint,i},\pi_{joint,-i}>))$: gives the \textbf{best response} $p_{joint,i}^*$ in the sense that the team cannot change any of the policies in the joint-policy $p_{joint,i}^*$ to get a better expected cumulative reward than $U_i^* = U_{joint,i}(<\pi_{joint,i}^*, \pi_{joint,-i}>)$;
    \item $SR_{joint,i}(\pi_{joint,i}, s) = \{\pi_{joint,i} | U(<\pi_{joint,i},\pi_{joint,-i}>) \geq s\}$: gives the \textbf{sufficient response} as the set of joint-policies getting at least $s \in \mathbb{R}, s \leq U_i^*$ as expected cumulative reward.
\end{itemize}

We refer to \textbf{solving} the Dec-POMDP for the team $t$ as finding a joint policy $\pi_{joint,i} \in \Pi_{joint}, \pi_{joint,i} = BR_{joint,i}(\pi_{joint,i})$ that maximize the expected cumulative reward over a finite horizon.
We refer to \textbf{sub-optimally solving} the Dec-POMDP at $s$ expectancy as finding a the joint policies $\pi_{joint,i} \in \Pi_{joint}, \pi_{joint,i} = SR_{joint,i}(\pi_{joint,i}, s)$ that gets the expected cumulative reward over a finite horizon at least at $s \in \mathbb{R}, s \leq U_i^*$.

We refer to \textquote{joint-history} as an approximation by observing received observation and played action. We note $h_{joint} \ in H_{joint}$ a \textbf{joint-history} so that $h_{joint} = {h \in H}$

\subsection{Organizational model}

Among, the existing organizational models \textquote{Agent/Group/Role}~\cite{Ferber2004} and $\mathcal{M}OISE^+$~\cite{Hubner2002} provide a relevant high-level description of the structures and interactions within the MAS. However, we favor $\mathcal{M}OISE^+$ because it provides an advanced formal description for an organization without incompatibilities with MARL, especially for formal description of agents' policies. It takes into account explicitly the social aspects between agents where \textquote{AGR} focuses on the integration of standards oriented towards design. Additionally, it provides a sufficiently detailed vision of organization to be understood at different point of views.
Based on $\mathcal{M}OISE^+$~\cite{Hubner2007} formalism, we only give the minimal elements of the formalism we used for our approach.

\paragraph{\textbf{Organization specifications (OS)}}: $OS = \langle SS, FS, DS \rangle$, the set of all organization specifications, where $SS$ are the \textbf{Structural Specifications}, $FS$ are the \textbf{Functional Specifications}, and $DS$ are the \textbf{Deontic Specifications}

\paragraph{\textbf{Structural Specifications (SS)}}: $SS = \langle \mathcal{R}, \mathcal{SG}, \mathcal{L}^{intra}, \mathcal{L}^{inter}, \mathcal{C}^{intra}, \mathcal{C}^{inter}, np, ng \rangle$, where:

\begin{itemize}
    \item $\mathcal{R}$: the set of non-abstract roles;
          
    \item $\mathcal{SG}$: the set of sub-groups;
          
    \item $\mathcal{L}$: the set of links. A link is a predicate $link(\rho_s,\rho_d,t) \in \mathcal{L}$, where $\rho_{s}$ is the link source, $\rho_{d}$ is the link destination, and $t \in\{acq, com, aut\}$ is the link type;
          \begin{itemize}
              \item If $t = acq$ (acquaintance), the agents playing the source role $\rho_{\mathrm{s}}$ are allowed to have a representation of the agents playing the destination role $\rho_{d}$;
              \item If $t = com$ (communication), the $\rho_{\mathrm{s}}$ agents are allowed to communicate with $\rho_{d}$ agents;
              \item If $t = aut$ (authority), the $\rho_{\mathrm{s}}$ agents are allowed to have authority on $\rho_{d}$ agents. It requires an acquaintance and communication link.
          \end{itemize}
    \item $\mathcal{L}^{intra}$: the set of intra-group links;
    \item $\mathcal{L}^{inter}$: the set of inter-group links;
          
    \item $\mathcal{C}$: the set of compatibilities. A compatibility $com(a,b) \in C$ (also noted $\rho_a \bowtie \rho_b$), means agents playing role $\rho_a$ can also play role $\rho_b$;
    \item $\mathcal{C}^{intra}$: the set of intra-group compatibilities;
    \item $\mathcal{C}^{inter}$: the set of inter-group compatibilities;
          
    \item $np: \mathcal{R} \rightarrow \mathbb{N} \times \mathbb{N}$: the relation giving the cardinality of each role;
    \item $ng: \mathcal{SG} \rightarrow \mathbb{N} \times \mathbb{N}$: the relation giving the cardinality of each sub-group.
          
\end{itemize}

\paragraph{\textbf{Functional Specifications (FS)}}: $FS = \langle SCH, PO \rangle$, where:

\begin{itemize}
    \item $SCH = \langle\mathcal{G}, \mathcal{M}, \mathcal{P}, mo, nm \rangle$: the set of \textbf{social scheme}, where:
          \begin{itemize}
              \item $\mathcal{G}$ is the set of global goal;
              \item $\mathcal{M}$ is the set of mission labels;
              \item $\mathcal{P}$ is the set of plans that builds the tree structure;
              \item $mo: \mathcal{M} \rightarrow \mathbb{P}(\mathcal{G})$: a function that specifies the mission set of goals;
              \item $nm: \mathcal{M} \rightarrow \mathbb{N} \times \mathbb{N}$ the cardinality of agents committed for each mission.
          \end{itemize}
    \item $PO$: the set of \textbf{preference order}. A preference order $pref(m_1, m_2)$ (also noted $m_{1} \prec m_{2}$) means if there is a moment when an agent is permitted to commit to $m_{1}$ and also $m_{2}$, it has a social preference for committing to $m_{1}$.
\end{itemize}

\paragraph{\textbf{Deontic Specifications (DS)}}: $DS = \langle OBL,PER \rangle$, the set of deontic specifications, where:

\begin{itemize}
    \item $TC$: the set of \textbf{time constraints}. A time constraint $tc \in TC$ specifies a set of periods during which a permission or obligation is valid;
    \item $OBL$: the set of \textbf{obligations}. An obligation $obl(\rho_a,m,tc)$ means a agent playing role $\rho_a \in \mathcal{R}$ is obliged to commit on mission $m \in \mathcal{M}$ for a given time constraint $tc \in TC$;
    \item $PER$: the set of \textbf{permissions}. A permission $per(\rho_a,m,tc) \in PER$ means a agent playing role $\rho_a \in \mathcal{R}$ is permitted to commit on mission $m \in \mathcal{M}$ for a given time constraint $tc \in TC$.
\end{itemize}


\subsection{Towards a DMO process linking MARL and Organizational model}

OMARL deals with all the processes linking MARL and Organization model as for being able to get information from joint-policies to build an organizational model, and constraining possible output joint-policies subset regarding organizational specifications. DMO is a first attempt to propose formally described OMARL processes using the Dec-POMDP for MARL and $\mathcal{M}OISE^+$ for the organizational model.

In a DMO process, we consider solving sub-optimally the Dec-POMDP $d \in D$ for a single team $i$ (comprising $n$ agents in $\mathcal{A}$) at a $s = U_i^* - \delta$ (with $\delta \in \mathbb{R}$) expectancy, we may obtain a set $S\Pi_{joint,s,i} = SR_{d,joint,i}(\pi_{joint,i},s) = \{\pi_{joint,i,s,1}, \pi_{joint,i,s,2} .. \pi_{joint,i,s,d}\}$ with the $S\Pi_{joint,s,i,k} \in \Pi_{joint}$ ($k \in \mathbb{N}, k \leq d$) and $SR_{d,joint,i}(\pi_{joint,i},s)$ gives the sub-optimal joint-policies at $s$ expectancy for the Dec-POMDP $d$ also noted $SR_{joint,i,s}(d)$. As an example, we may have $d$ different convergent joint-policies reaching a given expected cumulative reward after several training due to non-deterministic parameters in training.

Yet as an extra constraint, we only want joint-policies allowed by the initial specifications $OS_{init}$ we note $S\Pi_{joint,OS_{init}} = \{\pi_{joint,i,1}, \pi_{joint,i,2} .. \pi_{joint,i,b}\}$ with $b \in \mathbb{N}$ and the $\pi_{joint,k} \in \Pi_{joint}$ ($k \in \mathbb{N}, k \leq b$). Then, one can envision to use the sub-optimal joint-policies that satisfy the specifications $S\Pi_{joint,s,i} \cap S\Pi_{joint,OS_{init}}$ to infer some associated specifications $OS_{s,i}$ of the implicit resulting organization.

As a summary, we define an OMARL compliant DMO process if it can be described as a relation between the sub-optimal joint-policies set and associated organizational specifications set the following way:

\

\begin{relation}\label{rel:def_dmo}
    dmo: D \times OS_{init} \rightarrow (S\Pi_{joint,s,i} \cap S\Pi_{joint,OS_{init}}) \times OS_{s,i}
\end{relation}
Can be understood as for a given problem comprising a environment and a goal to be achieved by agents, and some design constraints; we can minimally get sufficiently successful trained agents that are compliant with given specifications and (at least partial) associated organizational specifications.

\

In \autoref{proof:to_extended_dmo}, we extended the \autoref{rel:def_dmo} DMO definition into another convenient form in \autoref{rel:def_extended_dmo}.

\

\begin{relation}\label{rel:def_extended_dmo}
    $\phantom{X}$ dmo: D \times OS_{init} \rightarrow (SR_{joint,i,s}[D] \cap sop[OS_{init}]) \times pos[SR[D] \cap sop[OS_{init}]]
\end{relation}

\begin{proof}\label{proof:to_extended_dmo}
    
    Let $dmo_{2}: D \times OS_{init} \rightarrow (SR_{joint,i,s}[D] \cap sop[OS_{init}]) \times \allowbreak pos[SR[D] \cap sop[OS_{init}]]$ \\
    
    By definition, $SR_{joint,i,s}[D] \cap sop[OS_{init}] = (S\Pi_{joint,s,i} \cap S\Pi_{joint,OS_{init}})$ \\
    
    By definition, $pos[\Pi_{joint, OS_{init}}] = OS_{s,i}$
    
    Hence, $dmo_{2}: D \times OS_{init} \rightarrow (S\Pi_{joint,s,i} \cap S\Pi_{joint,OS_{init}}) \times OS_{s,i}$
    
    Hence, $dmo_{2} \subseteq dmo$
\end{proof}

\noindent Where $pos$ and $sop$ are provided respectively in \autoref{rel:pos} and \autoref{rel:sop}.

\

\begin{relation}\label{rel:pos}
    pos: S\Pi_{joint,s,i} \rightarrow OS_{s,i}
\end{relation}
Represents how we can infer organizational specifications out of \textquote{trained} joint-policies.

\

\begin{relation}\label{rel:sop}
    sop: OS \rightarrow (S\Pi_{joint, OS_{init}})
\end{relation}
Represents how we can determine the joint-policies that satisfy some organizational specifications

\subsection{Proposed DMO process}

Continuing the previous decomposition in two activities, we detailed our process to get the specifications out of the agents' policies; and the process to get the joint-policies satisfying given specifications.

As a synthesis of these two processes, we propose the PAMID as a DMO compliant process presented in \autoref{alg:pamid}.

\RestyleAlgo{ruled}
\SetKwComment{Comment}{// }{}

\begin{algorithm}[hbt!]
    \caption{Partial Action-based $\mathcal{M}OISE^+$ Identification DMO (PAMID)}\label{alg:pamid}
    
    \KwData{$d \in D$, the Dec-POMDP to solve}  
    \KwData{$ep_{max} \in \mathbb{N}$, the maximum number of episodes}
    \KwData{$step_{max} \in \mathbb{N}$, the maximum number of steps per episodes}
    \KwData{$s \in \mathbb{R}$, the cumulative reward expectancy}
    \KwData{$\pi_{joint}$, the joint-policies to be trained}
    \KwData{$os_{init} \in OS_{init}$, the design specifications to respect}
    \KwData{$Solv: \Pi \times A \times O \times \mathbb{R} \rightarrow \Pi$, the MARL algorithm for updating a single policy in order to solve the Dec-POMDP}

    \KwResult{$(s\pi_{joint,s,i} \in S\Pi_{joint,s,i}, os_{s,i} \in OS_{s,i})$, the sub-optimal policies associated organization specifications}
    
    $s\pi_{joint,s,i} = PAMID-sop-training(s,i)$

    $os_{s,i} = PAMID-pos(s\pi_{joint,s,i})$
    
\end{algorithm}

\paragraph{\textbf{Infering OS from joint-policies}}

Seeking to implement a process represented by $pos$, we faced two main problems

First, for the agent $i$'s policy $\pi_{i} \in \pi_{joint}$, a minimal step to go further in the analyze of the policy is to know the $\{(\omega_k, a_k) \in \pi_{i}\}$ couples (the rules the agent follow). But it is not obvious to know them because of the explainability issues in black box policy approximation models such as NN-based ones.

To address that problem, rather than using directly agents' policy, we use the history as it may be built with observed resulting actions when observations are received during a series of test episodes. Indeed, for a given policy $\pi \in \Pi$ the associated history is by definition $h \in h_{joint} = ((\omega_k,a_k) | k \in \mathbb{N})$ and the $(\omega_k,a_k) \in \pi$. Taking into account several agents, joint-history can be seen as an approximation of the joint-policy that we refer to as reconstructed joint-policy $R\Pi_{joint} \subset \Pi_{joint}$. A reconstructed policy with a history is $r\pi_i = {(\omega, a) \in h | h \in H}$;

The second problem is linked to the difficulty to to infer information related to organizational specifications $OS$ such as roles, groups, goals\dots out of joint-histories (or reconstructed joint-policy).
To solve that difficulty we associate each action in MARL with some organization specifications as a \textquote{many to many} relation. It sets up a first frame for identifying organizational specifications in played action series. We address that problem in the remainder of this section.

We propose some relations between $\mathcal{M}OISE^+$ specifications and joint-histories. Their premises comes from noticing some specifications in the $\mathcal{M}OISE^+$ organizational model can be obviously mapped to subsets of actions from a single sub-optimal joint-policy, we propose the following relations:

\begin{itemize}
    \item $rh: \mathcal{R} \rightarrow \mathcal{P}(H)$: gives the histories associated with a role
    \item $sgh: \mathcal{SG} \rightarrow \mathcal{P}(H)$: gives the histories associated with a subgroup
    \item $lh: \mathcal{L} \rightarrow \mathcal{P}(H)$: gives the histories associated with an intra link
    \item $gh: \mathcal{G} \rightarrow \mathcal{P}(H)$: gives the histories associated with a global goal
    \item $mh: \mathcal{M} \rightarrow \mathcal{P}(H)$: gives the histories associated with a mission label
\end{itemize}

From these relation, we propose to rely on the previous relations through \emph{pamid-pos} presented in \autoref{alg:pamid-pos} that is consistent with $pos$.

\RestyleAlgo{ruled}
\SetKwComment{Comment}{// }{}

\begin{algorithm}[hbt!]
    \caption{PAMID-pos}\label{alg:pamid-pos}
    \KwData{$\pi_{joint}$, the joint-policies to be analyzed}
    \KwResult{$os \in OS$, the associated organization specifications}
    
    $h_{joint} = testEpisode(\pi_{joint})$
    
    \ForEach{$h_{joint,i} \in h_{joint}$}{
        \ForEach{$h_{j} \in h_{joint,i}$}{
        
            $r_j = \langle r, determineRoles(h_{j}) \rangle$
            
            $sg_j = determineSubgroups(h_{j})$
            
            $l_j = determineIntraLinks(h_{j})$
            
            $g_{ind,j} = determineIndGoals(h_{j})$
            
            $m_{ind,j} = determineIndMissions(h_{j})$
            
            $p_{ind,j} = determineIndPlans(h_{j}, m_{ind,j}, g_{ind,j})$
            
        }
        
        $g = determineGoals(g, h_{joint,i}, {g_{ind,j}})$
        
        $m = determineMissions(m, h_{joint,i}, {m_{ind,j}})$
        
        $p = determinePlans(p, h_{joint,i}, {p_{ind,j}})$
        
        $mo = determineMissionToGoals(mo, g_i, m_i)$
    }
    
    $ng = determineSgCard(\{r_j\}_i)$
    
    $np = determineRoleCard(\{r_j\}_i)$
    
    $c = determineIntraCompatibilities(\{r_j\}_i)$
    
    $per = determinePermissions(\{r_j\}_i, m)$
    
    $obl = determineObligations(\{r_j\}_i, m)$
    
    $nm = determineAgentCardPerMission(mo)$
    
    $ss = \langle \{r_j\}_i, \{sg_j\}_i, \{l_j\}_i, \emptyset, c, \emptyset, np, ng \rangle$
    
    $fs = \langle {g, m, p, mo, nm}, \emptyset \rangle$
    
    $ds = \langle per, obl \rangle$
    
    $os = \langle ss, fs, ds \rangle$
    
\end{algorithm}

As we have only one group, we do not consider the inter-links and inter-compatibilities. Additionally as a simplification, we consider only one social scheme. As a general remark, the \textquote{determine*} relations are not all fully described and rely on custom implementation (cf. \autoref{gym-wrapper} for further information).

PAMID-pos first look at the individual level.
First, it tries to figure out the roles played by agents ($determineRoles$) by sampling history sub-sequences $h \in H$ and comparing with known history sub-sequences whose we know the associated role via $ra$.
Then, it tries to get the intra links the agent has with other agents. ($determineIntraLinks$ and $determineInterLinks$) via $lh$.
Then, it tries to known the subgroup where the agent is ($determineSubgroup$) via $sgh$.
Then, it tries to know the individual goals the agent aim to ($determineIndGoals$) via $gh$.
Then, it tries to know the mission the agent is committed to ($determineIndMissions$) via $mh$.
Finally, it tries to build a part of the plan the agent is playing at its individual level ($determineIndPlans$) via $ph$ and the inferred individual goal and the inferred individual mission.

Then PAMID-pos look at the social and collective level.
At the end of each joint-policy we try to reinforce a global view of the goals $g$, missions $m$, plans $p$, and the knowledge of the mission to goal $mo$; with the partially inferred information at the individual level.

At the end of the algorithm, it tries to synthesize the knowledge infered until know to have better view of the number of agent per subgroup $ng$, the agent cardinality per mission $nm$, the role cardinality $np$, the compatibilities between roles $c$, the permissions and obligations $per$ and $obl$.


\paragraph{\textbf{Constraining joint-policies satisfying OS}}

\

We consider a given MARL algorithm that iteratively converges towards a joint-policy so that each agent's policy is updated at each step until a finite horizon. In \autoref{proof:jpc_to_ac} we proved that constraining the available action set for each agent to the action sets authorized by the organization specifications at each step of the MARL training; implies constraining the converged joint-policies to the ones that satisfy the given organization specifications. We used that result, to setup our $sop$ used jointly with MARL learning through \emph{PAMID-sop-training} presented in \autoref{alg:pamid-sop-training}.

\RestyleAlgo{ruled}
\SetKwComment{Comment}{// }{}

\begin{algorithm}[hbt!]
    \caption{PAMID-sop-training}\label{alg:pamid-sop-training}
    \KwData{$d \in D$, the Dec-POMDP to solve}
    \KwData{$ep_{max} \in \mathbb{N}$, the maximum number of episodes}
    \KwData{$step_{max} \in \mathbb{N}$, the maximum number of steps per episodes}
    \KwData{$s \in \mathbb{R}$, the cumulative reward expectancy}
    \KwData{$\pi_{joint}$, the joint-policies to be trained}
    \KwData{$os_{init} \in OS_{init}$, the design specifications to respect}
    \KwData{$solv: \Pi \times A \times O \times \mathbb{R} \rightarrow \Pi$, the MARL algorithm for updating a single policy in order to solve the Dec-POMDP}
    \KwResult{$s\pi_{joint} \in S\Pi_{joint,s,i}$, the sub-optimal joint-policies that satisfy the design specifications}

    $forbidden = \Pi_{joint} \setminus (DMO-pos^{-1}(os_{init}))$

    $s\pi_{joint} = \{\}$

    \ForEach{$1 \leq ep \leq ep_{max}$}{

        $h_{joint} = \{\emptyset^n\}$

        $\omega_{0} = \emptyset$

        $a_{0} = \emptyset$

        \ForEach{$1 \leq step \leq step_{max}$}{

            \ForEach{$\pi_i \in \pi_{joint}$}{
                
                $\omega_{step} = getObservation()$

                $r_{step} = getReward()$

                $\pi_{i,step} = Solv(\pi_{i,step-1}, \omega_{step-1}, a_{step-1}, r_{step})$  \Comment*[r]{Update the individual policy}

                $A_{step} = \{a | (\omega_{step}, a) \in \pi, \pi \in \pi_{joint},\pi_{joint} \notin forbidden, \{(\omega', a') \in h\} \in \pi\}$ \Comment*[r]{Restricted available actions}

                $a_{step} = \pi_{i,step}(\omega_{step})$ \Comment*[r]{Choose next action}

                $h_{joint,i} = \langle h_{joint,i}, (\omega_{step}, a_{step}) \rangle$ \Comment*[r]{Update the history}

            }

        }
    }

    $s\pi_{joint} = \{\pi_{joint}\}$
    
    
\end{algorithm}

\emph{PAMID-sop-training} first converts the $os_{init} \in OS_{init}$ design specifications into a $forbidden$ set of the non-authorized joint-policies. It uses a non-described reverse \emph{PAMID-pos} algorithm to achieve it.

Then, \emph{PAMID-sop-training} uses a given MARL algorithm to update the agents' individual policies with previous action, observation and resulting reward. Then, it first computes the authorized actions set $A_{step}$ according to the current history $h_{joint,i}$. Then, an action is chosen among authorized actions. That action $a_{step}$ is added in history to be used for updating the agent's policy in the next step. 
Finally, considering only one iteration of episode, it returns a sub-optimal joint-policy $\pi_{joint}$ satisfying $os_{init}$
We can note the $os_{init}$ could forbid the MARL algorithm to provide a joint-policy that reaches a expected cumulative reward.


\section{An approach for designing MAS integrating MARL within an organization model}

% ====> le titre devrait plutot etre le nom de ton approche... Le titre me plait pas du tout car ton objectif c'est pas d'integrer le MARL dans l'organisation... c'est juste ce qui te parait etre le meilleur moyen pour proposer une approche pour xxxxxxxxxxxxxxxxxx. C'est le xxxxxxxxxxxxxxxxxx qui est important.

% - We proposed PAMID (Partially Action-based MOISE+ Identification DMO): an algorithm that allows both getting the organizational specifications out of trained agents; and driving their training to satisfy extra design organizational specifications as constraints.
% - [Algo PAMID...]

% - We developped PAMID as a Gym-wrapper to help in applying the proposed MAS design approach...
% - [Lien vers dépôt GitHub]
% - Description des fonctionalités

% - The general approach workflow follows:
% 1) Reproducing target environment in simulation
% 2) Defining the reward function so agents aim to achieve the goals
% 3) Adding agents to be trained to solve the previously defined problem
% 4) Launching the training in simulation with PAMID
% 5) Getting the raw organizational specifications with PAMID
% 6) Curating these to produce a safe MAS model
% 7) Using that model to implement a proper MAS
% 8) Check viability and requirements respect in simulation
% 9) Deploy that MAS in the target system

% \begin{itemize}
%     \item Questions at the Multi-Agent System level (system-centric approach)
%           \begin{itemize}
%               \item Number of agents, what heterogeneity?
%               \item What is the common medium (Environment) shared by the agents?
%               \item What communication mechanisms are available to agents?
%               \item What are the communication languages, ontologies, interaction protocols used by the agents?
%               \item What is the organization within which the agents operate? How is it established?
%               \item How do the agents coordinate their actions? How to ensure coherent operation?
%           \end{itemize}

%     \item Agent level questions (agent-centered approach)
%           \begin{itemize}
%               \item What does an agent represent? What actions should be encapsulated in an agent?
%               \item How do agents represent the environment and organization in which they operate?
%               \item How do agents handle interactions with other agents?
%               \item What is the internal structure of the agents?
%           \end{itemize}
% \end{itemize}


\begin{figure}[h!]
  \centering
  \include{figures/approach}
  \caption{A summary view of our approach to MAS design}
  \label{fig:design_approach}
\end{figure}

Previous section formally described links between Dec-POMDP and $\mathcal{M}OISE^+$. Based on that idea, we introduce an approach for MAS design that enables constraining agents possible policies during training. Additionally, it allows extracting organization specifications from policies. An advantage of that approach is that only rely on \textquote{observed policies} we call \textquote{history} without considering the approximation function used such as neural networks.

Given a set of untrained policies within agents that are to be trained in a simulated environment, we can guide their training according to abstract organization specifications and get the non-abstract organization specifications following these steps:
\begin{enumerate}
  \item Labeling each action with related known specifications;
  \item Defining the training specifications befor launching a simulation of several episodes;
  \item Determining the action subspaces associated with each agent according to the training specifications
  \item Training agents with regular algorithms;
  \item Extracting organizational specifications out of the observed trained agents' policies in a single episode run;
  \item Curating the raw specifications to get safe and clean specifications for an organization to be implemented
  \item Implementing the curated organization and checking its validity
\end{enumerate}

An illustrative representation of our approach is presented in Figure \ref{fig:design_approach}. The organizational model $\mathcal{M}OISE^{+}$ is related to the policies learned in an \textquote{Organization Oriented MARL} (OOMARL) process. In this process, we seek to define the structural, functional and deontic specifications of $\mathcal{M}OISE^{+}$ with respect to the learned policies (1). For example, an analysis of the behavior of trained agents on the type of action and frequency can help determine their role. Conversely, the specifications of the organization constrain the space of possible agent policies (2). Some of the actions labeled for particular roles may or may not be accessible to an agent depending on the assigned role.

The core of this approach can be expressed as follows:\begin{equation} %\label{eq1}
  Design: T \cross R \cross Agent \cross S_{init} \rightarrow S \cross \Pi \cross S
\end{equation}

Where the environment and its constraints (represented by the state transition function $T$), the cyber defense objectives (represented by the reward function $R$), a set of agents ($Agents$) and the initial specifications ($S_{init}$) of $\mathcal{M}OISE^{+}$ are associated with the policies of the trained agents $\Pi$ with the associated organizational specifications $S$.

The interest is to provide at least the designer with indications on the specifications of an organization capable of satisfying the constraints of the environment and initial design. In light of these initial results, the designer can decide to propose specifications which he can ensure meet the safety and explainability requirements. These specifications then serve as a blueprint for implementing a MAS as described in Figure \ref{fig:design_approach}.

\paragraph{\textbf{OOMARL Gym Wrapper}\label{gym-wrapper}} We developed a \textquote{Gym Wrapper} to help in automate the setting up of OOMARL for a given Gym Environment. It enables linking actions with $\mathcal{M}OISE^+$ specifications, define the training specifications and provide functions to extract the resulting sub-optimal raw organizational specifications.

\section{Validation and application for cooperative game environments}

% =====> tu précices que ce que tu veux valider, le protocole expérimental que tu mets enn oeuvre pour ça et les résultats.
%  - We successfully assessed our approach in "Prey-pretador with communication" or "Knights, archers and zombies"
%    - Resulting PAMID organizational specification are indeed matching the expected ones
%    - Learning in MARL with PAMID is indeed impacted by constraining organizational specifications and allows designer to take hand on the way agents' learn at an organization level

%  - We also applied our approach the 3rd CAGE Challenge that consist in creating a suited MAS to protect an ad hoc drone swarm network being cyber-attacked by malware programs for it meets inherent embedded and distributed properties
%    - We used the MARL algorithms that give the best score in the end of the challenge
%    - Most resulting PAMID organizational specifications outlines some expected collective strategies
%    - The curated MAS model and the associated implemented MAS are giving equivalent scores by comparing with finalists' MARL algorithms

We selected 4 environments to assess our approach: \textquote{Predator-prey with communication}~\cite{Lowe2017},
%\textquote{DeepMind MuJoCo Multi-Agent Soccer Environment}~\cite{Liu2019},
\textquote{Google Research Football Environment}~\cite{Kurach2020},\textquote{Pistonball}~\cite{Terry2021}, and \textquote{Knights Archers Zombies}~\cite{Terry2021}.

% We selected 9 environments to assess our approach: \textquote{Predator-prey with comm}, \textquote{simple reference}, \textquote{simple speaker listener}, \textquote{DeepMind MuJoCo Multi-Agent Soccer Environment}, \textquote{Pistonball}, \textquote{Waterworld}, \textquote{Emtombed: Cooperative}, \textquote{Cooperative Pong}, \textquote{Knights Archers Zombies}, \textquote{Moving Company}, \textquote{Captain-sailor}.

We applied our approach in three cases:
\begin{itemize}
    \item No training specifications (NTS)
    \item Partially constraining training specifications (PTS)
    \item Fully constraining training specifications (FTS)
\end{itemize}

To evaluate the impact of our approach in training we payed attention to the following criteria: convergence time, stability, global performance, change sensibility. Results are presented in Table~\ref{tab:training_OOMARL_results}.

\input{tables/training_OMARL_results.tex}

We also took into account criteria after training: roles, links, compatibilities, social schemes. Results are presented in Table~\ref{tab:trained_OOMARL_results}

\input{tables/trained_OMARL_results.tex}

\section{Conclusion}

% =======> demande toi avant de rédiger la conclusion "Qu'est ce qui change dans l'état de l'art avec ce papier ?"... si tu avais lu ce papier et qu'il n'était pas de toi qu'est ce que tu te serai dis 

%  - Need for easing the MAS design by assisting human designer
%  - We proposed a novel general MAS design approach that consists in getting relevant organizational specifications out of agents in training satisfying design constraints. These insights allow implementing a suited safe MAS to be deployed on target system
%  - We validated that approach for various different environments including in a Cyberdefense context.
%  - Main perspective is to improve the reliability of PAMID between organizational specifications and agents' training satisfying to design constraints.
%  - Another future work is to reduce the manual parts of our approach with implementation of a full integrated development environment.

This article intends to address the need for easing the MAS designing when it can not be easily conducted by human designers due to the high complexity, low readability, highly changing and uncertainity of some environments. Our premise is to link good results obtained with MARL with some organizational model in order to get a human-readable idea of some suited organization in MAS to guide a safe monitored MAS design. Literature shows works have been provided focusing on emergent collective strategies using RL techniques or providing algorithms to make them emerge. Yet, we were unable to connect our idea with identified works as for using both MARL and an organizational model towards a MAS design approach. First, we formalized our idea for linking MARL with organizational model through Dec-POMDP and $\mathcal{M}OISE^+$. Relying on that formalism, we exposed the approach for MAS design using the link between MARL and organizational model. It resulted into a "Gym wrapper" for constraining agents' training according to their training organizational specifications and allowing to get the organizational specifications of the general OE in different runs. Finally, we applied our approach in four Gym environments to check the impact on training, explainability and viability for our MAS design approach. Despite, being agnostic of the policy approximation functions, we think worth to investigates on linking organizational model with those directly for they could be adapted to better match the frame of organizational models. Furthermore, it enables envisioning addressing the design of MAS with organization awareness.


%
% ---- Bibliography ----
%
% BibTeX users should specify bibliography style 'splncs04'.
% References will then be sorted and formatted in the correct style.
%
% \bibliographystyle{splncs04}
% \bibliography{mybibliography}
%
\section*{References}

% \bibliographystyle{abbrv}
\bibliographystyle{splncs04}

\bibliography{references}

\newpage

\section*{Annexes}

\subsection*{Action constraining during training implies result joint-policy constraining}
\input{proofs/jpc_to_ac.tex}

\end{document}
