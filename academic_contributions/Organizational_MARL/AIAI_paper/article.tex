\documentclass[runningheads]{llncs}

\usepackage[T1]{fontenc}
\usepackage{graphicx}
%\usepackage{color}
%\renewcommand\UrlFont{\color{blue}\rmfamily}

\usepackage{amsmath,amssymb,amsfonts}
\usepackage[inline, shortlabels]{enumitem}
\usepackage{tabularx}
\usepackage{caption}
\usepackage{listings}
% \usepackage{titlesec}
\usepackage[english]{babel}
\captionsetup{font=it}
\usepackage{ragged2e}
\usepackage[hyphens]{url}
\usepackage{hyperref}
\usepackage{xurl}
\usepackage{pifont}
\usepackage{footmisc}
\usepackage{multirow}
\usepackage{enumitem}
\usepackage{algorithm2e}
\usepackage{float}
\usepackage{listings}
\usepackage{xcolor}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}
 
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}
 
\lstset{style=mystyle}

% --- Tickz
\usepackage{physics}
\usepackage{amsmath}
\usepackage{tikz}
\usepackage{mathdots}
\usepackage{yhmath}
\usepackage{cancel}
\usepackage{color}
\usepackage{siunitx}
\usepackage{array}
\usepackage{multirow}
\usepackage{amssymb}
\usepackage{gensymb}
\usepackage{tabularx}
\usepackage{extarrows}
\usepackage{booktabs}
\usetikzlibrary{fadings}
\usetikzlibrary{patterns}
\usetikzlibrary{shadows.blur}
\usetikzlibrary{shapes}

% ---------
% \usepackage{titlesec}
\usepackage{pdfpages}
\usepackage{booktabs}
\usepackage{csquotes}
\usepackage{lipsum}  
\usepackage{arydshln}
\usepackage{smartdiagram}
\usepackage[inkscapeformat=png]{svg}
\usepackage{textcomp}
\usepackage{tabularray}\UseTblrLibrary{varwidth}
\usepackage{xcolor}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\usepackage{cite}
\usepackage{amsmath}
\newcommand{\probP}{\text{I\kern-0.15em P}}
\usepackage{etoolbox}
\patchcmd{\thebibliography}{\section*{\refname}}{}{}{}

\setlength{\extrarowheight}{2.5pt}

\renewcommand{\arraystretch}{1.7}

\setlength{\extrarowheight}{2.5pt}
\renewcommand{\arraystretch}{0.2}
\renewcommand{\arraystretch}{1.7}

% --------------
% \titleclass{\subsubsubsection}{straight}[\subsection]

% \newcounter{subsubsubsection}[subsubsection]
% \renewcommand\thesubsubsubsection{\thesubsubsection.\arabic{subsubsubsection}}
% \renewcommand\theparagraph{\thesubsubsubsection.\arabic{paragraph}} % optional; useful if paragraphs are to be numbered

% \titleformat{\subsubsubsection}
%   {\normalfont\normalsize\bfseries}{\thesubsubsubsection}{1em}{}
% \titlespacing*{\subsubsubsection}
% {0pt}{3.25ex plus 1ex minus .2ex}{1.5ex plus .2ex}

% \makeatletter
% \renewcommand\paragraph{\@startsection{paragraph}{5}{\z@}%
%   {3.25ex \@plus1ex \@minus.2ex}%
%   {-1em}%
%   {\normalfont\normalsize\bfseries}}
% \renewcommand\subparagraph{\@startsection{subparagraph}{6}{\parindent}%
%   {3.25ex \@plus1ex \@minus .2ex}%
%   {-1em}%
%   {\normalfont\normalsize\bfseries}}
% \def\toclevel@subsubsubsection{4}
% \def\toclevel@paragraph{5}
% \def\toclevel@paragraph{6}
% \def\l@subsubsubsection{\@dottedtocline{4}{7em}{4em}}
% \def\l@paragraph{\@dottedtocline{5}{10em}{5em}}
% \def\l@subparagraph{\@dottedtocline{6}{14em}{6em}}
% \makeatother

% \setcounter{secnumdepth}{4}
% \setcounter{tocdepth}{4}
% --------------

\newcommand{\before}[1]{\textcolor{red}{#1}}
\newcommand{\after}[1]{\textcolor{green}{#1}}

\newcommand{\old}[1]{\textcolor{orange}{#1}}
\newcommand{\rem}[1]{\textcolor{red}{#1}}
\newcommand{\todo}[1]{\textcolor{orange}{\newline \textit{\textbf{TODO:} #1}} \newline \newline }



\newcounter{relation}
\setcounter{relation}{0}
\renewcommand{\therelation}{\arabic{relation}}
\newcommand{\relationautorefname}{Relation}

\newenvironment{relation}[1][]{%
    \refstepcounter{relation}%
    \noindent \raggedright \textit{\textbf{Relation. \therelation}} \hfill$}
{%
$ \hfill \phantom{x}

}

\newcounter{proof}
\setcounter{proof}{0}
\renewcommand{\theproof}{\arabic{proof}}
\newcommand{\proofautorefname}{Proof}

\renewenvironment{proof}[1][]{
    \refstepcounter{proof}
    \noindent \raggedright \textit{\textbf{Proof. \theproof}}

    \setlength{\leftskip}{1em}

}
{

\
\setlength{\leftskip}{0pt}
}

% --------------------------------
%             DOCUMENT
% --------------------------------

\begin{document}
\title{A MARL-based Approach for Easing MAS Organization Engineering}
%
%\titlerunning{Abbreviated paper title}
% If the paper title is too long for the running head, you can set
% an abbreviated paper title here
%
\author{Julien Soulé\inst{1}\orcidID{0000-1111-2222-3333} \and
    Jean-Paul Jamont\inst{1}\orcidID{1111-2222-3333-4444} \and
    Michel Occelo\inst{1}\orcidID{2222--3333-4444-5555} \and
    Louis-Marie Traonouez\inst{2}\orcidID{2222--3333-4444-5555} \and
    Paul Théron\inst{3}\orcidID{2222--3333-4444-5555}}
%
\authorrunning{J. Soulé et al.}
% First names are abbreviated in the running head.
% If there are more than two authors, 'et al.' is used.
%
\institute{Univ. Grenoble Alpes, Grenoble INP, LCIS, 26000, Valence, France
    \email{\{julien.soule, jean-paul.jamont, michel.occello\}@lcis.grenoble-inp.fr}
    \and
    Thales Land and Air Systems, BU IAS, Rennes, France
    \email{louis-marie.traonouez@thalesgroup.com}
    \and
    AICA IWG, La Guillermie, France \\
    \email{paul.theron@orange.fr}
}


\maketitle              % typeset the header of the contribution

% MAS have been succefully
% For many MAS, the organization has become a critical success factor. 
% Several related methods exist to design MAS. 
% However, these methods are ...
% To enhance the quality and effectiveness of ... this paper presents an assisted approach for MAS Organization Engineering (AMOEA). 
% AMOEA guides the designer of...
% 1 phrase par contrib/key point

\begin{abstract}

    % context
    Multi-Agent Systems (MAS) have been successfully applied due to their ability to address complex, distributed problems especially in IoT based systems.
    Their efficiency regarding respective objectives and requirements is strongly dependant on the organization for engineering an application-specific MAS.
    Methods to develop MAS have been proposed relying on the knowledge of the deployment environment in order to design organization that can achieve the given goals.
    % Yet, in some cases, the deployment environment is not easily readable or handleable due to the complexity and may lead to unexpected emergent phenomena raising safety concerns.
    Yet, high complexity and low readability in deployment environments may lead the application of these methods to be costly or to raise safety concerns.
    % That stresses out the need for methodological works for assisted MAS design that could be addressed with collective AI techniques.
    % hypothesis / contribution
    In order to ease the organization designing regarding those concerns, we propose an novel Assisted MAS Organization Engineering Approach (AOMEA). AOMEA relies on linking a Multi-Agent Reinforcement Learning (MARL) process with an organizational model to suggest relevant organizational specifications to help in the MAS engineering.
    % We introduce , an novel design approach to assist the MAS design whose underlying idea is to use Multi-Agent Reinforcement Learning with organizational specifications for both understanding and constraining the training process regarding design constraints.
    % results
    % We applied our approach in cooperative Atari games and a Cyberdefense drone swarm scenario of the 3rd CAGE Challenge. Obtained specifications are indeed consistent with design constraints and provide insights of relevant collective strategies that led to develop an explainable MAS with scores close to finalists' ones.

    \keywords{Multi-Agent Systems \and Organization \and Design \and Assisted engineering}
\end{abstract}

\section{Introduction}

% Context:

Multi-agent systems (MAS) have gained significant interest in engineering and industry due to their ability to address complex, distributed problems. That paradigm enables decomposing a complex task into smaller ones delegated to autonomous cooperative agents that achieve them through re/self-organization mechanisms. Most notably, they provide ways to handle conflicting goals, parallel computation, system robustness, and scalability. The applications of MAS are diverse, including collective robotics, Vehicular Ad Hoc Network (VANET), Mobile Ad Hoc Network (MANET) or Wireless Sensor Network (WSN)\cite{Oliveira1999, Gembarski2020}.
In MAS, organization is a fundamental concept that impact how agents coordinate their activities to collaboratively achieve a common goal~\cite{Hubner2002}.
Organizational aspects are crucial for addressing the challenge of MAS design in dynamic and uncertain environments, where runtime behavior needs to be flexible~\cite{Kathleen2020}. The impact organization on MAS design is core in methodologies and frameworks that enable the engineering of application-specific MAS~\cite{Bakliwal2018}.

MAS designing/development methods, have been proposed jointly with organizational models to help designers finding suited specifications of an organization so that a MAS can reach a goal efficiently in a environment. Methods such as GAIA~\cite{Wooldridge2000}, ADELFE~\cite{Bernon2003} or DIAMOND~\cite{Jamont2005} lead to develop \textbf{self/re-organization} mechanisms from agents' rules (we call \textbf{policies}) enabling the MAS to adapt on the deployment environment.
These agents' policies are defined by the designer from Agent Centered Point of View (ACPV) or Organization Centered Point of View (OCPV) so that an optional emerging/chosen organization allows reaching a global goal~\cite{Picard2009}. It often takes place as an iterative process where designers are proceeding by trial and error. Despite the designer skills, that process may be hard and costly to converge towards a sufficiently estimated successful MAS. That process gets more difficult when the target deployment environment is not easily readable or handleable due to its complexity and may raise unexpected emergent phenomena.

As an example, research in Autonomous Intelligent Cyberdefense Agents~\cite{Kott2023} (AICA) aims to develop cooperative cyber-defense agents deployed in computer networks including highly complex ones and with non-visual interactions. Additionally, the lack of intuitive comprehension of the environment can make existing MAS methods difficult to apply regarding internal safety policies such as for company infrastructure networks.

% Problem:

Instead of adopting a risky direct empiric approach, some available methodological approaches rely on a intermediary step by simulating the target deployment system, analogous to \textquote{digital twins}. Indeed, simulation can provide a monitoring framework that leaves room for a safe design process, while having an assessment of the resulting MAS designs. If the simulation is close enough to the target system in terms of fidelity, then we can expect the designed MAS to be transferred to the target system for indeed reaching the goals.

However, none of these methods provide general automated ways to guide the designing process of a MAS that has to achieve a given goal regarding the deployment environment constraints and design requirements in light of results obtained in simulation. Indeed, there is a need not only for automatically finding suited agents' policies that lead to sufficient performance which is generally done through MARL; but also to make the implicit organizational mechanisms that emerges form trained agents, explicit to the designer point of view hence enabling to consider both explainability and safety concerns in the MAS engineering.

% Contribution

In order to address that issue, we introduce AMOEA, a novel general MAS design approach whose underlying idea is to link a given MARL process with an organizational model that links the on-training agents' policies with explicit organizational specifications. It can be viewed as a tool for engineering to automatically generate relevant exploitable organizational specifications only regarding the performance in achieving the given global goal and the satisfaction of the given designer's constraints. For the designer, the obtained organizational specifications can be used as insights of the organizational mechanisms to set up during the development towards a MAS that meets performance, explainability and safety requirements.

% Results

We applied AOMEA in three spatial Atari games with various required degree of cooperation among agents so they achieve a goal the best; and additionally respecting organizational specifications as design constraints. Obtained organizational specifications are indeed exploitable, coherent with expectations, and respect design constraints.
%We also applied our approach, in a Cyberdefense drone swarm environment whose resulting organizational specifications led to develop a MAS with scores comparable to the leading ones.

Section II starts introducing the theoretical background of AOMEA as for the key role of organization in the development of a MAS and available methodological works and the motivation for integrating a MAS organizational model into a MARL process in order to enhance the MAS design process. In section III, we globally introduce AOMEA before explaining the theoretical core the approach is built on. Then, we detail how AOMEA can be applied in MAS engineering through a proposed implementation. We assessed AOMEA in 4 simulated environments discussing obtained results in section IV. Finally, section V concludes on the viability and relevance of our approach as we highlight limitations to overcome and future works as well.

% ====================================================================================================

\section{Theoretical background}

% // Mettre en avant les briques du raisonnement en expliquant le titre pour préparer l'introduction de la contribution avec AOMEA sans les justifier (sans faire de comparaison avec l'existant, pas d'édt, dire juste les points forts)

% Organization
%   -> moise (justifier parmi les existants)

% MAS methodologies (ALAADIN, GAIAI mais pas de moyens pour trouver une organisation automatiquemenet)

% MARL (basiques) // DECPOMDP (basiques)


Developing a Multi-Agent System (MAS) involves various engineering methods and methodologies that are designed to address the complexities inherent in the design, implementation, and deployment of such systems. These methods are essential for ensuring that MAS can effectively coordinate, communicate, and execute tasks in a distributed and often dynamic environment.
Several comprehensive methodologies have been proposed for MAS development, including:

\begin{itemize}
    \item \emph{Tropos}: An agent-oriented software development methodology that emphasizes early requirements analysis and the continuous refinement of these requirements through the design and implementation phases~\cite{Bresciani2004}.
    \item \emph{Gaia}: A methodology for the analysis and design of MAS, focusing on the organizational structure of the system~\cite{Zambonelli2003}.
    \item \emph{INGENIAS}: A methodology that provides a suite of tools and a formal framework for the development of MAS~\cite{Pavon2003}.
    \item \emph{MaSE (Multi-agent Systems Engineering)}: A methodology that guides the analysis and design of MAS from initial system goals to the detailed agent design~\cite{Scott2004}.
\end{itemize}

Yet, these methodological works significantly rely on human designers while none of them enable automating the assistance of the MAS design process by guaranteeing sufficient efficiency while taking into account organizational aspects in a multiagent context.

Beside, these MAS designing methods, recent works in MARL make it possible benefitting of a MARL process that would automatically converge to an optimal or sub-optimally sufficient solution as for establishing the rules in agents (called \textbf{policies}) that drive the MAS to the goal. Yet, unlike human-based design where the agent's logic is explicitly specified, trained policies with MARL may be approximated by black box generalizing functions such as Neural Networks hence not explicitly describing emergent collective strategies among trained agents.
As a recent issue, we identified few related works partially linked to using MARL for better understanding trained agents collective behavior but none are oriented for a methodological approach for MAS designing.

Kazhdan et. al.~\cite{Kazhdan2020} proposed a library to enhance the explainability of MARL systems by providing means to extract symbolic models from MARL systems. It focuses on improving the interpretability of MARL systems.
Wang et. al.~\cite{Wang2020} introduced a role-oriented MARL approach where roles are emergent, and agents with similar roles tend to share their learning and specialize in certain sub-tasks. The approach is designed to learn specialized, dynamic, and identifiable roles without relying on predefined role structures and behaviors.
Tosic et. al~\cite{Tosic2010} proposed a framework for addressing coordination in collaborative multi-agent systems. It enable agents to learn how to coordinate effectively to enhance the coordination and communication capabilities of multi-agent systems.
Zheng et. al.~\cite{Zheng2018} presented a platform for MARL that aims to facilitate research on artificial collective intelligence by providing a comprehensive set of baselines and evaluation metrics to benchmark the performance of MARL algorithms.

Our approach to address the explainability in MARL with a methodological purpose for MAS, consists in considering that a joint-policy or joint-history can be described in terms of the organizational specifications at least partially.
We refer to research in the processes falling into that broad approach under the term of \textquote{Organization oriented MARL} (OMARL).
In that context, we propose the \emph{Partial Relations with Agent History and Organization Model} (PRAHOM) process to link agents' policies and their training to an organizational model.

The chosen MARL model is based on the Decentralized Dec-POMDP~\cite{Oliehoek2016} because it considers multiple agents in a similar MAS fashion. It relies on stochastic processes to model uncertainty of the environment for the changes induced by actions, in received observations, in communication\dots Additionally, unlike Partially Observable Stochastic Games (POSG), the reward function can be common to agents which fosters training for collaborative oriented actions~\cite{Beynier2013}. Formally, a Dec-POMDP is a 7-tuple $(S,\{A_i\},T,R,\{\Omega_i\},O,\gamma)$ , where: $S = \{s_1, ..s_{|S|}\}$: The set of the possible states; $A_{i} = \{a_{1}^{i},..,a_{|A_{i}|}^{i}\}$: The set of the possible actions for agent $i$; $T$ so that $T(s,a,s') = \probP{(s'|s,a)}$ : The set of conditional transition probabilities between states; $R: S \times A \times S \rightarrow \mathbb{R}$: The reward function; $\Omega_{i} = \{o_{1}^{i},..,o_{|\Omega_{i}|}^{i}\}$: The set of observations for agent $ag_i$; $O$ so that $O(s',a,o) = \probP{(o|s',a)}$ : The set of conditional observation probabilities; $\gamma \in [0,1]$, the discount factor.

Among, the existing organizational models \textquote{Agent/Group/Role}~\cite{Ferber2004} and \allowbreak $\mathcal{M}OISE^+$~\cite{Hubner2002} both provide a relevant high-level description of the structures and interactions within the MAS. However, we favor $\mathcal{M}OISE^+$ because it provides an advanced formal description for an organization without incompatibilities with MARL, especially for formal description of agents' policies. It takes into account explicitly the social aspects between agents where \textquote{AGR} focuses on the integration of standards oriented towards design. Additionally, it provides a sufficiently detailed vision of organization to be understood at different point of views. $\mathcal{M}OISE^+$ comprises: Structural specifications mainly related to roles, their links, compatibilities\dots; Functional specifications mainly related to goals, their repartition in missions and plans\dots; Deontic Specifications as the permission and obligations for a agent playing a role to commit to a mission~\cite{Hubner2002}.



% ====
% \paragraph{\textbf{Method Fragments and Model Transformations}}
% A set of method fragments for developing MAS, which is grounded on the development process of two multi-agent systems, emphasizes the importance of model transformations in the engineering process~\cite{Garcia2011}. These fragments can be seen as modular methodologies that can be adapted and reused across different MAS projects, providing a flexible approach to system development.

% \paragraph{\textbf{Meta-Models for Analysis and Design}}
% The use of meta-models is reported to improve analysis and design activities in MAS engineering~\cite{Gomez2004}. Meta-models provide a high-level abstraction that can help in understanding and designing the complex interactions and behaviors of agents within the system, facilitating a more structured approach to MAS development.

% \paragraph{\textbf{Agent-Oriented Software Engineering Paradigms}}

% Agent technology represents a new software engineering paradigm that offers fresh prospects for analyzing, designing, and building software systems~\cite{Li2016}. This paradigm shift towards agent-oriented software engineering (AOSE) encourages developers to think in terms of autonomous agents and their interactions, leading to more robust and adaptable MAS.

% \paragraph{\textbf{Comprehensive Methodologies}}

% \paragraph{\textbf{Characterization of emergent collective strategies}}

% % Collective explainable AI: Explaining cooperative strategies and agent contribution in multiagent reinforcement learning with shapley values
% Heuillet et. al.~\cite{Heuillet2022} proposes a novel approach to explain cooperative strategies in multiagent reinforcement learning (RL) using Shapley values, a game theory concept used in eXplainable AI (XAI). The study aims to make deep RL more comprehensible and address the need for methods that provide better understanding and interpretability. The experimental results on Multiagent Particle and Sequential Social Dilemmas demonstrate the effectiveness of Shapley values in explaining the rationale behind decisions taken by agents. However, the article also highlights that Shapley values can only provide general explanations about a model and cannot explain specific actions taken by agents. The authors suggest that future work should focus on addressing these limitations. The study's implications extend to areas such as non-discriminatory decision making, ethical and responsible AI-derived decisions, and policy making under fairness constraints.

% % Social Influence as Intrinsic Motivation for Multi-Agent Deep Reinforcement Learning
% Jaques et. al.~\cite{Jaques2019} proposes a mechanism for achieving coordination and communication in MARL by rewarding agents for having causal influence over other agents' actions. This causal influence is assessed using counterfactual reasoning, where agents simulate alternate actions to compute their effect on the behavior of other agents. The paper demonstrates that this approach leads to enhanced coordination and communication, as well as more meaningful learned communication protocols. The proposed method is shown to significantly increase the learning curves of the deep reinforcement learning agents, leading to more diversified team behavior and more successful performance of the population as a whole. The paper also highlights that the influence rewards for all agents can be computed in a decentralized way, opening up new opportunities for research in this area.

% \paragraph{\textbf{Adaptation of MARL to meet requirements}}

% % Efficient MARL through automated supervision
% Chongjie et. al.~\cite{Chongjie2008} proposes a unified mechanism for achieving coordination and communication in MARL. The approach involves training multiple agents to independently maximize their own individual reward without sharing weights. The paper introduces a method for automated supervision, which enables the agents to learn to coordinate and communicate effectively. This automated supervision mechanism leads to enhanced coordination, communication, and more meaningful learned communication protocols, ultimately improving the learning curves of the deep reinforcement learning agents and the overall performance of the agent population
% %
% % Self-Organized Group for Cooperative MARL
% Shao et. al.~\cite{Shao2022} introduces a method called Self-Organized Group (SOG) for cooperative MARL. In this approach, a certain number of agents are randomly elected to be conductors, and the corresponding groups are constructed with conductor-follower consensus, allowing the groups to be re-organized at regular intervals. The organized group under the unified command of a conductor is found to embed the multi-agent system with stronger zero-shot generalization ability compared to traditional methods. The SOG method provides strong adaptability to scenarios with varying numbers of agents and varying agent sight. The paper presents this approach as a mechanism to enhance cooperative multi-agent tasks with dynamic characteristics, aiming to improve the adaptability and generalization of MARL systems
%
% % A MARL model of common-pool resource appropriation
% Perolat et. al.~\cite{Perolat2017} introduces a model that focuses on common-pool resource appropriation, a multi-agent social dilemma that includes issues such as sustainable use of fresh water, common fisheries, grazing pastures, and irrigation systems. The model emphasizes the importance of trial-and-error learning in addressing the challenges of common-pool resource sustainability and inequality. It explores the emergent behavior of groups of independently learning agents in a partially observed Markov game, shedding light on the relationship between exclusion, cooperation, and sustainability in the context of resource appropriation. The research highlights the potential of deep reinforcement learning in understanding and addressing complex societal and environmental challenges related to common-pool resource management. The paper provides valuable insights into the application of MARL in the context of real-world social dilemmas and resource management
%
% Promoting Coordination through Policy Regularization in Multi-Agent Deep Reinforcement Learning
% Roy et. al.~\cite{Roy2020} addresses the challenge of inducing coordination between agents in MARL. The research investigates the use of policy regularization to promote inter-agent coordination and discusses two approaches based on inter-agent modeling and synchronized sub-policy selection. The proposed methods are designed to improve cooperative behaviors without relying on explicit communication channels, allowing agents to exhibit coordinated behaviors during testing when acting in a decentralized fashion. The paper presents two policy regularization methods, TeamReg and CoachReg, and evaluates their performance on challenging cooperative multi-agent problems, showing improved results. The research contributes to the advancement of coordination-driven multi-agent approaches in reinforcement learning and provides valuable insights into promoting inter-agent coordination through policy regularization.

% Eventually, among considered works, none are specifically using an organizational model as a general way for both expressing the MARL resulting through OCPV; and constraining the MARL itself according to organizational specifications. It appears that augmenting MARL with an organizational model so it can be core of a aided-engineering MAS design approach; is not explicitly covered as far as we know.

% ====================================================================================================

\section{AOMEA approach}

% Mettre d'abord en avant le shcéma général de l'approche
% Donner la philosophie de l'approche


% TODO: The Assisted Organization...

% AOMEA approach

% TODO: confirmer les choix faits en "theortecial background" vis à vis du problème

% 	Global overview (Fig 1)
% 	theoreetical core
% 	Engineering
%   Implémentation (vers une implémentation de type PoC)

\section{Global overview}

\begin{figure}[t!]
    \centering
    \include{figures/approach}
    \caption{A summary view of our approach to MAS design}
    \label{fig:design_approach}
\end{figure}

We introduce AOMEA as an approach for MAS design that automates the pre-designing of a MAS that has to respect some design constraints; while the obtained organizational specifications allow developing a curated MAS.

Since PRAHOM relies on joint-histories instead joint-policy, AOMEA is agnostic of the approximation function used for implementing the agents' policies such as neural networks based ones. Moreover, it can be generally applied in any environment likely to be simulated.

AOMEA first consider the environment with agents that are to achieve some goals. It automatically enables finding relevant insights under the form of organizational specifications. They are to transparently express how agents could individually act and collaborate to reach the goals. Then, the MAS design process can be assisted in light of these indications. As a high-level description AOMEA consists in:
\begin{enumerate}
    \item Linking some observation and action with related known specifications such as roles, links between roles, objectives or missions\dots;
    \item Defining the training specifications as design constraints before launching a simulation of several episodes;
    \item Training agents with regular algorithms such as MADDPG or PPO within PRAHOM;
    \item Extracting organizational specifications out of the  trained agents' policies in a single episode run via PRAHOM;
    \item Understanding and curating the raw specifications to get relevant, safe and clean organizational specifications
    \item Implementing the MAS in light of the curated organizational specifications and checking its validity
\end{enumerate}

An illustrative representation of our approach is presented in Figure \ref{fig:design_approach}. The organizational model $\mathcal{M}OISE^{+}$ is related to the policies learned via the PRAHOM process. In this process, we seek to define the structural, functional and deontic specifications of $\mathcal{M}OISE^{+}$ with respect to the learned policies (1). For example, we can determine the infered agents' roles, links and compatibilities between roles, how the determined goals are organized in plans and how agents can commit to them. Conversely, the specifications of the organization constrain the space of possible agent policies (2). For example, some of the actions labeled for particular roles may or may not be accessible to an agent depending on the assigned role.

The interest is to provide at least the designer with indications on the specifications of an organization capable of satisfying the constraints of the environment and initial design. In light of these initial results, the designer can decide to propose specifications which he can ensure meet the safety and explainability requirements. These specifications then serve as a blueprint for implementing a MAS as described in Figure \ref{fig:design_approach}.


\section{Theoretical core}

We proposed PRAHOM (in \autoref{alg:prahom}) as a synthesis of two processes that fall into the OMARL purposes. The first process gets the specifications out of the agents' policies; and the second process to get the joint-policies satisfying given specifications.
Here we just present the underlying idea at a high-level description for these two processes to avoid unnecessary formalism. More information can be found in \autoref{gym-wrapper} as for the use and implementation of PRAHOM.

\RestyleAlgo{ruled}
\SetKwComment{Comment}{// }{}

\begin{algorithm}[hbt!]
    \caption{Partial Relations with Agent History and Organization Model (PRAHOM)}\label{alg:prahom}

    \KwData{$d$, the Dec-POMDP to solve}
    \KwData{$ep_{max} \in \mathbb{N}$, the maximum number of episodes}
    \KwData{$step_{max} \in \mathbb{N}$, the maximum number of steps per episodes}
    \KwData{$s \in \mathbb{R}$, the cumulative reward expectancy}
    \KwData{$\pi_{joint}$, the joint-policies to be trained}
    \KwData{$os_{init}$, the design specifications to respect}
    \KwData{$Solv$, a MARL algorithm updating policies to solve a Dec-POMDP}

    \KwResult{$(s\pi_{joint,s,i} \in S\Pi_{joint,s,i}, os_{s,i} \in OS_{s,i})$, the sub-optimal policies associated organization specifications}

    $s\pi_{joint,s,i} = PRAHOM-sop-training(s,i)$

    $os_{s,i} = PRAHOM-pos(s\pi_{joint,s,i})$

\end{algorithm}

\paragraph{\textbf{Infering OS from joint-policies}}

Rather than using directly agents' policy, we use the history as it may be built with observed resulting actions when observations are received during a series of test episodes. Indeed, for a given policy $\pi \in \Pi$ the associated history is by definition $h \in h_{joint} = ((\omega_k,a_k) | k \in \mathbb{N})$ and the $(\omega_k,a_k) \in \pi$. Taking into account several agents, joint-history can be seen as an approximation of the joint-policy that we refer to as reconstructed joint-policy $R\Pi_{joint} \subset \Pi_{joint}$. A reconstructed policy with a history is $r\pi_i = {(\omega, a) \in h | h \in H}$;

Then, due to the difficulty to infer information related to organizational specifications, we associate each action in MARL with some organization specifications as a \textquote{many to many} relation. It sets up a first frame for identifying organizational specifications in played action series. We address that problem in the remainder of this section.

We propose some relations between $\mathcal{M}OISE^+$ specifications and joint-histories. Their premises comes from noticing some specifications in the $\mathcal{M}OISE^+$ organizational model can be obviously mapped to subsets of actions from a single sub-optimal joint-policy.
From these relation, we propose to rely on the previous relations to develop a empirical algorithm we call \emph{PRAHOM-pos} to infer organizational specifications out of joint-policies. Below we informally describe key points for understanding that process (cf. PRAHOM-Wrapper\ref{gym-wrapper} for the implementation).

As we have only one group, we do not consider the inter-links and inter-compatibilities. Additionally as a simplification, we consider only one social scheme. As a general remark, the \textquote{determine*} relations are not all fully described and rely on custom implementation (cf. \autoref{gym-wrapper} for further information).
PRAHOM-pos first look at the individual level.
First, it tries to figure out the roles, links, subgroups, individual goals, missions and plans played by agents by sampling history sub-sequences $h \in H$ and comparing with known history sub-sequences whose we know the associated role via established relations.

Then PRAHOM-pos look at the social and collective level.
At the end of each joint-policy we try to reinforce a global view of the goals, missions, plans, and the knowledge of the mission to goal; with the partially inferred information at the individual level.
At the end of the algorithm, it tries to synthesize the knowledge infered until know to have better view of the number of agent per subgroup, the agent cardinality per mission, the role cardinality, the compatibilities between roles, the permissions and obligations.


\paragraph{\textbf{Constraining joint-policies satisfying OS}}

We consider a given MARL algorithm that iteratively converges towards a joint-policy so that each agent's policy is updated at each step until a finite horizon. Indeed, constraining the available action set for each agent to the action sets authorized by the organization specifications at each step of the MARL training; implies constraining the converged joint-policies to the ones that satisfy the given organization specifications. We used that result, to setup \emph{PRAHOM-sop-training}, the process used jointly with MARL learning to satisfy organizational constraints.

\emph{PRAHOM-sop-training} first converts the $os_{init} \in OS_{init}$ design specifications into a $forbidden$ set of the non-authorized joint-policies. It uses a non-described reverse \emph{PRAHOM-pos} algorithm to achieve it.

Then, \emph{PRAHOM-sop-training} uses a given MARL algorithm to update the agents' individual policies with previous action, observation and resulting reward. Then, it first computes the authorized actions set $A_{step}$ according to the current history $h_{joint,i}$. Then, an action is chosen among authorized actions. That action $a_{step}$ is added in history to be used for updating the agent's policy in the next step.
Finally, considering only one iteration of episode, it returns a sub-optimal joint-policy $\pi_{joint}$ satisfying $os_{init}$
We can note the $os_{init}$ could forbid the MARL algorithm to provide a joint-policy that reaches a expected cumulative reward.


\section{Engineering tool}

We developed \emph{PRAHOM Gym Wrapper}\label{gym-wrapper}, a first Gym wrapper to help in automate the setting up of PRAHOM for a given Gym Environment.
It is a Proof of Concept that enables linking actions with $\mathcal{M}OISE^+$ specifications, define the training specifications and provide functions to extract the resulting sub-optimal raw organizational specifications. During training, actions are masked to guide an agent to learn to act according to the organizational specifications.

\begin{lstlisting}[language=Python, caption=PRAHOM Gy Wrapper basic use, label={lst:wrapper_basic_use}]
from omarl_experiments import PamidWrapper
env = gym_env.parallel_env(render_mode="human")
...
action_to_specs = {"agent_name": {
        "14": "role='leader';link='(agent_0,agent_1,auth);..."
        ...}    
    ...}
training_specs = {
    "leadadversary_0": {
        "must": ["(23,41)"],
        "must_not": ["(14,74)"]}...}
env = PrahomWrapper(env, action_to_specs, training_specs, unknown_specs_inference=True, pca_output=True)
# ...TRAINING...
env.prahom_render_pca()
trained_specs, agent_to_specs = env.prahom_specs()
\end{lstlisting}

In \autoref{lst:wrapper_basic_use}, we detailed a basic use of the wrapper to augment a Gym environment (l. 12) with known relations between actions and organizational specifications (l. 4) and the design constraints agents are to satisfy (l. 8). After training, we can print a Principal Component Analysis (PCA) of each agent history over an episode (l.14) which is an additional analysis tool to manually detect emerging roles. It also provides the infered $\mathcal{M}OISE^+$ specifications considering the last episodes and how agents are instantiated for the last one.

These results are constructed via an empirical or statistical approaches using provided know relations between observations, actions and organizational specifications. When no relation is available it tries to detect subsequence of joint-histories likely to match an organizational specification (l. 12). Due to these limitations, the results may not fully describe the underlying organization or may contain noisy infered organizational specifications. Since the results are compliant with $\mathcal{M}OISE^+$, it is then possible to use them with available MAS design methods to benefit from recurrent emerging organizational specifications during the design process.

% We start defining actions with known specifications Focusing of the "lead adversary", we have:

% "observations": [self_vel, self_pos, landmark_rel_positions, other_agent_rel_positions, other_agent_velocities, leader_comm]
% "actions": [say_0, say_1, say_2, say_3] X [no_action, move_left, move_right, move_down, move_up]

% TODO: décrire les fonctionalités et résultats attendues (reprendre github)

% =============

% \subsection{Premises for AOMEA's theoretical core}

% - Organizational oriented MARL: a general research study focusing in integrating the organization in MARL at several aspects
% - DMO (Dec-POMDP MOISE+ OMARL): a class of algorithm falling into OMARL purposes that use MOISE+ as organizational model and Dec-POMDP as a MARL model
% - PRAHOM (Partial Relations with Agent History and Organization Model): a DMO algorithm that allows both getting the organizational specifications out of trained agents; and driving their training to satisfy extra design organizational specifications as constraints.

% \subsection{AOMEA for engineering}

% - We proposed PRAHOM (Partial Relations with Agent History and Organization Model): an algorithm that allows both getting the organizational specifications out of trained agents; and driving their training to satisfy extra design organizational specifications as constraints.
% - [Algo PRAHOM...]

% - We developped PRAHOM as a Gym-wrapper to help in applying the proposed MAS design approach...
% - [Lien vers dépôt GitHub]
% - Description des fonctionalités

% - The general approach workflow follows:
% 1) Reproducing target environment in simulation
% 2) Defining the reward function so agents aim to achieve the goals
% 3) Adding agents to be trained to solve the previously defined problem
% 4) Launching the training in simulation with PRAHOM
% 5) Getting the raw organizational specifications with PRAHOM
% 6) Curating these to produce a safe MAS model
% 7) Using that model to implement a proper MAS
% 8) Check viability and requirements respect in simulation
% 9) Deploy that MAS in the target system

% \begin{itemize}
%     \item Questions at the Multi-Agent System level (system-centric approach)
%           \begin{itemize}
%               \item Number of agents, what heterogeneity?
%               \item What is the common medium (Environment) shared by the agents?
%               \item What communication mechanisms are available to agents?
%               \item What are the communication languages, ontologies, interaction protocols used by the agents?
%               \item What is the organization within which the agents operate? How is it established?
%               \item How do the agents coordinate their actions? How to ensure coherent operation?
%           \end{itemize}

%     \item Agent level questions (agent-centered approach)
%           \begin{itemize}
%               \item What does an agent represent? What actions should be encapsulated in an agent?
%               \item How do agents represent the environment and organization in which they operate?
%               \item How do agents handle interactions with other agents?
%               \item What is the internal structure of the agents?
%           \end{itemize}
% \end{itemize}

% ====================================================================================================

\section{Evaluation in cooperative game environments}

% Evaluation
%     In order to verify and demonstrate the approach, is applied on the following case study.
% 	Case study


% =====> tu précices que ce que tu veux valider, le protocole expérimental que tu mets enn oeuvre pour ça et les résultats.

% TODO: expliquer pourquoi on a choisit notre environnement
% ==> Montrer qu'on peut suggerer des modèles d'organisation
% Pour ce faire, on utilise des environnements simples classiques

% Mettre en avant les spécifications obtenues

% TODO: mettre une figure avec 4 cases avec less screnshots des figures avec un lien vers les figures avec une ancre
% footnote: cliquez pour acceder 

% todo: revoir l'intro -> c'est quoi le problème ? pk le MARL ? Il faut bien expliquer la démarche du papier (x phrases pour une même section)



In order to assess AOMEA, we considered using PRAHOM in available simulated environments made up from agents that have to achieve a goal with the best performance through various collective strategies whose some can be expected (presented in \autoref{fig:simulated_environments}).
We selected 3 Atari-like environments for their visual rendering is a convenient way to check the PRAHOM results with manual observations.
\footnote{Additional explaination and the examples discussed using \emph{PRAHOM Gym wrapper} are available at \url{https://github.com/julien6/omarl_experiments?tab=readme-ov-file\#tutorial-predator-prey-with-communication}}
We also considered a Cyberdefense environment as a first attempt to apply PRAHOM in non-visual environment and checking possibly emerging collective strategies against malware programs:

\phantom{X}

% Left column for text
\hspace{-0.6cm}
\begin{minipage}[t]{0.65\textwidth}

    \begin{itemize}
        \item \textquote{Predator-prey with communication}~\cite{Lowe2017} (PPY) that consists in predators monitored by a leader to better catch preys hence requiring collective hunting strategies;
        \item \textquote{Pistonball} (PBL)~\cite{Terry2021} that consists in a series of piston to bring a ball from right to left side hence requiring neighbourg representation;
        \item \textquote{Knights Archers Zombies}~\cite{Terry2021} (KAZ) that consists in a few knights and archers learning how to kill as much zombies a possible hence requiring efficient agents positioning;
        \item \textquote{Drone swarm - 3rd CAGE Challenge}~\cite{cage_challenge_3_announcement} (CYB) that consists in blue agents deployed in drones while trying to tackle embedded malware programs' impact. We may expect agents to \allowbreak collectively isolate compromised drones.
    \end{itemize}

\end{minipage}%
\hfill % Ajoute une marge horizontale
% Right column for a figure
\begin{minipage}[t]{0.37\textwidth}
    \vspace{-1.7cm}
    \begin{figure}[H] % Use the 'H' specifier from the 'float' package to force the figure placement
        \centering
        \includegraphics[width=0.6\linewidth]{figures/envs.png} % Replace "example-image" with your figure file name
        \caption{Overview of the selected environments: CYB, PBL, PPY, and KAZ}
        \label{fig:simulated_environments}
    \end{figure}
\end{minipage}

\

\noindent We applied AOMEA in three cases:
\begin{itemize}
    \item No training specifications (NTS): agents have to learn the most efficient collective strategies without any constraints or indications.
    \item Partially constraining training specifications (PTS): some constraints or indications are given to help converging faster or meeting requirements.
    \item Fully constraining training specifications (FTS): manually crafted joint-policies are given for they are a reference regarding learned joint-policies.
\end{itemize}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/prahom_learning_curve.png}
    \caption{Average reward for each iteration in Pistonball environment}
    \label{fig:prahom_learning_curve}
\end{figure}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/prahom_pca_analysis.png}
    \caption{Principal Component Analysis of the trained agent histories in Pistonball environment}
    \label{fig:prahom_pca_analysis}
\end{figure}

To evaluate the impact of our approach in training we payed attention to the following criteria: convergence time, stability, global performance, change sensibility. Results are presented in Table~\ref{tab:training_AOMEA_results}.

\input{tables/training_OMARL_results.tex}

As a general observation, we can notice convergence time is bigger for NTS than for PTS which is also bigger than for FTS. That makes sense for the search space is decreasing, hence a shorter convergence time. As an example, we noticed a way faster convergence to a sub-optimal solution in Pistonball environment by providing extra organizational specifications as presented in \autoref{fig:prahom_learning_curve}. Global performance indicated by cumulative reward show NTS to outperform PTS and FTS for agents are overfitted to solving the problem much more finely than the designer's specifications can do.

As for assessing AOMEA results, we also took into account criteria after training: roles, links, compatibilities, social schemes. Results are presented in Table~\ref{tab:trained_AOMEA_results}

\input{tables/trained_OMARL_results.tex}

% //TODO: Moise+ schemes and comparison with expected ones

For \textquote{Pistonball} we can notice roles to be equivalent which is logical for agents are expected to act the same. Indeed, trained agents' histories are close hence showing a common emerging role. In \autoref{fig:prahom_pca_analysis} we can see most agent's histories are in the left bottom zone of the Principal Component Analysis (PCA). We can also check no organizational specifications except roles have been generated for agents are indeed not expected to communicate. For \textquote{Knights, Archers, Zombies} we can notice two distinct roles that are indeed expected due to the agent's different capacities for knights and archers. Yet we can also a collective pattern to be drawn between archers and knights despite no communication. For \textquote{Prey, predators with Communication}, we can observe output specifications indicate authority links among two (predator leader and simple predator) of the emerging roles. That result is also expected for these links enable collective strategies to allow predators to circle a prey. Finally, the \textquote{Drone swarm - 3rd CAGE Challenge} shows communication between blue agents are indeed understood as communication links enabling to isolate infiltrated drone then trying to reimage and alert recently suspect drones.

For the \textquote{Drone swarm - 3rd CAGE Challenge} we developed our own custom MAS via a simple decision tree as preconised in AOMEA in light of the organizational specifications we curated by removing noisy results. The mean score we got about -2000 which is indeed close to the TOP 3 scores. This shows AOMEA to be indeed applicable for Cyberdefense context ultimately bringing more safety than just regular MARL trained MAS.

\section{Conclusion}

% =======> demande toi avant de rédiger la conclusion "Qu'est ce qui change dans l'état de l'art avec ce papier ?"... si tu avais lu ce papier et qu'il n'était pas de toi qu'est ce que tu te serai dis 

%  - Need for easing the MAS design by assisting human designer
%  - We proposed a novel general MAS design approach that consists in getting relevant organizational specifications out of agents in training satisfying design constraints. These insights allow implementing a suited safe MAS to be deployed on target system
%  - We validated that approach for various different environments including in a Cyberdefense context.
%  - Main perspective is to improve the reliability of PRAHOM between organizational specifications and agents' training satisfying to design constraints.
%  - Another future work is to reduce the manual parts of our approach with implementation of a full integrated development environment.

% 1er paragraphe: résumé + qu'est-ce qui est changé par rapport à l'edt
% 2ème paragraphe: perspectives et future works

% In this article we presented AOMEA, a novel general MAS design approach that aims to ease the MAS designing when it can not be easily conducted by designers in highly complex deployment environments.
On one side, literature shows some methodological works rely on the designer knowledge to design a suited MAS organisation but do not provide automatic or assisted ways to determine relevant organizational mechanisms solely regarding the design requirements and the global goal. On the other side, MARL techniques have been successfully applied to automatically train agents to reach the given goal but the concerns raised by explicit characterization of emergent collective strategies is still in its early stages.
In order to address both the lack of automation in MAS methods and the need for explicit organizational in MARL, we showed AOMEA's originality, which is to augment a MARL process with an explicit organizational model, has not been considered in literature yet. We first exposed how AOMEA is intented to be used in MAS engineering as an additional tool to assist the design process.
Then, we explained the AOMEA's theoretical core with formal links between Dec-POMDP and the $\mathcal{M}OISE^+$ through the PRAHOM algorithm.
Furthermore, we implemented the \emph{PRAHOM Gym wrapper} as a Proof of Concept for practically applying AOMEA and we showed how it enables getting the organizational specifications satisfying design constraints while allowing to achieve the given goal.
Finally, we applied our approach in four Gym environments to check the impact on training, explainability and viability for our MAS design approach.

Even though AOMEA is agnostic of the MARL algorithm because it uses agents' histories obtained a posteriori to infer organizational specifications; reconstructing agents' collective behavior show to be costly and difficult in analyse. Indeed, a major perspective for improving PRAHOM is to go further with supervised and non-supervised learning rather than an empirical statistical approach as for identifying valuable organizational specifications out of histories. Moreover, it is worth investigating recent works in MARL techniques such as hierarchical learning because they already seek to characterize emergent strategies throughut learning.
Eventually, we also aim to improve AOMEA applicability by developping dedicated interfaces built around PRAHOM making it more accessible for industrial and research contexts.


% ---- Bibliography ----
%
% BibTeX users should specify bibliography style 'splncs04'.
% References will then be sorted and formatted in the correct style.
%
% \bibliographystyle{splncs04}
% \bibliography{mybibliography}
%
\section*{References}

% \bibliographystyle{abbrv}
\bibliographystyle{splncs04}

\bibliography{references}

\end{document}
