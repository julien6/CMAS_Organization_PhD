\AtBeginSection[]{
	\begin{frame}
		\frametitle{}
		\tableofcontents[currentsection]
	\end{frame}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 
 \section{Case study}
	
	\subsection{Experimental setup}
	\begin{frame}{Experimental setup}
		{Network topology}

        Based on GALLIUM APT tactics we proposed a small company like networked environment:
        \begin{itemize}
            \item The cyber-attacker agents are initially deployed on At1 and At2 and the cyber-defender agents are deployed on WS and DB.
            \item The ultimate attackers' goals is to get data from the DB server and installing one spyware on the printer server PS.
        \end{itemize}

        \begin{figure}
            \centering
            \includesvg[width=0.65\linewidth]{figures/topology.svg}
            \caption{Proposed small-scale company network topology}
            \label{fig:scenario_network_topology}
        \end{figure}
 
	\end{frame}

    
	\begin{frame}{Experimental setup}
		{Attack/defense scenario}

        \vspace{-0.15cm}

        \begin{figure}
            \centering
            \includesvg[width=0.4\linewidth]{figures/ADTree.svg}

            \vspace{-0.2cm}
            
            \caption{An overview of the proposed attack/defense AD Tree}
            \label{fig:ADTree}
        \end{figure}
 
	\end{frame}


	\begin{frame}{Experimental setup}
		{Agent behavior implementation}

            \begin{block}{Random approach}
                \begin{itemize}
                    \item The agents only choose their actions by exploring the whole action space without any criteria until reaching the goal
                    \item Allows getting a benchmark of unexpected edge failure cases and to compare with other types of agent.
                \end{itemize}
            \end{block}

            \begin{block}{Decision Tree (DT) approach}
                Reference when cyber-attackers or cyber-defenders already know the best action to take as the role of each agent is defined by a DT.
            \end{block}

            \begin{block}{Multi-Agent Reinforcement Learning (MARL) approach}
                Q-Learning~\cite{CWatkins1992} with curriculum learning for first the attackers learn how to attack before adding defenders.
            \end{block}
 
	\end{frame}


 	\subsection{Results and discussion}

 	\begin{frame}{Results and discussion}
		{}

        \begin{itemize}
            \item After several iterations, chosen action paths by the attackers tend to be as efficient as the DT paths.
            \item When adding the defenders, we verified the attackers to be less and less able to reach the ultimate goal.
        \end{itemize}

        \begin{figure}
            \centering
            \includesvg[width=0.75\linewidth]{figures/graphs.svg}

            \vspace{-0.2cm}
            
            \caption{An evolution of the rewards average according to episodes in small-scale tests with MARL and Decision Tree Approaches with inactive cyber-defense
            }
            \label{fig:graphs}
        \end{figure}
 
	\end{frame}
