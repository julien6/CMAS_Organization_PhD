\AtBeginSection[]{
    \begin{frame}
        \frametitle{}
        \tableofcontents[currentsection]
    \end{frame}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Theoretical background}

\subsection{Multi-Agent Systems context}

\begin{frame}{Theoretical background}{Multi-Agent Systems context}
    An agent is an entity immersed in an environment that perceives observation and makes a decision to act autonomously in the environment to achieve the objectives assigned to it.
    Agent types include event-driven reactive to deal with uncertainties in an environment or cognitive proactive agents that leverage interactions with other agents. A MAS is a set of agents in a shared environment where each agent has only a local perception. These agents are to be endowed with self/re-organizing capabilities that allow them to adaptively modify their organizational structure according to their environment.

    A MAS is strongly linked to the organization entity (we simply call \textbf{organization}) we consider it to always exist through the running agents' interactions even though it may be implicit.
    An \textbf{organizational model} specifies (at least partially) the organization whether it is used as a medium to describe an explicit known organization in a top-down way, or describing an implicit organization in a bottom-up way. An example of organizational model is the \emph{Agent/Group/Role} (AGR) model~\cite{Ferber2004}. We refer to the \textbf{organizational specifications}, the components used in an organizational model to characterize the organization. $\mathcal{M}OISE^+$ is an organizational model with which it is possible to link agents' policies to organizational specifications. It takes into account the social aspects between agents explicitly whereas \emph{AGR} focuses on the integration of standards oriented towards design. $\mathcal{M}OISE^+$~\cite{Hubner2007} considers three types of specifications:

    The \textbf{structural specifications} describe the means agents can leverage to achieve a goal. It comprises the set of \emph{roles}, sub-groups, intra-group and inter-group \emph{links}, intra-group and inter-group \emph{compatibilities}, and the role and sub-group \emph{cardinalities}.
    A \emph{link} indicates whether two roles are related because of acquaintance, communication, or authority ties. A \emph{compatibility} indicates whether two roles can be adopted by the same agent. Role and sub-group \emph{cardinalities} respectively refer to the minimal and maximal number of roles and sub-groups.

    The \textbf{functional specifications} describe the way to achieve a goal. It comprises \emph{social schemes} and \emph{preference order}. A \emph{social scheme} is described by global goals, mission labels with plans, and the cardinality of agents committed to a mission. A \emph{preference order} means an agent has a social preference to commit to a specific mission among several possible ones.

    The \textbf{deontic specifications} enable linking functional and structural specifications through a set of \emph{permissions} and obligations. A \emph{permission} means an agent playing role $\rho_a$ is permitted to commit to mission $m$ for a given time constraint $tc$. Similarly, an \emph{obligation} means an agent playing role $\rho_a$ has to commit to mission $m$ for a given time constraint $tc$. A time constraint $tc $ specifies a set of periods determining whether a permission or an obligation is valid.

    % Yet, these methodological works significantly rely on human designers' experience while none of them enable automating the assistance of the MAS design process by guaranteeing sufficient efficiency while taking into account organizational aspects in a multi-agent context.
\end{frame}


\subsection{MARL basics}

\begin{frame}{Theoretical background}{Multi-Agent Systems context}
    Reinforcement learning is a machine learning paradigm where agents learn to make decisions by interacting with an environment. The goal is for the agent to maximize a cumulative reward signal over time through a trial-and-error process.
    MARL extends this concept to multiple agents that learn while considering the actions of other agents pushing agents to rely on cooperation mechanisms.

    MARL enables automatically converging towards agentsâ€™ policies that enable reaching the given goal. Yet, unlike human-based design, the trained agents' logic is explicitly specified from a collective point of view. Few works attempt to address that issue and few are oriented for methodological purposes.
    Kazhdan et. al.~\cite{Kazhdan2020} proposed means to extract symbolic models from MARL systems that improve the interpretability of MARL systems.
    Wang et. al.~\cite{Wang2020} introduced a role-oriented MARL approach where roles are emergent, and agents with similar roles tend to share their learning and specialize in certain sub-tasks.
    Tosic et. al~\cite{Tosic2010} proposed a framework for addressing coordination in collaborative MAS relying on the communication capabilities of multi-agent systems.
    Zheng et. al.~\cite{Zheng2018} presented a platform for MARL that aims to facilitate research on artificial collective intelligence by providing a comprehensive set of baselines and evaluation metrics to benchmark the performance of MARL algorithms.

    Markovian models are required to model the environment and apply MARL techniques. As a commonly used, Decentralized Dec-POMDP~\cite{Oliehoek2016} considers multiple agents in a similar MAS fashion. It relies on stochastic processes to model the uncertainty of the environment for the changes induced by the actions, the received observations, and the communications as well. Its reward function is common to agents which fosters training for collaborative oriented actions~\cite{Beynier2013}. Formally, a Dec-POMDP is a 7-tuple $(S,\{A_i\},T,R,\{\Omega_i\},O,\gamma)$ , where: $S = \{s_1, ..s_{|S|}\}$: The set of the possible states; $A_{i} = \{a_{1}^{i},..,a_{|A_{i}|}^{i}\}$: The set of the possible actions for agent $i$; $T$ so that $T(s,a,s') = \probP{(s'|s,a)}$ : The set of conditional transition probabilities between states; $R: S \times A \times S \rightarrow \mathbb{R}$: The reward function; $\Omega_{i} = \{o_{1}^{i},..,o_{|\Omega_{i}|}^{i}\}$: The set of observations for agent $ag_i$; $O$ so that $O(s',a,o) = \probP{(o|s',a)}$ : The set of conditional observation probabilities; $\gamma \in [0,1]$, the discount factor.

    We refer to \textbf{solving} the Dec-POMDP for the team $t$ as finding a joint policy $\pi_{joint,i} \in \Pi_{joint}$ that maximizes the expected cumulative reward over a finite horizon.
    We refer to \textbf{sub-optimally solving} the Dec-POMDP at $s$ expectancy as finding the joint policies $\pi_{joint,i} \in \Pi_{joint}$ that gets the expected cumulative reward over a finite horizon at least at $s \in \mathbb{R}$.
\end{frame}
