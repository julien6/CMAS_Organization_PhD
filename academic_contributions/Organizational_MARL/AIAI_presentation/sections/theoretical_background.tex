\AtBeginSection[]{
    \begin{frame}
        \frametitle{}
        \tableofcontents[currentsection]
    \end{frame}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Theoretical background}

\subsection{Multi-Agent Systems context}

\begin{frame}[allowframebreaks]{Theoretical background}{Multi-Agent Systems context}

    \begin{block}{Keywords}
        \begin{itemize}
            \item \textbf{Agent}: entity immersed in an environment perceiving observation and making decision autonomously to achieve some goals;
            \item \textbf{MAS}: a set of agents collaborating with self/re-organizing mechanisms to achieve their goal;
            \item \textbf{Organization}: the agents' interactions even though it may be implicit;
            \item \textbf{Organizational Model (OM)}: medium to formally describe an explicit/implicit organization;
            \item \textbf{Organizational Specifications (OS)}: components of an OM to characterize an organization
        \end{itemize}
    \end{block}

    \begin{block}{Organizational model: $\mathcal{M}OISE^+$}
        \begin{itemize}
            \item more complex than \emph{Agent Group Roles} (integration of standards);
            \item takes into account the social aspects between agents explicitly;
            \item possible to link agents' policies to organizational specifications.
        \end{itemize}
    \end{block}

    \input{figures/moise_model.tex}

\end{frame}


\subsection{MARL basics}

\begin{frame}[allowframebreaks]{Theoretical background}{MARL basics}

    \begin{block}{Keywords}
        \begin{itemize}
            \item \textbf{Policy}: the \textquote{logic} to choose next action according to observation for an agent;
            \item \textbf{History/trajectory}: the tuple of (observation, action) couples over an episode;
            \item \textbf{Joint-policy / Joint-history}: all of the agents' policies / histories as tuples;
            \item \textbf{Reinforcement learning}: an agent updates its policy to maximize a cumulative reward;
            \item \textbf{Multi-Agent Reinforcement Learning (MARL)}: extends to multiple agents that learn while considering the actions of other agents;
        \end{itemize}
    \end{block}

    \begin{block}{MARL for methodological purpose}

        Effective joint-policies but not explicitly specified/understandable
        $\Longrightarrow$ Few related works

        \begin{itemize}
            \item Kazhdan et. al.~\cite{Kazhdan2020} proposed means to extract symbolic models;
            \item Wang et. al.~\cite{Wang2020}: introduced a role-oriented MARL approach;
            \item Zheng et. al.~\cite{Zheng2018} presented a platform for MARL.
        \end{itemize}
    \end{block}

    \begin{block}{Markovian models for MARL: Dec-POMDP}
        Decentralized Partially Observable Markov Decision Process~\cite{Oliehoek2016}
        \begin{itemize}
            \item considers multiple agents in a similar MAS fashion
            \item stochastic processes for uncertainty in environmental changes including observations;
            \item reward function is common to agents which fosters training for collaborative oriented actions~\cite{Beynier2013}
        \end{itemize}

        \

        { \footnotesize

        $(S,\{A_i\},T,R,\{\Omega_i\},O,\gamma)$ , where
        \begin{itemize}
            \item $S = \{s_1, ..s_{|S|}\}$: The set of the possible states;
            \item $A_{i} = \{a_{1}^{i},..,a_{|A_{i}|}^{i}\}$: The set of the possible actions for agent $i$;
            \item $T$ so that $T(s,a,s') = \probP{(s'|s,a)}$ : The set of conditional transition probabilities between states;
            \item $R: S \times A \times S \rightarrow \mathbb{R}$: The reward function
            \item $\Omega_{i} = \{o_{1}^{i},..,o_{|\Omega_{i}|}^{i}\}$: The set of observations for agent $ag_i$;
            \item $O$ so that $O(s',a,o) = \probP{(o|s',a)}$ : The set of conditional observation probabilities;
            \item $\gamma \in [0,1]$, the discount factor.
        \end{itemize}

        }

    \end{block}

    \begin{block}{Solving a Dec-POMDP}
        \begin{itemize}
            \item \textbf{solving} the Dec-POMDP for the team $t$ as finding a joint policy $\pi_{joint,i} \in \Pi_{joint}$ that maximizes the expected cumulative reward over a finite horizon;
            \item \textbf{sub-optimally solving} the Dec-POMDP at $s$ expectancy as finding the joint policies $\pi_{joint,i} \in \Pi_{joint}$ that gets the expected cumulative reward over a finite horizon at least at $s \in \mathbb{R}$.
        \end{itemize}
    \end{block}

    \begin{exampleblock}{Examples of MARL Algorithms}
        \begin{itemize}
            \item \textbf{Independent Learning}: IQL, IDQN
            \item \textbf{Centralized Training, Decentralized Execution}: MADDPG, COMA, VDN
            \item \textbf{Cooperative MARL}: QMIX, MAPPO
            \item \textbf{Hierarchical MARL}: Feudal Networks, Hierarchical Actor-Critic
            \item \dots
        \end{itemize}
    \end{exampleblock}

\end{frame}
