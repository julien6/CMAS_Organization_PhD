\RequirePackage[2020-02-02]{latexrelease}
\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.

\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\usepackage[english]{babel}
\addto\extrasenglish{  
    \def\figureautorefname{Figure}
    \def\tableautorefname{Table}
    \def\algorithmautorefname{Algorithm}
    \def\sectionautorefname{Section}
    \def\subsectionautorefname{Subsection}
}

\newcommand{\supertiny}{\fontsize{1}{2}\selectfont}

\usepackage{catoptions}
\makeatletter

\def\Autoref#1{%
  \begingroup
  \edef\reserved@a{\cpttrimspaces{#1}}%
  \ifcsndefTF{r@#1}{%
    \xaftercsname{\expandafter\testreftype\@fourthoffive}
      {r@\reserved@a}.\\{#1}%
  }{%
    \ref{#1}%
  }%
  \endgroup
}
\def\testreftype#1.#2\\#3{%
  \ifcsndefTF{#1autorefname}{%
    \def\reserved@a##1##2\@nil{%
      \uppercase{\def\ref@name{##1}}%
      \csn@edef{#1autorefname}{\ref@name##2}%
      \autoref{#3}%
    }%
    \reserved@a#1\@nil
  }{%
    \autoref{#3}%
  }%
}
\makeatother

\usepackage[T1]{fontenc}
\usepackage{graphicx}
%\usepackage{color}
%\renewcommand\UrlFont{\color{blue}\rmfamily}

\usepackage{amsmath,amssymb,amsfonts}
\usepackage[inline, shortlabels]{enumitem}
\usepackage{tabularx}
\usepackage{caption}
\usepackage{listings}
% \usepackage{titlesec}
\usepackage{ragged2e}
% \usepackage[hyphens]{url}
\usepackage{hyperref}
\usepackage{xurl}
\usepackage{pifont}
\usepackage{footmisc}
\usepackage{multirow}
\usepackage[linesnumbered,ruled,vlined]{algorithm2e}
\usepackage{float}
\usepackage{listings}
\usepackage{xcolor}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}
 
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}
 
\lstset{style=mystyle}

% --- Tickz
\usepackage{physics}
\usepackage{amsmath}
\usepackage{tikz}
\usepackage{mathdots}
\usepackage{yhmath}
\usepackage{cancel}
\usepackage{color}
\usepackage{siunitx}
\usepackage{array}
\usepackage{multirow}
\usepackage{amssymb}
\usepackage{gensymb}
\usepackage{tabularx}
\usepackage{extarrows}
\usepackage{booktabs}
\usetikzlibrary{fadings}
\usetikzlibrary{patterns}
\usetikzlibrary{shadows.blur}
\usetikzlibrary{shapes}

% ---------
% \usepackage{titlesec}
\usepackage{pdfpages}
\usepackage{booktabs}
\usepackage{csquotes}
\usepackage{lipsum}  
\usepackage{arydshln}
\usepackage{smartdiagram}
\usepackage{textcomp}
\usepackage{tabularray}\UseTblrLibrary{varwidth}
\usepackage{xcolor}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\usepackage{cite}
\usepackage{amsmath}
\newcommand{\probP}{\text{I\kern-0.15em P}}
\usepackage{etoolbox}
\patchcmd{\thebibliography}{\section*{\refname}}{}{}{}

\setlength\tabcolsep{0.5pt}

\newcommand{\before}[1]{\textcolor{red}{#1}}
\newcommand{\after}[1]{\textcolor{green}{#1}}

\newcommand{\old}[1]{\textcolor{orange}{#1}}
\newcommand{\rem}[1]{\textcolor{red}{#1}}
\newcommand{\todo}[1]{\textcolor{orange}{\newline \textit{\textbf{TODO:} #1}} \newline \newline }



\newcounter{relation}
\setcounter{relation}{0}
\renewcommand{\therelation}{\arabic{relation}}
\newcommand{\relationautorefname}{Relation}

\newenvironment{relation}[1][]{%
    \refstepcounter{relation}%
    \noindent \raggedright \textit{\textbf{Relation. \therelation}} \hfill$}
{%
$ \hfill \phantom{x}

}

\makeatletter
\newcommand{\linebreakand}{%
  \end{@IEEEauthorhalign}
  \hfill\mbox{}\par
  \mbox{}\hfill\begin{@IEEEauthorhalign}
}
\makeatother

% =======================================

\begin{document}

\title{An Organization-oriented MARL Approach for Cyberdefense: Application for a Drone Scenario}

\author{

    \IEEEauthorblockN{Julien Soulé}
    \IEEEauthorblockA{\textit{Thales Land and Air Systems, BU IAS}}
    %Rennes, France \\
    \IEEEauthorblockA{\textit{Univ. Grenoble Alpes,} \\
        \textit{Grenoble INP, LCIS, 26000,}\\
        Valence, France \\
        julien.soule@lcis.grenoble-inp.fr}

    \and

    \IEEEauthorblockN{Louis-Marie Traonouez}
    \IEEEauthorblockA{\textit{Thales Land and Air Systems, BU IAS} \\
        Rennes, France \\
        louis-marie.traonouez@thalesgroup.com}

    \and

    \IEEEauthorblockN{Paul Théron}
    \IEEEauthorblockA{
        \textit{Co-leader of the IST-152 OTAN, 1st president of the AICA IWG} \\
        La Guillermie, France \\
        %lieu-dit Le Bourg, France \\
        paul.theron@orange.fr}

    % \linebreakand

    \and

    \IEEEauthorblockN{Jean-Paul Jamont\IEEEauthorrefmark{1}, Michel Occello\IEEEauthorrefmark{2}}
    \IEEEauthorblockA{\textit{Univ. Grenoble Alpes,} \\
        \textit{Grenoble INP, LCIS, 26000,}\\
        Valence, France \\
        \{\IEEEauthorrefmark{1}jean-paul.jamont,\IEEEauthorrefmark{2}michel.occello\}@lcis.grenoble-inp.fr
    }
}

% \IEEEauthorblockN{Michel Occello}
% \IEEEauthorblockA{\textit{Univ. Grenoble Alpes,} \\
% \textit{Grenoble INP, LCIS, 26000,}\\
% Valence, France \\
% michel.occello@lcis.grenoble-inp.fr}


\maketitle

\begin{abstract}
    Autonomous Cyber Defense is increasingly critical in protecting decentralized environments where centralized approaches are impractical. Multi-Agent Reinforcement Learning (MARL) has shown promise in training agents to achieve collective objectives without explicit organizational designs. However, enhancing the explainability and robustness of MARL in complex environments, such as cyberdefense scenarios, remains challenging. This paper introduces PRAHOM, an algorithmic approach that integrates the $\mathcal{M}OISE^+$ organizational model into the MARL process. PRAHOM constrains agents' policies according to organizational specifications derived from joint histories, ensuring alignment with predefined roles and missions. We apply PRAHOM to a cyberdefense scenario from the CAGE Challenge 3, involving a drone swarm facing malware attacks. The results demonstrate that PRAHOM can effectively guide the training of agents to detect, mitigate, and eliminate malware while adhering to organizational constraints, enhancing the swarm's resilience and operational effectiveness.
\end{abstract}

\begin{IEEEkeywords}
    Multi-Agent Reinforcement Learning, Cyberdefense, Drones, Autonomous Cyber Operation
\end{IEEEkeywords}

% ====================================================================================================

\section{Introduction}
\label{sec:introduction}

Autonomous Cyber Defense is increasingly critical in protecting decentralized environments where traditional centralized approaches are impractical or ineffective. One promising avenue for achieving robust and adaptive defense mechanisms in such settings is Multi-Agent Reinforcement Learning (MARL). MARL has been successfully applied to train agents that collectively achieve complex objectives, often without an explicit organizational design. However, a key challenge remains: enhancing the explainability and robustness of these learned behaviors, especially in high-stakes domains like cyberdefense.

In decentralized environments, agents must operate under uncertain and dynamic conditions, often requiring them to collaborate and adapt in real-time. The lack of a centralized authority necessitates a method to ensure that the collective behavior of agents aligns with predefined organizational goals and roles. This is where organizational models, such as $\mathcal{M}OISE^+$, can play a crucial role. By embedding organizational constraints directly into the MARL process, we can guide the training of agents to adhere to specific roles and missions, enhancing both their performance and interpretability.

This paper introduces PRAHOM, an algorithmic approach that integrates the $\mathcal{M}OISE^+$ organizational model into the MARL framework. PRAHOM aims to constrain the agents' policies according to organizational specifications derived from joint histories. This approach not only aligns agents' behaviors with predefined roles and missions but also provides a structured way to incorporate domain-specific knowledge into the training process.

We demonstrate the effectiveness of PRAHOM through a case study based on the CAGE Challenge 3, which involves a drone swarm tasked with defending against malware attacks. This scenario is particularly relevant to cyberdefense, as it encapsulates the challenges of detecting, mitigating, and eliminating threats in a decentralized and dynamic environment. By applying PRAHOM, we show that it is possible to train agents that not only achieve high performance but also exhibit collective strategies that are both effective and interpretable.

The rest of the paper is organized as follows. Section \ref{sec:related_works} reviews related works in the fields of MARL and organizational modeling. Section \ref{sec:theoretical_foundations} presents the theoretical foundations of PRAHOM. Section \ref{sec:methodology} details the implementation of PRAHOM and its application to the CAGE Challenge 3 scenario. Section \ref{sec:results} discusses the experimental setup, results, and analysis. Finally, Section \ref{sec:conclusion} concludes the paper and outlines future research directions.

\section{Related Works and Positioning}
\label{sec:related_works}

The integration of organizational models with Multi-Agent Reinforcement Learning (MARL) represents a convergence of two significant research areas in artificial intelligence and multi-agent systems. This section reviews the pertinent literature on MARL, organizational modeling in multi-agent systems, and the application of these techniques to cybersecurity, positioning our approach within this context.

\subsection{Multi-Agent Reinforcement Learning}
MARL has garnered substantial attention for its ability to train multiple agents to collaborate or compete in shared environments. Prominent algorithms such as MADDPG~\cite{lowe2017multi}, MAPPO~\cite{yu2022surprising}, and Q-MIX~\cite{rashid2018qmix} have demonstrated effectiveness in various domains, from game playing to robotic control. These methods focus on learning optimal policies for agents through interactions with the environment and other agents, often without explicit consideration of organizational structures or predefined roles.

\subsection{Organizational Modeling in Multi-Agent Systems}
Organizational models provide a framework to define roles, responsibilities, and interactions within a multi-agent system. The $\mathcal{M}OISE^+$ model~\cite{hubner2007jacamo}, for example, offers a comprehensive approach to specifying organizational structures, including roles, missions, and norms. Integrating such models into MARL can enhance the explainability and reliability of learned behaviors by ensuring that agents' actions align with organizational objectives and constraints. Previous works have explored similar integrations, such as the ORA4MAS framework~\cite{boissier2013ora4mas}, which embeds organizational artifacts into agent environments to guide their behaviors.

\subsection{Applications in Cybersecurity}
Cybersecurity is a critical application domain for MARL and organizational modeling. The dynamic and adversarial nature of cyber environments poses unique challenges that require adaptive and coordinated defense mechanisms. Recent studies have employed MARL for various cybersecurity tasks, including intrusion detection~\cite{gomes2019dynamic}, threat hunting~\cite{nguyen2021reinforcement}, and autonomous defense strategies~\cite{zhang2020multi}. However, these approaches often lack an explicit organizational perspective, which can limit their effectiveness in complex, decentralized scenarios.

\subsection{Positioning of PRAHOM}
The PRAHOM approach bridges the gap between MARL and organizational modeling by embedding $\mathcal{M}OISE^+$ specifications into the MARL training process. This integration allows for the training of agents that not only perform well but also adhere to predefined organizational roles and missions. By applying PRAHOM to a drone swarm scenario in the CAGE Challenge 3, we demonstrate its potential in enhancing both the performance and interpretability of autonomous cyber defense mechanisms in decentralized environments.

In summary, PRAHOM extends the current state of the art by combining the strengths of MARL and organizational models, addressing key challenges in explainability and robustness, particularly in the context of cyber defense.


\section{Theoretical foundations}

In this section, we first introduce the basics PRAHOM is relying on concerning MARL and the $\mathcal{M}OISE^{+}$ organizational model. Then, we  describe how joint-policies in MARL context can be linked to the $\mathcal{M}OISE^{+}$ organizational model.

\subsection{Multi-Agent Reinforcement Learning (MARL)}

Multi-Agent Reinforcement Learning (MARL) extends the principles of Reinforcement Learning (RL) to scenarios involving multiple interacting agents. Each agent aims to maximize its own cumulative reward through learning, but the presence of other agents introduces additional complexity due to the dynamic nature of the environment.

MARL is often modeled using the Decentralized Partially Observable Markov Decision Process (Dec-POMDP), a framework suitable for environments where agents have limited and differing observations. A Dec-POMDP is formally defined as a tuple $(S, \{A_i\}, T, R, \{\Omega_i\}, O, \gamma)$:

\begin{itemize}
    \item $S$: A finite set of states describing the environment.
    \item $\{A_i\}$: A set of action sets, one for each agent $i$.
    \item $T$: The state transition probability function $T(s, \vec{a}, s') = P(s'|s, \vec{a})$, where $\vec{a}$ is the joint action of all agents.
    \item $R$: The reward function $R(s, \vec{a}, s')$, mapping states and actions to a reward.
    \item $\{\Omega_i\}$: A set of observation sets, one for each agent $i$.
    \item $O$: The observation probability function $O(\vec{o} | s', \vec{a})$, where $\vec{o}$ is the joint observation of all agents.
    \item $\gamma \in [0,1]$: The discount factor for future rewards.
\end{itemize}

In MARL, each agent $i$ maintains a policy $\pi_i: H_i \rightarrow A_i$, mapping its observation history $H_i$ to actions. Joint policies $\pi = (\pi_1, \pi_2, \ldots, \pi_n)$ describe the behavior of all agents collectively.

\subsubsection{Solution Approaches}

Several approaches exist for solving MARL problems, often categorized into value-based, policy-based, and actor-critic methods.

\paragraph{Value-Based Methods}

Value-based methods, such as Q-learning, extend to the multi-agent context through techniques like Independent Q-learning (IQL) and Joint Action Learners (JAL). These methods estimate the value function $Q(s, \vec{a})$, representing the expected cumulative reward for state $s$ and joint action $\vec{a}$.

\paragraph{Policy-Based Methods}

Policy-based methods directly parameterize the policy $\pi_{\theta}$ with parameters $\theta$, and optimize $\theta$ to maximize the expected return. Multi-agent policy gradient methods, like MADDPG (Multi-Agent Deep Deterministic Policy Gradient), introduce centralized training with decentralized execution, using a critic that considers the actions and observations of all agents during training.

\paragraph{Actor-Critic Methods}

Actor-critic methods combine value-based and policy-based approaches. The actor updates the policy parameters in the direction suggested by the critic, which evaluates the current policy. In the multi-agent setting, COMA (Counterfactual Multi-Agent Policy Gradients) employs a centralized critic to compute counterfactual baselines, reducing the variance of policy gradient estimates.

\subsubsection{Optimization Objectives}

Key optimization objectives in MARL include:

\begin{itemize}
    \item \textbf{Maximizing Expected Cumulative Reward}: Finding policies $\pi_i$ that maximize the expected cumulative reward $U(\pi) = \mathbb{E}\left[\sum_{t=0}^{\infty} \gamma^t R(s_t, \vec{a}_t)\right]$.
    \item \textbf{Best Response (BR)}: Identifying policies that form a best response to the policies of other agents.
    \item \textbf{Sufficient Response (SR)}: Finding policies that achieve a satisfactory level of reward, often used in approximate solutions.
\end{itemize}

\subsubsection{Challenges and Research Directions}

MARL faces challenges such as non-stationarity, partial observability, and the scalability of algorithms. Ongoing research explores improved learning algorithms, communication protocols among agents, and mechanisms for cooperative and competitive scenarios.

In summary, MARL is a complex but powerful framework for multi-agent systems, leveraging techniques from RL to enable agents to learn optimal behaviors in dynamic and uncertain environments.


\subsection{Organizational Model}

The $\mathcal{M}OISE^+$ model~\cite{Hubner2007} provides a comprehensive framework for specifying the structure and interactions within a Multi-Agent System (MAS). We utilize $\mathcal{M}OISE^+$ due to its compatibility with MARL and its formal description capabilities for agents' policies.

In $\mathcal{M}OISE^+$, \textbf{Organizational Specifications (OS)} are defined as $\mathcal{OS} = \langle \mathcal{SS}, \mathcal{FS}, \mathcal{DS} \rangle$, where $\mathcal{SS}$ are the \textbf{Structural Specifications}, $\mathcal{FS}$ are the \textbf{Functional Specifications}, and $\mathcal{DS}$ are the \textbf{Deontic Specifications}.

\textbf{Structural Specifications (SS)} describe the organizational structure and are denoted as $\mathcal{SS} = \langle \mathcal{R}, \mathcal{IR}, \mathcal{G} \rangle$:

\begin{itemize}
    \item $\mathcal{R}_{ss}$: The set of all roles ($\rho \in \mathcal{R}$).
    \item $\mathcal{IR}: \mathcal{R} \rightarrow \mathcal{R}$: The inheritance relation between roles ($\rho_1 \sqsubset \rho_2$).
    \item $\mathcal{RG} \subseteq \mathcal{GR}$: The set of root groups, $\mathcal{GR} = \langle \mathcal{R}, \mathcal{SG}, \mathcal{L}^{intra}, \mathcal{L}^{inter}, \mathcal{C}^{intra}, \mathcal{C}^{inter}, np, ng \rangle$, where:
    \begin{itemize}
        \item $\mathcal{R} \subseteq \mathcal{R}_{ss}$: The set of non-abstract roles.
        \item $\mathcal{SG} \subseteq \mathcal{GR}$: The set of sub-groups.
        \item $\mathcal{L} = \mathcal{R} \times \mathcal{R} \times \mathcal{TL}$: The set of links $(\rho_s, \rho_d, t)$, where $t \in \{acq, com, aut\}$ indicates the type of link (acquaintance, communication, authority).
        \item $\mathcal{L}^{intra} \subseteq \mathcal{L}$: The set of intra-group links.
        \item $\mathcal{L}^{inter} \subseteq \mathcal{L}$: The set of inter-group links.
        \item $\mathcal{C} = \mathcal{R} \times \mathcal{R}$: The set of compatibilities $(\rho_a \bowtie \rho_b)$.
        \item $\mathcal{C}^{intra} \subseteq \mathcal{C}$: The set of intra-group compatibilities.
        \item $\mathcal{C}^{inter} \subseteq \mathcal{C}$: The set of inter-group compatibilities.
        \item $np: \mathcal{R} \rightarrow \mathbb{N} \times \mathbb{N}$: The cardinality of agents adopting a role.
        \item $ng: \mathcal{SG} \rightarrow \mathbb{N} \times \mathbb{N}$: The cardinality of each sub-group.
    \end{itemize}
\end{itemize}

\textbf{Functional Specifications (FS)} describe the tasks and goals and are denoted as $\mathcal{FS} = \langle \mathcal{SCH}, \mathcal{PO} \rangle$:

\begin{itemize}
    \item $\mathcal{SCH} = \langle \mathcal{G}, \mathcal{M}, \mathcal{P}, mo, nm \rangle$: The set of social schemes, where:
    \begin{itemize}
        \item $\mathcal{G}$: The set of global goals.
        \item $\mathcal{M}$: The set of mission labels.
        \item $\mathcal{P} = \langle \mathcal{G}, \{\mathcal{G}\}^s, OP, [0,1] \rangle$: The set of plans $(g_f, \{g_i\}_{0 \leq i \leq s}, op, ps)$.
        \item $mo: \mathcal{M} \rightarrow \mathbb{P}(\mathcal{G})$: The goals associated with each mission.
        \item $nm: \mathcal{M} \rightarrow \mathbb{N} \times \mathbb{N}$: The cardinality of agents committed to each mission.
    \end{itemize}
    \item $\mathcal{PO}: \mathcal{M} \times \mathcal{M}$: The set of preference orders $(m_1 \prec m_2)$.
\end{itemize}

\textbf{Deontic Specifications (DS)} define the permissions and obligations and are denoted as $\mathcal{DS} = \langle \mathcal{OBL}, \mathcal{PER} \rangle$:

\begin{itemize}
    \item $\mathcal{TC}$: The set of time constraints.
    \item $\mathcal{OBL}: \mathcal{R} \times \mathcal{M} \times \mathcal{TC}$: The set of obligations $(obl(\rho_a, m, tc))$.
    \item $\mathcal{PER}$: The set of permissions $(per(\rho_a, m, tc))$.
\end{itemize}

The $\mathcal{M}OISE^+$ formalism offers a robust framework for defining the organization within a MAS, ensuring a clear and structured approach to specifying roles, tasks, and interactions.



\section{Linking MARL and organizational model: PRAHOM}

This section outlines the construction of PRAHOM by establishing features that apply organizational specifications to policies:

\quad $(\mathbf{F_{DS}})$ \quad If an agent is constrained to a role and permitted/obligated to a mission, then $(\mathbf{F_{SS}})$ should be satisfied and $(\mathbf{F_{FS}})$ should be satisfied depending on the time constraint considering an agent is constrained to a mission.

\quad $(\mathbf{F_{SS}})$ \quad If an agent is constrained to a role, then its policy should necessarily belong to a policy subset associated with the given role throughout training.

\quad $(\mathbf{F_{FS}})$ \quad If an agent is constrained to a mission, then it should be encouraged to achieve the mission's goals according to the associated plan throughout training.

We will discuss the challenges in implementing these features and our proposed workarounds. Then, a synthesis of PRAHOM is presented in \Autoref{sec:prahom_alg}.

\

\subsection{\textbf{$(\mathbf{F_{SS}})$: Constraining joint-policies according to roles}}

\

\textbf{Sub-gap $\mathbf{F_{SS}.G_1}$} \quad Roles need to be represented by policies in MARL, addressing the lack of common ground between policies and roles to determine expected agent behavior.

\textbf{Workaround $\mathbf{F_{SS}.G_1}$} \quad We propose mapping a role to a history subset using the $rh: \mathcal{R} \rightarrow \mathcal{P}(H)$ bijection to show how an agent's policy should generate histories belonging to an expected history subset when playing a role.

\begin{figure}[h!]
    \centering
    \input{figures/PRAHOM_osm_rels.tex}
    \caption{Relations between organizational specifications and history subsets}
    \label{fig:PRAHOM_osm_rels}
\end{figure}

\

\textbf{Sub-gap $\mathbf{F_{SS}.G_2}$} \quad Representing complex behaviors with large history subsets is impracticable due to exponential complexity and handling non-convenient large observations or actions.

\textbf{Workaround $\mathbf{F_{SS}.G_{2.1}}$} \quad Using labels to represent observations, we introduce $ol: \mathcal{P}(\Omega) \rightarrow \mathcal{P}(L)$ to map simple strings to real observations. The relation $bld_{ol}$ enables creating a labels-observations mapping.

\begin{figure}[h!]
    \centering
    \input{figures/ol_scheme.tex}
    \caption{Observations-labels mapping and its creation}
    \label{fig:PRAHOM_ol}
\end{figure}

\

\textbf{Workaround $\mathbf{F_{SS}.G_{2.2}}$} \quad Simplify history subsets as constraints on policy $c\pi \in c\Pi$ by introducing an observable policy constraint $c\pi: H \times \Omega \rightarrow \mathcal{P}(A)$. The relation $bld_{opc}: \mathcal{P}(H) \rightarrow c\Pi$ enables creating an observable policy constraint from a given history subset.

\begin{figure}[h!]
    \centering
    \input{figures/opc_scheme.tex}
    \caption{Observable policy constraint and its creation}
    \label{fig:PRAHOM_opc}
\end{figure}

\

\textbf{Sub-gap $\mathbf{F_{SS}.G_3}$} \quad Ensuring an agent's policy generates histories belonging to a given history subset of a role.

\textbf{Workaround $\mathbf{F_{SS}.G_3}$} \quad Integrate constraints into an agent's policy $\pi \in \Pi$ via three modes:

\begin{itemize}
    \item \textbf{Correct}: Correct any chosen action $\pi(\omega)$ by an expected one in $c\pi(\omega)$. It ensures safety but is external to the agent's policy.
    \item \textbf{Penalize}: Add a penalty to the reward if a wrong action ($\pi(\omega) \notin c\pi(\omega)$) is made, encouraging agents to learn their constraints but without safety guarantees.
    \item \textbf{Correct\_Policy}: Create a constrained policy $\pi_c \in \Pi, \pi_c = bld_{\pi c}(\pi, c\pi)$, where $c\pi$ corrects the current policy $\pi$. This ensures safety internally.
\end{itemize}

\

\subsection{\textbf{$(\mathbf{F_{FS}})$: Constraining joint-policies according to missions}}

\

\textbf{Sub-gap $\mathbf{F_{FS}.G_1}$} \quad Representing a goal by a reward function in MARL and updating the joint reward function when an agent is committed to a mission.

\textbf{Workaround $\mathbf{F_{FS}.G_1}$} \quad Map a goal to a history subset using $gh: \mathcal{G} \rightarrow \mathcal{P}(H)$. Thus, $mh: \mathcal{M} \rightarrow \mathcal{P}(H) = \{(m,\bigcup gh[mo(m)]), m \in \mathcal{M}\}$ gives expected histories from missions.

\

\textbf{Sub-gap $\mathbf{F_{FS}.G_2}$} \quad Represent a goal in MARL formalism to entice agents to achieve it.

\textbf{Workaround $\mathbf{F_{FS}.G_2}$} \quad Represent a goal $g \in \mathcal{G}$ by introducing an observable reward function $R_{g}: H \rightarrow \mathbb{R} = bld_{grf}(H_g)$. The relation $dist: H \times H \rightarrow \mathbb{R}$ indicates how close a generated history is to a given history subset.

A mission $m \in \mathcal{M}$ containing goals $\mathcal{G}_{m} = mo(m)$ is represented as $R_m: H \rightarrow \mathbb{R} = comb_{grf}(\{bld_{grf}(H_g), g \in G\})$. The relation $comb_{grf}: \mathcal{P}(\mathcal{R}_{\mathcal{G}_{m}}) \rightarrow R_{\mathcal{M}}$ combines all observable reward functions of each goal into a single reward function for a mission.

\begin{figure}[h!]
    \centering
    \input{figures/goal_mission_scheme.tex}
    \caption{Observable reward function and its creation}
    \label{fig:goal_mission_scheme}
\end{figure}

\subsection{\textbf{$(\mathbf{F_{DS}})$: Constraining joint-policies according to permissions/obligations}}

\

\textbf{Sub-gap $\mathbf{F_{DS}.G_1}$} \quad Integrate time constraints within the training process using previous workarounds in $(\mathbf{F_{SS}})$ and $(\mathbf{F_{FS}})$.

\textbf{Workaround $\mathbf{F_{DS}.G_1}$} \quad Introduce the relation $da: \mathcal{PER} \cup \mathcal{OBL} \rightarrow \mathcal{P}(\mathcal{A})$ indicating how agents are constrained to roles and committed to missions for a given time constraint. Time-to-live for each permission/obligation is introduced via $dttl: \mathcal{PER} \cup \mathcal{OBL} \times \mathcal{A} \rightarrow \mathbb{N}$ and decreased at each step. Roles and missions may change, updating the reward function and policies via $update_{R,\Pi}$.

\

\textbf{Sub-gap $\mathbf{F_{DS}.G_2}$} \quad Differentiate between obligations and permissions, ensuring agents prioritize obligated missions over permitted ones.

\textbf{Workaround $\mathbf{F_{DS}.G_2}$} \quad Use $comb_{mrf}$ as a weighted sum with high weight for obligated missions and low weight for permitted ones.


\subsection{PRAHOM Algorithm}\label{sec:prahom_alg}

% $PRAHOM$ algorithm is presented in \autoref{alg:PRAHOM-A}. It fits within a regular MARL context: a joint-policy $\pi_{joint,ep}$ is updated after several steps allowing to have enough joint-histories $h_{joint,ep}$ and joint-rewards as feedback $r_{joint,ep}$. The training goes on over several episodes for better training until converging to a sufficient cumulative reward regarding $s$.

% $PRAHOM$ underlying idea is to represent roles and missions through permissions/obligations as history subset that agents are expected to generate while interacting.

The PRAHOM algorithm, as outlined in \autoref{alg:PRAHOM-A} (cf. annex), integrates organizational specifications into the learning process of multi-agent reinforcement learning (MARL) by constraining joint-policies according to predefined roles, missions, and permission/obligation relations. This method ensures that the agents' behaviors align with organizational requirements while optimizing joint-policy ($\pi_{joint,i,s}$) as output.

\textbf{Initialization and Input Parameters}: \quad
The algorithm starts by initializing the joint-policy ($\pi_{joint}$) with the initial joint-policy ($\pi_{joint,i,init}$) and setting the episode counter ($ep$) to zero (lines 1). The algorithm also sets a boolean variable ($sufficient$) to False, indicating that the cumulative reward expectancy has not yet been met.

\textbf{Step 1: Determine Joint Observable Policy Constraints}: \quad
PRAHOM determines the joint observable policy constraints ($c\pi_{joint}$) from the organizational specifications (lines 2). This step involves mapping roles and permissions/obligations to constraints on the joint-policy, ensuring that the agents' actions are in line with the organizational rules.

\textbf{Step 2: Initialize Constrained Policy}: \quad
If the mode of constraint integration is set to \textit{correct\_policy}, the algorithm creates and uses a joint constrained policy via $bld_{joint, \pi_c}$ based on the initial policy and the observable policy constraints ($c\pi_{joint}$) (line 4). This ensures that the initial policy is inherently aligned with the specified constraints from the beginning.

\textbf{Step 3: Determine Observable Reward Functions}: \quad
Next, PRAHOM determines the observable reward functions $R_{m,joint}$ from the organizational specifications (line 5). This involves mapping missions and goals to reward functions, which will guide the agents towards achieving organizational sub-goals.

\textbf{Step 4: Main Training Loop}: \quad
The main training loop runs for a maximum number of episodes ($ep_{max}$) or until the cumulative reward expectancy is met (line 6).

\begin{itemize}
    \item \textbf{Initialize Episode}:
          At the beginning of each episode, the environment, observation, and action histories are reinitialized (line 7). The cumulative reward and penalty are also reset. Time-to-live values for permissions/obligations are initialized using $bld_{dttl}$, and the initial observation and action are set by the environment ($d_{ep}.init()$).

    \item \textbf{Step Through Episode}:
          The algorithm steps through each episode for a maximum number of steps ($step_{max}$) (line 8). Within each step:

          \begin{itemize}
              \item \textbf{Policy Update}:
                    The joint-policy ($\pi_{joint}$) is updated using the MARL algorithm ($u_{marl}$) based on the current history and rewards (line 9).

              \item \textbf{Action Selection}:
                    The next action is selected based on the current observation ($\pi_{joint}(\omega_{joint,t=step})$) (line 10).

              \item \textbf{Action Validation}:
                    The expected actions are determined from the observable policy constraints ($c\pi_{joint}$), and if the selected action is not among the expected ones, it is either corrected or penalized based on the mode of constraint integration (lines 11-16).

              \item \textbf{Update Histories and Rewards}:
                    The current history is updated (line 17), and the action is applied to the environment to get the next observation and reward (line 18) adding any incurred penalties (line 19).

              \item \textbf{Update Constraints and Rewards}:
                    Time-to-live values are decreased, and the reward functions and policies are updated accordingly (line 20).
          \end{itemize}

    \item \textbf{Check for Sufficiency}:
          After each episode, the algorithm checks whether the cumulative reward meets the expectancy ($is\_sufficient$) and increments the episode counter ($ep$) (line 21).
\end{itemize}

\textbf{Complexity and Scalability}: \quad The computational complexity of PRAHOM primarily depends on the number of episodes ($ep_{max}$), the number of steps per episode ($step_{max}$), and the complexity of the MARL algorithm ($u_{marl}$). Each step involves updating the policy, selecting \& validating actions, and updating histories and rewards, which can be computationally intensive. The memory complexity depends on the need to store rewards, policies, and histories. The history graphs we introduced allow histories to be merged compactly, reducing redundancy with limited impact on read cost.

Scalability can be challenging as the number of agents, roles, missions increases. Evaluating PRAHOM scalability requires to consider an implemented version through these salient features:
\begin{itemize}
    \item \textbf{Parallelization:} Performance for updating/validating policies strongly depends on chosen MARL algorithms capabilities such as parallelization significantly improves it (especially in Policy Optimization algorithms);
    \item \textbf{Efficient Data Structures:} The choice of data structures for storing and accessing histories or policies is crucial for reducing overhead.;
    \item \textbf{Optimization of Reward Functions:} The logic for constructing/handling reward functions is important to enhance performance further.
\end{itemize}


\section{Case study: Predator-prey environment}

\subsection{Algorithm implementation: PRAHOM Wrapper}

We developed the \textit{PRAHOM Wrapper}
%
\footnote{Additional information are provided in addition to the code in \url{https://github.com/julien6/omarl_experiments}.}
%
as a proof-of-concept (PoC) tool to augment PettingZoo environments, facilitating the application of the PRAHOM algorithm. This wrapper includes additional class methods to implement PRAHOM's functionalities, streamlining the training process. The wrapper leverages the MARLlib~\cite{hu2022marllib} library, which offers a comprehensive range of state-of-the-art MARL algorithms and finely-tuned models.

\textbf{History Subsets} \quad We implemented a \textit{history\_subset} class to handle history subsets based on predefined patterns or rules via an implemented history graph when dealing with patterns. Alternatively, custom functions can be employed to instantiate a history\_subset, allowing more flexibility.

\textbf{Organizational Specifications to Relations} \quad The \textit{osr} class models a complete $\mathcal{M}OISE^+$ organizational model that includes roles, goals, and missions, which are directly mapped to \textit{history\_subset} objects initialized with expected patterns. This mapping implements \textit{rh}, \textit{gh}, and \textit{mh}. Permissions/obligations are also defined in the \textit{osr}, mapping the agents constrained to roles and committed to missions with time constraints, thus implementing the \textit{da} relation.

\textbf{Observation Labeling (ol)} \quad The observation labeling is managed by the singleton \textit{ol\_mngr} class, which uses a regular dictionary to map observations to labels. It leverages the HuggingFace transformer model \textit{tiiuae/falcon-7b} to learn the mappings alongside the dictionary. The \textit{ol\_mngr} class includes an interactive process for users to label each displayed observation during the labeling process. %Post-training, the LLM can be employed to identify desired observations based on keywords, enhancing usability.

\textbf{History Graph} \quad We constructed the \textit{history graph} class from scratch following the theoretical foundation of ordinal numbers and cardinalities. The class provides functions to add histories or history patterns and to check whether a given history matches the stored patterns.

\textbf{Observable Policy Constraint (opc)} \quad The \textit{opc} class links pairs of (history\_pattern, observation) to lists of actions. Each association is integrated into a unified history graph object, ensuring consistency in policy constraints.

\textbf{Constrained Policy (cons\_policy)} \quad The \textit{cons\_policy} class combines a regular MARLlib policy with an \textit{opc} object. The integration involves embedding policy constraints within the MARLlib policy framework, allowing the constrained policy to respect predefined action constraints.

\textbf{Observable Reward Function (orf)} \quad The \textit{orf} class constructs an observable reward functions based on history patterns. It includes a function that evaluates whether a given history matches the initial pattern: high rewards for matching patterns, low rewards otherwise.

\textbf{Time-to-Live (dttl)} \quad The \textit{dttl} class is built using a Python dictionary that replicates data from the \textit{osr} object. This class manages the time constraints to update the observable reward function and policy constraints.

Once the PettingZoo environment is wrapped with the PRAHOM Wrapper, a function is provided to execute the PRAHOM algorithm. This function accepts a JSON object containing MARLlib details, such as the chosen MARL algorithm (e.g., MADDPG or MAPPO). It processes all defined elements, retrieves roles and missions, converts them into \textit{opc} or \textit{orf} objects and so on. Ultimately outputs a dictionary mapping each agent to its trained policy.

\section{Experimental Setup}

\subsection{CybORG CAGE Challenge 3 Environment}

We selected the CybORG CAGE Challenge 3 environment, which simulates a swarm of autonomous drones operating in a complex cyber-infrastructure. The environment includes multiple drones (\texttt{drone\_0}, \texttt{drone\_1}, \ldots, \texttt{drone\_n}), each with specific capabilities and roles within the swarm. The goal is to secure critical assets and protect against cyber-attacks.

\textbf{Environment Details:}
\begin{itemize}
    \item \textbf{Agents:} Multiple autonomous drones with varying capabilities.
    \item \textbf{Actions:} Drone maneuvers, cyber-attacks, defensive actions.
    \item \textbf{Observation and State Shapes:} Diverse sensor data and cyber-threat indicators.
    \item \textbf{Rewards:} Points for securing assets, preventing attacks, and neutralizing threats.
\end{itemize}

We utilized the MAPPO and MADDPG algorithms from MARLlib tailored for cooperative behavior among autonomous agents.

\subsection{Organizational Models}

We implemented and evaluated four organizational models tailored for the CybORG CAGE Challenge 3 environment:

\begin{itemize}
    \item \textbf{Suspect Isolation Model:} Identifies and isolates drones exhibiting suspicious cyber-activities.
    \item \textbf{Cyberdefense Model 1:} Coordinates defensive actions against incoming cyber-attacks.
    \item \textbf{Cyberdefense Model 2:} Implements proactive measures to detect and mitigate potential threats.
    \item \textbf{Manual Control Model:} A baseline model with manually defined roles and actions for drones.
\end{itemize}

\subsection{Experimental Procedure}

We trained each organizational model using the PRAHOM Wrapper within the CybORG CAGE Challenge 3 environment. The wrapper integrates organizational specifications and constraints into the MARLlib policies, enabling agents to learn and adapt their behavior based on predefined roles and missions.

\subsection{Evaluation Criteria}

Our evaluation criteria include both qualitative and quantitative assessments:

\begin{itemize}
    \item $(\mathbf{C1.1})$: Qualitative assessment involves observing the adoption of specific roles and strategies by drones, such as isolating suspicious activities or coordinating defensive maneuvers.
    \item $(\mathbf{C2.1})$: Quantitative metrics measure the convergence time and average rewards obtained by each model, indicating the effectiveness of organizational constraints.
    \item $(\mathbf{C3.1})$: Comparative analysis between models to determine which organizational structure best enhances collective cyber-defense capabilities.
\end{itemize}

\section{Results and Discussion}

The results from our experiments are presented below:

\subsection{Learning Curves and Performance Metrics}

Figure \ref{fig:learning_curves} illustrates the learning curves for the different organizational models, indicating their convergence rates over training episodes.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/prahom_learning_curve.png}
    \caption{Learning curves for organizational models in CybORG CAGE Challenge 3.}
    \label{fig:learning_curves}
\end{figure}

Table \ref{tab:results} summarizes the convergence time and average rewards achieved by each model during the training process.

\begin{table}[ht]
    \centering
    \begin{tabular}{|c|c|c|}
        \hline
        Model                   & Convergence Time (episodes) & Average Reward \\
        \hline
        Suspect Isolation Model & 250                         & 17.2           \\
        Cyberdefense Model 1    & 300                         & 18.5           \\
        Cyberdefense Model 2    & 280                         & 19.8           \\
        Manual Control Model    & 400                         & 15.6           \\
        \hline
    \end{tabular}
    \caption{Performance metrics for organizational models in CybORG CAGE Challenge 3.}
    \label{tab:results}
\end{table}

\subsection{Discussion}

\textbf{Qualitative Observations}:
The organizational models effectively influenced drone behavior in the CybORG CAGE Challenge 3 environment. The Suspect Isolation Model demonstrated effective identification and isolation of drones exhibiting suspicious activities, enhancing overall security.

\textbf{Quantitative Analysis}:
Cyberdefense Model 2 showed the fastest convergence and highest average reward, indicating its effectiveness in proactive threat mitigation. This model significantly outperformed the Manual Control Model, highlighting the benefits of automated organizational frameworks in dynamic cyber-defense scenarios.

The experimental results confirm the importance of organizational models in enhancing coordination and effectiveness among autonomous agents in complex cyber-physical environments like CybORG CAGE Challenge 3. Future research could explore further refinements and adaptations of organizational specifications to optimize collective behaviors and response capabilities.



\section{Conclusion}

% résumer nos contributions et nos principales conclusions.

% importance d’améliorer l’explicabilité de l’IA dans les systèmes MARL et souligner l’impact potentiel de PRAHOM sur l’avancement du domaine.

% proposer des remarques finales et décrire les orientations des recherches futures afin d’explorer et d’affiner davantage notre algo.

% Multi-agent methods rely on the designer's knowledge to design a suitable MAS organization, but do not provide automatic means to determine the relevant organizational mechanisms only from the design requirements and the overall objective.
% MARL techniques have been successfully applied to automatically train agents to achieve a given objective without explicit characterization of emerging collective strategies.
% The PRAHOM's originality is to enrich a MARL process with an explicit organizational model towards a methodological objective to address these issues. It links the agents' policies (modeled in a Dec-POMDP) with $\mathcal{M}OISE^+$ through the process PRAHOM. Under the simplifying conditions of a group and a single social pattern, PRAHOM makes it possible to partially determine organizational specifications from joint histories and to constrain the training of policies in relation to organizational specifications.
% Additionally, we implemented the \emph{PRAHOM PettingZoo} wrapper as a proof of concept to apply PRAHOM.
% Finally, we applied our approach in four \emph{PettingZoo} environments to evaluate the impact during and after training. The performances obtained appear to be comparable to those known.

% Key point summary

\section{Conclusion}

Our main contribution is the PRAHOM algorithmic approach that aims to link MARL and agents' policies with the $\mathcal{M}OISE^+$ model leveraging on relations between joint-histories and organizational specifications. It seeks to constrain the agents' training according to expected behaviors expressed as organizational specifications.

As a first implementation, \emph{PRAHOM Wrapper} is assessed in a cooperative Atari game, where the inferred organizational specifications generally match the expected hand-crafted results. Despite the implementation's limited functionalities, the initial results showed that the PRAHOM approach can constrain the resulting agents' policy space according to the specified organizational constraints. Additionally, we manually verified that some valuable organizational specifications are inferred from trained agents' histories.

Even though PRAHOM is agnostic of any MARL algorithm, directing the collective behaviors of agents a priori can prove difficult. Major perspectives to improve PRAHOM are inspired by hierarchical learning, which contributes to better emergence of strategies during learning. 

Furthermore, while the initial results obtained with \emph{LLM} show it as a promising complementary tool for PRAHOM, it may also illuminate new avenues for explaining collective behavior beyond its intended use.

% Ultimately, we also aim to improve the applicability of PRAHOM by developing dedicated interfaces built around PRAHOM making it more accessible to industrial and research contexts.



\section*{Acknowledgment}

This work was supported by \emph{Thales Land Air Systems} within the framework of the \emph{Cyb'Air} chair and the \emph{AICA IWG}

\section*{References}

% \bibliographystyle{abbrv}
\bibliographystyle{IEEEtran}

\bibliography{references}

\end{document}
