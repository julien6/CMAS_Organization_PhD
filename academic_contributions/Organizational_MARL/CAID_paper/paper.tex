\RequirePackage[2020-02-02]{latexrelease}
\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.

\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\usepackage[english]{babel}
\addto\extrasenglish{  
    \def\figureautorefname{Figure}
    \def\tableautorefname{Table}
    \def\algorithmautorefname{Algorithm}
    \def\sectionautorefname{Section}
    \def\subsectionautorefname{Subsection}
}

\newcommand{\supertiny}{\fontsize{1}{2}\selectfont}

\usepackage{catoptions}
\makeatletter

\def\Autoref#1{%
  \begingroup
  \edef\reserved@a{\cpttrimspaces{#1}}%
  \ifcsndefTF{r@#1}{%
    \xaftercsname{\expandafter\testreftype\@fourthoffive}
      {r@\reserved@a}.\\{#1}%
  }{%
    \ref{#1}%
  }%
  \endgroup
}
\def\testreftype#1.#2\\#3{%
  \ifcsndefTF{#1autorefname}{%
    \def\reserved@a##1##2\@nil{%
      \uppercase{\def\ref@name{##1}}%
      \csn@edef{#1autorefname}{\ref@name##2}%
      \autoref{#3}%
    }%
    \reserved@a#1\@nil
  }{%
    \autoref{#3}%
  }%
}
\makeatother

\usepackage[T1]{fontenc}
\usepackage{graphicx}
%\usepackage{color}
%\renewcommand\UrlFont{\color{blue}\rmfamily}

\usepackage{amsmath,amssymb,amsfonts}
\usepackage[inline, shortlabels]{enumitem}
\usepackage{tabularx}
\usepackage{caption}
\usepackage{listings}
% \usepackage{titlesec}
\usepackage{ragged2e}
% \usepackage[hyphens]{url}
\usepackage{hyperref}
\usepackage{xurl}
\usepackage{pifont}
\usepackage{footmisc}
\usepackage{multirow}
\usepackage[linesnumbered,ruled,vlined]{algorithm2e}
\usepackage{float}
\usepackage{listings}
\usepackage{xcolor}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}
 
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}
 
\lstset{style=mystyle}

% --- Tickz
\usepackage{physics}
\usepackage{amsmath}
\usepackage{tikz}
\usepackage{mathdots}
\usepackage{yhmath}
\usepackage{cancel}
\usepackage{color}
\usepackage{siunitx}
\usepackage{array}
\usepackage{multirow}
\usepackage{amssymb}
\usepackage{gensymb}
\usepackage{tabularx}
\usepackage{extarrows}
\usepackage{booktabs}
\usetikzlibrary{fadings}
\usetikzlibrary{patterns}
\usetikzlibrary{shadows.blur}
\usetikzlibrary{shapes}

% ---------
% \usepackage{titlesec}
\usepackage{pdfpages}
\usepackage{booktabs}
\usepackage{csquotes}
\usepackage{lipsum}  
\usepackage{arydshln}
\usepackage{smartdiagram}
\usepackage{textcomp}
\usepackage{tabularray}\UseTblrLibrary{varwidth}
\usepackage{xcolor}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\usepackage{cite}
\usepackage{amsmath}
\newcommand{\probP}{\text{I\kern-0.15em P}}
\usepackage{etoolbox}
\patchcmd{\thebibliography}{\section*{\refname}}{}{}{}

\setlength\tabcolsep{0.5pt}

\newcommand{\before}[1]{\textcolor{red}{#1}}
\newcommand{\after}[1]{\textcolor{green}{#1}}

\newcommand{\old}[1]{\textcolor{orange}{#1}}
\newcommand{\rem}[1]{\textcolor{red}{#1}}
\newcommand{\todo}[1]{\textcolor{orange}{\newline \textit{\textbf{TODO:} #1}} \newline \newline }



\newcounter{relation}
\setcounter{relation}{0}
\renewcommand{\therelation}{\arabic{relation}}
\newcommand{\relationautorefname}{Relation}

\newenvironment{relation}[1][]{%
    \refstepcounter{relation}%
    \noindent \raggedright \textit{\textbf{Relation. \therelation}} \hfill$}
{%
$ \hfill \phantom{x}

}

\makeatletter
\newcommand{\linebreakand}{%
  \end{@IEEEauthorhalign}
  \hfill\mbox{}\par
  \mbox{}\hfill\begin{@IEEEauthorhalign}
}
\makeatother

% =======================================

\begin{document}

\title{An Organization-oriented MARL Approach for Cyberdefense: Application for a Drone Scenario}

\author{

    \IEEEauthorblockN{Julien Soulé}
    \IEEEauthorblockA{\textit{Thales Land and Air Systems, BU IAS}}
    %Rennes, France \\
    \IEEEauthorblockA{\textit{Univ. Grenoble Alpes,} \\
        \textit{Grenoble INP, LCIS, 26000,}\\
        Valence, France \\
        julien.soule@lcis.grenoble-inp.fr}

    \and

    \IEEEauthorblockN{Jean-Paul Jamont\IEEEauthorrefmark{1}, Michel Occello\IEEEauthorrefmark{2}}
    \IEEEauthorblockA{\textit{Univ. Grenoble Alpes,} \\
        \textit{Grenoble INP, LCIS, 26000,}\\
        Valence, France \\
        \{\IEEEauthorrefmark{1}jean-paul.jamont,\IEEEauthorrefmark{2}michel.occello\}@lcis.grenoble-inp.fr
    }

    % \and

    % \IEEEauthorblockN{Michel Occello}
    % \IEEEauthorblockA{\textit{Univ. Grenoble Alpes,} \\
    % \textit{Grenoble INP, LCIS, 26000,}\\
    % Valence, France \\
    % michel.occello@lcis.grenoble-inp.fr}

    \and

    \IEEEauthorblockN{Paul Théron}
    \IEEEauthorblockA{
        \textit{Co-leader of the IST-152 OTAN, 1st president of the AICA IWG} \\
        La Guillermie, France \\
        %lieu-dit Le Bourg, France \\
        paul.theron@orange.fr}

    % \linebreakand

    \and

    \IEEEauthorblockN{Louis-Marie Traonouez}
    \IEEEauthorblockA{\textit{Thales Land and Air Systems, BU IAS} \\
        Rennes, France \\
        louis-marie.traonouez@thalesgroup.com}}

\maketitle

\begin{abstract}
    Designing a Multi-Agent Systems to achieve a goal in an environment often requires an organizational structure to coordinate and delegate tasks among agents. However, defining the internal logic of each agent can be challenging in complex environments. Multi-Agent Reinforcement Learning enables agents to learn how to reach a goal without explicitly considering the organization.
    While previous studies have introduced guided training in individual agents, a multi-agent context requires clarifying the implicit cooperation among multiple agents after training. We propose a novel algorithmic approach leveraging the $\mathcal{M}OISE^+$ Organizational Model that consists in linking organizational specifications, such as roles or missions, to the respective agents' histories, characterizing their behaviors. Our algorithm constrains the learning process based on organizational constraints. Evaluations conducted in a mixed competitive/cooperative Predator-Prey environment validate the impact of organizational specifications as constraints during training.
\end{abstract}

\begin{IEEEkeywords}
    Multi-Agent Reinforcement Learning, Design, Organizational Models, Multi-Agent Systems
\end{IEEEkeywords}

% ====================================================================================================

\section{Introduction}

% Context
In a top-down design approach, designing a Multi-Agent System (MAS) requires to establish an organization emendable into the agents' internal logics that explicates how agents coordinate their activities to collaboratively achieve a common expected goal~\cite{Picard2009}.
%
Consequently, designing a MAS can be viewed as an optimization problem, aiming to find the organization that promotes the best performance in achieving a goal. Methods such as GAIA~\cite{Wooldridge2000,Cernuzzi2014}, ADELFE~\cite{Mefteh2015}, MaSE~\cite{Deloach2001}, or KB-ORG~\cite{Sims2008} rely on an iterative process of trial and error to gain empirical knowledge of the environment to search for an appropriate MAS organization~\cite{Sims2008}.

% Problem
However, increasing this empirical knowledge can be costly for numerous factors including the environment complexity or its restricted access, the designers' unavailability\dots Moreover, relying solely on a limited knowledge may prevent to design a MAS that meets sufficient performances or that guarantees safety requirement~\cite{Mefteh2013}. Currently, no methods fully automate the design process of MAS. Addressing this would improve the trust and adoption of MARL systems in real-world applications~\cite{kok2006collaborative,omidshafiei2019learning}.

% Contribution
We propose an assisted MAS design approach called \textit{Partial Relations between Agents' Histories and Organizational Model} (PRAHOM). PRAHOM fits within a simulated version of the environment to leverage Multi-Agent Reinforcement Learning (MARL) by introducing organizational specifications from the $\mathcal{M}OISE^+$ model. To link $\mathcal{M}OISE^+$ and MARL, we one-to-one map an organizational specification (i.e a role or a mission) to a history subset characterizing an expected behavior impacting action space or the reward function accordingly.

Unlike, classical MARL where agents' logic (referred to as \textbf{policies}) is freely updated solely based on maximizing the reward, our approach also forces/entices agents to satisfy some hand-crafted organizational specifications during the training. By restricting the policy search space, our approach shortens the convergence time to stable policies showing a fine-tuned organization among trained agents that may prove to be relevant insights into future design in the real environment.

% Outline
The remainder is organized as follows: Section 2 gives an overview of the available works dealing with design within MARL. This work leads to present the theoretical foundations we retained for building our contribution in section 3. Section 4 introduces the PRAHOM algorithmic approach. Section 5 first presents our first PRAHOM implementation as a library we used in our experimental setup. Then we discuss the results of our empirical evaluation. Section 6 concludes the paper and outlines future research directions.


\section{Related works and positioning}

The integration of organizational specifications in multi-agent systems (MAS) learning processes is not largely addressed explicitly in literature. Yet, various approaches have been proposed to incorporate organizational constraints and policies in MAS to ensure agents adhere to certain requirements.

\

\textbf{Learning with Organizational Constraints} \quad
%
The integration of organizational constraints into the learning process of agents has been explored to various extents. In \cite{cruz2020norms}, the authors present a method for incorporating norms into the learning algorithms of agents, ensuring that their behavior remains within acceptable bounds. Additionally, \cite{villatoro2011social} proposes a mechanism for agents to learn and adapt to social norms in dynamic environments, highlighting the importance of norm adaptation in MAS.

A relevant work set belong to the \emph{Specification-Guided Reinforcement Learning} that aims to generate policies that accomplish a specific task using external specifications to guide learning in achieving an objective under given constraints~\cite{Bansal2022}. Jothimurugan et al.~\cite{Jothimurugan2021} propose logical specification learning as exploiting the compositional structure of specifications to generate policies for complex tasks.

\textbf{Policy-Based Approaches} \quad
%
Policy-based approaches provide a way to enforce organizational constraints by defining explicit policies that govern agent behavior. In \cite{krupanski2015norm}, the use of normative policies is investigated to guide agent interactions and decision-making processes. Moreover, \cite{vos2020governing} explores the use of governance mechanisms to enforce compliance with organizational policies in decentralized systems.

\textbf{Evaluation and Applications} \quad
%
Several studies have evaluated the effectiveness of organizational models and policies in various applications. For example, \cite{dignum2004agent} evaluates the application of organizational models in e-commerce systems, demonstrating their utility in ensuring reliable and predictable agent behavior. Similarly, \cite{andrighetto2013normative} investigates the role of normative systems in regulating agent behavior in collaborative environments.

\textbf{Frameworks Integrating Organizational Aspects} \quad
%
Wang et al.~\cite{Wang2020} introduce an approach in which similar emerging roles are pushed to jointly specialize in specific tasks. Tosic et al.~\cite{Tosic2010} propose a framework for coordination based on the communication capabilities of multi-agent systems. Zheng et al.~\cite{Zheng2018} present a platform for MARL that aims to facilitate research on artificial collective intelligence by providing a comprehensive set of evaluation metrics to compare the performance of MARL algorithms.

\

Despite these advancements, there is still a lack of works that explicitly use organizational specifications as a means of constraining agent learning regarding requirements. To our knowledge, no work can be used to generate a MAS that satisfy additional organizational constraints explicitly. Unlike these works, our originality is to explicitly use an organizational model as a general means of constraining learning regarding requirements.


\section{Theoretical foundations}

In this section, we first introduce the basics PRAHOM is relying on concerning MARL and the $\mathcal{M}OISE^{+}$ organizational model. Then, we  describe how joint-policies in MARL context can be linked to the $\mathcal{M}OISE^{+}$ organizational model.

\subsection{Markovian model for MARL}

To apply MARL techniques, we rely on the Decentralized Partially Observable Markov Decision Process (Dec-POMDP)~\cite{Oliehoek2016} because it considers multiple agents in a similar MAS manner. It relies on stochastic processes to model uncertainty of the environment for the changes induced by actions, received observations, communication\dots \ Additionally, unlike Partially Observable Stochastic Games (POSG), the reward function can be common to agents which fosters training for collaborative-oriented actions~\cite{Beynier2013}.
A Dec-POMDP $d \in D$ (with $D$ the set of Dec-POMDP) is a 7-tuple $d = (S,\{A_i\},T,R,\{\Omega_i\},O,\gamma)$ , where:
\begin{itemize}
    \item $S = \{s_1,...,s_{|S|}\}$: the set of possible states;
    \item $A_{i} = \{a_{1}^{i},...,a_{|A_{i}|}^{i}\}$: the set of possible actions for agent $i$;
    \item $T$ so that $T(s,a,s') = \probP{(s'|s,a)}$ : the set of conditional transition probabilities between states;
    \item $R: S \times A \times S \rightarrow \mathbb{R}$: the reward function;
    \item $\Omega_{i} = \{o_{1}^{i},...,o_{|\Omega_{i}|}^{i}\}$: the set of observations for agent $i$;
    \item $O$ so that $O(s',a,o) = \probP{(o|s',a)}$ : the set of conditional observation probabilities;
    \item $\gamma \in [0,1]$ : the discount factor.
\end{itemize}

Considering $m$ \textbf{teams} (also referred to as \textbf{groups}) each containing several agents among $\mathcal{A}$, we also detail the minimal formalism notation we re-used for solving the Dec-POMDP for a given team $i, 0 \leq i \leq m$ containing $n$ agents~\cite{Beynier2013,Albrecht2024}:

\begin{itemize}

    \item $\Pi$: the set of policies. A \textbf{policy} $\pi \in \Pi, \pi: H \times \Omega \rightarrow A$ associate an observation to an action optionally using previous experiences as histories. It represents the agent's internal logic;
    \item $\Pi_{joint}$: the set of joint-policies. A \textbf{joint-policy} $\pi_{joint} \in \Pi_{joint}, \pi_{joint}: H_{joint} \times \Omega^n \rightarrow A^n = \Pi^n$ chooses an action for each agent regarding their respective observation optionally using previous experiences as histories. It can be viewed as a set of policies used in agents;
    \item $H$: the set of histories. A \textbf{history} over $z \in \mathbb{N}$ steps (where $z$ is generally the maximum number of steps for an episode) is the $z$-tuple $h = ((\omega_{k}, a_{k}) | k \leq z, \omega \in \Omega, a \in A)$;
    \item $H_{joint}$: the set of joint-histories. A \textbf{joint-history} over $z \in \mathbb{N}$ steps $h_{joint} \in H_{joint}, h_{joint} = \{h_1,h_2...h_n\}$ is the set of agents' histories;
    \item $U_{joint,i}(<\pi_{joint,i}, \pi_{joint,-i}>): \Pi_{joint} \rightarrow \mathbb{R}$: gives the \textbf{expected cumulative reward} over a finite horizon (if $\gamma < 1$ or the number of steps in an episode is finite), with $\pi_{joint,i}$ the joint policy for team $i$ and $\pi_{joint,-i}$ all of the other concatenated joint-policies (considered as fixed);
    \item $BR_{joint,i}(\pi_{joint,i}) = argmax_{\pi_{joint,i}}(U(<\pi_{joint,i},\pi_{joint,-i}>))$: gives the \textbf{best response} $\pi_{joint,i}^*$ in the sense that the team cannot change any of the policies in the joint-policy $\pi_{joint,i}^*$ to get a better expected cumulative reward than $U_i^* = U_{joint,i}(<\pi_{joint,i}^*, \pi_{joint,-i}>)$;
    \item $SR_{joint,i}(\pi_{joint,i}, s) = \{\pi_{joint,i} | U(<\pi_{joint,i},\pi_{joint,-i}>) \geq s\}$: gives the \textbf{sufficient response} as the set of joint-policies getting at least $s \in \mathbb{R}, s \leq U_i^*$ as expected cumulative reward.
\end{itemize}

We refer to \textbf{solving} the Dec-POMDP for the team $i$ as finding a joint policy $\pi_{joint,i} \in \Pi_{joint}, \pi_{joint,i} = BR_{joint,i}(\pi_{joint,i})$ that maximizes the expected cumulative reward over a finite horizon.
We refer to \textbf{sub-optimally solving} the Dec-POMDP at $s$ expectancy as finding the joint policies $\pi_{joint,i} \in \Pi_{joint}, \pi_{joint,i} = SR_{joint,i}(\pi_{joint,i}, s)$ that gets the expected cumulative reward over a finite horizon at least at $s \in \mathbb{R}, s \leq U_i^*$.


\subsection{Organizational model}

$\mathcal{M}OISE^+$~\cite{Hubner2007} provides a relevant high-level description of the structures and interactions within the MAS. However, we favor $\mathcal{M}OISE^+$ because it provides an advanced formal description for an organization without incompatibilities with MARL, especially for a formal description of agents' policies.
Based on $\mathcal{M}OISE^+$~\cite{Hubner2007} formalism, we only give the elements of the formalism we used.

\textbf{Organizational specifications (OS)} refer to some information describing an organization, we denote them as $\mathcal{OS} = \langle \mathcal{SS}, \mathcal{FS}, \mathcal{DS} \rangle$, where $\mathcal{SS}$ are the \textbf{Structural Specifications}, $\mathcal{FS}$ are the \textbf{Functional Specifications}, and $\mathcal{DS}$ are the \textbf{Deontic Specifications}

\textbf{Structural Specifications (SS)} refer to the structured means left to agents to achieve a goal, we denote them as $\mathcal{SS} = \langle \mathcal{R}, \mathcal{IR}, \mathcal{G} \rangle$, where:

\begin{itemize}

    \item $\mathcal{R}_{ss}$: the set of all roles (denoted $\rho \in \mathcal{R}$);

    \item $\mathcal{IR}: \mathcal{R} \rightarrow \mathcal{R}$: the inheritance relation between roles ($\mathcal{IR}(\rho_1) = \rho_2$ means $\rho_1$ inherits from $\rho_2$ also denoted $\rho_1 \sqsubset \rho_2$);

    \item $\mathcal{RG} \subseteq \mathcal{GR}$ the set of root groups, $\mathcal{GR} = \langle \mathcal{R}, \mathcal{SG}, \mathcal{L}^{intra}, \mathcal{L}^{inter}, \mathcal{C}^{intra}, \mathcal{C}^{inter}, np, ng \rangle$, the set of all groups, where

          \begin{itemize}

              \item $\mathcal{R} \subseteq \mathcal{R}_{ss}$: the set of non-abstract roles;

              \item $\mathcal{SG} \subseteq \mathcal{GR}$: the set of sub-groups;

              \item $\mathcal{L} = \mathcal{R} \cross \mathcal{R} \cross \mathcal{TL}$: the set of links. A link is a 3-tuple $(\rho_s,\rho_d,t) \in \mathcal{L}$ (also denoted as a predicate $link(\rho_s,\rho_d,t))$, where $\rho_{s}$ is the source role, $\rho_{d}$ is the destination role, and $t \in \mathcal{TL}, \mathcal{TL} = \{acq, com, aut\}$ is the link type;
                    \begin{itemize}
                        \item If $t = acq$ (acquaintance), the agents playing the source role $\rho_{\mathrm{s}}$ are allowed to have a representation of the agents playing the destination role $\rho_{d}$;
                        \item If $t = com$ (communication), the $\rho_{\mathrm{s}}$ agents are allowed to communicate with $\rho_{d}$ agents;
                        \item If $t = aut$ (authority), the $\rho_{\mathrm{s}}$ agents are allowed to have authority on $\rho_{d}$ agents. It requires an acquaintance and communication link.
                    \end{itemize}
              \item $\mathcal{L}^{intra} \subseteq \mathcal{L}$: the set of intra-group links;
              \item $\mathcal{L}^{inter} \subseteq \mathcal{L}$: the set of inter-group links;

              \item $\mathcal{C} = \mathcal{R} \cross \mathcal{R}$: the set of compatibilities. A compatibility is a couple $(a,b) \in \mathcal{C}$ (also denoted $\rho_a \bowtie \rho_b$), means agents playing role $\rho_a \in \mathcal{R}$ can also play role $\rho_b \in \mathcal{R}$;
              \item $\mathcal{C}^{intra} \subseteq \mathcal{C}$: the set of intra-group compatibilities;
              \item $\mathcal{C}^{inter} \subseteq \mathcal{C}$: the set of inter-group compatibilities;

              \item $np: \mathcal{R} \rightarrow \mathbb{N} \times \mathbb{N}$: the relation giving the cardinality of agents adopting a role;
              \item $ng: \mathcal{SG} \rightarrow \mathbb{N} \times \mathbb{N}$: the relation giving the cardinality of each sub-group.

          \end{itemize}

\end{itemize}

\textbf{Functional Specifications (FS)} globally refer to the tasks and goals agents have to achieve, we denote them as $\mathcal{FS} = \langle \mathcal{SCH}, \mathcal{PO} \rangle$, where:

\begin{itemize}
    \item $\mathcal{SCH} = \langle\mathcal{G}, \mathcal{M}, \mathcal{P}, mo, nm \rangle$: the set of \textbf{social scheme}, where:
          \begin{itemize}
              \item $\mathcal{G}$ is the set of global goal;

              \item $\mathcal{M}$ is the set of mission labels;
              \item $\mathcal{P} = \langle \mathcal{G}, \{\mathcal{G}\}^s, OP, [0,1] \rangle, s \in \mathbb{N}^*$ is the set of plans that builds the tree structure of the goals.
                    %
                    A plan $p \in \mathcal{P}$ is 4-tuple $p=(g_f,\{g_i\}_{0 \leq i \leq s}, op, ps), g_f \in \mathcal{G}, g_i \in \mathcal{G}, op \in OP, OP = \{sequence, choice, parallel\}, ps \in [0,1]$, meaning that the goal $g_f$ is achieved if some of the sub-goals $g_i$ are achieved with a success probability $ps$ and according to the operator $op$:
                    %
                    \begin{itemize}
                        \item if $op = sequence$, the $g_i$ can only be achieved in the same order sequentially;
                        \item if $op = choice$, only one of the $g_i$ has to be achieved;
                        \item if $op = parallel$, the $g_i$ can only be achieved sequentially or simultaneously.
                    \end{itemize}

              \item $mo: \mathcal{M} \rightarrow \mathbb{P}(\mathcal{G})$: specifies the set of goals a mission is associated with;
              \item $nm: \mathcal{M} \rightarrow \mathbb{N} \times \mathbb{N}$ the cardinality of agents committed for each mission.
          \end{itemize}
    \item $\mathcal{PO}: \mathcal{M} \cross \mathcal{M}$: the set of \textbf{preference orders}. A preference order is a couple $(m_1, m_2), m_1 \in \mathcal{M}, m_2 \in \mathcal{M}$ (also denoted $m_{1} \prec m_{2}$) meaning that if there is a moment when an agent is permitted to commit to $m_{1}$ and also $m_{2}$, it has a social preference for committing to $m_{1}$.
\end{itemize}

\textbf{Deontic Specifications (DS)} refer to how SS are to be used to achieve the FS, we denote them as $\mathcal{DS} = \langle \mathcal{OBL},\mathcal{PER} \rangle$, the set of deontic specifications, where:

\begin{itemize}
    \item $\mathcal{TC}$: the set of \textbf{time constraints}. A time constraint $tc \in \mathcal{TC}$ specifies a set of periods during which a permission or obligation is valid ($Any \in \mathcal{TC}$ means \textquote{everytime});
    \item $\mathcal{OBL}: \mathcal{R} \cross \mathcal{M} \cross \mathcal{TC}$: the set of \textbf{obligations}. An obligation is a 3-tuple $(\rho_a,m,tc)$ (also denoted $obl(\rho_a,m,tc)$) meaning an agent playing role $\rho_a \in \mathcal{R}$ is obliged to commit on mission $m \in \mathcal{M}$ for a given time constraint $tc \in \mathcal{TC}$;
    \item $\mathcal{PER}$: the set of \textbf{permissions}. A permission is a 3-tuple $(\rho_a,m,tc)$ (also denoted $per(\rho_a,m,tc)$) meaning an agent playing role $\rho_a \in \mathcal{R}$ is permitted to commit on mission $m \in \mathcal{M}$ for a given time constraint $tc \in \mathcal{TC}$;
\end{itemize}



\section{Linking MARL and organizational model: PRAHOM}

In this section, we incrementally present the construction of PRAHOM by setting up the following features to apply organizational specifications to policies:

\quad $(\mathbf{F_{DS}})$ \quad If an agent is constrained to a role and permitted/obligated to a mission, then $(\mathbf{F_{SS}})$ should be satisfed and $(\mathbf{F_{FS}})$ should be satisfied depending on the time constraint considering an agent is constrained to a mission;

\quad $(\mathbf{F_{SS}})$ \quad If an agent is constrained to a role, then its policy should necessarly belong to a policy subset associated with the given role throughout the training;

\quad $(\mathbf{F_{FS}})$ \quad If an agent is constrained to a mission, then it should be enticed to achieve the mission's goals according to the associated plan throughout training.

\

In the remainder of this section, we present the issues we face to implement these features and the workarounds we proposed to develop PRAHOM. Then, a synthesis of $PRAHOM$ is presented in \Autoref{sec:prahom_alg}.

\subsection{\textbf{$(\mathbf{F_{SS}})$: Constraining joint-policies according to roles}}

\

\textbf{Sub-gap $\mathbf{F_{SS}.G_1}$} \quad Considering a role to be represented by a policy in MARL, we need to face the lack of common ground between policies and roles enabling to know how an agent is expected to behave when it is constrained to a role.

\textbf{Workaround $\mathbf{F_{SS}.G_1}$} \quad As illustrated in \Autoref{fig:PRAHOM_osm_rels}, we propose to map a role to a history subset introducig the $rh: \mathcal{R} \rightarrow \mathcal{P}(H)$ bijection that show how an agent playing a role and committed to a mission should have its policy constrained to generate histories belonging to an expected history subset.

\begin{figure}[h!]
    \centering
    \input{figures/PRAHOM_osm_rels.tex}
    \caption{An view of relations between organizational specification to history subsets}
    \label{fig:PRAHOM_osm_rels}
\end{figure}

\

\textbf{Sub-gap $\mathbf{F_{SS}.G_2}$} \quad We need to consider that defining exhaustively a large history subset to represent complex behavior may be impracticable for its exponential complexity (Sub-gap $\mathbf{F_{SS}.G_{2.1}}$); and handling non-convenient large observations or actions (Sub-gap $\mathbf{F_{SS}.G_{2.2}}$).

\textbf{Workaround $\mathbf{F_{SS}.G_{2.1}}$} \quad Considering observation are not convenient to be handled directly, we propose using short-way strings such as labels. we introduce the \textbf{labels-observations mapping} $ol: \mathcal{P}(\Omega) \rightarrow \mathcal{P}(L)$. $ol$ associates some simple strings to real observations. The relation $bld_{ol}$ enables creating a labels-observations mapping. As illustrated in \Autoref{fig:PRAHOM_ol}, we implemented an $ol$ via a simple mapping or using a Large Language Model (LLM) both established or trained by labeling observations in a rendered environment at each step for implementing $bld_{ol}$ (frame by frame in visual environments).

\begin{figure}[h!]
    \centering
    \input{figures/ol_scheme.tex}
    \caption{An abstract view of observations-labels mapping and its crafting}
    \label{fig:PRAHOM_ol}
\end{figure}

\

\textbf{Workaround $\mathbf{F_{SS}.G_{2.2}}$} \quad Considering a history subset is aimed to constrain a policy, we propose to simplify it as a constraint on policy $c\pi \in c\Pi$ by introducing an \textbf{observable policy constraint} $c\pi: H \times \Omega \rightarrow \mathcal{P}(A)$. $c\pi$ indicates the actions an agent's policy should be allowed to choose among when it receives an observation at each step. We denote, $bld_{opc}: \mathcal{P}(H) \rightarrow c\Pi$ the relation that enables creating an observable policy constraint from a given history subset.

As illustrated in \Autoref{fig:PRAHOM_opc}, we propose a pattern format to simplify its creation. It follows the following rules: a sequence is denoted \textquote{[$label_{obs_1},label_{act_1}\dots$] \allowbreak ($card_{min},card_{max}$)}; the \textquote{Any} label refer to any observation/action. Using this pattern, we defined rules to associate an action set depending on history and a received observation. We implemented a $c\pi$ as an oriented graph denoted \textbf{history graph} where nodes are observations/actions and edges represent expected transitions relying on ordinal number and cardinalities. A history graph enables implementing $bld_{opc}$ by merging each history.

\begin{figure}[h!]
    \centering
    \input{figures/opc_scheme.tex}
    \caption{An abstract view of observable policy constraint and its crafting}
    \label{fig:PRAHOM_opc}
\end{figure}

\

\textbf{Sub-gap $\mathbf{F_{SS}.G_3}$} \quad We need to consider how an agent's policy could be constrained to generate histories that belong to a given history subset of a role.

\textbf{Workaround $\mathbf{F_{SS}.G_3}$} \quad When an agent receives an observation $\omega \in \Omega$ we propose to integrate constraints into an agent policy $\pi \in \Pi$ via three modes:
\begin{itemize}
    \item \textbf{correct}: Correct any chosen action $\pi(\omega)$ by an expected one in $c\pi(\omega)$. It aims to converge faster by reducing the search space with safety guarantees. Yet, it is external to the agent's policy;
    \item \textbf{penalize}: Add a penalty to the reward if any wrong action ($\pi(\omega) \notin c\pi(\omega)$) has been made among agents. This mode aims to make agents \textquote{learn} to respect their constraints. Yet, no safety guarantee is ensured since agents only approximate their expected behavior;
    \item \textbf{correct\_policy}: Create and use a \textbf{constrained policy} $\pi_c \in \Pi, \pi_c = bld_{\pi c}(\pi, c\pi) = \{sample(c\pi_{joint}(\omega_{joint})) \ if \ \omega_{joint} \in Dom(c\pi_{joint}) \ and \ \omega_{joint} \notin Dom(\pi_{joint}), \ else \ \pi_{joint}(\omega_{joint})\}$, where an observable policy constraint $c\pi$ corrects the current policy $\pi$. $sample$ gets an element from a set randomly. $bld_{\pi c}: \Pi \times c\Pi \rightarrow \Pi$ enables creating a constrained policy by combining a regular policy and an observable policy constraint. This mode enables to respect safety guarantees internally.
\end{itemize}

In the annex, we provide \Autoref{proof:jpc_to_ac}, which outlines why constraining the action space during training at each step implies that the resulting joint-policy will necessarily be constrained.

\

\subsection{\textbf{$(\mathbf{F_{FS}})$: Constraining joint-policies according to missions}}

\

\textbf{Sub-gap $\mathbf{F_{FS}.G_1}$} \quad Considering a goal to be represented by a reward function in MARL, we therefore need a way to update the joint reward function when an agent is committed to a mission which is a goal set.

\textbf{Workaround $\mathbf{F_{FS}.G_1}$} \quad As illustrated in \Autoref{fig:PRAHOM_osm_rels}, we propose to map a goal to a history subset introducig the $gh: \mathcal{G} \rightarrow \mathcal{P}(H)$ the bijection that show how an agent committed to a mission should have its policy enticed to generate histories belonging to an expected history subset. Thus, $mh: \mathcal{M} \rightarrow \mathcal{P}(H) = \{(m,\bigcup gh[mo(m)]), m \in \mathcal{M}\}$ gives expected histories from missions.

\

\textbf{Sub-gap $\mathbf{F_{FS}.G_2}$} \quad Considering an agent needs to achieve a goal by generating histories belonging to a theoretically defined history subset, we need to know how to represent such a goal into MARL formalism to entice agents to achieve it.

\textbf{Workaround $\mathbf{F_{FS}.G_2}$} \quad As illustrated in \Autoref{fig:goal_mission_scheme}, we propose to represent a goal $g \in \mathcal{G}$ in MARL formalism by introducing an \textbf{observable reward function} $R_{g}: H \rightarrow \mathbb{R} = bld_{grf}(H_g) = \{(h, dist(H_g,h)), h \in H\}$ so that a history generated by an agent $h \in H$ be associated with a positive reward if it belongs to the observable/expected $H_g$ or negative otherwise. The relation $dist: H \times H \rightarrow \mathbb{R}$ associate a real number indicating how close a generated history belong to a given history subset. Indeed, we implemented $dist$ to return a high reward if generated belongs to expected history using history graph else a negative reward at the end of the episode. The relation $bld_{grf}: H \rightarrow R$ generates an observable reward function from a reference given history subset.

\

A mission $m \in \mathcal{M}$ containing goals $\mathcal{G}_{m} = mo(m) = \{g_i \in \mathcal{G}_{m}\}$ is represented as $R_m: H \rightarrow \mathbb{R} = bld_{mrf}(m) = comb_{grf}(\{bld_{grf}(H_g), g \in G, H_g = gh(g) \subset H\})$. The relation $comb_{grf}: \mathcal{P}(\mathcal{R}_{\mathcal{G}_{m}}) \rightarrow R_{\mathcal{M}}$ combines all observable reward function of each goal of $m$ into a single observable reward function for a mission. We implemented $comb_{grf}$ as a simple weighted sum for each observable reward function (usually all weights are equal).

\

Then, an observable reward function can be integrated in the global reward function in the whole environment through $comb_{mrf}$. We implemented $comb_{mrf}$ to add individual reward obtained from mission observable reward functions in addition to the initial common reward. This way, agents are individually enticed to achieve their respective sub-goals hence speeding up the convergence to achieving the ultimate goal.

\begin{figure}[h!]
    \centering
    \input{figures/goal_mission_scheme.tex}
    \caption{An abstract view of observable reward function and its crafting}
    \label{fig:goal_mission_scheme}
\end{figure}

\subsection{\textbf{$(\mathbf{F_{DS}})$: Constraining joint-policies according to permissions/obligations}}

\

\textbf{Sub-gap $\mathbf{F_{DS}.G_1}$} \quad Using previous workarounds in $(\mathbf{F_{SS}})$ and $(\mathbf{F_{FS}})$, we can change MARL functionning to integrate roles and missions. Nevertheless, time constraints is not currently modeled and integrated within the training process.

\textbf{Workaround $\mathbf{F_{DS}.G_1}$} \quad As illustrated in \Autoref{fig:PRAHOM_osm_rels}, we introduced the relation $da: \mathcal{PER} \cup \mathcal{OBL} \rightarrow \mathcal{P}(\mathcal{A})$ that indicates how agents are constrained to roles and committed on missions for a given time constraint. In order to take into account time constraints, we introduce a time-to-live for each permission/obligation through the relation $dttl: \mathcal{PER} \cup \mathcal{OBL} \times \mathcal{A} \rightarrow \mathbb{N}$. Associated time-to-lives are initialized with time constraint values via the relation $bld_{dttl}: da \rightarrow dttl = Dom(da) \crossproduct [Dom(da)]_{TC}$. Then, they are to be decreased at each step if the given time constraint is not \textquote{Any} with the relation $dec: dttl \rightarrow dttl$. Then, the roles constrained to agents or committed missions may change. Consequently, the reward function and policies are changed via $update_{R,\Pi}(dttl, rh, mh, \pi, \dots)$ to return new $R$ and the $c\pi$ so changes take effect during training.

\

\textbf{Sub-gap $\mathbf{F_{DS}.G_2}$} \quad Difference between obligation and permission introduce an additional complexity when adapting the training to take into account. Agents obligated to commit on missions, should seek to prioritize maximizing the associated observable reward function accordingly over permitted missions.

\textbf{Workaround $\mathbf{F_{DS}.G_2}$} \quad As shown in \Autoref{fig:goal_mission_scheme}, as a first approximation to address this issue, we propose $comb_{mrf}$ as weighted sum with high weight for obligated missions and low weight for permitted ones.


\subsection{PRAHOM Algorithm}\label{sec:prahom_alg}

% $PRAHOM$ algorithm is presented in \autoref{alg:PRAHOM-A}. It fits within a regular MARL context: a joint-policy $\pi_{joint,ep}$ is updated after several steps allowing to have enough joint-histories $h_{joint,ep}$ and joint-rewards as feedback $r_{joint,ep}$. The training goes on over several episodes for better training until converging to a sufficient cumulative reward regarding $s$.

% $PRAHOM$ underlying idea is to represent roles and missions through permissions/obligations as history subset that agents are expected to generate while interacting.

The PRAHOM algorithm, as outlined in \autoref{alg:PRAHOM-A} (cf. annex), integrates organizational specifications into the learning process of multi-agent reinforcement learning (MARL) by constraining joint-policies according to predefined roles, missions, and permission/obligation relations. This method ensures that the agents' behaviors align with organizational requirements while  optimizing joint-policy ($\pi_{joint,i,s}$) as output.

\textbf{Initialization and Input Parameters}: \quad
The algorithm starts by initializing the joint-policy ($\pi_{joint}$) with the initial joint-policy ($\pi_{joint,i,init}$) and setting the episode counter ($ep$) to zero (lines 1). The algorithm also sets a boolean variable ($sufficient$) to False, indicating that the cumulative reward expectancy has not yet been met.

\textbf{Step 1: Determine Joint Observable Policy Constraints}: \quad
PRAHOM determines the joint observable policy constraints ($c\pi_{joint}$) from the organizational specifications (lines 2). This step involves mapping roles and permissions/obligations to constraints on the joint-policy, ensuring that the agents' actions are in line with the organizational rules.

\textbf{Step 2: Initialize Constrained Policy}: \quad
If the mode of constraint integration is set to \textit{correct\_policy}, the algorithm creates and uses a joint constrained policy via $bld_{joint, \pi_c}$ based on the initial policy and the observable policy constraints ($c\pi_{joint}$) (line 4). This ensures that the initial policy is inherently aligned with the specified constraints from the beginning.

\textbf{Step 3: Determine Observable Reward Functions}: \quad
Next, PRAHOM determines the observable reward functions $R_{m,joint}$ from the organizational specifications (line 5). This involves mapping missions and goals to reward functions, which will guide the agents towards achieving organizational sub-goals.

\textbf{Step 4: Main Training Loop}: \quad
The main training loop runs for a maximum number of episodes ($ep_{max}$) or until the cumulative reward expectancy is met (line 6).

\begin{itemize}
    \item \textbf{Initialize Episode}:
          At the beginning of each episode, the environment, observation, and action histories are reinitialized (line 7). The cumulative reward and penalty are also reset. Time-to-live values for permissions/obligations are initialized using $bld_{dttl}$, and the initial observation and action are set by the environment ($d_{ep}.init()$).

    \item \textbf{Step Through Episode}:
          The algorithm steps through each episode for a maximum number of steps ($step_{max}$) (line 8). Within each step:

          \begin{itemize}
              \item \textbf{Policy Update}:
                    The joint-policy ($\pi_{joint}$) is updated using the MARL algorithm ($u_{marl}$) based on the current history and rewards (line 9).

              \item \textbf{Action Selection}:
                    The next action is selected based on the current observation ($\pi_{joint}(\omega_{joint,t=step})$) (line 10).

              \item \textbf{Action Validation}:
                    The expected actions are determined from the observable policy constraints ($c\pi_{joint}$), and if the selected action is not among the expected ones, it is either corrected or penalized based on the mode of constraint integration (lines 11-16).

              \item \textbf{Update Histories and Rewards}:
                    The current history is updated (line 17), and the action is applied to the environment to get the next observation and reward (line 18) adding any incurred penalties (line 19).

              \item \textbf{Update Constraints and Rewards}:
                    Time-to-live values are decreased, and the reward functions and policies are updated accordingly (line 20).
          \end{itemize}

    \item \textbf{Check for Sufficiency}:
          After each episode, the algorithm checks whether the cumulative reward meets the expectancy ($is\_sufficient$) and increments the episode counter ($ep$) (line 21).
\end{itemize}

\textbf{Complexity and Scalability}: \quad The computational complexity of PRAHOM primarily depends on the number of episodes ($ep_{max}$), the number of steps per episode ($step_{max}$), and the complexity of the MARL algorithm ($u_{marl}$). Each step involves updating the policy, selecting \& validating actions, and updating histories and rewards, which can be computationally intensive. The memory complexity depends on the need to store rewards, policies and histories. The history graphs we introduced allow histories to be merged compactly, reducing redundancy with limited impact on read cost.

Scalability can be challenging as the number of agents, roles, missions increases. Evaluating PRAHOM scalability requires to consider an implemented version through these salient features:
\begin{itemize}
    \item \textbf{Parallelization:} Performance for updating/validating policies strongly depends on chosen MARL algorithms capabilities such as parallelization significantly improves it (especially in Policy Optimization algorithms);
    \item \textbf{Efficient Data Structures:} The choice of data structures for storing and accessing histories or policies is crucial for reducing overhead.;
    \item \textbf{Optimization of Reward Functions:} The logic for constructing/handling reward functions is important to enhance performance further.
\end{itemize}

\section{Case study: Predator-prey environment}

\subsection{Algorithm implementation: PRAHOM Wrapper}

We developed the \textit{PRAHOM Wrapper}
%
\footnote{Additional information are provided in addition to the code in \url{https://github.com/julien6/omarl_experiments}.}
%
as a proof-of-concept (PoC) tool to augment PettingZoo environments, facilitating the application of the PRAHOM algorithm. This wrapper includes additional class methods to implement PRAHOM's functionalities, streamlining the training process. The wrapper leverages the MARLlib~\cite{hu2022marllib} library, which offers a comprehensive range of state-of-the-art MARL algorithms and finely-tuned models.

\textbf{History Subsets} \quad We implemented a \textit{history\_subset} class to handle history subsets based on predefined patterns or rules via an implemented history graph when dealing with patterns. Alternatively, custom functions can be employed to instantiate a history\_subset, allowing more flexibility.

\textbf{Organizational Specifications to Relations} \quad The \textit{osr} class models a complete $\mathcal{M}OISE^+$ organizational model that includes roles, goals, and missions, which are directly mapped to \textit{history\_subset} objects initialized with expected patterns. This mapping implements \textit{rh}, \textit{gh}, and \textit{mh}. Permissions/obligations are also defined in the \textit{osr}, mapping the agents constrained to roles and committed to missions with time constraints, thus implementing the \textit{da} relation.

\textbf{Observation Labeling (ol)} \quad The observation labeling is managed by the singleton \textit{ol\_mngr} class, which uses a regular dictionary to map observations to labels. It leverages the HuggingFace transformer model \textit{tiiuae/falcon-7b} to learn the mappings alongside the dictionary. The \textit{ol\_mngr} class includes an interactive process for users to label each displayed observation during the labeling process. %Post-training, the LLM can be employed to identify desired observations based on keywords, enhancing usability.

\textbf{History Graph} \quad We constructed the \textit{history graph} class from scratch following the theoretical foundation of ordinal numbers and cardinalities. The class provides functions to add histories or history patterns and to check whether a given history matches the stored patterns.

\textbf{Observable Policy Constraint (opc)} \quad The \textit{opc} class links pairs of (history\_pattern, observation) to lists of actions. Each association is integrated into a unified history graph object, ensuring consistency in policy constraints.

\textbf{Constrained Policy (cons\_policy)} \quad The \textit{cons\_policy} class combines a regular MARLlib policy with an \textit{opc} object. The integration involves embedding policy constraints within the MARLlib policy framework, allowing the constrained policy to respect predefined action constraints.

\textbf{Observable Reward Function (orf)} \quad The \textit{orf} class constructs an observable reward functions based on history patterns. It includes a function that evaluates whether a given history matches the initial pattern: high rewards for matching patterns, low rewards otherwise.

\textbf{Time-to-Live (dttl)} \quad The \textit{dttl} class is built using a Python dictionary that replicates data from the \textit{osr} object. This class manages the time constraints to update the observable reward function and policy constraints.

Once the PettingZoo environment is wrapped with the PRAHOM Wrapper, a function is provided to execute the PRAHOM algorithm. This function accepts a JSON object containing MARLlib details, such as the chosen MARL algorithm (e.g., MADDPG or MAPPO). It processes all defined elements, retrieves roles and missions, converts them into \textit{opc} or \textit{orf} objects and so on. Ultimately outputs a dictionary mapping each agent to its trained policy.

\section{Experimental Setup}

\subsection{CybORG CAGE Challenge 3 Environment}

We selected the CybORG CAGE Challenge 3 environment, which simulates a swarm of autonomous drones operating in a complex cyber-infrastructure. The environment includes multiple drones (\texttt{drone\_0}, \texttt{drone\_1}, \ldots, \texttt{drone\_n}), each with specific capabilities and roles within the swarm. The goal is to secure critical assets and protect against cyber-attacks.

\textbf{Environment Details:}
\begin{itemize}
    \item \textbf{Agents:} Multiple autonomous drones with varying capabilities.
    \item \textbf{Actions:} Drone maneuvers, cyber-attacks, defensive actions.
    \item \textbf{Observation and State Shapes:} Diverse sensor data and cyber-threat indicators.
    \item \textbf{Rewards:} Points for securing assets, preventing attacks, and neutralizing threats.
\end{itemize}

We utilized the MAPPO and MADDPG algorithms from MARLlib tailored for cooperative behavior among autonomous agents.

\subsection{Organizational Models}

We implemented and evaluated four organizational models tailored for the CybORG CAGE Challenge 3 environment:

\begin{itemize}
    \item \textbf{Suspect Isolation Model:} Identifies and isolates drones exhibiting suspicious cyber-activities.
    \item \textbf{Cyberdefense Model 1:} Coordinates defensive actions against incoming cyber-attacks.
    \item \textbf{Cyberdefense Model 2:} Implements proactive measures to detect and mitigate potential threats.
    \item \textbf{Manual Control Model:} A baseline model with manually defined roles and actions for drones.
\end{itemize}

\subsection{Experimental Procedure}

We trained each organizational model using the PRAHOM Wrapper within the CybORG CAGE Challenge 3 environment. The wrapper integrates organizational specifications and constraints into the MARLlib policies, enabling agents to learn and adapt their behavior based on predefined roles and missions.

\subsection{Evaluation Criteria}

Our evaluation criteria include both qualitative and quantitative assessments:

\begin{itemize}
    \item $(\mathbf{C1.1})$: Qualitative assessment involves observing the adoption of specific roles and strategies by drones, such as isolating suspicious activities or coordinating defensive maneuvers.
    \item $(\mathbf{C2.1})$: Quantitative metrics measure the convergence time and average rewards obtained by each model, indicating the effectiveness of organizational constraints.
    \item $(\mathbf{C3.1})$: Comparative analysis between models to determine which organizational structure best enhances collective cyber-defense capabilities.
\end{itemize}

\section{Results and Discussion}

The results from our experiments are presented below:

\subsection{Learning Curves and Performance Metrics}

Figure \ref{fig:learning_curves} illustrates the learning curves for the different organizational models, indicating their convergence rates over training episodes.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/prahom_learning_curve.png}
    \caption{Learning curves for organizational models in CybORG CAGE Challenge 3.}
    \label{fig:learning_curves}
\end{figure}

Table \ref{tab:results} summarizes the convergence time and average rewards achieved by each model during the training process.

\begin{table}[ht]
    \centering
    \begin{tabular}{|c|c|c|}
        \hline
        Model & Convergence Time (episodes) & Average Reward \\
        \hline
        Suspect Isolation Model & 250 & 17.2 \\
        Cyberdefense Model 1 & 300 & 18.5 \\
        Cyberdefense Model 2 & 280 & 19.8 \\
        Manual Control Model & 400 & 15.6 \\
        \hline
    \end{tabular}
    \caption{Performance metrics for organizational models in CybORG CAGE Challenge 3.}
    \label{tab:results}
\end{table}

\subsection{Discussion}

\textbf{Qualitative Observations}:
The organizational models effectively influenced drone behavior in the CybORG CAGE Challenge 3 environment. The Suspect Isolation Model demonstrated effective identification and isolation of drones exhibiting suspicious activities, enhancing overall security.

\textbf{Quantitative Analysis}:
Cyberdefense Model 2 showed the fastest convergence and highest average reward, indicating its effectiveness in proactive threat mitigation. This model significantly outperformed the Manual Control Model, highlighting the benefits of automated organizational frameworks in dynamic cyber-defense scenarios.

In conclusion, the experimental results confirm the importance of organizational models in enhancing coordination and effectiveness among autonomous agents in complex cyber-physical environments like CybORG CAGE Challenge 3. Future research could explore further refinements and adaptations of organizational specifications to optimize collective behaviors and response capabilities.



\section{Conclusion}

% résumer nos contributions et nos principales conclusions.

% importance d’améliorer l’explicabilité de l’IA dans les systèmes MARL et souligner l’impact potentiel de PRAHOM sur l’avancement du domaine.

% proposer des remarques finales et décrire les orientations des recherches futures afin d’explorer et d’affiner davantage notre algo.

% Multi-agent methods rely on the designer's knowledge to design a suitable MAS organization, but do not provide automatic means to determine the relevant organizational mechanisms only from the design requirements and the overall objective.
% MARL techniques have been successfully applied to automatically train agents to achieve a given objective without explicit characterization of emerging collective strategies.
% The PRAHOM's originality is to enrich a MARL process with an explicit organizational model towards a methodological objective to address these issues. It links the agents' policies (modeled in a Dec-POMDP) with $\mathcal{M}OISE^+$ through the process PRAHOM. Under the simplifying conditions of a group and a single social pattern, PRAHOM makes it possible to partially determine organizational specifications from joint histories and to constrain the training of policies in relation to organizational specifications.
% Additionally, we implemented the \emph{PRAHOM PettingZoo} wrapper as a proof of concept to apply PRAHOM.
% Finally, we applied our approach in four \emph{PettingZoo} environments to evaluate the impact during and after training. The performances obtained appear to be comparable to those known.

% Key point summary

Our main contribution is the PRAHOM algorithmic approach that aims to link MARL and agents' policies with the $\mathcal{M}OISE^+$ model leveraging on relations between joint-histories and organizational specifications. It seeks to constrain the agents' training according to expected behaviors expressed as organizational specifications.
As a first implementation, \emph{PRAHOM Wrapper} is assessed in cooperative Atari game, where the inferred organizational specifications generally match the expected hand-crafted results.
Despite the implementation's limited functionalities, the first results showed that the PRAHOM approach can be used to constrain the resulting agents' policy space according to the specified organizational constraints. Additionally, we manually verified that some valuable organizational specifications are inferred from trained agents' histories.

% Perspectives
Even if PRAHOM is agnostic of any MARL algorithm, directing the collective behaviors of agents a priori can prove difficult. Indeed, major perspectives to improve PRAHOM are inspired by hierarchical learning which contributes to better making emerge strategies during learning.
Eventually, even though the first results obtained with LLM show it as a promising complementary help for PRAHOM, it may also bring out new ways for explaining collective behavior beyond its intended use.
% Ultimately, we also aim to improve the applicability of PRAHOM by developing dedicated interfaces built around PRAHOM making it more accessible to industrial and research contexts.



\section*{Acknowledgment}

This work was supported by \emph{Thales Land Air Systems} within the framework of the \emph{Cyb'Air} chair and the \emph{AICA IWG}

\section*{References}

% \bibliographystyle{abbrv}
\bibliographystyle{IEEEtran}

\bibliography{references}

\end{document}
