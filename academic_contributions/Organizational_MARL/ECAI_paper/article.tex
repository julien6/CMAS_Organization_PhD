%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% LaTeX Template for ECAI Papers 
%%% Prepared by Ulle Endriss (version 1.0 of 2023-12-10)

%%% To be used with the ECAI class file ecai.cls.
%%% You also will need a bibliography file (such as mybibfile.bib).

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% Start your document with the \documentclass{} command.
%%% Use the first variant for the camera-ready paper.
%%% Use the second variant for submission (for double-blind reviewing).

\documentclass{ecai} 

%\documentclass[doubleblind]{ecai} 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% Load any packages you require here. 

\usepackage{latexsym}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{booktabs}
\usepackage{enumitem}
\usepackage{graphicx}
\usepackage{color}

\usepackage[T1]{fontenc}
\usepackage{graphicx}
%\usepackage{color}
%\renewcommand\UrlFont{\color{blue}\rmfamily}

\usepackage{amsmath,amssymb,amsfonts}
% \usepackage[inline, shortlabels]{enumitem}
\usepackage{tabularx}
\usepackage{caption}
% \usepackage{titlesec}
\usepackage[english]{babel}
\captionsetup{font=it}
\usepackage{ragged2e}
\usepackage{hyperref}
\usepackage{pifont}
\usepackage{footmisc}
\usepackage{multirow}
\usepackage{algorithm2e}

% --- Tickz
\usepackage{physics}
\usepackage{amsmath}
\usepackage{tikz}
\usepackage{mathdots}
\usepackage{yhmath}
\usepackage{cancel}
\usepackage{color}
\usepackage{siunitx}
\usepackage{array}
\usepackage{multirow}
\usepackage{amssymb}
\usepackage{gensymb}
\usepackage{tabularx}
\usepackage{extarrows}
\usepackage{booktabs}
\usetikzlibrary{fadings}
\usetikzlibrary{patterns}
\usetikzlibrary{shadows.blur}
\usetikzlibrary{shapes}

% ---------

\usepackage{pdfpages}
\usepackage{booktabs}
\usepackage{csquotes}
\usepackage{lipsum}  
\usepackage{arydshln}
\usepackage{smartdiagram}
\usepackage[inkscapeformat=png]{svg}
\usepackage{textcomp}
\usepackage{tabularray}\UseTblrLibrary{varwidth}
\usepackage{xcolor}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
% \usepackage{cite}
\usepackage{amsmath}
\newcommand{\probP}{\text{I\kern-0.15em P}}
\usepackage{etoolbox}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% Define any theorem-like environments you require here.

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{fact}[theorem]{Fact}
\newtheorem{definition}{Definition}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% Define any new commands you require here.

% \newcommand{\BibTeX}{B\kern-.05em{\sc i\kern-.025em b}\kern-.08em\TeX}

\renewcommand{\arraystretch}{1.7}

\setlength{\extrarowheight}{2.5pt}
\renewcommand{\arraystretch}{0.2}
\renewcommand{\arraystretch}{1.7}

\newcommand{\before}[1]{\textcolor{red}{#1}}
\newcommand{\after}[1]{\textcolor{green}{#1}}

\newcommand{\old}[1]{\textcolor{orange}{#1}}
\newcommand{\rem}[1]{\textcolor{red}{#1}}
\newcommand{\todo}[1]{\textcolor{orange}{\newline \textit{\textbf{TODO:} #1}} \newline \newline }



\newcounter{relation}
\setcounter{relation}{0}
\renewcommand{\therelation}{\arabic{relation}}
\newcommand{\relationautorefname}{Relation}

\newenvironment{relation}[1][]{%
    \refstepcounter{relation}%
    \noindent \raggedright \textit{\textbf{Relation. \therelation}} \hfill$}
{%
$ \hfill \phantom{x}

}

\newcounter{proof}
\setcounter{proof}{0}
\renewcommand{\theproof}{\arabic{proof}}
\newcommand{\proofautorefname}{Proof}

\renewenvironment{proof}[1][]{
    \refstepcounter{proof}
    \noindent \raggedright \textit{\textbf{Proof. \theproof}}

    \setlength{\leftskip}{1em}

}
{

\
\setlength{\leftskip}{0pt}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frontmatter}

    %%% Use this command to specify your submission number.
    %%% In doubleblind mode, it will be printed on the first page.

    \paperid{123}

    %%% Use this command to specify the title of your paper.

    \title{Explicitizing Cooperative Intelligence: Towards Explainable Multi-Agent Reinforcement Learning with Organizational Models}

    % JS: keywords:
    %     Multi-Agent Reinforcement Learning
    %     Explainability
    %     Organizational Models
    %     Cooperative Intelligence

    %%% Use this combinations of commands to specify all authors of your 
    %%% paper. Use \fnms{} and \snm{} to indicate everyone's first names 
    %%% and surname. This will help the publisher with indexing the 
    %%% proceedings. Please use a reasonable approximation in case your 
    %%% name does not neatly split into "first names" and "surname".
    %%% Specifying your ORCID digital identifier is optional. 
    %%% Use the \thanks{} command to indicate one or more corresponding 
    %%% authors and their email address(es). If so desired, you can specify
    %%% author contributions using the \footnote{} command.

    \author[A,B]{\fnms{Julien}~\snm{Soulé}\thanks{Corresponding Author. Email: julien.soule@lcis.grenoble-inp.fr}}
    \author[A]{\fnms{Jean-Paul}~\snm{Jamont}}
    \author[A]{\fnms{Michel}~\snm{Occello}}
    \author[B]{\fnms{Louis-Marie}~\snm{Traonouez}}
    \author[C]{\fnms{Paul}~\snm{Théron}}

    \address[A]{Univ. Grenoble Alpes, Grenoble INP, LCIS, 26000, Valence, France}
    \address[B]{Thales Land and Air Systems, BL IAS, Rennes, France}
    \address[C]{AICA IWG, La Guillermie, France}

    %%% Use this environment to include an abstract of your paper.

    \begin{abstract}
        This paper addresses the challenge of explainability in Multi-Agent Reinforcement Learning (MARL) by proposing a novel algorithmic approach leveraging the $\mathcal{M}OISE^+$ Organizational Model. While previous studies have focused on understanding individual agent behaviors in traditional RL, our work emphasizes the need to elucidate the implicit cooperation among multiple agents in MARL systems. We introduce the \emph{Partial Relation between Agents' History and Organizational Model} (PRAHOM) algorithm, which links agents' histories to organizational specifications, facilitating the inference of cooperative structures from trained agents' behaviors. PRAHOM serves dual purposes: constraining the learning process based on organizational constraints and inferring organizational specifications from trained agents' histories. PRAHOM is designed to constrain agent policy spaces and generate valuable organizational specifications. Empirical evaluations conducted in cooperative Atari-like game environments validate the effectiveness of our algorithm, showing alignment with hand-crafted expectations and superior performance in some scenarios. This work seeks to contribute to AI explainability in MARL systems by offering a principled framework for understanding emergent cooperative behaviors. By bridging the gap between individual agent decision-making and cooperation, our algorithmic approach aims to enhance transparency and interpretability in complex Multi-Agent Systems.
    \end{abstract}

\end{frontmatter}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}

% Context
Explainable Artificial Intelligence (XAI) has emerged as a critical requirement for the widespread adoption of AI systems, particularly in multi-agent settings where multiple agents interact and cooperate to achieve complex goals~\citep{doshivelez2017rigorous,gunning2019xai}. While significant progress has been made in explaining the behavior of single agents~\citep{ribeiro2016classifier,lundberg2017unified}, the challenge of explicating the cooperative strategies and emergent organizational structures in MARL~\citep{busoniu2008survey} is not largely addressed~\citep{kok2006collaborative,omidshafiei2019learning}.

% Problem
For systems using MARL, a set of agents learn to achieve goals that may imply implicit cooperation and coordination. However, very few works have attempted to analyze the trained agents' policies to make this cooperation explicit and attempted to handle it directly~\citep{albrecht2018survey,perolat2017pool}. This lack of explainability hinders: \quad i) the understanding of the complex social, cooperation, and coordination schemes that emerge between the trained agents in a bottom-up manner; \quad ii) the application of these schemes to agents to guide their training in a top-down manner, hence providing safety guarantees.
Addressing these concerns would improve the trust and adoption of MARL systems in real-world applications~\citep{kok2006collaborative,omidshafiei2019learning}.

% The idea of benefitting of the particularly adaptive and general MARL mechanism to get an approximated suited MAS organization and producing associated specifications, requires to link the MARL training of a set of policies in a bidirectional way with a MAS organizational model. For instance, a hierarchy described in a MAS model would constrain the possible policies to get ultimately trained in MARL. Reversely, a set of trained policies could be described in an organizational model, thus indicating resemblance with known MAS organization architectures.

% Contribution
To address this problem, we propose a novel algorithmic approach called PRAHOM that leverages the $\mathcal{M}OISE^+$ organizational model~\citep{hubner2007moise} as a common ground to provide explicating the cooperative aspects in the observed agents' behaviors, or making these agents' policies constrained regarding expected cooperative schemes. The underlying idea is to establish relations between the agents' histories and the organizational specifications, such as roles, goals, or social links. These relations enable translating a joint-policy into a $\mathcal{M}OISE^+$ organizational description, or constraining the joint-policy training regarding organizational specifications.

The characterization of organizational aspects uses predefined relations to identify matching histories as organizational specifications. Otherwise it proposes a first incremental approach relying on a series of statistical and unsupervised learning techniques to automatically infer new raw organizational specifications from histories hence enriching known relations. A Large Language Model (LLM) provides complementary suggested labels and explanation for these raw data in a human-like manner.
When agents are constrained to organizational specifications, joint-policies are constrained by leveraging on the relations to know expected joint-histories. These enable constraining agents to choose expected actions when required so they stick to the expected behaviors.

% This paper first informally introduce the aforementioned broad idea as a new specific research study we refer to as \textquote{Organizational Multi-Agent Reinforcement Learning} (OMARL). We present DMO (Dec-POMDP $\mathcal{M}OISE^+$ OMARL) a first attempt to formally describe OMARL processes using the Decentralized Partially Observable Markov Decision Process (Dec-POMDP) as model for MARL and $\mathcal{M}OISE^+$ as an organizational model. Then, we propose PRAHOM (Partial Relations between Agents' Histories and Organizational Model), a DMO process that allows both getting some organizational specifications from \textquote{trained} policies and constraining policies training with respect to given specifications. As PRAHOM only changes the action set and relies \textquote{observed} actions, it is agnostic of the MARL algorithmic approach used jointly with function approximators as policies. We used PRAHOM within a proposed design approach to generate an efficient organization and associated OCPV specifications based on the environment, the initial design constraints, and goals.

% Outline
The remainder is organized as follows: Section 2 gives an overview of related works for XAI in MARL. Section 3 provides fundamental background in MARL and organizational models and presents how to link them. Section 4 introduces the PRAHOM algorithmic approach with its components. Section 5 describes the experimental setup and evaluation in an Atari-game environment. Section 6 presents and discusses the results of our empirical evaluation. Section 7 concludes the paper and outlines future research directions.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% JS : A GARDER SI ON EN A BESOIN

% \begin{eqnarray}\label{eq:vcg}
% p_i(\boldsymbol{\hat{v}}) & = &
% \sum_{j \neq i} \hat{v}_j(f(\boldsymbol{\hat{v}}_{-i})) - 
% \sum_{j \neq i} \hat{v}_j(f(\boldsymbol{\hat{v}})) 
% \end{eqnarray}

% \begin{theorem}[Fermat, 1637]\label{thm:fermat}
% No triple $(a,b,c)$ of natural numbers satisfies the equation 
% $a^n + b^n = c^n$ for any natural number $n > 2$.
% \end{theorem}

% \begin{proof}
% A full proof can be found in the supplementary material.
% \end{proof}

% \begin{table}[h]
% \caption{Locations of selected conference editions.}
% \centering
% \begin{tabular}{ll@{\hspace{8mm}}ll} 
% \toprule
% AISB-1980 & Amsterdam & ECAI-1990 & Stockholm \\
% ECAI-2000 & Berlin & ECAI-2010 & Lisbon \\
% ECAI-2020 & \multicolumn{3}{l}{Santiago de Compostela (online)} \\
% \bottomrule
% \end{tabular}
% \end{table}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Related works and positioning}

% présenter une revue "complète" de la littérature connexe dans les domaines de l'apprentissage par renforcement multi-agents (MARL) et de l'IA explicable.

% discuter des algos précédents pour améliorer l'explicabilité dans les systèmes MARL et identifier les lacunes dans la recherche existante.

MARL is a machine learning paradigm where agents learn to make decisions by interacting between each other and their environment. The goal is for a set of agents to maximize the cumulative reward over time through a process of trial and error.
MARL makes it possible to automatically converge towards policies enabling the given objective to be achieved. Reinterpreting these individual policies into organizational specifications requires work on explainability at a collective level that is rarely addressed in the literature.

% Understanding Cooperative Strategies and Emergent Organizational Structures:

% "Interpretable Policies for Multi-Agent Systems" by Foerster et al.
% "Towards Interpretable Multi-Agent Systems" by Hu et al.
% "Explainable AI for Cooperative Multi-Agent Systems: A Survey" by Li et al.
% "Interpretable Policies for Multi-Agent Reinforcement Learning" by Raileanu et al.
% "Learning to Explain: An Interpretable Approach to Policy Improvement in Multi-Agent Systems" by Saunders et al.


.% Safety Guarantees and Risk Mitigation:

% "Safety-Aware Learning for Multi-Agent Reinforcement Learning" by Alshiekh et al.


% Causal Understanding and Mechanisms:

% "Explainable Reinforcement Learning Through a Causal Lens" by Armstrong et al.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Interpretable Policy Learning:

% "Interpretable Policies for Multi-Agent Systems" by Foerster et al.
% "Interpretable Policies for Multi-Agent Reinforcement Learning" by Raileanu et al.
% "Learning to Explain: An Interpretable Approach to Policy Improvement in Multi-Agent Systems" by Saunders et al.


% Causal Understanding and Analysis:

% "Explainable Reinforcement Learning Through a Causal Lens" by Armstrong et al.


% Survey and Overview:

% "Explainable AI for Cooperative Multi-Agent Systems: A Survey" by Li et al.


% Safety and Risk Considerations:

% "Safety-Aware Learning for Multi-Agent Reinforcement Learning" by Alshiekh et al.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% "Towards Interpretable Multi-Agent Systems" by Hu et al.

% This paper focuses on developing interpretable models for Multi-Agent Reinforcement Learning (MARL) systems, addressing the need for explainability (XAI) in understanding the emergent organizational structures and cooperation schemes among agents.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\paragraph{\textbf{Framework for MARL with organizational aspects}}
%
Some proposed frameworks attempt to include organizational concepts within the MARL framework.
Kazhdan and. al.~\cite{Kazhdan2020} present a library to improve the explainability of MARL systems by bringing them closer to symbolic models, in particular to infer roles.%todo
%
Wang et. al.~\cite{Wang2020} introduce an approach in which similar emerging roles are pushed to jointly specialize on specific tasks.
%
Tosic et. al~\cite{Tosic2010} proposes a framework for coordination based on the communication capabilities of multi-agent systems.
%
Zheng et. al.~\cite{Zheng2018} presented a platform for MARL that aims to facilitate research on artificial collective intelligence by providing a comprehensive set of evaluation metrics to compare the performance of MARL algorithms.

\paragraph{\textbf{Characterization of emerging collective strategies}}
%
Heuillet and. al.~\cite{Heuillet2022} propose an approach to explain cooperative strategies using Shapley values. Its effectiveness has been demonstrated in the context of applications on multi-agent particle environments by explaining certain decisions taken.
%
Jaques and. al.~\cite{Jaques2019} propose a mechanism to benefit from communication between agents by rewarding agents having a causal influence on other agents. This approach leads to learned communication protocols allowing for overall more efficient collective behavior.

\paragraph{\textbf{Adaptation of MARL to meet design requirements}}
%
Shao et. al.~\cite{Shao2022} introduces an approach based on the leader-follower model as a mechanism to improve multi-agent cooperative tasks with dynamic characteristics, aiming to improve the adaptability and generalization of MARL systems.
%
Roy and. al.~\cite{Roy2020} presents two policy regularization methods aimed at improving coordination in reinforcement learning.
% %
\emph{Specification-Guided Reinforcement Learning} aims to generate policies that accomplish a specific task using external specifications to guide learning in achieving an objective and under given constraints~\cite{Bansal2022}.% ~\cite
%
~Jothimurugan et. al.~\cite{Jothimurugan2021} propose logical specification learning as exploiting the compositional structure of specifications to generate policies for complex tasks.

\

To our knowledge, there is no work that can be used to generate organizational specifications for an MAS achieving a given objective in an environment and respecting possible additional organizational constraints.
Unlike these works, our originality is to explicitly use an organizational model as a general means of expressing policies at a collective level and/or constraining their learning with respect to requirements.

\section{Theoretical foundations}

% introduire les fondements théoriques de notre algo:
%   - apprentissage par renforcement multi-agents (MARL)
%   - cadre du modèle organisationnel des systèmes multi-agents (MOISE+) et montrer sa pertinence pour améliorer l'explicabilité de l'IA au sein du MARL.

% détailler l'algorithme PRAHOM en décrivant ses composants et fonctionnalités...

% fournir des preuves formelles de son efficacité dans la restriction des espaces de politiques des agents durant et après l'apprentissage par rapport à des spécifications organisationnelles considérées comme des contraintes.

% fournir des preuves (formelles dans l'idéal) de son efficacité dans la génération de spécifications organisationnelles à partir des historiques des agents entrainés.

In this section, we first introduce the basics PRAHOM is relying on concerning MARL and the $\mathcal{M}OISE^{+}$ organizational model. Then, we formally describe how agents' policies and training process within MARL can be linked to the $\mathcal{M}OISE^{+}$ organizational model.

\subsection{MARL model}

The MARL model we rely on is based on the Decentralized Partially Observable Markov Decision Process (Dec-POMDP)~\citep{Oliehoek2016} because it considers multiple agents in a similar MAS fashion. It relies on stochastic processes to model uncertainty of the environment for the changes induced by actions, in received observations, in communication\dots \ Additionally, unlike Partially Observable Stochastic Games (POSG), the reward function can be common to agents which fosters training for collaborative oriented actions~\citep{Beynier2013}.
A Dec-POMDP $d \in D$ (with $D$ the set of Dec-POMDP) is a 7-tuple $d = (S,\{A_i\},T,R,\{\Omega_i\},O,\gamma)$ , where:
\begin{itemize}
    \item $S = \{s_1, ..s_{|S|}\}$: the set of the possible states;
    \item $A_{i} = \{a_{1}^{i},..,a_{|A_{i}|}^{i}\}$: the set of the possible actions for agent $i$;
    \item $T$ so that $T(s,a,s') = \probP{(s'|s,a)}$ : the set of conditional transition probabilities between states;
    \item $R: S \times A \times S \rightarrow \mathbb{R}$: The reward function
    \item $\Omega_{i} = \{o_{1}^{i},..,o_{|\Omega_{i}|}^{i}\}$: the set of observations for agent $i$;
    \item $O$ so that $O(s',a,o) = \probP{(o|s',a)}$ : the set of conditional observation probabilities;
    \item $\gamma \in [0,1]$, the discount factor.
\end{itemize}

Considering $m$ \textbf{teams} (also referred as \textbf{groups}) each containing several agents among $\mathcal{A}$, we also detail the minimal formalism notation we re-used for solving the Dec-POMDP for a given team $i, 0 \leq i \leq m$ containing $n$ agents~\citep{Beynier2013,Albrecht2024}:

\begin{itemize}

    \item $\Pi$: the set of policies. A \textbf{policy} $\pi \in \Pi, \pi: \Omega \rightarrow A$ deterministically maps an observation to an action. It represents the agent's internal logic;
    \item $\Pi_{joint}$: the set of joint-policies. A \textbf{joint-policy} $\pi_{joint} \in \Pi_{joint}, \pi_{joint}: \Omega^n \rightarrow A^n = \Pi^n$ chooses an action for each agent regarding their respective observation. It can be viewed as a set of the policies used in agents;
    \item $H$: the set of histories. A \textbf{history} over $z \in \mathbb{N}$ steps (where $z$ is generally the maximum number of steps for an episode) is the $z$-tuple $h = ((\omega_{k}, a_{k}) | k \leq z, \omega \in \Omega, a \in A)$;
    \item $H_{joint}$: the set of joint-histories. A \textbf{joint-history} over $z \in \mathbb{N}$ steps $h_{joint} \in H_{joint}, h_{joint} = \{h_1,h_2..h_n\}$ is the set of the agents' histories;
    \item $U_{joint,i}(<\pi_{joint,i}, \pi_{joint,-i}>): \Pi_{joint} \rightarrow \mathbb{R}$: gives the \textbf{expected cumulative reward} over a finite horizon (if $\gamma < 1$ or the number of steps in an episode is finite), with $\pi_{joint,i}$ the joint policy for team $i$ and $\pi_{joint,-i}$ all of the other concatenated joint-policies (considered as fixed);
    \item $BR_{joint,i}(\pi_{joint,i}) = argmax_{\pi_{joint,i}}(U(<\pi_{joint,i},\pi_{joint,-i}>))$: gives the \textbf{best response} $\pi_{joint,i}^*$ in the sense that the team cannot change any of the policies in the joint-policy $\pi_{joint,i}^*$ to get a better expected cumulative reward than $U_i^* = U_{joint,i}(<\pi_{joint,i}^*, \pi_{joint,-i}>)$;
    \item $SR_{joint,i}(\pi_{joint,i}, s) = \{\pi_{joint,i} | U(<\pi_{joint,i},\pi_{joint,-i}>) \geq s\}$: gives the \textbf{sufficient response} as the set of joint-policies getting at least $s \in \mathbb{R}, s \leq U_i^*$ as expected cumulative reward.
\end{itemize}

We refer to \textbf{solving} the Dec-POMDP for the team $i$ as finding a joint policy $\pi_{joint,i} \in \Pi_{joint}, \pi_{joint,i} = BR_{joint,i}(\pi_{joint,i})$ that maximize the expected cumulative reward over a finite horizon.
We refer to \textbf{sub-optimally solving} the Dec-POMDP at $s$ expectancy as finding a the joint policies $\pi_{joint,i} \in \Pi_{joint}, \pi_{joint,i} = SR_{joint,i}(\pi_{joint,i}, s)$ that gets the expected cumulative reward over a finite horizon at least at $s \in \mathbb{R}, s \leq U_i^*$.


\subsection{Organizational model}

$\mathcal{M}OISE^+$~\citep{Hubner2002} provide a relevant high-level description of the structures and interactions within the MAS. However, we favor $\mathcal{M}OISE^+$ because it provides an advanced formal description for an organization without incompatibilities with MARL, especially for formal description of agents' policies.
Based on $\mathcal{M}OISE^+$~\citep{Hubner2007} formalism, we only give the minimal elements of the formalism we used.

\paragraph{\textbf{Organizational specifications (OS)}}: $\mathcal{OS} = \langle \mathcal{SS}, \mathcal{FS}, \mathcal{DS} \rangle$, the set of all organization specifications, where $\mathcal{SS}$ are the \textbf{Structural Specifications}, $\mathcal{FS}$ are the \textbf{Functional Specifications}, and $\mathcal{DS}$ are the \textbf{Deontic Specifications}

\paragraph{\textbf{Structural Specifications (SS)}}: refer to the structured means left to agents to achieve a goal, we denote them as $\mathcal{SS} = \langle \mathcal{R}, \mathcal{IR}, \mathcal{G} \rangle$, where:

\begin{itemize}

    \item $\mathcal{R}_{ss}$: the set of all roles (denoted $\rho \in \mathcal{R}$);

    \item $\mathcal{IR}: \mathcal{R} \rightarrow \mathcal{R}$: the inheritance relation between roles ($\mathcal{IR}(\rho_1) = \rho_2$ means $\rho_1$ inherits from $\rho_2$ also denoted $\rho_1 \sqsubset \rho_2$);

    \item $\mathcal{RG} \subseteq \mathcal{GR}$ the set of root groups, $\mathcal{GR} = \langle \mathcal{R}, \mathcal{SG}, \mathcal{L}^{intra}, \mathcal{L}^{inter}, \mathcal{C}^{intra}, \mathcal{C}^{inter}, np, ng \rangle$, the set of all groups, where

          \begin{itemize}

              \item $\mathcal{R} \subseteq \mathcal{R}_{ss}$: the set of non-abstract roles;

              \item $\mathcal{SG} \subseteq \mathcal{GR}$: the set of sub-groups;

              \item $\mathcal{L} = \mathcal{R} \cross \mathcal{R} \cross \mathcal{TL}$: the set of links. A link is a 3-tuple $(\rho_s,\rho_d,t) \in \mathcal{L}$ (also denoted as a predicate $link(\rho_s,\rho_d,t))$, where $\rho_{s}$ is the source role, $\rho_{d}$ is the destination role, and $t \in \mathcal{TL}, \mathcal{TL} = \{acq, com, aut\}$ is the link type;
                    \begin{itemize}
                        \item If $t = acq$ (acquaintance), the agents playing the source role $\rho_{\mathrm{s}}$ are allowed to have a representation of the agents playing the destination role $\rho_{d}$;
                        \item If $t = com$ (communication), the $\rho_{\mathrm{s}}$ agents are allowed to communicate with $\rho_{d}$ agents;
                        \item If $t = aut$ (authority), the $\rho_{\mathrm{s}}$ agents are allowed to have authority on $\rho_{d}$ agents. It requires an acquaintance and communication link.
                    \end{itemize}
              \item $\mathcal{L}^{intra} \subseteq \mathcal{L}$: the set of intra-group links;
              \item $\mathcal{L}^{inter} \subseteq \mathcal{L}$: the set of inter-group links;

              \item $\mathcal{C} = \mathcal{R} \cross \mathcal{R}$: the set of compatibilities. A compatibility is a couple $(a,b) \in \mathcal{C}$ (also denoted $\rho_a \bowtie \rho_b$), means agents playing role $\rho_a \in \mathcal{R}$ can also play role $\rho_b \in \mathcal{R}$;
              \item $\mathcal{C}^{intra} \subseteq \mathcal{C}$: the set of intra-group compatibilities;
              \item $\mathcal{C}^{inter} \subseteq \mathcal{C}$: the set of inter-group compatibilities;

              \item $np: \mathcal{R} \rightarrow \mathbb{N} \times \mathbb{N}$: the relation giving the cardinality of agents adopting a role;
              \item $ng: \mathcal{SG} \rightarrow \mathbb{N} \times \mathbb{N}$: the relation giving the cardinality of each sub-group.

          \end{itemize}

\end{itemize}

\paragraph{\textbf{Functional Specifications (FS)}}: globally refer to the tasks and goals agents have to achieve, we denote them as $\mathcal{FS} = \langle \mathcal{SCH}, \mathcal{PO} \rangle$, where:

\begin{itemize}
    \item $\mathcal{SCH} = \langle\mathcal{G}, \mathcal{M}, \mathcal{P}, mo, nm \rangle$: the set of \textbf{social scheme}, where:
          \begin{itemize}
              \item $\mathcal{G}$ is the set of global goal;

              \item $\mathcal{M}$ is the set of mission labels;
              \item $\mathcal{P} = \langle \mathcal{G}, \{\mathcal{G}\}^s, OP, [0,1] \rangle, s \in \mathbb{N}^*$ is the set of plans that builds the tree structure of the goals.
                    %
                    A plan $p \in \mathcal{P}$ is 4-tuple $p=(g_f,\{g_i\}_{0 \leq i \leq s}, op, ps), g_f \in \mathcal{G}, g_i \in \mathcal{G}, op \in OP, OP = \{sequence, choice, parallel\}, ps \in [0,1]$, meaning that the goal $g_f$ is achieved if some of the sub-goals $g_i$ are achieved with a success probability $ps$ and according to the operator $op$:
                    %
                    \begin{itemize}
                        \item if $op = sequence$, the $g_i$ can only be achieved in the same order sequentially;
                        \item if $op = choice$, only one of the $g_i$ has to be achieved;
                        \item if $op = parallel$, the $g_i$ can only be achieved sequentially or simultaneously.
                    \end{itemize}

              \item $mo: \mathcal{M} \rightarrow \mathbb{P}(\mathcal{G})$: specifies the set of goals a mission is associated to;
              \item $nm: \mathcal{M} \rightarrow \mathbb{N} \times \mathbb{N}$ the cardinality of agents committed for each mission.
          \end{itemize}
    \item $\mathcal{PO}: \mathcal{M} \cross \mathcal{M}$: the set of \textbf{preference order}. A preference order is couple $(m_1, m_2), m_1 \in \mathcal{M}, m_2 \in \mathcal{M}$ (also denoted $m_{1} \prec m_{2}$) meaning that if there is a moment when an agent is permitted to commit to $m_{1}$ and also $m_{2}$, it has a social preference for committing to $m_{1}$.
\end{itemize}

\paragraph{\textbf{Deontic Specifications (DS)}}: refer how SS are to be used to achieve the FS, we denote them as $\mathcal{DS} = \langle \mathcal{OBL},\mathcal{PER} \rangle$, the set of deontic specifications, where:

\begin{itemize}
    \item $\mathcal{TC}$: the set of \textbf{time constraints}. A time constraint $tc \in \mathcal{TC}$ specifies a set of periods during which a permission or obligation is valid ($Any \in \mathcal{TC}$ means everytime);
    \item $\mathcal{OBL}: \mathcal{R} \cross \mathcal{M} \cross \mathcal{TC}$: the set of \textbf{obligations}. An obligation is a 3-tuple $(\rho_a,m,tc)$ (aslo denoted $obl(\rho_a,m,tc)$) meaning an agent playing role $\rho_a \in \mathcal{R}$ is obliged to commit on mission $m \in \mathcal{M}$ for a given time constraint $tc \in \mathcal{TC}$;
    \item $\mathcal{PER}$: the set of \textbf{permissions}. A permission is a 3-tuple $(\rho_a,m,tc)$ (aslo denoted $per(\rho_a,m,tc)$) meaning an agent playing role $\rho_a \in \mathcal{R}$ is permitted to commit on mission $m \in \mathcal{M}$ for a given time constraint $tc \in \mathcal{TC}$;
\end{itemize}

\subsection{Linking MARL and Organizational model}

We refer to Organizational oriented MARL (OMARL) as all the processes linking MARL and Organization model as for being able to get information from joint-policies to build an organizational model, and constraining possible output joint-policies subset regarding organizational specifications. We refer to \emph{Dec-POMDP $\mathcal{M}OISE^+$ OMARL} (DMO) as all the formally described OMARL process using the Dec-POMDP for MARL and $\mathcal{M}OISE^+$ for the organizational model.

In a DMO process, we consider solving sub-optimally the Dec-POMDP $d \in D$ for a single team $i$ (comprising $n$ agents in $\mathcal{A}$) at a $s = U_i^* - \delta$ (with $\delta \in \mathbb{R}$) expectancy, we may obtain a set $S\Pi_{joint,i,s} = SR_{d,joint,i}(\pi_{joint,i},s) = \{\pi_{joint,i,s,1}, \pi_{joint,i,s,2} .. \pi_{joint,i,s,m}\}$ with the $\pi_{joint,i,s,k} \in \Pi_{joint}$ ($k \in \mathbb{N}, k \leq m$) and $SR_{d,joint,i}(\pi_{joint,i},s)$ gives the sub-optimal joint-policies at $s$ expectancy for the Dec-POMDP $d$ also denoted $SR_{joint,i,s}(d)$. For instance, we may have $m$ different convergent joint-policies reaching a given expected cumulative reward after several training due to non-deterministic parameters in training.

Yet as an extra constraint, it is required to have only the joint-policies satisfying the constraining organizational specifications $\mathcal{OS}_{cons}$ we denote $S\Pi_{joint,i,\mathcal{OS}_{cons}} = \{\pi_{joint,i,\mathcal{OS}_{cons},1}, \pi_{joint,i,\mathcal{OS}_{cons},2} .. \pi_{joint,i,\mathcal{OS}_{cons},b}\}$ with $b \in \mathbb{N}$ and the $\pi_{joint,i,\mathcal{OS}_{cons},k} \in \Pi_{joint}$ ($k \in \mathbb{N}, k \leq b$). Then, one can envision to use the sub-optimal joint-policies that satisfy the specifications $S\Pi_{joint,i,s} \cap S\Pi_{joint,i,\mathcal{OS}_{cons}}$, on which we can infer further associated specifications $\mathcal{OS}_{i,s}$ of the implicit resulting organization.

As a first approach, a first general definition for a DMO process is a relation between the sub-optimal joint-policies set and associated organizational specifications set the following way:

\

\begin{relation}\label{rel:def_dmo}
    dmo: D \times \mathcal{OS}_{cons} \rightarrow (S\Pi_{joint,i,s} \cap S\Pi_{joint,i,\mathcal{OS}_{cons}}) \times \mathcal{OS}_{i,s}
\end{relation}
Can be understood as for a given problem comprising a environment and a goal to be achieved by agents, and some design constraints; we can possibly get sufficiently successful trained agents that are compliant with given specifications, and some associated organizational specifications.

In practice, implementing a DMO process faces two main issues:
%
\begin{enumerate}
    \item The intractability in analyzing policies directly because policy function approximations (such as neural networks) are generally difficult to explain hence hindering the extraction of organizational specifications;
    \item The lack of common ground between policies and organizational specifications enabling to know how agents are expected to behave when they are constrained to stick to organizational specifications. Reversly, there is no way to characterize any organizational specifications regarding behavior induced by agents' policies.
\end{enumerate}

To address (1), we hypothetize that for a single deterministic policy $\pi_q: \Omega_q \rightarrow A_q$ having been played over $l \in \mathbb{N}$ episodes hence $H_l = \{\langle (\omega_{j,k}, a_{j,k}), 0 \leq k \leq p_l, p_l \in \mathbb{N}, \omega_k \in \Omega_q, a_k \in A_q \rangle, 0 \leq j \leq l\}$, if there is $e \in \mathbb{N}$, $l \geq e$ and if many different observations $|\{\omega_{j,k}, 0 \leq j \leq l, 0 \leq k \leq p_l\}|$ are obtained, then the difference between the actual policy and the reconstructed policy $\pi_q^r: = {(\omega_{j,k}, a_{j,k}), 0 \leq j \leq l, 0 \leq k \leq p_l}$ is small $\delta_q^r = \pi_q \backslash \pi_q^r, |\delta_q^r| \leq b, 0 \leq b \leq |\pi_q|$.

Indeed, we hypothetize we can approximate the apparent behavior of a deterministic policy by saving all of the observation-action couples we can observe as a set. Thus, instead of analyzing policies directly, we only take agents' histories (obtained by letting play policies) into account to analyse their behaviors.

Continuing the previous approach, we address (2) by introducing a relation $osh: \mathcal{OS} \rightarrow \mathcal{P}(H_{joint})$ theoretically linking any organizational specification to a subset of expected joint-histories (practically expressed in short way as patterns).
Using that relation, we proposed the \autoref{rel:def_extended_dmo} that we showed is indeed compliant to initial $dmo$.% definition \autoref{rel:def_dmo}.
% TODO: En annexes -> in \autoref{proof:to_extended_dmo}.

\

\begin{relation}\label{rel:def_extended_dmo}
    $\\\phantom{}$ dmo_{ext}: D \times \mathcal{OS}_{cons} \rightarrow (SR_{joint,i,s}[D] \cap rct[osh[\mathcal{OS}_{cons}]]) \times osh^{-1}[obs[SR_{joint,i,s}[D]] \cap osh[\mathcal{OS}_{cons}]]
\end{relation}

\

\noindent With $rct: H_{joint} \rightarrow \Pi_{joint} = \{\pi = \{(\omega_i, a_i), 0 \leq i \leq z\}, h = \langle (\omega_i, a_i), 0 \leq i \leq z \rangle, h \in H_{joint}\}$. The relation to reconstruct a joint-policy from a joint-history.
\noindent With $obs: \Pi_{joint} \rightarrow H_{joint} = \{h = \langle (\omega_l, a_l) \in \pi \rangle, l \in \mathbb{N}, \pi = \{ (\omega_i, a_i), 0 \leq i \leq z \}, \pi \in \Pi_{joint}\}$. The relation to save a joint-history from a joint-policy.

\

From \autoref{rel:def_extended_dmo}, except from regular MARL training we notice a DMO process intended contribution to be represented by:
\begin{enumerate}
    \item $rct[osh[\mathcal{OS}_{cons}]]$: highlighting the need to know the expected joint-histories for given organizational specifications and how to use these joint-histories to constrain the joint-policies during training. We define this process according to the relation $osh-training: \mathcal{OS}_{cons} \rightarrow \Pi_{joint}$.
    \item $osh^{-1}[obs[SR_{joint,i,s}[D]] \cap osh[\mathcal{OS}_{cons}]]$: highlighting the need to sample (optionally constrained) joint-histories and to infer organizational specifications from them. We define this process according to the relation $hos: SH_{joint} \rightarrow \mathcal{OS}$
\end{enumerate}

Addressing the implementation of these intended contributions led to the development of the PRAHOM algorithmic approach as a first attempt to implement a DMO process.

\section{PRAHOM algorithmic approach}

Continuing the previous decomposition in two activities, we detailed our process to get the specifications out of the agents' policies as \emph{PRAHOM-hos} presented in \autoref{alg:PRAHOM-hos}; and the process to get the joint-policies satisfying given specifications as \emph{PRAHOM-osh-training} presented in \autoref{alg:PRAHOM-osh-training}. PRAHOM is then a synthesis of these two processes presented in \autoref{alg:PRAHOM}.

PRAHOM first launches the training of a joint-policy model on the given environment under organizational constraints relying on given relations. It enables getting the joint-policies reaching the given expectancy. Then, each of these joint-policies is played within differently generated environments to get all the joint-histories. Finally, these joint-histories are analyzed to infer the associated organizational specifications enriching known relations.

\RestyleAlgo{ruled}
\SetKwComment{Comment}{// }{}

\begin{algorithm}[hbt!]
    \caption{\emph{PRAHOM}}\label{alg:PRAHOM}

    \KwData{$d \in D$, Dec-POMDP to solve}
    \KwData{$ep_{max} \in \mathbb{N}$, maximum number of episodes}
    \KwData{$it_{max} \in \mathbb{N}$, maximum number of iteration}
    \KwData{$step_{max} \in \mathbb{N}$, maximum number of step per episode}
    \KwData{$s \in \mathbb{R}$, cumulative reward expectancy}
    \KwData{$cons: \mathcal{A} \rightarrow \mathcal{P}(\mathcal{OS})$, agents' org. specs. constraints}
    \KwData{$marl\_alg: \Pi_{joint} \times H_{joint} \times \mathbb{R}^{step_{max}} \rightarrow \Pi_{joint}$, algorithm to train from histories and rewards}
    \KwData{$osh: \mathcal{OS} \rightarrow \mathcal{P}(H_{joint})$, relations OS to histories}

    \KwResult{$(s\pi_{joint,i,s} \in S\Pi_{joint,i,s}, os_{i,s} \in \mathcal{OS}_{i,s})$, sub-optimal policies and associated org. specs.}

    \Comment{Init policy models}

    $\pi_{joint} = marl\_alg.init()$

    \Comment{Training under org. specs. constraints}

    $s\pi_{joint,i,s} = \mathbf{PRAHOM-osh-training}(cons,\dots)$

    % \phantom{X}

    \Comment{Get the $|s\pi_{joint,i,s}| \times ep_{max}$ joint-histories}
    $sh_{joint,i,s} \in SH_{joint,i,s} = gen-jth(ep_{max}, s\pi_{joint,i,s})$

    \Comment{Get org. specs. from joint-histories}
    $os_{i,s} = \mathbf{PRAHOM-hos}(sh_{joint,i,s},\dots)$

\end{algorithm}


\subsection{\textbf{Infering OS from joint-histories}}

\input{algos/PRAHOM_hos.tex}\label{alg:prahom-hos}

\begin{figure*}[h!]
    \centering
    \input{figures/gosia_illustrative_view.tex}
    \caption{GOSIA illustrative view}
    \label{fig:gosia_illustrative_view}
\end{figure*}

The \autoref{alg:PRAHOM-hos} implements \emph{PRAHOM-hos}. Its underlying principle is first to rely on \emph{Knowledge-based Organizational Specification Identification Approach} (KOSIA) to indentify some organizational specifications thanks to the current $hos$ relation. KOSIA is aimed to be relevant when knowledge of the relations between histories and organizational specifications is significant enough to be used generally.
Otherwise, if it is not possible getting all known associated organizational specifications, the General Organizational Specification Inference Approach (GOSIA) suggests an empirical approach to infer the rest of them.
GOSIA is based on some proposed definitions for each $\mathcal{M}OISE^+$ organizational specification regarding joint-histories, to use suggested specific statistical, unsupervised learning techniques to infer them incrementally. We explain GOSIA's principle through \autoref{fig:gosia_illustrative_view}.
%
\paragraph{1) Infer roles and their inheritance}

We propose a role $\rho$ is defined as a policy whose the associated histories of agents having adopted it all contain a common discontinuous sequence. We proposed a role $\rho_2$ inherits $\rho_2$ if the common discontinuous sequence of the histories associated with $\rho_2$ are also contained in the $\rho_1$'s ones.
From these definitions, we leverage hierarchical sequence clustering to find longest common discontinuous sequences among agent's histories. Results can represented as a dendogram. It enables inferring roles and inheritance relations, their respective relation with histories and the current agents as well.

\paragraph{2) Infer possible organizations}

We propose an organization is linked to only one set of all instantiable roles sharing closely similar inheritance relations. Indeed, for two trained joint-policies $H_{joint,i,s,1}$ and $H_{joint,i,s,2}$, even though both achieve a goal relying the roles $\mathcal{R}_{ss,1}$ and $\mathcal{R}_{ss,2}$ may be far from other. For instance, their roles may not use the same responsability distribution.
We used a K-means algorithm to get the $q$ clusters of the vectorized $\mathcal{IR}_{i}$ we consider as organizations. The roles in a same cluster inherit from to the K-means' centroid $\mathcal{IR}_j$. Indeed, they are representative general roles regarding all the similar roles adopted by agents of the same organization over all joint-histories.
For the next steps of GOSIA we choose only one organization and considered only the related joint-histories. When it exists we aim to choose an organization close to KOSIA's one.

\paragraph{3) Infer links and sub-groups}

We propose two agents have a \emph{social impact link} $(ag_1,ag_2, \kappa, \delta, f)$ with $h_1$ associated to $ag_1$ and $h_2$ associated to $ag_2$ if a sequence $h_{1,s}$ in $h_1$ is correlated at a $\kappa \in [0,1]$ index to another sequence $h_{2,s}$ in $h_2$ positioned at a relative delay $\delta \in [0,1]$ after the beginning of $h_{1,s}$, and these two correlated sequences are $f$ frequently present among all joint-policies.
We propose two agents are in the same group if there is a social impact link such as $f \geq 0.9$. Considering that $\kappa$ indicates the likeliness of an agent' sequence to impact another one, and that $\delta$ indicates its reactivity, we propose:
\begin{itemize}
    \item an acquaintance link $(ag_1,ag_2,acq)$ is defined if there is a social impact link with $\kappa \geq 0.1$, $\delta \geq 0$, $f \geq 0$;
    \item a communication link $(ag_1,ag_2,com)$ is defined if there is a social impact link with $\kappa \geq 0.3$, $\delta \geq 0$, $f \geq 0$;
    \item an authority link $(ag_1,ag_2,aut)$ is defined if there is a social impact link with $\kappa \geq 0.9$, $\delta \geq 0.5$, $f \geq 0$.
\end{itemize}
% TODO: Generaliser à plusieurs agents ou le dire en tant que limitation
We used empirical techniques and the cross-correlation techniques to compute a graph of the social impact links between agents. Frequency enables determining clusters as groups agents and their associated roles. We can also infer acquaintance, communication, and authority links between roles. From the information concerning roles associated to groups, we can infer whether links are intra-group or inter-groups.

\paragraph{4) Infer goals, plans, and missions}

We propose a sub-goal/goal is a set of common states that are reached following histories of the successful agents.
For each joint-history we computed the state transition graph that we merged into a general one. Measuring distance between two vectorized states within K-means enables finding the clusters of trajectories that some agents may follow. Then, we sampled some sets of states for each trajectory as goals. For instance, we may choose the narrowest set of states in which agents collectively seem to transition at some point to achieve their goal. Otherwise, a balanced sampling over lower variance trajectories could be made. Knowing what goal belongs to what trajectory, we can infer plans for choice and sequence only.

This enables getting goals, plans at a global state but these goals could be indeed splitted into specific goals for each sub-group and agent. To do this, we respectively proposed conducting the same process replacing states by the observations of the agents in a same sub-group, and observations for agents.

We propose a mission as the set of sub-goals one or several agents are achieving.
Knowing what shared goals are achieved by agents, we can determine some sets of representative goals as missions.

\paragraph{5) Infer compatibilities, obligations, and permissions and cardinalities}

We propose an obligation is when an agent playing role $\rho$ is achieving a mission's goals and no other one at some time constraints.
We propose an obligation is when an agent playing role $\rho$ is achieving a mission's goals and some other ones at some time constraints.
We can determine what agents' are associated to what mission and if they are restricted to some hence these are obligations or just permission

We propose a compatibility $(\rho_1,\rho_2)$ is defined if a agent playing a history associated to $\rho_1$ in a joint-history with a given groups and roles' configuration also plays a history associated to $\rho_2$ in the same configuration. If this change operates only in a same group, then it is intra-group. Else it is inter-group.

Finally from inferred organizational specifications, we can compute the cardinalities.By counting the number of agent playing a role in each joint-history, we determine the role's cardinality. Similarly, we can determine the sub-group's cardinality.

\paragraph{Complementary help of Large Language Models (LLM)}

We set up the \emph{EleutherAI} \emph{gpt-neo-2.7B} pre-trained transformer using \emph{Transformers}' \emph{Huggingface} library. Within \emph{PRAHOM-hos} it is automatically prompt-engineered with generated specific contextual description of the environment, its functioning, and the $\mathcal{M}OISE^+$ organizational specifications. This prompt-engineering is to better guide the answers towards three main purposes:
\begin{enumerate}
    \item Labeling, tagging the infered organizational specification from related joint-histories in a human-like manner;
    \item Giving specific human-like textual description about each organizational specification;
    \item Giving general human-like textual description about the whole organization.
\end{enumerate}

% Schéma général de PRAHOM-HOS (PRAHOM-History to Organizational Specification) :

% PRAHOM-HOS:
%
%  if h_i is (at least partially) known
%     apply KOSI (Knowledge-based Organizational Identification) approach
%
%  else apply GOSI (General Organizational Specification Infering) approach
%    1) SS:
%       1) infer roles & roles inheritance relations (via sequence clustering)
%       2) groups, (via analysis of agents social impact)
%       3) infer compatibilities (via comparison between analyzed joint histories)
%       4) infer intra & extra links (via analysis of roles social impact in a given group / all groups)
%       5) infer roles agents cardinality (via comparison between analyzed joint histories)
%       6) infer sub-groups cardinality (via comparison betwee analyzed joint-histories)
%
%    2) FS:
%       1) infer goals, plans (via threshold states analysis, generalized role behavior analysis...)
%       2) infer missions (unifying goals and associated agents and analysis of goal tree structure to determine goals shared by several agents)
%       3) infer mission to goals (via generalization over analyzed joint-histories)
%       4) infer agents' cardinality for each mission (via generalization over analyzed joint-histories)
%
%    3) DS
%       1) infer permissions & obligations (via generalization over analyzed joint-histories)

%%%%%%%%%%%%%%%%%%%%%%%%%%


\subsection{\textbf{Constraining joint-policies satisfying OS}}

\

We consider a given MARL algorithm that iteratively converges towards a joint-policy so that each agent's policy is updated at each step until a finite horizon.
%In \autoref{proof:jpc_to_ac}
We proved that constraining the available action set for each agent to the action sets authorized by the organization specifications at each step of the MARL training, implies constraining the converged joint-policies to the ones that satisfy the given organization specifications. We used that result, to setup \emph{PRAHOM-osh-training} presented in \autoref{alg:PRAHOM-osh-training}.

\emph{PRAHOM-osh-training} fits within a regular MARL context: joint-policies $\Pi_{joint,ep}$ are updated after several steps allowing to have enough joint-histories $h_{joint,ep}$ and joint-rewards as feedbacks $r_{joint,ep}$. The training goes on over several episode for better training until converging to a sufficient cumulative reward regarding $s$. New training are launched until $it_{max}$ times to get more joint-policies to add in $s\pi_{joint}$ as a result.

It augments that framework by changing the way agents choose their action to meet the expectations of the organizational specifications. Using the known relations $osh$ with agents' constraints $cons[\mathcal{A}]$, $cons\_act$ has to guess the most expected actions regarding current $h_{joint,ep}$. In practice, we implemented it as a simple pattern matching engine. These expected joint-actions $a_{joint,exp}$ are to be chosen by agents. Two mode are available to integrate these constraints:
\begin{itemize}
    \item \textquote{correct-mode}: Correct any unexpected action by sampling an expected one (such as a simple random selection). This mode mainly aims agent to converge faster by reducing the search space with safety guarantees. Yet, it is external to the agent's policy;
    \item \textquote{penalize-mode}: Add a penalty to the joint-reward if any wrong action has bee made among agents. This mode aims to make agents \textquote{learn} to respect the given organizational constraints. Yet, since agents simply approximate the expcted behavior, it is not possible to have safety guarantees.
\end{itemize}


\RestyleAlgo{ruled}
\SetKwComment{Comment}{// }{}

\begin{algorithm}[hbt!]

    \caption{PRAHOM-osh-training}\label{alg:PRAHOM-osh-training}

    \KwData{$cons: \mathcal{A} \rightarrow \mathcal{P}(\mathcal{OS})$, agents' org. specs. constraints}
    
    \Comment{PRAHOM context}

    \KwData{...}

    \KwResult{$s\pi_{joint,i,s} \in S\Pi_{joint,i,s}$, the sub-optimal joint-policies that satisfy the design specifications}

    $\pi_{joint,init}$ = $\pi_{joint}$

    \ForEach{$1 \leq it \leq it_{max}$}{

        $\pi_{joint}$ = $\pi_{joint,init}$

        \ForEach{$1 \leq ep \leq ep_{max}$}{

            \Comment{Reinit the env., obs., act. hist.}
            $h_{joint,ep} = ()$ ;
            $rh_{joint,ep} = ()$ ;
            $\omega_{joint,ep,t=0} = (0)^{|\mathcal{A}|}$ ;
            $a_{joint,ep,t=0} = (0)^{|\mathcal{A}|}$ ;
            $r_{joint,ep,t=0} = (0)^{|\mathcal{A}|}$ ;
            $d_{ep} = d$ ;
            $r_{joint,penalty} = (0)^{|\mathcal{A}|}$

            \ForEach{$1 \leq step \leq step_{max}$}{

                \Comment{Update jt-policy from hist., rew.}

                $\pi_{joint} = marl\_alg(\pi_{joint},h_{joint,ep}, rh)$


                \Comment{Choose next act. from obs.}
                $a_{joint,t=step} = \pi_{joint}(\omega_{joint,t=step})$

                \Comment{Get expct. act. from OS and rels.}
                $a_{joint,exp} = cons\_act(osh,h_{joint,ep},cons[\mathcal{A}])$

                \uIf{$a_{joint,t=step} \notin a_{joint,exp}$}{
                    \uIf{correct-mode}{
                        $a_{joint,t=step} = sample(a_{joint,exp})$
                    }
                    \uElseIf{penalize-mode}{
                        $r_{joint,penalty} += penalty()$
                    }
                }

                \Comment{Record obs. and chosen act.}

                $h_{joint,ep} = h_{joint,ep} \cup (\omega_{joint,t=step}, a_{joint,t=step})$

                \Comment{Apply action}

                $\omega_{joint,t=step+1}, r_{joint,ep,t=step+1} = d_{ep}.step(a_{joint,t=step})$

                $rh_{joint,ep} = rh_{joint,ep} \cup r_{joint,ep,t=step+1}$

                \Comment{Record resulting reward}

                $rh_{joint,ep} = rh_{joint,ep} \cup (r_{joint,ep,t=step+1} + r_{joint, penalty})$

            }

            \uIf{sufficient($rh_{joint,ep}$, $s$)}{
                $s\pi_{joint} = s\pi_{joint} \cup \{\pi_{joint,ep}\}$
            }

        }

    }

\end{algorithm}


\section{Methodology}

% décrire la méthodologie pour évaluer PRAHOM
%   - configuration expérimentale: les environnements de jeu coopératifs PettingZoo de type Atari avec les paramètres spécifiques utilisés, présentation et utilisation de "PRAHOM Wrapper" pour intégrer PRAHOM dans les environnements de jeu.
%   - mesures utilisées pour évaluer l'efficacité de notre algo et fournir un aperçu du processus de collecte et d'analyse des données.

In order to assess PRAHOM, we considered using \emph{PRAHOM} in available simulated environments made up of agents that have to achieve a goal with the best performance through various collective strategies whose some can be easily understood.
We selected three Atari-like environments for their visual rendering is a convenient way to assess the results with manual observations\footnotemark[1].

\footnotetext[1]{Additional explanation and the examples discussed using \emph{PRAHOM PettingZoo wrapper} are attached to this paper}

\begin{itemize}
    \item \textquote{Pistonball} (PBL)~\cite{Terry2021} is a series of pistons to bring a ball from right to left side hence requiring neighbors' representation;
    \item \textquote{Predator-prey with communication}~\cite{Lowe2017} (PPY) consists of predators monitored by a leader to catch faster prey hence requiring hunting strategies;
    \item \textquote{Knights Archers Zombies}~\cite{Terry2021} (KAZ) consists in knights and archers learning how to kill zombies hence requiring efficient agent spatial positioning.
\end{itemize}
%
\noindent We applied PRAHOM in three cases:
\begin{itemize}
    \item No organizational specifications (NTS): agents have to learn an efficient collective strategies without any constraints or indications.
    \item Partially constraining organizational specifications (PTS): some constraints are given to help converge faster or meet requirements.
    \item Fully constraining organizational specifications (FTS): manually crafted joint-policies are given for they are a reference regarding learned joint-policies.
\end{itemize}

\noindent Here, we do not present the details of the constraints that were given in NTS and FTS (available in Git repository\footnotemark[1]).


\section{Results and discussion}

% présenter les résultats de nos évaluations empiriques
%   - performances de l'algorithme PRAHOM vis-à-vis des contraintes dans l'espaces des politiques des agents et la génération de spécifications organisationnelles à partir des historiques des agents entrainés

% discuter des implications de nos résultats et mettre en évidence toutes les observations ou tendances notables observées au cours des expériences.

% discussion complète des implications de nos résultats et de leur signification plus large dans le contexte de MARL et du XAI

% explorer les explications potentielles des résultats observés

% discuter des limites de notre algo et proposons des pistes de recherche future pour remédier à ces limites.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.46\textwidth]{figures/prahom_learning_curve.png}
    \caption{Average reward for each iteration in the PBL environment for the NTS, PTS, and FTS cases}
    \label{fig:prahom_learning_curve}
\end{figure}
%
% \begin{figure}[h!]
%     \centering
%     \includegraphics[width=0.45\textwidth]{figures/prahom_pca_analysis.png}
%     \caption{PCA of the trained agents' histories in the PBL environment}
%     \label{fig:prahom_pca_analysis}
% \end{figure}
%
We evaluate the impact of \emph{PRAHOM} on the following criteria: convergence time ratios between PTS, NTS, and FTS for reaching a threshold cumulative reward. Performance stability shows how the trained agents can achieve the goal generally by assessing several environments generated with different parameters. Results are presented in Table~\ref{tab:training_AOMEA_results}.
%
\input{tables/training_OMARL_results.tex}
%
As a general observation, we can notice convergence time is longer for NTS than for PTS which is also longer than for FTS. As expected, the search space is decreasing, hence a shorter convergence time. For instance, we noticed a faster convergence to a sub-optimal solution in the PBL environment by providing organizational specifications as presented in \autoref{fig:prahom_learning_curve}. Although PTS converges faster than NTS to a comparable cumulative reward, NTS may outperform PTS because trained agents' policies are hand-tailored to solve the problem much more finely than the designer's organizational specifications can do.

We also took into account the following criteria after training: roles, links, and global performance.
%A qualitative analysis is presented in Table~\ref{tab:trained_AOMEA_results}
%
% \input{tables/trained_OMARL_results.tex}
%
% //TODO: Moise+ schemes and comparison with expected ones
%
For the PBL environment, we can notice roles being equivalent for agents are expected to act the same. Indeed, trained agents' histories are close hence showing a common emerging role.
% We generate the PCA presented in \autoref{fig:prahom_pca_analysis} by expressing agents' histories as vectors containing the observation-action couples. We can notice most agents’ histories are in the left bottom zone (circled in red)
It shows most pistons seem to act similarly as expected. We observe no organizational specifications except roles have been generated because agents cannot communicate. For the KAZ environment, we can notice two distinct roles: archers tend to move away from zombies, while knights tend to approach them. For the PPY environment, we can observe the output specifications indicate authority links between the leader predator and the simple predators to enable collective strategies for circling prey.

\section{Conclusion}

% résumer nos contributions et nos principales conclusions.

% importance d’améliorer l’explicabilité de l’IA dans les systèmes MARL et souligner l’impact potentiel de PRAHOM sur l’avancement du domaine.

% proposer des remarques finales et décrire les orientations des recherches futures afin d’explorer et d’affiner davantage notre algo.

% Multi-agent methods rely on the designer's knowledge to design a suitable MAS organization, but do not provide automatic means to determine the relevant organizational mechanisms only from the design requirements and the overall objective.
% MARL techniques have been successfully applied to automatically train agents to achieve a given objective without explicit characterization of emerging collective strategies.
% The PRAHOM's originality is to enrich a MARL process with an explicit organizational model towards a methodological objective to address these issues. It links the agents' policies (modeled in a Dec-POMDP) with $\mathcal{M}OISE^+$ through the process PRAHOM. Under the simplifying conditions of a group and a single social pattern, PRAHOM makes it possible to partially determine organizational specifications from joint histories and to constrain the training of policies in relation to organizational specifications.
% Additionally, we implemented the \emph{PRAHOM PettingZoo} wrapper as a proof of concept to apply PRAHOM.
% Finally, we applied our approach in four \emph{PettingZoo} environments to evaluate the impact during and after training. The performances obtained appear to be comparable to those known.

% Key point summary
We showed that PRAHOM effectively constrains the resulting agents' policy space according to the specified organizational constraints. Additionally, we demonstrate that PRAHOM can produce valuable organizational specifications from trained agents' histories. The PRAHOM algorithmic approach is evaluated in cooperative Atari-like game environments, where the inferred organizational specifications generally match or outperform the expected hand-crafted expected results.

% Perspectives
Even if PRAHOM is agnostic of any MARL algorithm. Hower, reconstructing the collective behaviors of agents a posteriori can prove difficult. Indeed, major perspectives to improve PRAHOM are inspired by hierarchical learning which contribute to better characterizing emerging strategies during learning.
% Ultimately, we also aim to improve the applicability of PRAHOM by developing dedicated interfaces built around PRAHOM making it more accessible to industrial and research contexts.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% Use this environment to include acknowledgements (optional).
%%% This will be omitted in doubleblind mode.

\begin{ack}
    This work was supported by \emph{Thales Land Air Systems} within the framework of the \emph{Cyb'Air} chair and the \emph{AICA IWG}.
\end{ack}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage

%%% Use this command to include your bibliography file.

\bibliography{references}

% \newpage

% \section*{Annexes}

% \subsection*{Action constraining during training implies result joint-policy constraining}
% \input{proofs/jpc_to_ac.tex}

\end{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
