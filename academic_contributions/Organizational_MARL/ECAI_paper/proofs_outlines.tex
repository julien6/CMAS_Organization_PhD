%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% LaTeX Template for ECAI Papers 
%%% Prepared by Ulle Endriss (version 1.0 of 2023-12-10)

%%% To be used with the ECAI class file ecai.cls.
%%% You also will need a bibliography file (such as mybibfile.bib).

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% Start your document with the \documentclass{} command.
%%% Use the first variant for the camera-ready paper.
%%% Use the second variant for submission (for double-blind reviewing).

\documentclass{ecai} 

% \documentclass[doubleblind]{ecai} 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% Load any packages you require here. 

\usepackage{latexsym}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{booktabs}
% \usepackage{enumitem}
\usepackage{graphicx}
\usepackage{color}

\usepackage[T1]{fontenc}
\usepackage{graphicx}
%\usepackage{color}
%\renewcommand\UrlFont{\color{blue}\rmfamily}

\usepackage{amsmath,amssymb,amsfonts}
\usepackage[inline, shortlabels]{enumitem}
\usepackage{tabularx}
\usepackage{caption}
% \usepackage{titlesec}
\usepackage[english]{babel}
\captionsetup{font=it}
\usepackage{ragged2e}
\usepackage{hyperref}
\usepackage{pifont}
\usepackage{footmisc}
\usepackage{multirow}
\usepackage{algorithm2e}

% --- Tickz
\usepackage{physics}
\usepackage{amsmath}
\usepackage{tikz}
\usepackage{mathdots}
\usepackage{yhmath}
\usepackage{cancel}
\usepackage{color}
\usepackage{siunitx}
\usepackage{array}
\usepackage{multirow}
\usepackage{amssymb}
\usepackage{gensymb}
\usepackage{tabularx}
\usepackage{extarrows}
\usepackage{booktabs}
\usetikzlibrary{fadings}
\usetikzlibrary{patterns}
\usetikzlibrary{shadows.blur}
\usetikzlibrary{shapes}

% ---------

\usepackage{pdfpages}
\usepackage{booktabs}
\usepackage{csquotes}
\usepackage{lipsum}  
\usepackage{arydshln}
\usepackage{smartdiagram}
\usepackage[inkscapeformat=png]{svg}
\usepackage{textcomp}
\usepackage{tabularray}\UseTblrLibrary{varwidth}
\usepackage{xcolor}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
% \usepackage{cite}
\usepackage{amsmath}
\newcommand{\probP}{\text{I\kern-0.15em P}}
\usepackage{etoolbox}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% Define any theorem-like environments you require here.

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{fact}[theorem]{Fact}
\newtheorem{definition}{Definition}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% Define any new commands you require here.

% \newcommand{\BibTeX}{B\kern-.05em{\sc i\kern-.025em b}\kern-.08em\TeX}

\renewcommand{\arraystretch}{1.7}

\setlength{\extrarowheight}{2.5pt}
\renewcommand{\arraystretch}{0.2}
\renewcommand{\arraystretch}{1.7}

\newcommand{\before}[1]{\textcolor{red}{#1}}
\newcommand{\after}[1]{\textcolor{green}{#1}}

\newcommand{\old}[1]{\textcolor{orange}{#1}}
\newcommand{\rem}[1]{\textcolor{red}{#1}}
\newcommand{\todo}[1]{\textcolor{orange}{\newline \textit{\textbf{TODO:} #1}} \newline \newline }



\newcounter{relation}
\setcounter{relation}{0}
\renewcommand{\therelation}{\arabic{relation}}
\newcommand{\relationautorefname}{Relation}

\newenvironment{relation}[1][]{%
    \refstepcounter{relation}%
    \noindent \raggedright \textit{\textbf{Relation. \therelation}} \hfill$}
{%
$ \hfill \phantom{x}

}

\newcounter{proof}
\setcounter{proof}{0}
\renewcommand{\theproof}{\arabic{proof}}
\newcommand{\proofautorefname}{Proof}

\renewenvironment{proof}[1][]{
    \refstepcounter{proof}
    \noindent \raggedright \textit{\textbf{Proof. \theproof}}

    \setlength{\leftskip}{1em}

}
{

\
\setlength{\leftskip}{0pt}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frontmatter}

    \title{Appendix: Proofs Outlines}

\end{frontmatter}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{$dmo_{ext}$ is compliant with $dmo$}

\begin{proof}\label{proof:to_extended_dmo}

    Let $dmo_{ext}: D \times \mathcal{OS}_{cons} \rightarrow (SR_{joint,i,s}[D] \cap rct[osh[\mathcal{OS}_{cons}]]) \times osh^{-1}[obs[SR_{joint,i,s}[D]] \cap osh[\mathcal{OS}_{cons}]]$ \\

    By definition, $(SR_{joint,i,s}[D] \cap rct[osh[\mathcal{OS}_{cons}]]) = (S\Pi_{joint,i,s} \cap S\Pi_{joint,i,\mathcal{OS}_{cons}})$ \\

    By definition, $osh^{-1}[obs[SR_{joint,i,s}[D]] \cap osh[\mathcal{OS}_{cons}]] = osh^{-1}[obs[\Pi_{joint,i,s}] \cap H_{joint,i,\mathcal{OS}_{cons}}] = osh^{-1}[H_{joint,i,s} \cap H_{joint,i,\mathcal{OS}_{cons}}]$

    With $H_{joint,i,s} \cap H_{joint,i,\mathcal{OS}_{cons}} = H_{joint,i,\mathcal{OS}_{cons}}^{'} \subseteq H_{joint,i,\mathcal{OS}_{cons}}$

    Hence, $osh^{-1}[H_{joint,i,\mathcal{OS}_{cons}}^{'}] = \mathcal{OS}_{cons}^{'} \subseteq \mathcal{OS}_{cons}$

    Finally, $dmo_{ext}: D \times \mathcal{OS}_{cons} \rightarrow (S\Pi_{joint,i,s} \cap S\Pi_{joint,i,\mathcal{OS}_{cons}}) \times \mathcal{OS}_{i,cons}$

    Hence, $dmo_{ext} \subseteq dmo$
\end{proof}


\section{Action constraining during training implies constrained joint-policy}

\begin{proof}\label{proof:jpc_to_ac}
    
    Let $n \in \mathbb{N}$ agents
    
    Let a Dec-POMDP, $d \in D$
    
    Let a cumulative reward expectancy, $s \in \mathbb{R}$
    
    Let some specifications constraints, $os_{init} \in OS$
    
    Let a joint-policy $\pi_{joint} \in \Pi_{joint}, \allowbreak \pi_{joint} = \{\pi_1,\pi_2..\pi_n\}, \pi_k \in \Pi,(k \in \mathbb{N}, k \leq n)$
    
    Let's consider only one episode
    
    Let $step_{max} \in \mathbb{N}$, the maximum number of steps per episode
    
    Let $Solv: \Pi \times \Omega \times A \times \mathbb{R} \rightarrow \Pi$, the RL algorithm that uses the reward, observation to improve a given agent's policy (possibly at each step) by updating the observation-action couples of the policy so that $Solv(\pi, \omega, r) = (\pi \setminus \{(\omega_p, a_p) | p \in \mathbb{N}\}) \cup \{(\omega_q, a_q) | q \in \mathbb{N}\}$
    
    For each step, the joint-policy is updated by the RL algorithm.
    
    Let $solutions = \{\pi_{joint,1}, \pi_{joint,1}.. \pi_{joint,d}\} (d \in \mathbb{N})$ the set of joint-policies obtained at a $s$ cumulative reward expectancy
    
    Let $satisfying = \{\pi_{joint,1}, \pi_{joint,1}.. \pi_{joint,f}\} (f \in \mathbb{N})$ the set of joint-policies that are allowed by the organization specifications $os_{init}$
    
    The joint-policies that satisfy the organization specifications and reach at least a cumulative reward expectancy are
    Let $compliant = solution \cap solutions = \{\pi_{joint,1}, \pi_{joint,1}.. \pi_{joint,q}\} (q \in \mathbb{N})$
    It means that for a given joint-policy in compliant, for a given agent's policy we cannot find a couple observation-action that is not in the matching satisfying joint-policy.

    Now, let's suppose we restrict the action space at each step for each agent.

    % -------
    At first step, the agent $i$ has a empty history $h = ()$ and its initial policy is $\pi_{i,0} \in \Pi$, has an empty previous action $a_0 = \emptyset$ and observation $\omega_0 = \emptyset$.
    (we choose $\pi_{i,0} \in \pi_{joint} | \pi_{joint} \in satisfying$)

    When agent $i$ receives an observation $\omega_{1}$ and reward $r_1$:
    The available actions are $A_1 = \{a | (\omega, a) \in \pi, \pi \in \pi_{joint},\pi_{joint} \in satisfying\}$

    It updates its policy based on previous action, observation and reward: $\pi_{i,1} = Solv(\pi_{i,0}, \omega_0, a_0, r_1)$.
    $\pi_{i,1} = (\pi_{i,0} \setminus \{(\omega_p, a_p) | p \in \mathbb{N}\}) \cup \{(\omega_q, a_q) | q \in \mathbb{N}\}$ with the $(\omega_p, a_p) \in (\pi_{i,0} \cup {(\omega_0, a_0)})$ and $(\omega_q, a_q) \in (\pi_{i,0} \cup {(\omega_0, a_0)})$. Consequently, $\pi_{i,1} \in \pi_{joint} | \pi_{joint} \in satisfying$.


    It uses its policy to determine next action: $a_1 = \pi_{i,1}(\omega_1)$

    % -----
    At second step, the agent $i$ has now a non-empty history $h = ((\omega_1, a_1))$ and its policy is $\pi_{i,1} \in \Pi$, has a previous action $a_1$ and observation $\omega_1$.

    When agent $i$ receives an observation $\omega_{2}$ and reward $r_2$:
    The available actions are $A_1 = \{a | (\omega_2, a) \in \pi, \pi \in \pi_{joint},\pi_{joint} \in satisfying, \{(\omega_1, a_1)\} \in \pi\}$

    (note: $A_1 \subset A_0$)

    It updates its policy based on previous action, observation and reward: $\pi_{i,2} = Solv(\pi_{i,1}, \omega_0, a_0, r_2)$.
    $\pi_{i,2} = (\pi_{i,1} \setminus \{(\omega_p, a_p) | p \in \mathbb{N}\}) \cup \{(\omega_q, a_q) | q \in \mathbb{N}\}$ with the $(\omega_p, a_p) \in (\pi_{i,1} \cup {(\omega_1, a_1)})$ and $(\omega_q, a_q) \in (\pi_{i,1} \cup {(\omega_1, a_1)})$. Consequently, $\pi_{i,2} \in \pi_{joint} | \pi_{joint} \in satisfying$.

    It uses its policy to determine next action: $a_2 = \pi_{i,2}(\omega_2)$

    % -----
    At step $step_{max}$, the agent $i$ has now a non-empty history $h = ((\omega_1, a_1), (\omega_2, a_2)..(\omega_{step_{max}-1}, a_{step_{max}-1}))$

    When agent $i$ receives an observation $\omega_{step_{max}}$
    The available actions are $A_{step_{max}} = \{a | (\omega_{step_{max}}, a) \in \pi, \pi \in \pi_{joint},\pi_{joint} \in satisfying, \{(\omega', a') \in h\} \in \pi\}$

    (note: $A_1 \subset A_0 \subset \dots \subset A_{step_{max}}$)

    It updates its policy based on previous action, observation and reward: $\pi_{i,step_{max}} = Solv(\pi_{i,step_{max}-1}, \omega_0, a_0, r_{step_{max}})$.
    $\pi_{i,step_{max}} = (\pi_{i,step_{max}-1} \setminus \{(\omega_p, a_p) | p \in \mathbb{N}\}) \cup \{(\omega_q, a_q) | q \in \mathbb{N}\}$ with the $(\omega_p, a_p) \in (\pi_{i,step_{max}-1} \cup {(\omega_{step_{max}-1}, a_{step_{max}-1})})$ and $(\omega_q, a_q) \in (\pi_{i,step_{max}-1} \cup {(\omega_{step_{max}-1}, a_{step_{max}-1})})$. Consequently, $\pi_{i,step_{max}} \in \pi_{joint} | \pi_{joint} \in satisfying$.

    It uses its policy to determine next action: $a_{step_{max}} = \pi_{i,step_{max}}(\omega_{step_{max}})$

    % -----
    At the end, its history is now $h = ((\omega_1, a_1), (\omega_2, a_2)..(\omega_{step_{max}}, a_{step_{max}}))$

    By construction, the $(\omega_i,a_i) \in h$ ($i \leq step_{max}$) belong to the policies that belong to the joint-policies $satisfying$.
    
    By construction, $\pi_{i,step_{max}} \in \pi_{joint}, \pi_{joint} \in satisfying$.

    Applying several episodes does not change that result as it keeps the same policy from the end of an episode to the beginning of another one.

    Thus, the resulting joint-policy $\pi_{joint} = \{\pi_{i,step_{max}}, i \leq n\} \in satisfying$

    We assume the given design specification do not prevent the MARL algorithm to provide agents' policies that reach $s$ cumulative reward expectancy, then:

    Thus, the resulting joint-policy $\pi_{joint} = \{\pi_{i,step_{max}}, i \leq n\} \in solutions$

\end{proof}

\end{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
