\begin{proofoutline}\label{proof:jpc_to_ac}

    We provide an overview of our approach to constrain the possible policies of trained agents through a simple abstract example. While this example is somewhat artificial, it serves to illustrate the general principle of our approach and gives insights into why it is indeed effective in constraining policies.

    \noindent Let's consider an example with this initial configuration:

    \begin{itemize}
        \item $d=\langle S,A,T,R,\Omega, O, \gamma \rangle \in D$, the Dec-POMDP to solve (i.e maximizing $R$);
        \item $\mathcal{A}, |\mathcal{A}| = n \in \mathbb{N}$, the  $n$ agents involved in the Dec-POMDP;
        \item $s \in \mathbb{R}$, the cumulative reward expectancy to reach;
        \item $\pi_{joint} \in \Pi_{joint}, \allowbreak \pi_{joint} = \{\pi_1..\pi_n\}, \pi_k \in \Pi (k \leq n)$, the joint-policy to update;
        \item $ep_{max}$, the maximum number of episodes;
        \item $step_{max}$, the maximum number of steps per episode;
        \item $u_{marl}: \Pi_{joint} \times H_{joint} \times R_{joint} \rightarrow \Pi_{joint}$, the MARL algorithm that uses the joint-reward and joint-history to update a joint-policy;
    \end{itemize}
    %
    \noindent We assume some organizational specifications are defined, applied to agents, and associated with matching history subsets (at least from a theoretical point view):
    \item $os \in \mathcal{OS}$, the organizational specifications containing: $\mathcal{R}$, the roles that agents may be constrained to; $\mathcal{M}$, the missions that agents may be committed to; $\mathcal{OBL}$, the obligations indicating whether an agent playing a role $\rho \in \mathcal{R}$ is obligated to commit on mission $m \in \mathcal{M}$. In this example, we do not consider permissions; $rh: \mathcal{R} \rightarrow \mathcal{P}(H)$: gives the expected history subset for a role; $mh: \mathcal{M} \rightarrow \mathcal{P}(H)$: gives the expected history subset for a mission; $da: \mathcal{OBL} \rightarrow \mathcal{P}(A)$: gives the agents constrained to a role and obligated to commit on a mission.

    \

    \noindent We suppose there exists a set of joint-policies $S\Pi_{joint} = \{s\pi_{joint,1}.. s\pi_{joint,d}\} \allowbreak (d \in \mathbb{N})$, that enables reaching at least the $s$ cumulative reward expectancy.

    \noindent We suppose there exists a set of joint-policies $O\Pi_{joint} = \{o\pi_{joint,1}.. o\pi_{joint,d'}\} (d' \in \mathbb{N})$ that satisfy the applied organizational specifications, so that an agent playing role $\rho \in \mathcal{R}$ and obligated to commit on mission $m \in \mathcal{M}$ should have its policy $o\pi_{joint,i} \ (i \leq d')$ to generate any matching history $h \in (rh(\rho) \cap mh(m))$.

    \noindent We assume there exists a non-empty set of joint-policies $P\Pi = S\Pi \cap O\Pi \allowbreak = \{p\pi_{joint,1}..p\pi_{joint,q}\}, q \in \mathbb{N}$ that both reach at least the $s$ cumulative reward expectancy and satisfy the organizational specifications $os$.

    \

    Based on these assumptions and initial data, we apply PRAHOM on the first iterations and generalize it to indefinite number of iteration, in order to determine whether it enables building a policy that does belong to $P\Pi$. Although all constraints integration modes are effective in constraining policies, in this example, we chose the $correct\_policy$ mode to apply our algorithm for it offers a clear way to understand the proof outline.
    We consider the first episode. Initially, a constrained policy $\pi_{joint} = \pi_{joint,c}$ built from the initial policy $\pi_{joint,init,0}$ and the observable policy constraint $c\pi_{joint}$.

    At first step, agents have an empty history $h_{joint} = \langle \rangle$, null rewards $rh_{joint} = \langle (0)^n \rangle $. Thus, the initial policies $\pi_{joint,0} \in \Pi_{joint}$ are not updated for now. Receiving the initial observations for each agents $\omega_{joint,0} \in \Omega_{joint}$, agents choose their respective next actions $a_{joint,0}$ using their policies $\allowbreak \pi_{joint,0}$. The observations and actions are stored in the joint-histories $h_{joint} \allowbreak = \allowbreak \langle \allowbreak (\omega_{joint,0}, \allowbreak a_{joint,0}) \rangle$. Then, the action are applied, hence generating new observations $\omega_{joint,1}$ and rewards $r_{joint,1}$ stored in $rh_{joint}$ for the next step.

    % \

    % At second step, agents have the current history $h_{joint} = \langle (\omega_{joint,0}, a_{joint,0}) \rangle$, and rewards $rh_{joint} = \langle (0)^n, r_{joint,1} \rangle$. Thus, the policies are updated accordingly $\pi_{joint,1} = u_{marl}(\pi_{joint,0},h_{joint},rh_{joint})$. From received observation $\omega_{joint,1}$, agents choose their next actions $a_{joint,1}$ using their policies $\pi_{joint,1}$. The observations and actions are stored in the joint-histories $h_{joint} = \langle (\omega_{joint,0},a_{joint,0}), (\omega_{joint,1},a_{joint,1}) \rangle$. Then, the actions are applied, hence generating new observations $\omega_{joint,2}$ and rewards $r_{joint,2}$ stored in $rh_{joint}$ for the next step.

    Generalizing until the $p < step_{max}$ step, agents have the current history $h_{joint} = \langle (\omega_{joint,0}, a_{joint,0}), (\omega_{joint,1}, a_{joint,1})..(\omega_{joint,p-1}, a_{joint,p-1}) \rangle$, and rewards $rh_{joint} = \langle (0)^n, r_{joint,1}, r_{joint,2}..r_{joint,p} \rangle$. Thus, the policies are updated accordingly $\pi_{joint,p} = u_{marl}(\pi_{joint,p-1},h_{joint},rh_{joint})$. From received observation $\omega_{joint,p}$, agents choose their next actions $a_{joint,p}$ using their policies $\pi_{joint,p}$. The observations and actions are stored in the joint-histories $h_{joint} \allowbreak = \allowbreak \langle \allowbreak (\omega_{joint,0},a_{joint,0}), \allowbreak (\omega_{joint,1},a_{joint,1}), (\omega_{joint,2},a_{joint,2})..(\omega_{joint,p},a_{joint,p}) \rangle$. Then, the actions are applied, hence generating new observations $\omega_{joint,p+1}$ and rewards $r_{joint,p+1}$ stored in $rh_{joint}$ for the next step.

    \

    When $p = step_{max}$, the episode is finished, we assume the cumulative reward reaches at least $s$. The generated histories are $h_{joint} = \langle (\omega_{joint,0},a_{joint,0}) .. \allowbreak (\omega_{joint,step_{max}},a_{joint,step_{max}}) \rangle$. Throughout all steps, it is built using the $\pi_{joint,k}, \allowbreak k < step_{max}$ and $\pi_{joint,k} \allowbreak = \allowbreak \{sample(c\pi_{joint}(\omega_{joint})) \allowbreak \ \allowbreak if \allowbreak \ \allowbreak \omega_{joint} \in Dom(c\pi_{joint}) \allowbreak \ \allowbreak else \allowbreak \ \allowbreak \pi_{joint,k}(\omega_{joint,init,k})\}$.
    %
    By definition, $\langle (\omega_{joint,j}, \allowbreak sample(c\pi_{joint}(\omega_{joint,j})))_{j < step_{max}} \rangle, \allowbreak \omega_{joint,j} \allowbreak \in \Omega_{joint}$, the joint-history generated using the observable constrained policy satisfy the organizational specifications. Thus, the policy represented by $sample(c\pi_{joint}(\omega_{joint,j}))$ belongs to $O\Pi_{joint}$ and possibly $S\Pi_{joint}$.
    %
    By construction, $\langle (\pi_{joint,init,k}(\omega_{joint,k}))_{k < step_{max}} \rangle$, the joint-history generated using the initial policy trained over $k$ steps so that the cumulative reward reach at least $s$. Thus, the policy $\pi_{joint,init,step_{max}}$ belong to $S\Pi_{joint}$.

    Considering several episodes, being reached thanks to a policy in $\allowbreak S\Pi_{joint,ep_{max},step_{max}}$. Moreover, since a history $h_{joint}$ belongs, at least, to histories generated by a policy in $O\Pi_{joint}$. Thus, $\pi_{joint,ep_{max},step_{max}} \in S\Pi \cap O\Pi, \pi_{joint,ep_{max},step_{max}} \in P\Pi$. So, built policies indeed satisfy organizational specifications while reaching sufficient cumulative reward expectancy.

    \

    As for the other constraint integration modes, we briefly outline the main ideas supporting why it is also effective as well as the $correct\_policy$ mode:
    $\mathbf{correct}$, corrects the action according to an observable policy constraint after the initial policy has chosen it. Without other consideration, it can be modeled by building a constrained policy $\pi_c$ encompassing both the observable policy constraint and the initial policy. Therefore, this goes back to the $correct\_policy$ case;
    $\mathbf{penalize}$, adjust the reward comparing chosen action by the policy and the expected ones according to an observable policy constraint. We assume that the policy can be updated according to rewards so that it asymptotically tends be equal to any constrained policy formed from the current policy and the observable policy constraint. Therefore, it also goes back to the $correct\_policy$ case.

\end{proofoutline}