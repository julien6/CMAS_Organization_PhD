\documentclass[runningheads]{llncs}

\usepackage[T1]{fontenc}
\usepackage{graphicx}
%\usepackage{color}
%\renewcommand\UrlFont{\color{blue}\rmfamily}

\usepackage{amsmath,amssymb,amsfonts}
\usepackage[inline, shortlabels]{enumitem}
\usepackage{tabularx}
\usepackage{caption}
\usepackage{listings}
% \usepackage{titlesec}
\usepackage[english]{babel}
\captionsetup{font=it}
\usepackage{ragged2e}
\usepackage[hyphens]{url}
\usepackage{hyperref}
\usepackage{xurl}
\usepackage{pifont}
\usepackage{footmisc}
\usepackage{multirow}
\usepackage{enumitem}
\usepackage{algorithm2e}
\usepackage{float}
\usepackage{listings}
\usepackage{xcolor}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}
 
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}
 
\lstset{style=mystyle}

% --- Tickz
\usepackage{physics}
\usepackage{amsmath}
\usepackage{tikz}
\usepackage{mathdots}
\usepackage{yhmath}
\usepackage{cancel}
\usepackage{color}
\usepackage{siunitx}
\usepackage{array}
\usepackage{multirow}
\usepackage{amssymb}
\usepackage{gensymb}
\usepackage{tabularx}
\usepackage{extarrows}
\usepackage{booktabs}
\usetikzlibrary{fadings}
\usetikzlibrary{patterns}
\usetikzlibrary{shadows.blur}
\usetikzlibrary{shapes}

% ---------
% \usepackage{titlesec}
\usepackage{pdfpages}
\usepackage{booktabs}
\usepackage{csquotes}
\usepackage{lipsum}  
\usepackage{arydshln}
\usepackage{smartdiagram}
\usepackage[inkscapeformat=png]{svg}
\usepackage{textcomp}
\usepackage{tabularray}\UseTblrLibrary{varwidth}
\usepackage{xcolor}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\usepackage{cite}
\usepackage{amsmath}
\newcommand{\probP}{\text{I\kern-0.15em P}}
\usepackage{etoolbox}
\patchcmd{\thebibliography}{\section*{\refname}}{}{}{}

\setlength\tabcolsep{0.5pt}

% \renewcommand{\arraystretch}{1.7}

% \setlength{\extrarowheight}{2.5pt}
% \renewcommand{\arraystretch}{0.2}
% \renewcommand{\arraystretch}{1.7}

% --------------
% \titleclass{\subsubsubsection}{straight}[\subsection]

% \newcounter{subsubsubsection}[subsubsection]
% \renewcommand\thesubsubsubsection{\thesubsubsection.\arabic{subsubsubsection}}
% \renewcommand\theparagraph{\thesubsubsubsection.\arabic{paragraph}} % optional; useful if paragraphs are to be numbered

% \titleformat{\subsubsubsection}
%   {\normalfont\normalsize\bfseries}{\thesubsubsubsection}{1em}{}
% \titlespacing*{\subsubsubsection}
% {0pt}{3.25ex plus 1ex minus .2ex}{1.5ex plus .2ex}

% \makeatletter
% \renewcommand\paragraph{\@startsection{paragraph}{5}{\z@}%
%   {3.25ex \@plus1ex \@minus.2ex}%
%   {-1em}%
%   {\normalfont\normalsize\bfseries}}
% \renewcommand\subparagraph{\@startsection{subparagraph}{6}{\parindent}%
%   {3.25ex \@plus1ex \@minus .2ex}%
%   {-1em}%
%   {\normalfont\normalsize\bfseries}}
% \def\toclevel@subsubsubsection{4}
% \def\toclevel@paragraph{5}
% \def\toclevel@paragraph{6}
% \def\l@subsubsubsection{\@dottedtocline{4}{7em}{4em}}
% \def\l@paragraph{\@dottedtocline{5}{10em}{5em}}
% \def\l@subparagraph{\@dottedtocline{6}{14em}{6em}}
% \makeatother

% \setcounter{secnumdepth}{4}
% \setcounter{tocdepth}{4}
% --------------

\newcommand{\before}[1]{\textcolor{red}{#1}}
\newcommand{\after}[1]{\textcolor{green}{#1}}

\newcommand{\old}[1]{\textcolor{orange}{#1}}
\newcommand{\rem}[1]{\textcolor{red}{#1}}
\newcommand{\todo}[1]{\textcolor{orange}{\newline \textit{\textbf{TODO:} #1}} \newline \newline }



\newcounter{relation}
\setcounter{relation}{0}
\renewcommand{\therelation}{\arabic{relation}}
\newcommand{\relationautorefname}{Relation}

\newenvironment{relation}[1][]{%
    \refstepcounter{relation}%
    \noindent \raggedright \textit{\textbf{Relation. \therelation}} \hfill$}
{%
$ \hfill \phantom{x}

}

\newcounter{proof}
\setcounter{proof}{0}
\renewcommand{\theproof}{\arabic{proof}}
\newcommand{\proofautorefname}{Proof}

\renewenvironment{proof}[1][]{
    \refstepcounter{proof}
    \noindent \raggedright \textit{\textbf{Proof. \theproof}}

    \setlength{\leftskip}{1em}

}
{

\
\setlength{\leftskip}{0pt}
}

% --------------------------------
%             DOCUMENT
% --------------------------------

\begin{document}
\title{An Approach for Explicating and Handling Implicit Organization in MARL}
%
%\titlerunning{Abbreviated paper title}
% If the paper title is too long for the running head, you can set
% an abbreviated paper title here
%
% \author{Julien Soulé\inst{1}\orcidID{0000-1111-2222-3333} \and
% Jean-Paul Jamont\inst{1}\orcidID{1111-2222-3333-4444} \and
% Michel Occello\inst{1}\orcidID{2222--3333-4444-5555} \and
% Louis-Marie Traonouez\inst{2}\orcidID{2222--3333-4444-5555} \and
% Paul Théron\inst{3}\orcidID{2222--3333-4444-5555}}

%%% Double blind review %%%
\author{}
\authorrunning{}
\institute{}
% \author{Julien Soulé\inst{1} \and
% Jean-Paul Jamont\inst{1} \and
% Michel Occello\inst{1} \and
% Louis-Marie Traonouez\inst{2} \and
% Paul Théron\inst{3}}
% %
% \authorrunning{J. Soulé et al.}
% % First names are abbreviated in the running head.
% % If there are more than two authors, 'et al.' is used.
% %
% \institute{Univ. Grenoble Alpes, Grenoble INP, LCIS, 26000, Valence, France
%     \email{\{julien.soule, jean-paul.jamont, michel.occello\}@lcis.grenoble-inp.fr}
%     \and
%     Thales Land and Air Systems, BL IAS, Rennes, France
%     \email{louis-marie.traonouez@thalesgroup.com}
%     \and
%     AICA IWG, La Guillermie, France \\
%     \email{paul.theron@orange.fr}
% }


\maketitle              % typeset the header of the contribution

% MAS have been succefully
% For many MAS, the organization has become a critical success factor. 
% Several related methods exist to design MAS. 
% However, these methods are ...
% To enhance the quality and effectiveness of ... this paper presents an assisted approach for MAS Organization Engineering (AMOEA). 
% AMOEA guides the designer of...
% 1 phrase par contrib/key point

\begin{abstract}

    This paper addresses the challenge of leveraging Multi-Agent Reinforcement Learning (MARL) for Multi-Agent Systems (MAS) design by making it usable for both constraining agents' training regarding organizational requirements and explaining collective aspects of trained agents in a human-readable manner. While previous studies have focused on guiding a single agent's training or explicating its behavior, a multi-agent context necessitates elucidating the implicit cooperation among multiple agents after training. We propose a novel algorithmic approach leveraging the $\mathcal{M}OISE^+$ Organizational Model, called Partial Relation between Agents' History and Organizational Model (PRAHOM). The core principle of our algorithm links organizational specifications, such as roles or missions, to the respective agents' histories characterizing their behaviors. PRAHOM serves dual purposes: constraining the learning process based on organizational constraints and inferring organizational specifications from trained agents' histories. Evaluations conducted in a cooperative Predator-Prey environment validate the impact of organizational specifications as constraints for training and demonstrate that the inferred organizational specifications align with the expected ones.
    %This work seeks to contribute to AI explainability in MARL systems by offering a principled framework for understanding emergent cooperative behaviors.
    % By bridging the gap between individual agent decision-making and cooperation, PRAHOM aims to enhance transparency and interpretability in complex Multi-Agent Systems.
    \keywords{Multi-Agent Reinforcement Learning \and Collective XAI \and Organizational Models \and Multi-Agent Systems}

    % % context
    % Multi-Agent Systems (MAS) have been successfully applied in industry for their ability to address complex, distributed problems, especially in IoT-based systems.
    % Their efficiency in achieving given objectives and meeting design requirements is strongly dependent on the MAS organization during the engineering process of an application-specific MAS. To design a MAS that can achieve given goals, available methods rely on the designer's knowledge of the deployment environment.
    % % Yet, in some cases, the deployment environment is not easily readable or handleable due to the complexity and may lead to unexpected emergent phenomena raising safety concerns.
    % However, high complexity and low readability in some deployment environments make the application of these methods to be costly or raise safety concerns.
    % % That stresses out the need for methodological works for assisted MAS design that could be addressed with collective AI techniques.
    % % hypothesis / contribution
    % In order to ease the MAS organization design regarding those concerns, we introduce an original Assisted MAS Organization Engineering Approach (AOMEA). AOMEA relies on combining a Multi-Agent Reinforcement Learning (MARL) process with an organizational model to suggest relevant organizational specifications to help in MAS engineering.
    % % We introduce , an novel design approach to assist the MAS design whose underlying idea is to use Multi-Agent Reinforcement Learning with organizational specifications for both understanding and constraining the training process regarding design constraints.
    % % results
    % % We applied our approach in cooperative Atari games and a Cyberdefense drone swarm scenario of the 3rd CAGE Challenge. Obtained specifications are indeed consistent with design constraints and provide insights of relevant collective strategies that led to develop an explainable MAS with scores close to finalists' ones.

    % \keywords{Multi-Agent Systems \and Design \and Assisted engineering}
\end{abstract}


% Context:

% General problem: Need for automating/assisting MAS design -> through XAI... -> short general explanation -> split into 2 gaps:

% Gaps:
%   - (G1): Automating the search for joint-policies taking into account organizational constraints;
%   - (G2): Giving means to explain and understand the trained joint-policies for MAS design purposes.

% Proposition: An algorithm relying on MARL and OM -> short description

% Plan:
% II) Background and motivations:
% - What are the related works?
% - Why choosing to use MARL and OM?
% - Theoretical basics of MARL and OM
% III) Proposed algorithm
% - How we combined OMARL and OM formally to get PRAHOM?
% - What are the sub-gaps and how we addressed these
%       - Addressing (G1)
%           - Sub-gaps: (G1.A1), (G1.A2)
%       - Addressing (G2)
%           - Sub-gaps: (G2.A1), (G2.A1)
% IV) Algorithm implementation
% - How we implemented PRAHOM technically into the PRAHOM Wrapper?
% - What are the sub-gaps and how we addressed these
%       - Addressing (G1.A1)
%           - Sub-gaps: (G1.A2.T1), (G1.A2.T2)
%       ...

% V) Case study: Prey-predator
% - Show a full use of the PRAHOM Wrapper / Tutorial-like of PRAHOM Wrapper step by step
% 1) Configuration of obs/act-label
% 2) Setting up the OSH model,
% 3) Configuring and launching training
% 4) Configuring and running OS determination

% - Discuss raw results (with generated figures)

% VI) Conclusion
% ====================================================================================================

\section{Introduction}

% Context
Explainable Artificial Intelligence (XAI) has emerged as a critical requirement for the widespread adoption of AI systems, particularly in multi-agent settings where multiple agents interact and cooperate to achieve complex goals~\cite{doshivelez2017rigorous,gunning2019xai}. While significant progress has been made in explaining the behavior of single agents~\cite{ribeiro2016classifier,lundberg2017unified}, the challenge of explicating the cooperative strategies and emergent organizational structures in MARL~\cite{busoniu2008survey} is not largely addressed~\cite{kok2006collaborative,omidshafiei2019learning}.

% Problem
For systems using MARL, a set of agents learn to achieve goals that may imply implicit cooperation and coordination. Very few works have attempted to analyze the trained agents' policies to make this cooperation explicit and attempted to handle it directly~\cite{albrecht2018survey,perolat2017pool}. This lack of explainability is core to two main gaps we intend to cover:
%
\begin{itemize}[wide, labelsep = 1em]
    \item[$(\mathbf{G_E})$] Understanding the organizational schemes such as social or collective links that emerge between the trained agents in a bottom-up manner;
    \item[$(\mathbf{G_A})$] Applying hand-crafted organizational specifications to agents to guide their training in a top-down manner, hence providing safety guarantees.
\end{itemize}
%
Addressing these concerns would improve the trust and adoption of MARL systems in real-world applications~\cite{kok2006collaborative,omidshafiei2019learning}.

% Contribution
To address these gaps, we propose an algorithmic approach called PRAHOM that leverages the $\mathcal{M}OISE^+$ organizational model~\cite{Hubner2007}. Our premise was to use an organizational model as a medium to explicate the cooperative aspects in the observed agents' behaviors into concepts such as roles, social links, goals, missions to address $(\mathbf{G1})$. Reversely, one can also use $\mathcal{M}OISE^+$ to constrain agents' policies regarding organizational specifications hence addressing $(\mathbf{A})$. To link $\mathcal{M}OISE^+$ and MARL, we one-to-one map an organizational specification, mostly a role or a mission, to an agent's history subset that characterize an expected behavior in MARL. PRAHOM relies on this mapping to automatically describe a trained joint-policy into a $\mathcal{M}OISE^+$ organizational description if the generated histories are already in the mapping. Otherwise, we propose a first incremental approach relying on a series of statistical, unsupervised learning techniques, and a complimentary Large Language Model (LLM) to automatically infer new organizational specifications from histories. Finally, PRAHOM enables constraining the joint-policy training to satisfy some organizational specifications using the mapped expected histories by dynamically modifying MARL elements such as the action space or the reward function accordingly.

% Outline
The remainder is organized as follows: Section 2 gives an overview of the available works dealing with XAI or design in MARL. This work leads to present the theoretical foundations we retained for building our contribution in section 3. Section 4 introduces the PRAHOM algorithmic approach. Section 5 first presents our first PRAHOM implementation as a library we used in our experimental setup. Then we discuss the results of our empirical evaluation. Section 6 concludes the paper and outlines future research directions.



\section{Related works and positioning}

% présenter une revue "complète" de la littérature connexe dans les domaines de l'apprentissage par renforcement multi-agents (MARL) et de l'IA explicable.

% discuter des algos précédents pour améliorer l'explicabilité dans les systèmes MARL et identifier les lacunes dans la recherche existante.

MARL is a machine learning paradigm where agents learn to make decisions by interacting with each other and their environment. The goal is for a set of agents to maximize the cumulative reward over time through a process of trial and error.
MARL makes it possible to automatically converge towards policies enabling the given objective to be achieved. Reinterpreting these individual policies into organizational specifications requires work on explainability at a collective level that is rarely addressed in the literature.

% Understanding Cooperative Strategies and Emergent Organizational Structures:

% "Interpretable Policies for Multi-Agent Systems" by Foerster et al.
% "Towards Interpretable Multi-Agent Systems" by Hu et al.
% "Explainable AI for Cooperative Multi-Agent Systems: A Survey" by Li et al.
% "Interpretable Policies for Multi-Agent Reinforcement Learning" by Raileanu et al.
% "Learning to Explain: An Interpretable Approach to Policy Improvement in Multi-Agent Systems" by Saunders et al.


% Safety Guarantees and Risk Mitigation:

% "Safety-Aware Learning for Multi-Agent Reinforcement Learning" by Alshiekh et al.


% Causal Understanding and Mechanisms:

% "Explainable Reinforcement Learning Through a Causal Lens" by Armstrong et al.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Interpretable Policy Learning:

% "Interpretable Policies for Multi-Agent Systems" by Foerster et al.
% "Interpretable Policies for Multi-Agent Reinforcement Learning" by Raileanu et al.
% "Learning to Explain: An Interpretable Approach to Policy Improvement in Multi-Agent Systems" by Saunders et al.


% Causal Understanding and Analysis:

% "Explainable Reinforcement Learning Through a Causal Lens" by Armstrong et al.


% Survey and Overview:

% "Explainable AI for Cooperative Multi-Agent Systems: A Survey" by Li et al.


% Safety and Risk Considerations:

% "Safety-Aware Learning for Multi-Agent Reinforcement Learning" by Alshiekh et al.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% "Towards Interpretable Multi-Agent Systems" by Hu et al.

% This paper focuses on developing interpretable models for Multi-Agent Reinforcement Learning (MARL) systems, addressing the need for explainability (XAI) in understanding the emergent organizational structures and cooperation schemes among agents.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\paragraph{\textbf{Framework for MARL with organizational aspects}}
%
Some proposed frameworks attempt to include organizational concepts within the MARL framework.
Kazhdan and. al.~\cite{Kazhdan2020} present a library to improve the explainability of MARL systems by bringing them closer to symbolic models, in particular, to infer roles.%todo
%
Wang et. al.~\cite{Wang2020} introduce an approach in which similar emerging roles are pushed to jointly specialize on specific tasks.
%
Tosic et. al~\cite{Tosic2010} propose a framework for coordination based on the communication capabilities of multi-agent systems.
%
Zheng et. al.~\cite{Zheng2018} presented a platform for MARL that aims to facilitate research on artificial collective intelligence by providing a comprehensive set of evaluation metrics to compare the performance of MARL algorithms.

\paragraph{\textbf{Characterization of emerging collective strategies}}
%
Heuillet and. al.~\cite{Heuillet2022} propose an approach to explain cooperative strategies using Shapley values. Its effectiveness has been demonstrated in the context of applications on multi-agent particle environments by explaining certain decisions taken.
%
Jaques and. al.~\cite{Jaques2019} propose a mechanism to benefit from communication between agents by rewarding agents having a causal influence on other agents. This approach leads to learned communication protocols allowing for overall more efficient collective behavior.

\paragraph{\textbf{Adaptation of MARL to meet design requirements}}
%
Shao et. al.~\cite{Shao2022} introduce an approach based on the leader-follower model as a mechanism to improve multi-agent cooperative tasks with dynamic characteristics, aiming to improve the adaptability and generalization of MARL systems.
%
Roy and. al.~\cite{Roy2020} present two policy regularization methods aimed at improving coordination in reinforcement learning.
% %
\emph{Specification-Guided Reinforcement Learning} aims to generate policies that accomplish a specific task using external specifications to guide learning in achieving an objective under given constraints~\cite{Bansal2022}.% ~\cite
%
~Jothimurugan et. al.~\cite{Jothimurugan2021} propose logical specification learning as exploiting the compositional structure of specifications to generate policies for complex tasks.

\

To our knowledge, no work can be used to generate organizational specifications for a MAS achieving a given objective in an environment and respecting possible additional organizational constraints.
Unlike these works, our originality is to explicitly use an organizational model as a general means of expressing policies at a collective level and/or constraining their learning regarding requirements.



\section{Theoretical foundations}

% introduire les fondements théoriques de notre algo:
%   - apprentissage par renforcement multi-agents (MARL)
%   - cadre du modèle organisationnel des systèmes multi-agents (MOISE+) et montrer sa pertinence pour améliorer l'explicabilité de l'IA au sein du MARL.

% détailler l'algorithme PRAHOM en décrivant ses composants et fonctionnalités...

% fournir des preuves formelles de son efficacité dans la restriction des espaces de politiques des agents durant et après l'apprentissage par rapport à des spécifications organisationnelles considérées comme des contraintes.

% fournir des preuves (formelles dans l'idéal) de son efficacité dans la génération de spécifications organisationnelles à partir des historiques des agents entrainés.

In this section, we first introduce the basics PRAHOM is relying on concerning MARL and the $\mathcal{M}OISE^{+}$ organizational model. Then, we  describe how joint-policies in MARL context can be linked to the $\mathcal{M}OISE^{+}$ organizational model.

\subsection{Markovian model for MARL}

To apply MARL techniques, we rely on the Decentralized Partially Observable Markov Decision Process (Dec-POMDP)~\cite{Oliehoek2016} because it considers multiple agents in a similar MAS manner. It relies on stochastic processes to model uncertainty of the environment for the changes induced by actions, received observations, communication\dots \ Additionally, unlike Partially Observable Stochastic Games (POSG), the reward function can be common to agents which fosters training for collaborative-oriented actions~\cite{Beynier2013}.
A Dec-POMDP $d \in D$ (with $D$ the set of Dec-POMDP) is a 7-tuple $d = (S,\{A_i\},T,R,\{\Omega_i\},O,\gamma)$ , where:
\begin{itemize}
    \item $S = \{s_1,...,s_{|S|}\}$: the set of possible states;
    \item $A_{i} = \{a_{1}^{i},...,a_{|A_{i}|}^{i}\}$: the set of possible actions for agent $i$;
    \item $T$ so that $T(s,a,s') = \probP{(s'|s,a)}$ : the set of conditional transition probabilities between states;
    \item $R: S \times A \times S \rightarrow \mathbb{R}$: the reward function;
    \item $\Omega_{i} = \{o_{1}^{i},...,o_{|\Omega_{i}|}^{i}\}$: the set of observations for agent $i$;
    \item $O$ so that $O(s',a,o) = \probP{(o|s',a)}$ : the set of conditional observation probabilities;
    \item $\gamma \in [0,1]$ : the discount factor.
\end{itemize}

Considering $m$ \textbf{teams} (also referred to as \textbf{groups}) each containing several agents among $\mathcal{A}$, we also detail the minimal formalism notation we re-used for solving the Dec-POMDP for a given team $i, 0 \leq i \leq m$ containing $n$ agents~\cite{Beynier2013,Albrecht2024}:

\begin{itemize}

    \item $\Pi$: the set of policies. A \textbf{policy} $\pi \in \Pi, \pi: H \times \Omega \rightarrow A$ associate an observation to an action optionally using previous experiences as histories. It represents the agent's internal logic;
    \item $\Pi_{joint}$: the set of joint-policies. A \textbf{joint-policy} $\pi_{joint} \in \Pi_{joint}, \pi_{joint}: H_{joint} \times \Omega^n \rightarrow A^n = \Pi^n$ chooses an action for each agent regarding their respective observation optionally using previous experiences as histories. It can be viewed as a set of policies used in agents;
    \item $H$: the set of histories. A \textbf{history} over $z \in \mathbb{N}$ steps (where $z$ is generally the maximum number of steps for an episode) is the $z$-tuple $h = ((\omega_{k}, a_{k}) | k \leq z, \omega \in \Omega, a \in A)$;
    \item $H_{joint}$: the set of joint-histories. A \textbf{joint-history} over $z \in \mathbb{N}$ steps $h_{joint} \in H_{joint}, h_{joint} = \{h_1,h_2...h_n\}$ is the set of agents' histories;
    \item $U_{joint,i}(<\pi_{joint,i}, \pi_{joint,-i}>): \Pi_{joint} \rightarrow \mathbb{R}$: gives the \textbf{expected cumulative reward} over a finite horizon (if $\gamma < 1$ or the number of steps in an episode is finite), with $\pi_{joint,i}$ the joint policy for team $i$ and $\pi_{joint,-i}$ all of the other concatenated joint-policies (considered as fixed);
    \item $BR_{joint,i}(\pi_{joint,i}) = argmax_{\pi_{joint,i}}(U(<\pi_{joint,i},\pi_{joint,-i}>))$: gives the \textbf{best response} $\pi_{joint,i}^*$ in the sense that the team cannot change any of the policies in the joint-policy $\pi_{joint,i}^*$ to get a better expected cumulative reward than $U_i^* = U_{joint,i}(<\pi_{joint,i}^*, \pi_{joint,-i}>)$;
    \item $SR_{joint,i}(\pi_{joint,i}, s) = \{\pi_{joint,i} | U(<\pi_{joint,i},\pi_{joint,-i}>) \geq s\}$: gives the \textbf{sufficient response} as the set of joint-policies getting at least $s \in \mathbb{R}, s \leq U_i^*$ as expected cumulative reward.
\end{itemize}

We refer to \textbf{solving} the Dec-POMDP for the team $i$ as finding a joint policy $\pi_{joint,i} \in \Pi_{joint}, \pi_{joint,i} = BR_{joint,i}(\pi_{joint,i})$ that maximizes the expected cumulative reward over a finite horizon.
We refer to \textbf{sub-optimally solving} the Dec-POMDP at $s$ expectancy as finding the joint policies $\pi_{joint,i} \in \Pi_{joint}, \pi_{joint,i} = SR_{joint,i}(\pi_{joint,i}, s)$ that gets the expected cumulative reward over a finite horizon at least at $s \in \mathbb{R}, s \leq U_i^*$.


\subsection{Organizational model}

$\mathcal{M}OISE^+$~\cite{Hubner2007} provides a relevant high-level description of the structures and interactions within the MAS. However, we favor $\mathcal{M}OISE^+$ because it provides an advanced formal description for an organization without incompatibilities with MARL, especially for a formal description of agents' policies.
Based on $\mathcal{M}OISE^+$~\cite{Hubner2007} formalism, we only give the elements of the formalism we used.

\paragraph{\textbf{Organizational specifications (OS)}} refer to some information describing an organization, we denote them as $\mathcal{OS} = \langle \mathcal{SS}, \mathcal{FS}, \mathcal{DS} \rangle$, where $\mathcal{SS}$ are the \textbf{Structural Specifications}, $\mathcal{FS}$ are the \textbf{Functional Specifications}, and $\mathcal{DS}$ are the \textbf{Deontic Specifications}

\paragraph{\textbf{Structural Specifications (SS)}} refer to the structured means left to agents to achieve a goal, we denote them as $\mathcal{SS} = \langle \mathcal{R}, \mathcal{IR}, \mathcal{G} \rangle$, where:

\begin{itemize}

    \item $\mathcal{R}_{ss}$: the set of all roles (denoted $\rho \in \mathcal{R}$);

    \item $\mathcal{IR}: \mathcal{R} \rightarrow \mathcal{R}$: the inheritance relation between roles ($\mathcal{IR}(\rho_1) = \rho_2$ means $\rho_1$ inherits from $\rho_2$ also denoted $\rho_1 \sqsubset \rho_2$);

    \item $\mathcal{RG} \subseteq \mathcal{GR}$ the set of root groups, $\mathcal{GR} = \langle \mathcal{R}, \mathcal{SG}, \mathcal{L}^{intra}, \mathcal{L}^{inter}, \mathcal{C}^{intra}, \mathcal{C}^{inter}, np, ng \rangle$, the set of all groups, where

          \begin{itemize}

              \item $\mathcal{R} \subseteq \mathcal{R}_{ss}$: the set of non-abstract roles;

              \item $\mathcal{SG} \subseteq \mathcal{GR}$: the set of sub-groups;

              \item $\mathcal{L} = \mathcal{R} \cross \mathcal{R} \cross \mathcal{TL}$: the set of links. A link is a 3-tuple $(\rho_s,\rho_d,t) \in \mathcal{L}$ (also denoted as a predicate $link(\rho_s,\rho_d,t))$, where $\rho_{s}$ is the source role, $\rho_{d}$ is the destination role, and $t \in \mathcal{TL}, \mathcal{TL} = \{acq, com, aut\}$ is the link type;
                    \begin{itemize}
                        \item If $t = acq$ (acquaintance), the agents playing the source role $\rho_{\mathrm{s}}$ are allowed to have a representation of the agents playing the destination role $\rho_{d}$;
                        \item If $t = com$ (communication), the $\rho_{\mathrm{s}}$ agents are allowed to communicate with $\rho_{d}$ agents;
                        \item If $t = aut$ (authority), the $\rho_{\mathrm{s}}$ agents are allowed to have authority on $\rho_{d}$ agents. It requires an acquaintance and communication link.
                    \end{itemize}
              \item $\mathcal{L}^{intra} \subseteq \mathcal{L}$: the set of intra-group links;
              \item $\mathcal{L}^{inter} \subseteq \mathcal{L}$: the set of inter-group links;

              \item $\mathcal{C} = \mathcal{R} \cross \mathcal{R}$: the set of compatibilities. A compatibility is a couple $(a,b) \in \mathcal{C}$ (also denoted $\rho_a \bowtie \rho_b$), means agents playing role $\rho_a \in \mathcal{R}$ can also play role $\rho_b \in \mathcal{R}$;
              \item $\mathcal{C}^{intra} \subseteq \mathcal{C}$: the set of intra-group compatibilities;
              \item $\mathcal{C}^{inter} \subseteq \mathcal{C}$: the set of inter-group compatibilities;

              \item $np: \mathcal{R} \rightarrow \mathbb{N} \times \mathbb{N}$: the relation giving the cardinality of agents adopting a role;
              \item $ng: \mathcal{SG} \rightarrow \mathbb{N} \times \mathbb{N}$: the relation giving the cardinality of each sub-group.

          \end{itemize}

\end{itemize}

\paragraph{\textbf{Functional Specifications (FS)}} globally refer to the tasks and goals agents have to achieve, we denote them as $\mathcal{FS} = \langle \mathcal{SCH}, \mathcal{PO} \rangle$, where:

\begin{itemize}
    \item $\mathcal{SCH} = \langle\mathcal{G}, \mathcal{M}, \mathcal{P}, mo, nm \rangle$: the set of \textbf{social scheme}, where:
          \begin{itemize}
              \item $\mathcal{G}$ is the set of global goal;

              \item $\mathcal{M}$ is the set of mission labels;
              \item $\mathcal{P} = \langle \mathcal{G}, \{\mathcal{G}\}^s, OP, [0,1] \rangle, s \in \mathbb{N}^*$ is the set of plans that builds the tree structure of the goals.
                    %
                    A plan $p \in \mathcal{P}$ is 4-tuple $p=(g_f,\{g_i\}_{0 \leq i \leq s}, op, ps), g_f \in \mathcal{G}, g_i \in \mathcal{G}, op \in OP, OP = \{sequence, choice, parallel\}, ps \in [0,1]$, meaning that the goal $g_f$ is achieved if some of the sub-goals $g_i$ are achieved with a success probability $ps$ and according to the operator $op$:
                    %
                    \begin{itemize}
                        \item if $op = sequence$, the $g_i$ can only be achieved in the same order sequentially;
                        \item if $op = choice$, only one of the $g_i$ has to be achieved;
                        \item if $op = parallel$, the $g_i$ can only be achieved sequentially or simultaneously.
                    \end{itemize}

              \item $mo: \mathcal{M} \rightarrow \mathbb{P}(\mathcal{G})$: specifies the set of goals a mission is associated with;
              \item $nm: \mathcal{M} \rightarrow \mathbb{N} \times \mathbb{N}$ the cardinality of agents committed for each mission.
          \end{itemize}
    \item $\mathcal{PO}: \mathcal{M} \cross \mathcal{M}$: the set of \textbf{preference orders}. A preference order is a couple $(m_1, m_2), m_1 \in \mathcal{M}, m_2 \in \mathcal{M}$ (also denoted $m_{1} \prec m_{2}$) meaning that if there is a moment when an agent is permitted to commit to $m_{1}$ and also $m_{2}$, it has a social preference for committing to $m_{1}$.
\end{itemize}

\paragraph{\textbf{Deontic Specifications (DS)}} refer to how SS are to be used to achieve the FS, we denote them as $\mathcal{DS} = \langle \mathcal{OBL},\mathcal{PER} \rangle$, the set of deontic specifications, where:

\begin{itemize}
    \item $\mathcal{TC}$: the set of \textbf{time constraints}. A time constraint $tc \in \mathcal{TC}$ specifies a set of periods during which a permission or obligation is valid ($Any \in \mathcal{TC}$ means \textquote{everytime});
    \item $\mathcal{OBL}: \mathcal{R} \cross \mathcal{M} \cross \mathcal{TC}$: the set of \textbf{obligations}. An obligation is a 3-tuple $(\rho_a,m,tc)$ (also denoted $obl(\rho_a,m,tc)$) meaning an agent playing role $\rho_a \in \mathcal{R}$ is obliged to commit on mission $m \in \mathcal{M}$ for a given time constraint $tc \in \mathcal{TC}$;
    \item $\mathcal{PER}$: the set of \textbf{permissions}. A permission is a 3-tuple $(\rho_a,m,tc)$ (also denoted $per(\rho_a,m,tc)$) meaning an agent playing role $\rho_a \in \mathcal{R}$ is permitted to commit on mission $m \in \mathcal{M}$ for a given time constraint $tc \in \mathcal{TC}$;
\end{itemize}



\section{PRAHOM algorithmic approach}

\subsection{Linking MARL and organizational model}

Addressing gaps $(\mathbf{G_E})$ and $(\mathbf{G_A})$ requires linking joint-policies and a set of organizational specifications whose triplet (role,permission/obligation, mission) is core. We defined addressing the issue $(\mathbf{G_E})$ as estabishing the following features for explicating policies through organizational specifications:
%
\begin{itemize}[wide, labelsep = 1em]
    \item[$(\mathbf{G_E.F_D})$] For any given joint-policy obtained after training, it should be possible to satisfy $(\mathbf{G_E.F_S})$ and $(\mathbf{G_E.F_F})$ to determine deontic specifications containing at least a set of permissions/obligations for each agent even if no agents have been explicitely constrained to a role and permitted/obligated to missions;
    \item[$(\mathbf{G_E.F_S})$] For any given joint-policy obtained after training, it should be possible to determine structural specifications containing at least a set of  roles for each agent even if no agents have been explicitely constrained to roles;
    \item[$(\mathbf{G_E.F_F})$] For any given joint-policy obtained after training, it should be possible to determine functional specifications containing at least a set of  missions for each agent even if no agents have been explicitely constrained to missions.
\end{itemize}

\noindent We defined addressing $(\mathbf{G_A})$ as estabishing the following features for applying organizational specifications onto policies:
%
\begin{itemize}[wide, labelsep = 1em]
    \item[$(\mathbf{G_A.F_D})$] If an agent is constrained to a role and permitted/obligated to a mission, then $(\mathbf{G_A.F_S})$ should be satisfed and $(\mathbf{G_A.F_F})$ should be satisfied depending on the time constraint considering an agent is constrained to a mission;
    \item[$(\mathbf{G_A.F_S})$] If an agent is constrained to a role, then its policy should necessarly belong to a policy subset associated with the given role throughout the training;
    \item[$(\mathbf{G_A.F_F})$] If an agent is constrained to a mission, then it should be enticed to achieve the mission's goals according to the associated plan throughout training.
\end{itemize}

\noindent We identified two main issues preventing any implementation of these features in both $(\mathbf{G_A})$ and $(\mathbf{G_E})$ gaps:

\begin{itemize}[wide, labelsep = 1em]
    \item The lack of common ground between policies and organizational specifications enabling to know how agents are expected to behave when they are constrained to stick to organizational specifications $(\mathbf{G_A})$. Reversely, there is no way to characterize any organizational specifications $(\mathbf{G_E})$ regarding known behavior induced by agents' policies;
    \item A significant challenge is the intractability of directly analyzing or modifying policies. This difficulty arises because policy function approximations, such as neural networks, are generally opaque, making it hard to extract organizational specifications $(\mathbf{G_E})$; and update policies to meet the desired behaviors of roles or missions $(\mathbf{G_A})$.
\end{itemize}

To address these issues, the premise of our contribution is to leverage history as a core concept in MARL as an explicit way to represent organizational specifications in MARL. It requires the following additional theoretical tools we propose as prior requirements.

We introduce $rh: \mathcal{R} \rightarrow \mathcal{P}(H)$ and $mh: \mathcal{M} \rightarrow \mathcal{P}(H)$ the bijections that respectively show how an agent playing a role or committed to a mission should have its policy constrained or enticed to generate histories that belong to an expected history subset during/after learning.

We propose to represent a role $\rho \in \mathcal{R}$ in MARL formalism by introducing an \textbf{observable policy} $\pi_{\rho} \in \Pi, \pi_{\rho} = rcnt \circ play$ so that generated histories by a policy $h \in H$ belong to the agent observable/expected history subset $H_{\rho}$. The relation $rcnt: \mathcal{P}(H) \rightarrow \Pi_{\mathcal{R}}$ enables reconstructing an observable policy based upon collected histories. The relation $play: \Pi \rightarrow \mathcal{P}(H)$ enables generating histories according to a policy. For a representative amount of histories in $H = play(\pi)$, $\pi_{\mathcal{R}} = rcnt(H)$ is considered to be an approximation of $\pi$.
Unlike black-box policies, the mapping of an observable policy associating couples (history, observation) to action is explicit.

We propose to represent a goal $g \in \mathcal{G}$ in MARL formalism by introducing an \textbf{observable reward function} $R_{g}: H \rightarrow \mathbb{R} = \{(h, dist(H_g,h)), h \in H\}$ so that a history generated by an agent $h \in H$ be associated with a positive reward if it belongs to the observable/expected $H_g$ or negative otherwise. The relation $dist: H \times H \rightarrow \mathbb{R}$ associate a real number indicating how close a generated history belong to a given history subset.
A mission $m \in \mathcal{M}$ containing goals $\mathcal{G} = \{g_i \in \mathcal{G}\}$ is represented as $R_{\mathcal{m}}: H \rightarrow \mathbb{R} = comb(G)$. The relation $comb: \mathcal{M} \times \mathcal{P}(\mathcal{G}) \rightarrow R_{\mathcal{M}}$ combines all observable reward function of each goal into a single observable reward function.

Using proposed observable policy and reward function, we implemented the previously mentioned features to develop the PRAHOM algorithmic approach as a first attempt to address $(\mathbf{G_E})$ and $(\mathbf{G_A})$.


\subsection{PRAHOM Algorithmic Approach}

Continuing the previous decomposition in two activities, we detailed our process to get the specifications out of the agents' policies as \emph{PRAHOM\_hos} presented in \autoref{alg:PRAHOM-hos} for addressing $(\mathbf{G_E})$; and the process to get the joint-policies satisfying given specifications as \emph{PRAHOM\_osh\_training} presented in \autoref{alg:PRAHOM-osh-training} for addressing $(\mathbf{G_A})$. PRAHOM is then a synthesis of these two processes
%presented in \autoref{alg:PRAHOM}
\footnotemark[1].

\footnotetext[1]{Additional information are provided in addition to developed code in \url{https://github.com/julien6/omarl_experiments}.}

PRAHOM first launches the training of a joint-policy model on the given environment under organizational constraints relying on given relations. It enables getting the joint-policies that reach the given expectancy. Then, each of these joint-policies is played within differently generated environments to get all the joint-histories. Finally, these joint-histories are analyzed to infer the associated organizational specifications enriching known relations.

% \RestyleAlgo{ruled}
% \SetKwComment{Comment}{// }{}

% \begin{algorithm}[hbt!]
%     \caption{\emph{PRAHOM}}\label{alg:PRAHOM}

%     \KwData{$d \in D$, Dec-POMDP to solve}
%     \KwData{$ep_{max} \in \mathbb{N}$, maximum number of episodes}
%     \KwData{$it_{max} \in \mathbb{N}$, maximum number of iteration}
%     \KwData{$step_{max} \in \mathbb{N}$, maximum number of step per episode}
%     \KwData{$s \in \mathbb{R}$, cumulative reward expectancy}
%     \KwData{$cons: \mathcal{A} \rightarrow \mathcal{P}(\mathcal{OS})$, agents' org. specs. constraints}
%     \KwData{$marl\_alg: \Pi_{joint} \times H_{joint} \times \mathbb{R}^{step_{max}} \rightarrow \Pi_{joint}$, algorithm to train from histories and rewards}
%     \KwData{$osh: \mathcal{OS} \rightarrow \mathcal{P}(H_{joint})$, relations OS to histories}

%     \KwResult{$(s\pi_{joint,i,s} \in S\Pi_{joint,i,s}, os_{i,s} \in \mathcal{OS}_{i,s})$, sub-optimal policies and associated org. specs.}

%     \Comment{Init policy models}

%     $\pi_{joint} = marl\_alg.init()$

%     \Comment{Training under org. specs. constraints}

%     $s\pi_{joint,i,s} = \mathbf{PRAHOM\_osh\_training}(cons,\dots)$

%     % \phantom{X}

%     \Comment{Get the $|s\pi_{joint,i,s}| \times ep_{max}$ joint-histories}
%     $sh_{joint,i,s} \in SH_{joint,i,s} = gen\_jth(ep_{max}, s\pi_{joint,i,s})$

%     \Comment{Get org. specs. from joint-histories}
%     $os_{i,s} = \mathbf{PRAHOM\_hos}(sh_{joint,i,s},\dots)$

% \end{algorithm}


\subsubsection{\textbf{$(\mathbf{G_E})$: Inferring OS from histories}}

% \paragraph{\textbf{$(\mathbf{G_E.F_S})$: Inferring SS}}

% \paragraph{\textbf{$(\mathbf{G_E.F_F})$: Inferring FS}}

% \paragraph{\textbf{$(\mathbf{G_E.F_D})$: Inferring DS}}

\input{algos/PRAHOM_hos.tex}

\

\noindent The proposed \autoref{alg:PRAHOM-hos} implements \emph{PRAHOM\_hos}. Its underlying principle is first to rely on the proposed \emph{Knowledge-based Organizational Specification Identification Approach} (KOSIA) to identify some organizational specifications thanks to the current $osh$ relations. % KOSIA is aimed to be relevant when knowledge of the relations between histories and organizational specifications is significant enough to be used generally.
For instance, we implemented KOSIA as a pattern-matching engine finding similar joint-histories $Im(osh)$ to a given one hence finding the associated organizational specifications.

Otherwise, if it is not possible to get all known associated organizational specifications, the proposed \emph{General Organizational Specification Inference Approach} (GOSIA) suggests an empirical approach to infer the rest of them.
GOSIA is based on some proposed definitions for each $\mathcal{M}OISE^+$ organizational specification regarding joint-histories or other organizational specifications, to use suggested specific statistical, unsupervised learning techniques to infer them incrementally. The \autoref{fig:gosia_illustrative_view} summarizes the five steps of GOSIA (represented as arrow labels in \autoref{fig:gosia_illustrative_view}) that are detailed below.
%
\paragraph{1) Infer roles and their inheritance}

We propose a role $\rho$ is defined as a policy whose the associated histories of agents having adopted it all contain a common discontinuous sequence. We proposed a role $\rho_2$ inherits $\rho_2$ if the common discontinuous sequence of the histories associated with $\rho_2$ is also contained in the $\rho_1$'s one.
From these definitions, GOSIA leverages hierarchical sequence clustering to find the longest common discontinuous sequences among agent's histories. Results can represented as a dendrogram. It enables inferring roles and inheritance relations, their respective relation with histories, and the current agents as well.

\paragraph{2) Infer possible organizations}

We propose an organization is linked to only one set of all instantiable roles sharing closely similar inheritance relations. Indeed, considering two trained joint-policies $H_{joint,i,s,1}$ and $H_{joint,i,s,2}$, even though both achieve a goal relying on the roles $\mathcal{R}_{ss,1}$ and $\mathcal{R}_{ss,2}$ may be far from other. For instance, their roles may not use the same responsibility distribution.
GOSIA uses a K-means algorithm to get the $q$ clusters of the vectorized $\mathcal{IR}_{i}$ considered as organizations. The roles in the same cluster share the inheritance relations of the K-means' centroid $\mathcal{IR}_j$. Indeed, they are representative general roles regarding all the similar roles adopted by agents of the same organization over all joint-histories.
For the next steps, only one chosen organization and its related joint-histories are considered. When it exists it chooses an organization close to KOSIA's one.

\paragraph{3) Infer links and sub-groups}

We propose two agents have a \emph{social impact link} $(ag_1,ag_2, \kappa, \delta, f)$ with $h_1$ associated with $ag_1$ and $h_2$ associated with $ag_2$ if a sequence $h_{1,s}$ in $h_1$ is correlated at a $\kappa \in [0,1]$ index to another sequence $h_{2,s}$ in $h_2$ positioned at a relative delay $\delta \in [0,1]$ after the beginning of $h_{1,s}$, and these two correlated sequences are $f$ frequently present among all joint-policies.
We consider two agents to be in the same group if there is a social impact link such as $f \geq 0.9$. Considering that $\kappa$ indicates the likeliness of an agent' sequence to impact another one and that $\delta$ indicates the receiver's reactivity, we consider:
\begin{itemize}
    \item an acquaintance link $(ag_1,ag_2,acq)$ is defined if there is a social impact link with $\kappa \geq 0.1$, $\delta \geq 0$, $f \geq 0$;
    \item a communication link $(ag_1,ag_2,com)$ is defined if there is a social impact link with $\kappa \geq 0.3$, $\delta \geq 0$, $f \geq 0$;
    \item an authority link $(ag_1,ag_2,aut)$ is defined if there is a social impact link with $\kappa \geq 0.9$, $\delta \geq 0.5$, $f \geq 0$.
\end{itemize}
% TODO: Generaliser à plusieurs agents ou le dire en tant que limitation
GOSIA uses empirical techniques to compute a graph of the social impact links between agents. Frequency enables determining clusters as agents' groups and their associated roles. It enables inferring the acquaintance, communication, and authority links between roles. From the information concerning roles associated with groups, it is possible to infer whether links are intra-group or inter-group.

\paragraph{4) Infer goals, plans, and missions}

We propose a sub-goal/goal is a set of common states that are reached following the histories of the successful agents.
For each joint-history GOSIA computes the state transition graph that is merged into a general one. Measuring the distance between two vectorized states within K-means enables finding the clusters of trajectories that some agents may follow. Then, we sampled some sets of states for each trajectory as goals. For instance, one may choose the narrowest set of states in which agents collectively seem to transition at some point to achieve their goal. Otherwise, a balanced sampling over lower variance trajectories could be made. Knowing what goal belongs to what trajectory, GOSIA infers plans for choice and sequence only.

This enables getting goals and plans at a global state but these goals could be indeed split into specific goals for each sub-group and agent. To do this, GOSIA conducts the same process replacing the states with the observations of the agents in the same sub-group for sub-groups, and observations for agents.

We propose a mission as the set of sub-goals one or several agents are achieving.
Knowing what shared goals are achieved by agents, GOSIA determines some sets of representative goals as missions.

\paragraph{5) Infer compatibilities, obligations, permissions, and cardinalities}

We propose an obligation is when an agent playing role $\rho$ is achieving a mission's goals and no other one at some time constraints while an obligation is when an agent playing role $\rho$ may achieve some other ones at some time constraints.
GOSIA determines what agents' are associated with what mission and if they are restricted to some hence these are obligations or just permission

We propose a compatibility $(\rho_1,\rho_2)$ is defined if an agent playing a history associated with $\rho_1$ in a joint-history also plays a history associated with $\rho_2$ in another joint-history. If this change operates only in the same group, then it is intra-group. Else it is inter-group.

Finally, by counting the number of agents playing a role in each joint-history, each role's cardinality is computed. Similarly, GOSIA computes each sub-group's cardinality counting the inferred ones.


% \begin{figure*}[h!]
%     \centering
%     \input{figures/gosia_illustrative_view.tex}
%     \caption{GOSIA illustrative view}
%     \label{fig:gosia_illustrative_view}
% \end{figure*}


\paragraph{Complementary help from LLMs}

We use \emph{tiiuae/falcon-7b} pre-trained transformer using \emph{Transformers}\emph{Huggingface} library. Within \emph{PRAHOM\_hos} it is automatically prompt-engineered with generated specific contextual descriptions of the environment, its functioning, and the $\mathcal{M}OISE^+$ organizational specifications. This prompt-engineering is to guide the answers towards three main purposes:
\begin{enumerate*}[label=\roman*),itemjoin={;\quad}]
    \item Labeling, tagging the inferred organizational specification from related joint-histories in a human-like manner;
    \item Giving specific human-like textual description about each organizational specification;
    \item Giving general human-like textual description of the whole organization.
\end{enumerate*}

%%%%%%%%%%%%%%%%%%%%%%%%%%


\subsubsection{\textbf{$(\mathbf{G_A})$: Constraining joint-policies}}

% \paragraph{\textbf{$(\mathbf{G_A.F_S})$: Constraining joint-policies according to SS}}

% \paragraph{\textbf{$(\mathbf{G_A.F_F})$: Constraining joint-policies according to FS}}

% \paragraph{\textbf{$(\mathbf{G_A.F_D})$: Constraining joint-policies according to DS}}

\input{algos/PRAHOM_osh_training.tex}

\noindent We consider a given MARL algorithm that iteratively converges towards a joint-policy so that the joint-policy is updated at each step until a finite horizon.
%In \autoref{proof:jpc_to_ac}
We consider that restricting the actions according to the organizational specifications at each step allows constraining the converged joint-policies to the ones that satisfy the organizational specifications\footnotemark[1]. Relying on that principle we propose \emph{PRAHOM\_osh\_training} presented in \autoref{alg:PRAHOM-osh-training}.

\emph{PRAHOM\_osh\_training} fits within a regular MARL context: joint-policy $\pi_{joint,ep}$ is updated after several steps allowing to have enough joint-histories $h_{joint,ep}$ and joint-rewards as feedback $r_{joint,ep}$. The training goes on over several episodes for better training until converging to a sufficient cumulative reward regarding $s$. New training are launched until $it_{max}$ times to get more joint-policies to add in $s\pi_{joint}$ as a final result.

\emph{PRAHOM\_osh\_training} augments that framework by changing the way agents choose their actions to meet the expectations of the organizational specifications. Using the known relations $osh$ with agents' constraints $cons[\mathcal{A}]$, $cons\_act$ has to guess the most expected actions regarding the current $h_{joint,ep}$. In practice, a decision tree reconstructed from the $osh[cons[\mathcal{A}]]$ allows getting the expected actions by following the specific episode joint-history $h_{joint,ep}$. These expected joint-actions $a_{joint,exp}$ are to be chosen by agents.

Two modes are available to integrate these constraints:
%
\begin{enumerate*}[label=\roman*),itemjoin={;\quad}]
    \item \textquote{correct-mode}: Correct any unexpected action by an expected one (such as a random sample). It aims to converge faster by reducing the search space with safety guarantees. Yet, it is external to the agent's policy;
    \item \textquote{penalize-mode}: Add a penalty to the reward if any wrong action has been made among agents. This mode aims to make agents \textquote{learn} to respect their constraints. Yet, no safety guarantee is ensured since agents only approximate their expected behavior.
\end{enumerate*}

\paragraph{Complementary help from LLMs}

Aiming to ease the handling of observation and action from a user perspective, we use the previously mentioned \emph{tiiuae/falcon-7b} within \emph{PRAHOM\_osh\_training} for:
\begin{enumerate*}[label=\roman*),itemjoin={;\quad}]
    \item Helping mapping a observation label and text description to expected real observation in MARL;
    \item Helping mapping a action label and text description to an expected real action in MARL;
\end{enumerate*}


\section{Case study: Predator-prey environment}

\subsection{Algorithm implementation: PRAHOM Wrapper}

% To implement PRAHOM

To assess PRAHOM, we assess it in PettingZoo~\cite{Terry2021} environments where several agents have to achieve a goal with the best performance through various collective strategies. Pettingzoo is a library that offers a standard API simplifying the development of multi-agent environments and facilitates the use of MARL algorithms.

We developed \emph{PRAHOM PettingZoo Wrapper}\footnotemark[2] as a \emph{PoC} tool augmenting a Pettingzoo environment to help apply PRAHOM. It provides a series of additional class methods implementing some PRAHOM's functionalities to ease the training (with \emph{Proximal Policy Optimization}) and the organizational specifications inference.

\subsection{Experimental setup}

We selected three Atari environments for their visual rendering is a convenient way to assess the results with manual observations\footnotemark[1].
\begin{enumerate*}[label=\roman*),itemjoin={;\quad}]
    \item \textquote{Pistonball} (PBL)~\cite{Terry2021} is a series of pistons to bring a ball from right to left side hence requiring neighbors' representation;
    \item \textquote{Predator-prey with communication}~\cite{Lowe2017} (PPY) consists of predators monitored by a leader to catch faster prey hence requiring hunting strategies;
    \item \textquote{Knights Archers Zombies}~\cite{Terry2021} (KAZ) consists in knights and archers learning how to kill zombies hence requiring efficient agent spatial positioning.
\end{enumerate*}
%
We applied PRAHOM in three cases:
\begin{enumerate*}[label=\roman*),itemjoin={;\quad}]
    \item No organizational specifications (NTS): agents have to learn efficient collective strategies without any constraints or indications
    \item Partially constraining organizational specifications (PTS): some constraints are given to help converge faster
    \item Fully constraining organizational specifications (FTS): manually crafted joint-policies are given for they are a reference regarding learned ones.
\end{enumerate*}
%
Details of the constraints for NTS and FTS are not presented here (available in code\footnotemark[1]).

\

We conducted an empirical evaluation of the way \emph{PRAHOM} may impact the training when constrained by organizational specifications. We determined approximated reward ratios between PTS, NTS, and FTS after 1000 episodes. For all episodes, performance stability is the average performance over the maximal one. It shows how the trained agents can achieve the goal in a consistent way in differently generated environments. Results are presented in Table~\ref{tab:training_AOMEA_results}.

\subsection{Results and discussion}

\input{tables/training_OMARL_results.tex}


As a general observation, we can notice convergence time is longer for NTS than for PTS which is also longer than for FTS. As expected, the search space is decreasing, hence a shorter convergence time. For instance, we noticed a faster convergence to a sub-optimal solution in the PBL environment by providing organizational specifications as presented in \autoref{fig:prahom_learning_curve}. Although PTS converges faster than NTS to a comparable cumulative reward, NTS may outperform PTS because trained agents' policies are hand-tailored to solve the problem finely.
%
%A qualitative analysis is presented in Table~\ref{tab:trained_AOMEA_results}
%
% \input{tables/trained_OMARL_results.tex}
%
% //TODO: Moise+ schemes and comparison with expected ones
%
%
\begin{figure}[h!]
    \centering
    \includegraphics[width=0.46\textwidth]{figures/prahom_learning_curve.png}
    \caption{Average reward every training iteration (100 episodes) in the PBL environment for the NTS, PTS, and FTS cases\\ \phantom{X} \\}
    \label{fig:prahom_learning_curve}
\end{figure}

We also conducted a qualitative analysis of the resulting roles, links, and goals determined by PRAHOM after training.
%
For the PBL environment, we noticed roles being equivalent for agents are expected to act the same. Indeed, a Principal Component Analysis (PCA) presented in \autoref{fig:prahom_pca_analysis} of the agents' histories, shows most agents’ histories are in the left bottom zone (circled in red). It shows most pistons seem to act similarly as expected. One of the inferred sub-goals consists in having the ball in mid-way which is indeed coherent with expectations.
%
For the KAZ environment, we noticed acquaintance links due to each other's representation and two distinct roles: archers tend to move away from zombies, and knights tend to approach them. Despite the inferred sub-goals diversity, some indicate to have knights at the top while archers stay at the bottom as expected.
%
For the PPY environment, we noticed the output specifications indicate authority links between the leader predator and the simple predators. These are probably to be related to some inferred sub-goals that seem to indicate an approximated circling purpose.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.45\textwidth]{figures/prahom_pca_analysis.png}
    \caption{PCA of the agents' histories in the PBL environment after training for the NTS case}
    \label{fig:prahom_pca_analysis}
\end{figure}



\section{Conclusion}

% résumer nos contributions et nos principales conclusions.

% importance d’améliorer l’explicabilité de l’IA dans les systèmes MARL et souligner l’impact potentiel de PRAHOM sur l’avancement du domaine.

% proposer des remarques finales et décrire les orientations des recherches futures afin d’explorer et d’affiner davantage notre algo.

% Multi-agent methods rely on the designer's knowledge to design a suitable MAS organization, but do not provide automatic means to determine the relevant organizational mechanisms only from the design requirements and the overall objective.
% MARL techniques have been successfully applied to automatically train agents to achieve a given objective without explicit characterization of emerging collective strategies.
% The PRAHOM's originality is to enrich a MARL process with an explicit organizational model towards a methodological objective to address these issues. It links the agents' policies (modeled in a Dec-POMDP) with $\mathcal{M}OISE^+$ through the process PRAHOM. Under the simplifying conditions of a group and a single social pattern, PRAHOM makes it possible to partially determine organizational specifications from joint histories and to constrain the training of policies in relation to organizational specifications.
% Additionally, we implemented the \emph{PRAHOM PettingZoo} wrapper as a proof of concept to apply PRAHOM.
% Finally, we applied our approach in four \emph{PettingZoo} environments to evaluate the impact during and after training. The performances obtained appear to be comparable to those known.

% Key point summary

Our main contribution is the PRAHOM algorithmic approach that aims to link MARL and agents' policies with the $\mathcal{M}OISE^+$ model leveraging on relations between joint-histories and organizational specifications. It seeks to determine organizational specifications from trained agents to help understand the agents' collective behavior. It also seeks to constrain the agents' training according to expected behaviors expressed as organizational specifications.
As a first implementation, \emph{PRAHOM PettingZoo Wrapper} is assessed in cooperative Atari games, where the inferred organizational specifications generally match the expected hand-crafted results.
Despite the implementation's limited functionalities, the first results showed that the PRAHOM approach can be used to constrain the resulting agents' policy space according to the specified organizational constraints. Additionally, we manually verified that some valuable organizational specifications are inferred from trained agents' histories.

% Perspectives
Even if PRAHOM is agnostic of any MARL algorithm, reconstructing the collective behaviors of agents a posteriori can prove difficult. Indeed, major perspectives to improve PRAHOM are inspired by hierarchical learning which contributes to better characterizing emerging strategies during learning.
Eventually, even though the first results obtained with LLM show it as a promising complementary help for PRAHOM, it may also bring out new ways for explaining collective behavior beyond its intended use.
% Ultimately, we also aim to improve the applicability of PRAHOM by developing dedicated interfaces built around PRAHOM making it more accessible to industrial and research contexts.


\section*{References}

\bibliographystyle{splncs04}

\bibliography{references}

\end{document}