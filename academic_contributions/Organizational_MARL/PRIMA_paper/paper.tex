\RequirePackage[2020-02-02]{latexrelease}
\documentclass[runningheads]{llncs}

% save the proof environment defined by the class
\let\lncsproof\proof \let\lncsendproof\endproof \let\lncsqed\qed
% remove the definitions in order to load amsthm
\let\proof\relax\let\endproof\relax

\usepackage{amsthm}

% restore the class defined proof
\let\proof\lncsproof \let\endproof\lncsendproof \let\qed\lncsqed

\newtheoremstyle{freethm}% <name>
 {3pt}% <Space above>
 {3pt}% <Space below>
 {\itshape}% <Body font>
 {}% <Indent amount>
 {\bfseries}% <Theorem head font>
 {}% <Punctuation after theorem head>
 {\newline}% <Space after theorem headi>
 {\thmname{#1}\thmnumber{ #2}\thmnote{ (#3)}.}

\theoremstyle{freethm}
\newtheorem{mytheorem}{Theorem}

% Define a new theorem style for proof outlines
\newtheoremstyle{proofoutline}
  {3pt}    % Space above
  {3pt}    % Space below
  {\itshape} % Body font
  {}        % Indent amount
  {\bfseries} % Theorem head font
  {.}       % Punctuation after theorem head
  { }       % Space after theorem head
  {\thmname{#1}\thmnumber{ #2}\thmnote{ (#3)}} % Theorem head spec

% Apply the new theorem style
\theoremstyle{proofoutline}
\newtheorem{proofoutline}{Proof Outline}

\usepackage[english]{babel}
\addto\extrasenglish{  
    \def\figureautorefname{Figure}
    \def\tableautorefname{Table}
    \def\algorithmautorefname{Algorithm}
    \def\sectionautorefname{Section}
    \def\subsectionautorefname{Subsection}
    \def\proofoutlineautorefname{Proof Outline}
}

\newcommand{\supertiny}{\fontsize{1}{2}\selectfont}

\usepackage{catoptions}
\makeatletter

\def\Autoref#1{%
  \begingroup
  \edef\reserved@a{\cpttrimspaces{#1}}%
  \ifcsndefTF{r@#1}{%
    \xaftercsname{\expandafter\testreftype\@fourthoffive}
      {r@\reserved@a}.\\{#1}%
  }{%
    \ref{#1}%
  }%
  \endgroup
}
\def\testreftype#1.#2\\#3{%
  \ifcsndefTF{#1autorefname}{%
    \def\reserved@a##1##2\@nil{%
      \uppercase{\def\ref@name{##1}}%
      \csn@edef{#1autorefname}{\ref@name##2}%
      \autoref{#3}%
    }%
    \reserved@a#1\@nil
  }{%
    \autoref{#3}%
  }%
}
\makeatother

\usepackage[T1]{fontenc}
\usepackage{graphicx}
%\usepackage{color}
%\renewcommand\UrlFont{\color{blue}\rmfamily}

\usepackage{amsthm,amssymb,amsfonts}
\usepackage[inline, shortlabels]{enumitem}
\usepackage{tabularx}
\usepackage{caption}
\usepackage{listings}
% \usepackage{titlesec}
\usepackage{ragged2e}
% \usepackage[hyphens]{url}
\usepackage{hyperref}
\usepackage{xurl}
\usepackage{pifont}
\usepackage{footmisc}
\usepackage{multirow}
\usepackage[linesnumbered,ruled,vlined]{algorithm2e}
\usepackage{float}
\usepackage{listings}
\usepackage{xcolor}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}
 
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}
 
\lstset{style=mystyle}

% --- Tickz
\usepackage{physics}
\usepackage{amsmath}
\usepackage{tikz}
\usepackage{mathdots}
\usepackage{yhmath}
\usepackage{cancel}
\usepackage{color}
\usepackage{siunitx}
\usepackage{array}
\usepackage{multirow}
\usepackage{amssymb}
\usepackage{gensymb}
\usepackage{tabularx}
\usepackage{extarrows}
\usepackage{booktabs}
\usetikzlibrary{fadings}
\usetikzlibrary{patterns}
\usetikzlibrary{shadows.blur}
\usetikzlibrary{shapes}

% ---------
% \usepackage{titlesec}
\usepackage{pdfpages}
\usepackage{booktabs}
\usepackage{csquotes}
\usepackage{lipsum}  
\usepackage{arydshln}
\usepackage{smartdiagram}
\usepackage{textcomp}
\usepackage{tabularray}\UseTblrLibrary{varwidth}
\usepackage{xcolor}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\usepackage{cite}
\usepackage{amsmath}
\newcommand{\probP}{\text{I\kern-0.15em P}}
\usepackage{etoolbox}
\patchcmd{\thebibliography}{\section*{\refname}}{}{}{}

\setlength\tabcolsep{0.5pt}

% \renewcommand{\arraystretch}{1.7}

% \setlength{\extrarowheight}{2.5pt}
% \renewcommand{\arraystretch}{0.2}
% \renewcommand{\arraystretch}{1.7}

% --------------
% \titleclass{\subsubsubsection}{straight}[\subsection]

% \newcounter{subsubsubsection}[subsubsection]
% \renewcommand\thesubsubsubsection{\thesubsubsection.\arabic{subsubsubsection}}
% \renewcommand\theparagraph{\thesubsubsubsection.\arabic{paragraph}} % optional; useful if paragraphs are to be numbered

% \titleformat{\subsubsubsection}
%   {\normalfont\normalsize\bfseries}{\thesubsubsubsection}{1em}{}
% \titlespacing*{\subsubsubsection}
% {0pt}{3.25ex plus 1ex minus .2ex}{1.5ex plus .2ex}

% \makeatletter
% \renewcommand\paragraph{\@startsection{paragraph}{5}{\z@}%
%   {3.25ex \@plus1ex \@minus.2ex}%
%   {-1em}%
%   {\normalfont\normalsize\bfseries}}
% \renewcommand\subparagraph{\@startsection{subparagraph}{6}{\parindent}%
%   {3.25ex \@plus1ex \@minus .2ex}%
%   {-1em}%
%   {\normalfont\normalsize\bfseries}}
% \def\toclevel@subsubsubsection{4}
% \def\toclevel@paragraph{5}
% \def\toclevel@paragraph{6}
% \def\l@subsubsubsection{\@dottedtocline{4}{7em}{4em}}
% \def\l@paragraph{\@dottedtocline{5}{10em}{5em}}
% \def\l@subparagraph{\@dottedtocline{6}{14em}{6em}}
% \makeatother

% \setcounter{secnumdepth}{4}
% \setcounter{tocdepth}{4}
% --------------

\newcommand{\before}[1]{\textcolor{red}{#1}}
\newcommand{\after}[1]{\textcolor{green}{#1}}

\newcommand{\old}[1]{\textcolor{orange}{#1}}
\newcommand{\rem}[1]{\textcolor{red}{#1}}
\newcommand{\todo}[1]{\textcolor{orange}{\newline \textit{\textbf{TODO:} #1}} \newline \newline }



\newcounter{relation}
\setcounter{relation}{0}
\renewcommand{\therelation}{\arabic{relation}}
\newcommand{\relationautorefname}{Relation}

\newenvironment{relation}[1][]{%
    \refstepcounter{relation}%
    \noindent \raggedright \textit{\textbf{Relation. \therelation}} \hfill$}
{%
$ \hfill \phantom{x}

}

\newcounter{proof}
\setcounter{proof}{0}
\renewcommand{\theproof}{\arabic{proof}}
\newcommand{\proofautorefname}{Proof}

\renewenvironment{proof}[1][]{
    \refstepcounter{proof}
    \noindent \raggedright \textit{\textbf{Proof. \theproof}}

    \setlength{\leftskip}{1em}

}
{

\
\setlength{\leftskip}{0pt}
}


% --------------------------------
%             DOCUMENT
% --------------------------------

\begin{document}

\title{Leveraging Organizations in MARL for Designing Multi-Agent Systems}
%
%\titlerunning{Abbreviated paper title}
% If the paper title is too long for the running head, you can set
% an abbreviated paper title here
%
% \author{Julien Soulé\inst{1}\orcidID{0000-1111-2222-3333} \and
% Jean-Paul Jamont\inst{1}\orcidID{1111-2222-3333-4444} \and
% Michel Occello\inst{1}\orcidID{2222--3333-4444-5555} \and
% Louis-Marie Traonouez\inst{2}\orcidID{2222--3333-4444-5555} \and
% Paul Théron\inst{3}\orcidID{2222--3333-4444-5555}}

%%% Double blind review %%%
\author{}
\authorrunning{}
\institute{}
% \author{Julien Soulé\inst{1} \and
% Jean-Paul Jamont\inst{1} \and
% Michel Occello\inst{1} \and
% Louis-Marie Traonouez\inst{2} \and
% Paul Théron\inst{3}}
% %
% \authorrunning{J. Soulé et al.}
% % First names are abbreviated in the running head.
% % If there are more than two authors, 'et al.' is used.
% %
% \institute{Univ. Grenoble Alpes, Grenoble INP, LCIS, 26000, Valence, France
%     \email{\{julien.soule, jean-paul.jamont, michel.occello\}@lcis.grenoble-inp.fr}
%     \and
%     Thales Land and Air Systems, BL IAS, Rennes, France
%     \email{louis-marie.traonouez@thalesgroup.com}
%     \and
%     AICA IWG, La Guillermie, France \\
%     \email{paul.theron@orange.fr}
% }


\maketitle              % typeset the header of the contribution

% MAS have been succefully
% For many MAS, the organization has become a critical success factor. 
% Several related methods exist to design MAS. 
% However, these methods are ...
% To enhance the quality and effectiveness of ... this paper presents an assisted approach for MAS Organization Engineering (AMOEA). 
% AMOEA guides the designer of...
% 1 phrase par contrib/key point

\begin{abstract}

    Designing a Multi-Agent System to achieve a goal in an environment often requires an organizational structure to coordinate and delegate tasks among agents. However, defining the agents' internal logic can be challenging in complex environments. Multi-Agent Reinforcement Learning enables agents to learn how to reach a goal without explicitly considering the organization. While previous studies have introduced guided training in individual agents, a multi-agent context requires clarifying the implicit cooperation among multiple agents after training. We propose a novel algorithmic approach leveraging the $\mathcal{M}OISE^+$ Organizational Model that consists in linking organizational specifications, such as roles or missions, to the respective agents' histories, characterizing their behaviors. Our algorithm constrains the learning process based on organizational constraints. Evaluations conducted in a mixed competitive/cooperative Predator-Prey environment validate the impact of organizational specifications as constraints during training.

    \keywords{Agent-Oriented Software Engineering \and Multi-Agent Reinforcement Learning \and Assisted-Design \and Organizational Models}
\end{abstract}

% ====================================================================================================

\section{Introduction}

% Context
In a top-down design approach, designing a Multi-Agent System (MAS) requires establishing an organization embeddable into the agents' internal logic (referred to as \textbf{policies}) that explains how agents coordinate their activities to collaboratively achieve a common expected goal~\cite{Picard2009}.
%
Consequently, designing a MAS can be viewed as an optimization problem, aiming to find the organization that promotes the best performance in achieving a goal. Methods such as GAIA~\cite{Wooldridge2000,Cernuzzi2014}, ADELFE~\cite{Mefteh2015}, or KB-ORG~\cite{Sims2008} rely on an iterative process of trial and error to gain knowledge of the environment and find an appropriate organization~\cite{Sims2008}.

% Problem
However, increasing this empirical knowledge can be costly for numerous factors including the environment's complexity or its restricted access, and the designers' unavailability. Moreover, relying solely on limited knowledge may prevent the designing of a MAS that meets sufficient performance or that guarantees safety requirements~\cite{Mefteh2013}. Currently, no method fully automate the design process of MAS. Meeting this challenge would improve the trust and adoption of MARL systems in real-world applications~\cite{kok2006collaborative,omidshafiei2019learning}.

% Contribution
To address this concern, we propose extending \textit{Partial Relation between Agents' History and Organizational Model}~\cite{soule2024} (PRAHOM). PRAHOM is a general approach that leverages histories to integrate $\mathcal{M}OISE^+$~\cite{Hubner2007} organizational model into the MARL framework. $\mathcal{M}OISE^+$ is suitable to represent organizational specifications in the MARL framework. It makes it possible to envision constraining the learning. However, the current development in this area remains limited and does not scale with the increasing number of organizational specifications. It is unsuitable for further application in more realistic scenarios.

Partially relying on PRAHOM, we propose a novel algorithm called \textit{PRAHOM Training} (PRAHOMT), which only requires the concepts of role and mission, thus addressing scalability issues.
% PRAHOMT can be seen as a way to assist MAS design.
PRAHOMT links $\mathcal{M}OISE^+$ and MARL by one-to-one mapping an organizational specification (i.e a role or a mission) to a history subset characterizing an expected behavior impacting action space or the reward function accordingly.
%
\Autoref{fig:policy_histories} illustrates this underlying idea: a policy can be represented as the set of all its possibly generated histories/trajectories (arrows) in an abstract space representing all couples of joint-observations ($\Omega_{joint}$) and joint-actions ($A_{joint}$). A history starts from an initial observation-action couple (blue area) and possibly reaches one of the expected observation-action couples that characterize the goal (red area). The idea is to see a history subset as a way to characterize an organizational specification (orange area).

\begin{figure}[h!]
    \centering
    \input{figures/history_representation.tex}
    \caption{An abstract view of joint-policy as joint-histories and constraints}
    \label{fig:policy_histories}
\end{figure}

Unlike, classical MARL where an agent's policy is freely updated solely based on maximizing the reward, our approach also forces/entices agents to satisfy some hand-crafted organizational specifications during the training. By restricting the policy search space, our approach may shorten the convergence time. It enables stabilizing resulting policies during/after training. From a design point of view, the obtained policies can be analyzed as fine-tuned organizations leveraging known introduced organizational specifications to help out with explainability. This led to getting relevant insights into future design in the real environment. Although adding organizational specifications may also prevent new organizations from emerging, our contribution provides practical means for users to find a tradeoff while addressing safety guarantees.

% Outline
The remainder is organized as follows: \Autoref{sec:marl_background} presents the MARL framework our contribution is built on. \Autoref{sec:related_works} gives an overview of the available works dealing with modifying policies within MARL. \Autoref{sec:linking_marl_moise} presents the proposed principles to link $\mathcal{M}OISE^+$ and MARL. \Autoref{sec:prahom_alg} introduces the PRAHOMT algorithm. \Autoref{sec:case_study} first presents our PRAHOMT implementation as a library we used in our experimental setup, and discusses the results of our evaluation. \Autoref{sec:conclusion} concludes the paper and outlines future research directions.


\section{MARL background}\label{sec:marl_background}

Multi-Agent Reinforcement Learning (MARL) extends the concepts of Reinforcement Learning (RL) to multi-agent context. Agents' policies are optimized to maximize the total cumulative reward through learning.
To apply MARL techniques, we chose the Decentralized Partially Observable Markov Decision Process (Dec-POMDP)~\cite{Oliehoek2016} because it considers multiple agents in a similar MAS manner. It relies on stochastic processes to model the uncertainty of the environment for the changes induced by actions, received observations, and communication. Additionally, unlike Partially Observable Stochastic Games (POSG), the reward function can be common to agents which fosters training for collaborative-oriented actions~\cite{Beynier2013}.

A Dec-POMDP $d \in D$ (with $D$ the set of Dec-POMDP) is a 7-tuple $d = (S,\{A_i\},T,R,\{\Omega_i\},O,\gamma)$ , where:
$S = \{s_1,\dots,s_{|S|}\}$: the set of possible states;
$A_{i} = \{a_{1}^{i},\dots,a_{|A_{i}|}^{i}\}$: the set of possible actions for agent $i$;
$T$ so that $T(s,a,s') = \probP{(s'|s,a)}$ : the set of conditional transition probabilities between states;
$R: S \times A \times S \rightarrow \mathbb{R}$: the reward function;
$\Omega_{i} = \{o_{1}^{i},\dots,o_{|\Omega_{i}|}^{i}\}$: the set of observations for agent $i$;
$O$ so that $O(s',a,o) = \probP{(o|s',a)}$ : the set of conditional observation probabilities;
$\gamma \in [0,1]$ : the discount factor.

Considering $m$ \textbf{teams} (also referred to as \textbf{groups}) each containing several agents among $\mathcal{A}$, we also detail the minimal formalism notation we re-used for solving the Dec-POMDP for a given team $i, 0 \leq i \leq m$ containing $n$ agents~\cite{Beynier2013,Albrecht2024}:

\begin{itemize}

    \item $\Pi$: the set of policies. A \textbf{policy} $\pi \in \Pi, \pi: H \times \Omega \rightarrow A$ associates an observation to an action optionally using previous experiences as histories. It represents the agent's internal logic;
    \item $\Pi_{joint}$: the set of joint-policies. A \textbf{joint-policy} $\pi_{joint} \in \Pi_{joint}, \pi_{joint}: H_{joint} \times \Omega^n \rightarrow A^n = \Pi^n$ chooses an action for each agent regarding their respective observation optionally using previous experiences as histories. It can be viewed as a set of policies used in agents;
    \item $H$: the set of histories. A \textbf{history} over $z \in \mathbb{N}$ steps (where $z$ is generally the maximum number of steps for an episode) is the $z$-tuple $h = ((\omega_{k}, a_{k}) | k \leq z, \omega \in \Omega, a \in A)$;
    \item $H_{joint}$: the set of joint-histories. A \textbf{joint-history} over $z \in \mathbb{N}$ steps $h_{joint} \in H_{joint}, h_{joint} = \{h_1,h_2 \dots h_n\}$ is the set of agents' histories;
    \item $U_{joint,i}(<\pi_{joint,i}, \pi_{joint,-i}>): \Pi_{joint} \rightarrow \mathbb{R}$: gives the \textbf{expected cumulative reward} over a finite horizon (if $\gamma < 1$ or the number of steps in an episode is finite), with $\pi_{joint,i}$ the joint policy for team $i$ and $\pi_{joint,-i}$ all of the other concatenated joint-policies (considered as fixed).
          % \item $BR_{joint,i}(\pi_{joint,i}) = argmax_{\pi_{joint,i}}(U(<\pi_{joint,i},\pi_{joint,-i}>))$: gives the \textbf{best response} $\pi_{joint,i}^*$ in the sense that the team cannot change any of the policies in the joint-policy $\pi_{joint,i}^*$ to get a better expected cumulative reward than $U_i^* = U_{joint,i}(<\pi_{joint,i}^*, \pi_{joint,-i}>)$;
          % \item $SR_{joint,i}(\pi_{joint,i}, s) = \{\pi_{joint,i} | U(<\pi_{joint,i},\pi_{joint,-i}>) \geq s\}$: gives the \textbf{sufficient response} as the set of joint-policies getting at least $s \in \mathbb{R}, s \leq U_i^*$ as expected cumulative reward.
\end{itemize}

Various approaches enable solving MARL problems. We favored \textbf{Policy-based} methods, such as Multi-Agent Proximal Policy Optimization~\cite{yu2022surprising} (MAPPO) and Multi-Agent Deep Deterministic Policy Gradient~\cite{Lowe2017} (MADDPG). These algorithms directly parameterize the policy as $\pi_\theta$ to optimize it possibly using centralized training and decentralized execution.
We refer to \textbf{solving} the Dec-POMDP at $s$ expectancy as finding the joint policies $\pi_{joint,i} \in \Pi_{joint}, \pi_{joint,i} = \{\pi_{joint,i} | U(<\pi_{joint,i},\pi_{joint,-i}>) \geq s\}$ that gets the expected cumulative reward over a finite horizon at least at $s \in \mathbb{R}$.


\section{Related works and positioning}\label{sec:related_works}

The integration of organizational specifications in MAS learning processes is not largely addressed explicitly in the literature. Yet, various approaches have been proposed to incorporate organizational constraints and policies in MAS to ensure agents adhere to certain requirements.

\textbf{Learning with Organizational Constraints} \quad
%
The integration of organizational constraints into the learning process of agents has been explored to various extents. In \cite{cruz2020norms}, the authors present a method for incorporating norms into the learning algorithms of agents, ensuring that their behavior remains within acceptable bounds. Additionally, \cite{villatoro2011social} proposes a mechanism for agents to learn and adapt to social norms in dynamic environments, highlighting the importance of norm adaptation in MAS. However, these norms cannot be individual.

\emph{Specification-Guided Reinforcement Learning} aims to generate policies that accomplish a specific task using external specifications to guide learning in achieving an objective under given constraints~\cite{Bansal2022}. Jothimurugan et al.~\cite{Jothimurugan2021} propose logical specification learning as exploiting the compositional structure of specifications to generate policies for complex tasks. However, it does not completely falls into the MARL framework we set for it requires introducing logical specifications.

\textbf{Policy-Based Approaches} \quad
%
Policy-based approaches provide a way to enforce organizational constraints by defining explicit policies that govern agent behavior. In \cite{krupanski2015norm}, the use of normative policies is investigated to guide agent interactions and decision-making processes. Moreover, \cite{vos2020governing} explores the use of governance mechanisms to enforce compliance with organizational policies in decentralized systems. However, these approaches face a lack of explainability since most policies are black-box and cannot be easily modified.

\textbf{Frameworks Integrating Organizational Aspects} \quad
%
Wang et al.~\cite{Wang2020} introduce an approach in which similar emerging roles are pushed to jointly specialize in specific tasks. Tosic et al.~\cite{Tosic2010} propose a framework for coordination based on the communication capabilities of multi-agent systems. Zheng et al.~\cite{Zheng2018} present a platform for MARL that aims to facilitate research on artificial collective intelligence by providing a comprehensive set of evaluation metrics to compare the performance of MARL algorithms. However, it does not take into account specifications likely to entice agents to adhere to an expected behavior such as missions.

\

Despite these advancements, there is still a lack of works that explicitly use organizational specifications as a means of constraining agent learning regarding requirements. To our knowledge, no work can be used to generate a MAS that satisfies additional organizational constraints explicitly. Unlike these works, our originality is to explicitly use an organizational model as a general means of constraining learning regarding requirements.


\section{Linking MARL and the $\mathcal{M}OISE^+$ model}\label{sec:linking_marl_moise}
\label{sec:marl_moise_linking}

In this section, we incrementally present the principles we propose to adapt the MARL according to organizational specifications. This includes constraining a policy to adhere to the expected behavior of a role and enticing the policy to achieve a mission for a given time duration.


The $\mathcal{M}OISE^+$ model provides a comprehensive framework for specifying the structure and interactions within a Multi-Agent System (MAS). We utilize $\mathcal{M}OISE^+$ due to its compatibility with MARL and its formal description capabilities for agents' policies. The PRAHOMT's underlying approach bridges the gap between MARL and organizational modeling by embedding $\mathcal{M}OISE^+$ specifications into the MARL training process. This integration allows for the training of agents that adhere to predefined organizational roles and missions.
%
In $\mathcal{M}OISE^+$, \textbf{Organizational Specifications (OS)} are defined as $\mathcal{OS} = \langle \mathcal{SS}, \mathcal{FS}, \mathcal{DS} \rangle$, where $\mathcal{SS}$ are the \textbf{Structural Specifications}, $\mathcal{FS}$ are the \textbf{Functional Specifications}, and $\mathcal{DS}$ are the \textbf{Deontic Specifications}.

\subsection{Structural Specifications and Constraining Joint-Policies According to Roles}

\textbf{Structural Specifications} describe the organizational structure and are denoted as $\mathcal{SS} = \langle \mathcal{R}, \mathcal{IR}, \mathcal{G} \rangle$:

\begin{itemize}
    \item $\mathcal{R}_{ss}$: The set of all roles ($\rho \in \mathcal{R}$).
    \item $\mathcal{IR}: \mathcal{R} \rightarrow \mathcal{R}$: The inheritance relation between roles ($\rho_1 \sqsubset \rho_2$).
    \item $\mathcal{RG} \subseteq \mathcal{GR}$: The set of root groups, $\mathcal{GR} = \langle \mathcal{R}, \mathcal{SG}, \mathcal{L}^{intra}, \mathcal{L}^{inter}, \mathcal{C}^{intra}, \mathcal{C}^{inter}, \allowbreak np, ng \rangle$, where:
          %   \begin{itemize}
          $\mathcal{R} \subseteq \mathcal{R}_{ss}$: The set of non-abstract roles.
          $\mathcal{SG} \subseteq \mathcal{GR}$: The set of sub-groups.
          $\mathcal{L} = \mathcal{R} \times \mathcal{R} \times \mathcal{TL}$: The set of links $(\rho_s, \rho_d, t)$, where $t \in \{acq, com, aut\}$ indicates the type of link (acquaintance, communication, authority).
          $\mathcal{L}^{intra} \subseteq \mathcal{L}$: The set of intra-group links.
          $\mathcal{L}^{inter} \subseteq \mathcal{L}$: The set of inter-group links.
          $\mathcal{C} = \mathcal{R} \times \mathcal{R}$: The set of compatibilities $(\rho_a \bowtie \rho_b)$.
          $\mathcal{C}^{intra} \subseteq \mathcal{C}$: The set of intra-group compatibilities.
          $\mathcal{C}^{inter} \subseteq \mathcal{C}$: The set of inter-group compatibilities.
          $np: \mathcal{R} \rightarrow \mathbb{N} \times \mathbb{N}$: The cardinality of agents adopting a role.
          $ng: \mathcal{SG} \rightarrow \mathbb{N} \times \mathbb{N}$: The cardinality of each sub-group.
          %   \end{itemize}
\end{itemize}

Constraining policies directly is not feasible because most policy implementations rely on intractable black-box models such as neural networks. We propose to formally represent a role as a history subset $\mathcal{P}(H)$ containing histories an agent playing a role should generate, hence characterizing the role's expected behavior. As illustrated in \Autoref{fig:PRAHOM_osm_rels}, we propose each role to be mapped to a history subset through the $rh: \mathcal{R} \rightarrow \mathcal{P}(H)$ bijection. An agent playing a role should have its policy constrained to generate histories belonging to the mapped history subset (at least from a theoretical point of view).

\begin{figure}[h!]
    \centering
    \input{figures/PRAHOM_osm_rels.tex}
    \caption{Relations between organizational specifications and history subsets}
    \label{fig:PRAHOM_osm_rels}
\end{figure}

Since defining roles into a history subset faces issues for handling possibly numerous large and non-manageable observations (such as pixel tables), we first propose to use labels to represent observations in a short-way. We introduce $ol: \mathcal{P}(\Omega) \rightarrow \mathcal{P}(L)$ to map some simple strings to real observations. In addition to a simple mapping, we also considered using a Large Language Model (LLM) for that purpose. % as illustrated in \Autoref{fig:PRAHOMT_ol}.
The LLM is trained after real observations have been rendered visually and labeled by hand. Once trained, the LLM can be used conveniently to get real observations from labels, and may also be used to label some other observations.

% \begin{figure}[h!]
%     \centering
%     \input{figures/ol_scheme.tex}
%     \caption{Observations-labels mapping and its creation}
%     \label{fig:PRAHOMT_ol}
% \end{figure}

Second, defining a history subset exhaustively may require taking into account many cases, hence leading to an important amount of histories. %As illustrated in \Autoref{fig:PRAHOM_opc}
Rather than defining a history subset exhaustively, we propose three means to simplify its definition:
%
\quad i) a \textbf{pattern} format that conforms to the following rules: a sequence is denoted \textquote{[$label_{obs_1},label_{act_1}\dots$] \allowbreak ($card_{min},card_{max}$)} the \textquote{Any} label refer to any observation/action. This pattern is implemented an oriented graph denoted \textbf{history graph} where nodes are observations/actions and edges represent expected transitions relying on ordinal number and cardinalities; \quad
%
ii) \textbf{rules} to associate an action set depending on a history (possibly defined as a pattern) belonging to the history subset and a received observation. Once the observations and the associated actions are added to the history, this history should still belong to the history subset; \quad
%
iii) a \textbf{custom script} logic taking into account a history belonging to the history subset and a new observation to indicate the actions to add in the current history so it still belongs to the history subset.


Considering a history subset is ultimately aimed to be used to constrain a policy to make it adhere to a role, we introduce an \textbf{observable policy constraint} $c\pi: H \times \Omega \rightarrow \mathcal{P}(A)$. $c\pi$ indicates the actions an agent's policy should be allowed to choose among when it receives an observation at each step.

% \begin{figure}[h!]
%     \centering
%     \input{figures/opc_scheme.tex}
%     \caption{An abstract view of observable policy constraint and its crafting}
%     \label{fig:PRAHOM_opc}
% \end{figure}

We propose to integrate observable policy constraints into an agent's policy via three modes:
\textbf{correct}: Corrects any chosen action $\pi(\omega)$ to an expected one in $c\pi(\omega)$. It ensures safety but is external to the agent's policy; \quad
\textbf{penalize}: Adds a penalty to the reward if a wrong action ($\pi(\omega) \notin c\pi(\omega)$) is made, encouraging agents to learn their constraints but without safety guarantees; \quad
\textbf{correct\_policy}: Creates a \textbf{constrained policy} $\pi_c \in \Pi, \pi_c = \{smpl(c\pi_{joint}(\omega_{joint})) \allowbreak if \ \omega_{joint} \in Dom(c\pi_{joint}) \ \allowbreak, \ \allowbreak else \ \allowbreak \pi_{joint}(\omega_{joint})\}$ where $smpl$ gets an element from a set randomly. An observable policy constraint $c\pi$ corrects the current policy $\pi$, hence respecting safety guarantees internally.

In the annex, we provide \autoref{proof:jpc_to_ac}, which outlines why constraining the action decision-making process dynamically during training at each step implies that the resulting joint-policy will necessarily be constrained.


\subsection{Functional Specifications and Constraining Joint-Policies According to Missions}

\textbf{Functional Specifications} describe the tasks and goals and are denoted as $\mathcal{FS} = \langle \mathcal{SCH}, \mathcal{PO} \rangle$:

\begin{itemize}
    \item $\mathcal{SCH} = \langle \mathcal{G}, \mathcal{M}, \mathcal{P}, mo, nm \rangle$: The set of social schemes, where:
          \begin{itemize}
              \item $\mathcal{G}$: The set of global goals.
              \item $\mathcal{M}$: The set of mission labels.
              \item $\mathcal{P} = \langle \mathcal{G}, \{\mathcal{G}\}^s, OP, [0,1] \rangle$: The set of plans $(g_f, \{g_i\}_{0 \leq i \leq s}, op, ps)$.
              \item $mo: \mathcal{M} \rightarrow \mathbb{P}(\mathcal{G})$: The goals associated with each mission.
              \item $nm: \mathcal{M} \rightarrow \mathbb{N} \times \mathbb{N}$: The cardinality of agents committed to each mission.
          \end{itemize}
    \item $\mathcal{PO}: \mathcal{M} \times \mathcal{M}$: The set of preference orders $(m_1 \prec m_2)$.
\end{itemize}

We consider a goal to be theoretically represented as a history subset. We propose $gh: \mathcal{G} \rightarrow \mathcal{P}(H)$ that shows how an agent committed to a mission should have its policy enticed to generate histories belonging to an expected history subset (at least from a theoretical point of view). Thus, $mh: \mathcal{M} \rightarrow \mathcal{P}(H) = \{(m,\bigcup gh[mo(m)]), m \in \mathcal{M}\}$ gives expected histories from missions. Ultimately, a goal should impact MARL by updating the reward function when an agent is committed to a mission. We introduce an \textbf{observable reward function} $R_{g}: H \rightarrow \mathbb{R}$, the relation indicating how close a generated history is to a given history subset. A reward function for a mission $m \in \mathcal{M}$ containing goals $\mathcal{G}_{m} = mo(m)$ is computed as a weighted addition of all observable reward functions of each goal. This way, agents are individually enticed to achieve their respective sub-goals hence speeding up the convergence to achieving the ultimate goal.

% \begin{figure}[h!]
%     \centering
%     \input{figures/goal_mission_scheme.tex}
%     \caption{An abstract view of observable reward function and its crafting}
%     \label{fig:goal_mission_scheme}
% \end{figure}

\subsection{Deontic Specifications and Constraining Joint-Policies According to Permissions/Obligations}

\textbf{Deontic Specifications} define the permissions and obligations and are denoted as $\mathcal{DS} = \langle \mathcal{OBL}, \mathcal{PER} \rangle$:

\begin{itemize}
    \item $\mathcal{TC}$: The set of time constraints.
    \item $\mathcal{OBL}: \mathcal{R} \times \mathcal{M} \times \mathcal{TC}$: The set of obligations $(obl(\rho_a, m, tc))$.
    \item $\mathcal{PER}$: The set of permissions $(per(\rho_a, m, tc))$.
\end{itemize}

We introduced the relation $da: \mathcal{PER} \cup \mathcal{OBL} \rightarrow \mathcal{P}(\mathcal{A})$ that indicates how agents are constrained to roles and committed on missions for a given time constraint. In order to take into account time constraints, we introduce a time-to-live for each permission/obligation through the relation $dttl: \mathcal{PER} \cup \mathcal{OBL} \times \mathcal{A} \rightarrow \mathbb{N}$. Then, they are to be decreased at each step if the given time constraint is not \textquote{Any} with the relation $dec: dttl \rightarrow dttl$. Then, the roles constrained to agents or committed missions may change after the reward function is changed.

To differentiate between obligations and permissions and ensure agents prioritize obligated missions over permitted ones, we propose multiplying the observable reward function of this mission by a high factor for obligated missions and a low factor for permitted ones.

Relying on these principles to integrate organizational specifications within the MARL framework, we established the PRAHOM algorithm.


\section{PRAHOMT Algorithm}\label{sec:prahom_alg}

% $PRAHOMT$ algorithm is presented in \autoref{alg:PRAHOM-A}. It fits within a regular MARL context: a joint-policy $\pi_{joint,ep}$ is updated after several steps allowing to have enough joint-histories $h_{joint,ep}$ and joint-rewards as feedback $r_{joint,ep}$. The training goes on over several episodes for better training until converging to a sufficient cumulative reward regarding $s$.

In this section, we provide an explanation of the PRAHOMT algorithm as outlined in \autoref{alg:PRAHOM-A}. It integrates organizational specifications into the learning by constraining joint-policies according to predefined roles, missions, and permission/obligation relations. This ensures that the agents' behaviors align with organizational requirements while  optimizing joint-policy ($\pi_{joint,i,s}$) as output.

\textbf{Initialization and Input Parameters}: \quad
The algorithm starts by initializing the joint-policy ($\pi_{joint}$) with the initial joint-policy ($\pi_{joint,i,init}$) and setting the episode counter ($ep$) to zero (line 1). The algorithm also sets a boolean variable ($sufficient$) to False, indicating that the cumulative reward expectancy has not yet been met.

\textbf{Step 1: Determine Joint Observable Policy Constraints}: \quad
PRAHOMT determines the joint observable policy constraints ($c\pi_{joint}$) from the organizational specifications via $bld_{opc}$ (line 2). This step involves mapping roles ($rh$), missions ($mh$), and permissions/obligations ($da$) to constraints on the joint-policy, ensuring that the agents' actions are in line with the organizational rules.

\textbf{Step 2: Initialize Constrained Policy}: \quad
If the mode of constraint integration is set to \textit{correct\_policy}, the algorithm creates and uses a joint constrained policy via $bld_{joint, \pi_c}$ based on the initial policy and the observable policy constraints ($c\pi_{joint}$) (line 4). This ensures that the initial policy is inherently aligned with the specified constraints from the beginning.

\textbf{Step 3: Determine Observable Reward Functions}: \quad
Next, PRAHOMT determines the observable reward functions $R_{m,joint}$ from the organizational specifications via $bld_{mrf}$ (line 5). This involves mapping missions and goals to reward functions, which will guide the agents towards achieving organizational sub-goals. Then, these observable reward functions are integrated within the global reward function via $comb_{mrf}$.

\textbf{Step 4: Main Training Loop}: \quad
The main training loop runs for a maximum number of episodes ($ep_{max}$) or until the cumulative reward expectancy is met (line 6).

\begin{itemize}
    \item \textbf{Initialize Episode}:
          At the beginning of each episode, the environment, observation, and action histories are reinitialized (line 7). The cumulative reward and penalty are also reset. Time-to-live values for permissions/obligations are initialized using $bld_{dttl}$, and the initial observation and action are set by the environment ($d_{ep}.init()$).

    \item \textbf{Step Through Episode}:
          The algorithm steps through each episode for a maximum number of steps ($step_{max}$) (line 8). Within each step:

          \begin{itemize}
              \item \textbf{Policy Update}:
                    The joint-policy ($\pi_{joint}$) is updated using the MARL algorithm ($u_{marl}$) based on the current history ($rh_{joint,ep}$) and last rewards ($rh$) (line 9).

              \item \textbf{Action Selection}:
                    The next action is selected based on the current observation ($\pi_{joint}(\omega_{joint,t=step})$) (line 10).

              \item \textbf{Action Validation}:
                    The expected actions are determined from the observable policy constraints ($c\pi_{joint}$), and if the selected action is not among the expected ones, it is either corrected or penalized based on the mode of constraint integration (lines 11-16).

              \item \textbf{Update Histories and Rewards}:
                    The current history is updated (line 17), and the action is applied to the environment to get the next observation and reward (line 18) adding any incurred penalties (line 19).

              \item \textbf{Update Constraints and Rewards}:
                    Time-to-live values are decreased, and the reward functions and policies are updated accordingly if some organizational specifications have changed (line 20).
          \end{itemize}

    \item \textbf{Check for Sufficiency}:
          After each episode, the algorithm checks whether the cumulative reward meets the expectancy ($is\_sufficient$) and increments the episode counter ($ep$) (line 21).
\end{itemize}

\textbf{Complexity and Scalability}: \quad The computational complexity of PRAHOMT primarily depends on the number of episodes ($ep_{max}$), the number of steps per episode ($step_{max}$), and the complexity of the MARL algorithm ($u_{marl}$). Each step involves updating the policy, selecting \& validating actions, and updating histories and rewards, which can be computationally intensive. The memory complexity depends on the need to store rewards, policies, and histories. The history graphs we introduced allow histories to be merged compactly, reducing redundancy with limited impact on read cost.

Scalability can be challenging as the number of agents, roles, and missions increases. Evaluating PRAHOMT scalability requires considering an implemented version through these salient features: \quad
\textbf{Parallelization:} Performance for updating/validating policies strongly depends on chosen MARL algorithms capabilities such as parallelization significantly improves it (especially in Policy Optimization algorithms); \quad
\textbf{Efficient Data Structures:} The choice of data structures for storing and accessing histories or policies is crucial for reducing overhead.; \quad
\textbf{Optimization of Reward Functions:} The logic for constructing/handling reward functions is important to enhance performance further.



\section{Case study: Predator-prey environment}\label{sec:case_study}

\subsection{Algorithm implementation: PRAHOMT Wrapper}

\emph{PettingZoo} is a library that offers a standard API simplifying the development of multi-agent environments and facilitating the application of MARL algorithms.
%
We developed the \textit{PRAHOMT Wrapper}
%
% \footnote{Additional information are provided in addition to the code in \url{https://github.com/julien6/omarl_experiments}.}
%
as a proof-of-concept (PoC) tool to augment PettingZoo environments, facilitating the application of the PRAHOMT algorithm. The wrapper leverages the MARLlib~\cite{hu2022marllib} library, which offers a wide range of state-of-the-art MARL algorithms and fine-tuned policy models for various environments.
This wrapper provides an API of additional auxiliary classes to define organizational specifications and link them to their expected behavior:

\textbf{Observation Labeling (ol)} \quad The observation labeling is handled by the singleton class \textit{ol\_mngr}, which is part of the API. This class employs the HuggingFace transformer model \textit{tiiuae/falcon-7b} to learn these mappings in conjunction with a simple dictionary. This class also provides an interactive process that allows users to label each observation during the labeling procedure.

\textbf{History Subsets} \quad The internal \textit{history\_subset} class is used to handle history subsets based on predefined patterns or rules via an implemented history graph when dealing with patterns. Alternatively, custom functions can be employed to instantiate a history\_subset, allowing more flexibility.

\textbf{Observable Policy Constraint (opc)} \quad The \textit{opc} class is to be used by users to link pairs of (history\_pattern, observation) to lists of expected actions. Each association is integrated into a unified \textit{history\_subset} object.

\textbf{Constrained Policy (cons\_policy)} \quad The internal \textit{cons\_policy} class combines a regular MARLlib policy with an \textit{opc} object. The integration involves embedding policy constraints within the MARLlib policy framework, allowing the constrained policy to respect predefined action constraints.

\textbf{Observable Reward Function (orf)} \quad The \textit{orf} class enables users to instantiate an observable reward function based on \textit{history\_subset} objects. It includes a function that evaluates whether a given history matches the initial pattern: high rewards for matching patterns, and low rewards otherwise.

\textbf{Organizational Specifications to Relations (osr)} \quad The \textit{osr} class enables users to gather all previously instantiated elements into a complete $\mathcal{M}OISE^+$ organizational model (JSON representable). In this class, roles and goals are directly mapped respectively to \textit{opc} and \textit{orf} objects using either patterns, rules, or custom functions. This class implements \textit{rh}, \textit{gh}, and \textit{mh}. Permissions/obligations are also defined in this class: each obligation/permission as (role,missions,time constraint) triplet is mapped to agents, thus implementing the \textit{da} relation.

\textbf{Time-to-Live (dttl)} \quad The internal \textit{dttl} class uses the \textit{osr} object to manage time constraints and update the observable reward function / policy constraints.

\textbf{Train under constraints} \quad Once the PettingZoo environment is wrapped with the PRAHOMT Wrapper and all auxiliary classes have been used to get an \textit{ol\_mngr} and \textit{os}. The wrapper' API provides the \textit{train\_under\_constraint()} function that enables executing the PRAHOMT algorithm taking \textit{ol\_mngr} and and \textit{os} objects as arguments (in addition to other MARLlib parameters). Ultimately, it generates MARLlib policy objects and statistical data.


\subsection{Experimental Setup}

The goal of our experiments is to show whether PRAHOMT indeed impacts the agents' policies during/after training as expected. For that purpose, we selected the \textquote{Simple World Comm}~\cite{Lowe2017} PettingZoo environment. This environment simulates a predator-prey scenario where predators, including a leader predator, collaborate to catch faster-moving prey. Prey gain points by collecting food and avoiding predators. The leader predator can observe the entire map, navigate freely (except in obstacles), and communicate directional orders to other predators. This setup is well-suited to induce agents to adopt organizational patterns, thus testing the effectiveness of PRAHOMT in training agents.

%The environment includes features such as obstacles and food items, with the following key attributes:

% \begin{itemize}
%     \item \textbf{Actions}: Discrete (5), Continuous (20)
%     \item \textbf{Observation Shape}: (28) for prey, (34) for predators
%     \item \textbf{State Shape}: (192)
%     \item \textbf{Rewards}: Prey gain points by collecting food and avoiding predators, while predators gain points by catching prey.
% \end{itemize}

\noindent We selected the MAPPO algorithm for its proven effectiveness in cooperative multi-agent environments without the need for domain-specific algorithmic modifications or architectures~\cite{Yu2022}. We used their respective fine-tuned model provided by MARLlib to launch training.
%
To train the LLM in \texttt{ol}, we observed observations showing specific behaviors (\textit{three agents circling a single prey}). This labeling helps in easily identifying collective behaviors based on visual observations, enabling better control of agents' actions.

\

\noindent To evaluate PRAHOMT, we implemented two organizational models:

\begin{itemize}
    \item \textbf{circle\_model} \quad This model includes two roles and a mission for predators:
          \begin{itemize}
              \item \textit{circle\_leader}: This role maps observations where three normal predators are close to the same prey, to directing them to move towards it.
              \item \textit{circle\_follower}: This role ensures any order received from the leader predator is followed with the corresponding action.
              \item \textit{prey\_proximity}: This mission contains a single goal indicating a high reward when prey is near any predator.
          \end{itemize}

    \item \textbf{manual\_model} \quad This model serves as a fully controlled organizational model with no room for learning. It includes two roles \textit{manual\_leader} and \textit{manual\_redator} that both dictate optimal actions for the leader predator according to a hand-crafted policy (custom function).
\end{itemize}

% \noindent Here is an overview of the PRAHOMT Wrapper code used for \textbf{circle\_model} :
% %
% \begin{lstlisting}[language=Python, basicstyle=\scriptsize]
% from prahom_wrapper import prahom_wrapper
% pz_env = raw_pz_env() ; pz_env = prahom_wrapper(pz_env)
% pz_env.train_under_constraints(ol=ol(), cons_mode="CORRECT",alg_conf="default_MAPPO",osr=organizational_model(
% structural_specifications(
%     {"circle_leader": leader_opc.add_pattern("[o1,a3,[#any]...](1,1)"),"circle_follower": normal_opc.add_pattern("[o1,a2,[#any]...](1,1)")}, None,None),
% functional_specifications(social_scheme={"sch_1": social_scheme(
%     goals=["goal_1": orf.add_pattern("[[#any],o4,a4](1,1)")], missions=["prey_proximity"], goals_structure=None, mission_to_goals={"prey_proximity": ["goal_0"]}, mission_to_agent_cardinality={"prey_proximity": cardinality(1, "*")})}, social_preferences=None),
% deontic_specifications(None, {
%     obligation("circle_leader", None, time_constraint_type.ANY): ["leadadversary_0"], obligation("circle_follower", None, time_constraint_type.ANY): ["adversary_0", "adversary_1", "adversary_2"]})))
% \end{lstlisting}

\subsubsection{Evaluation criteria and metrics}

Our evaluation criteria rely both on qualitative and quantitative aspects during and after training. We considered three cases to compare models:
\begin{itemize}
    \item \textbf{No Organizational Specifications (NTS)}: Agents learn without any constraints.
    \item \textbf{Partially Constraining Organizational Specifications (PTS)}: Agents are guided by the \textit{circle\_model}.
    \item \textbf{Fully Constraining Organizational Specifications (FTS)}: Agents follow hand-crafted policies from the \textit{manual\_model}.
\end{itemize}

We used various metrics to help measure the impact of PRAHOMT during/after training:
\begin{itemize}
    \item \textbf{Scalability}: Assesses qualitatively the ability to manage a growing number of agents and obstacles based on computational resource usage.
    \item \textbf{Convergence Time}: The number of episodes required for the algorithm to reach a stable solution. Shorter convergence times indicate faster learning.
    \item \textbf{Standard Deviation}: Indicates the variability in the rewards obtained by the agents. A lower standard deviation signifies more consistent performance and possibly more stable organization.
    \item \textbf{Average Reward}: The mean reward obtained per episode reflects the overall performance of the algorithm.
    \item \textbf{Constraint Respect}: Assesses how well the agents adhere to the given organizational constraints. It is calculated as the inverse of the number of times the constraints are not satisfied. High constraint respect means that the agents are effectively following the rules.
\end{itemize}

Relying on these metrics, we summarized the criteria likely to show an effective expected impact of PRAHOMT during/after training:
\begin{itemize}
    \item $(\mathbf{C_1})$: We expected to manually notice some collective hunting strategies, such as circling, in a visually rendered environment.
    \item $(\mathbf{C_2})$: We expect to see faster convergence in the PTS case compared to the NTS case, with the FTS case showing a constant learning curve.
    \item $(\mathbf{C_3})$: We expect higher rewards in the PTS case compared to the NTS case.
    \item $(\mathbf{C_4})$: We expect the standard deviation to decrease from NTS to PTS and from PTS to FTS since agents are increasingly more constrained in their behavior.
    \item $(\mathbf{C_5})$: We expect the constraint respect to be fully covered in \textit{correct} and \textit{correct\_policy} modes but not in the \textit{penalize} mode.
    \item $(\mathbf{C_6})$: We expect the scalability to be handled in all of the cases.
\end{itemize}


\subsection{Results and Discussion}

The learning curves for the three cases (NTS, PTS, FTS) for the \textit{penalize} mode, are displayed in Figure~\ref{fig:learning_curves}. \Autoref{tab:results} summarizes the results according to metrics for each mode after training.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/penalize_learning_curves.png}
    \caption{Learning curves for NTS, PTS, and FTS cases using the \textit{penalize} mode.}
    \label{fig:learning_curves}
\end{figure}


\begin{table}[h!]
    \centering
    \caption{Comparison of NTS, PTS, and FTS cases under different modes (correct, penalize, correct\_policy) based on various metrics.\\}\label{tab:results}
    \setlength{\tabcolsep}{5pt}
    \begin{tabular}{cccccccccc}
        \cline{2-10}
                           & \multicolumn{3}{c}{correct} & \multicolumn{3}{c}{penalize} & \multicolumn{3}{c}{correct\_policy}                                                         \\
        \cline{2-10}
                           & NTS                         & PTS                          & FTS                                 & NTS         & PTS  & FTS  & NTS         & PTS  & FTS  \\
        \hline
        Scalability        & +                           & +                            & +                                   & ++          & ++   & ++   & +           & +    & +    \\
        Convergence Time   & 100                         & 60                           & 0                                   & 75          & 24   & 0    & 90          & 55   & 0    \\
        Standard Deviation & 2.3                         & 1.8                          & 0.5                                 & 2.5         & 1.9  & 0.5  & 2.1         & 1.7  & 0.5  \\
        Average Reward     & 500                         & 1200                         & 1800                                & 400         & 1100 & 1750 & 550         & 1300 & 1900 \\
        Constraint Respect & $\emptyset$                 & 1                            & 1                                   & $\emptyset$ & 0.8  & 1    & $\emptyset$ & 1    & 1    \\
    \end{tabular}
\end{table}
%
%
% The qualitative analysis shows that the \textit{circle\_model} leads to observable circling behaviors around prey, demonstrating the effective impact of organizational specifications on agent behavior. This visually \footnotemark[2] confirms that PRAHOMT can induce desired collective strategies ($\mathbf{C1}$). Additionally, as expected, the \textit{manual\_model} gives good results while being stable.
%
%
%
% Quantitatively, for any mode, the PTS case exhibits faster convergence and higher average rewards compared to the NTS case ($\mathbf{C_3}$), validating the hypothesis that organizational constraints can enhance learning efficiency ($\mathbf{C_2}$). Additionally, we verify the standard deviation to decrease from NTS to PTS and from PTS to FTS $(\mathbf{C_4})$.
%
% The FTS case, serving as a reference, consistently achieves the highest rewards due to pre-defined optimal policies. We also verify the constraint respect to be fully covered in \textit{correct} and \textit{correct\_policy} modes but the \textit{penalize} mode ($\mathbf{C_5}$). Finally, we did not see scalability difficulties when adding up to 30 agents and obstacles for all cases encountered ($\mathbf{C_6}$). From the experiments conducted, the \textit{penalize} mode better faces scalability because it updates its internal policy using optimized computations while \textit{correct} and \textit{correct\_policy} require an aside correction function that may impact the overall computational performance.
%
% The experiments demonstrate that PRAHOMT significantly impacts agent behavior and learning efficiency. The organizational models, especially the \textit{circle\_model}, effectively guide agents towards efficient collective strategies, as evidenced by both qualitative observations and quantitative metrics.
%
The qualitative analysis shows that the \textit{circle\_model} induces observable circling behaviors around prey, confirming the impact of organizational specifications on agent actions
%
\footnote{A visual animated representation is provided in \url{https://drive.google.com/file/d/1fT8gDo91mraRU5Wq8s_GBtP_YxM4lBfr/view?usp=sharing}}
%
. This visual representation shows alignment with expected collective strategies ($\mathbf{C1}$). The \textit{manual\_model} also exhibited consistent further demonstrating the robustness of predefined policies.

From a quantitative perspective, the metrics provide compelling evidence for the advantages of applying organizational constraints. In the PTS case, agents exhibited faster convergence compared to the NTS case across all modes, confirms that organizational constraints can accelerate the learning ($\mathbf{C2}$). The FTS case, which utilized hand-crafted policies, achieved immediate convergence, as expected, due to the absence of learning.

The average reward metrics reveal that agents guided by the \textit{circle\_model} (PTS) consistently outperformed those in the unconstrained scenario (NTS), achieving higher rewards ($\mathbf{C3}$). This indicates that organizational constraints not only improve learning efficiency but also enhance overall performance. The FTS case, leveraging optimal policies, consistently achieved the highest rewards, highlighting the effectiveness of well-defined constraints. We also noticed that the \textit{penalize} mode shows slightly better average rewards than \textit{correct} and \textit{correct\_policy} modes. Indeed, \textit{penalize} may authorize not satisfy some organizational specifications if it increases the overall reward.

The standard deviation of rewards, an indicator of performance variability, decreased progressively from NTS to PTS and from PTS to FTS ($\mathbf{C4}$). This reduction in variability suggests that organizational constraints contribute to more stable and consistent agent behavior. Lower standard deviations in the FTS case underscore the stability imparted by predefined policies. Again, the \textit{penalize} mode shows a slightly higher deviation because the policy is not strongly constrained by organizational specifications.

Constraint respect, assessed as the adherence to organizational rules, was fully met in the \textit{correct} and \textit{correct\_policy} modes, but not in the \textit{penalize} mode ($\mathbf{C5}$). This demonstrates that while agents can learn to follow constraints effectively, the constraint mode plays a crucial role in their adherence.

Scalability, evaluated by increasing the number of agents and obstacles, was effectively handled in all cases ($\mathbf{C6}$). The \textit{penalize} mode demonstrated superior scalability, as it incorporates optimized computations for policy updates, in contrast to the aside correction functions required by the \textit{correct} and \textit{correct\_policy} modes, which can impact computational performance.

In summary, the experimental results show that PRAHOMT enhances both agent behavior and learning efficiency. The \textit{circle\_model} successfully guides agents towards efficient collective strategies, as evidenced by the observed behaviors and quantitative metrics. These findings underscore the utility of organizational constraints in MARL environments and is a first step towards a framework for future research and application.


\section{Conclusion}\label{sec:conclusion}

This paper proposes to see the design of a MAS as a problem under constraint where agents' policies are to be optimized. To integrate the constraints, we proposed the PRAHOMT algorithm to augment the MARL framework with the $\mathcal{M}OISE^+$ model. PRAHOMT constrains agents' training according to organizational specifications. We evaluated PRAHOMT using our proposed proof of concept (PoC) implementation for the \textquote{Simple World Comm} predator-prey environment. Two organizational models were established: a minimally constrained model, \textquote{circle\_model}, and a fully constrained one, \textquote{manual\_model}. We conducted an evaluation of the fully-learned, fine-tuned, or predefined policies based on performance criteria in or after training.
%
The results show that \textquote{circle\_model} provides a relevant tradeoff between constraints and free learning. Despite comprising simple predefined rules, it effectively complements learned behavior.

In addition to constraining agents according to organizational specifications, we also aim to integrate explainability mechanisms. We intend to characterize and curate relevant emergent strategies to include as new organizational constraints in future training. The idea of iterative, mutually beneficial improvement between constrained training and explainability could greatly benefit from hierarchical learning, which helps better characterize and reveal strategies during learning. Furthermore, the initial results obtained with LLM suggest it as a promising complementary tool for PRAHOMT, potentially offering new avenues for explaining collective behavior, especially in scenarios where environments are not visually or intuitively representable.


\section*{References}

\bibliographystyle{splncs04}

\bibliography{references}

\newpage

\section*{Annexes}

\subsection*{Partial Relation between Agents' History and Organizational Model (PRAHOM) Algorithm}
\input{algos/PRAHOM_A.tex}

\subsection*{Constraining the action decision-making process dynamically enables constraining policies}
\input{proofs/jpc_to_ac.tex}

\end{document}