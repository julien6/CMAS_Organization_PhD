\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage[inline, shortlabels]{enumitem}
\usepackage{tabularx}
\usepackage{caption}
\usepackage{titlesec}
\usepackage[T2A,T1]{fontenc}
\usepackage[english]{babel}
\captionsetup{font=it}
\usepackage{ragged2e}
\usepackage{hyperref}
\addto\extrasenglish{%
  \renewcommand{\sectionautorefname}{Section}%
  \renewcommand{\subsectionautorefname}{Subsection}%
  \renewcommand{\subsubsectionautorefname}{Subsubsection}%
  \renewcommand{\tableautorefname}{Table}%
  \renewcommand{\figureautorefname}{Figure}%
}
\usepackage{pifont}
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%
\usepackage{footmisc}
\usepackage{multirow}

% --- Tickz
\usepackage{physics}
\usepackage{amsmath}
\usepackage{tikz}
\usepackage{mathdots}
\usepackage{yhmath}
\usepackage{cancel}
\usepackage{color}
\usepackage{siunitx}
\usepackage{array}
\usepackage{multirow}
\usepackage{amssymb}
\usepackage{gensymb}
\usepackage{tabularx}
\usepackage{extarrows}
\usepackage{booktabs}
\usetikzlibrary{fadings}
\usetikzlibrary{patterns}
\usetikzlibrary{shadows.blur}
\usetikzlibrary{shapes}

% ---------

\usepackage{pdfpages}
\usepackage{booktabs}
\usepackage{csquotes}
\usepackage{lipsum}  
\usepackage{arydshln}
\usepackage{smartdiagram}
\usepackage[inkscapeformat=png]{svg}
\usepackage{textcomp}
\usepackage{tabularray}\UseTblrLibrary{varwidth}
\usepackage{xcolor}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\usepackage{cite}
\usepackage{amsmath}
\newcommand{\probP}{\text{I\kern-0.15em P}}
\usepackage{etoolbox}
\patchcmd{\thebibliography}{\section*{\refname}}{}{}{}

\setlength{\extrarowheight}{2.5pt}

% \renewcommand{\arraystretch}{1.7}

% \setlength{\extrarowheight}{2.5pt}
% \renewcommand{\arraystretch}{0.2}
% \renewcommand{\arraystretch}{1.7}

% --------------
\titleclass{\subsubsubsection}{straight}[\subsection]

\newcounter{subsubsubsection}[subsubsection]
\renewcommand\thesubsubsubsection{\thesubsubsection.\arabic{subsubsubsection}}
\renewcommand\theparagraph{\thesubsubsubsection.\arabic{paragraph}} % optional; useful if paragraphs are to be numbered

\titleformat{\subsubsubsection}
  {\normalfont\normalsize\bfseries}{\thesubsubsubsection}{1em}{}
\titlespacing*{\subsubsubsection}
{0pt}{3.25ex plus 1ex minus .2ex}{1.5ex plus .2ex}

\makeatletter
\renewcommand\paragraph{\@startsection{paragraph}{5}{\z@}%
  {3.25ex \@plus1ex \@minus.2ex}%
  {-1em}%
  {\normalfont\normalsize\bfseries}}
\renewcommand\subparagraph{\@startsection{subparagraph}{6}{\parindent}%
  {3.25ex \@plus1ex \@minus .2ex}%
  {-1em}%
  {\normalfont\normalsize\bfseries}}
\def\toclevel@subsubsubsection{4}
\def\toclevel@paragraph{5}
\def\toclevel@paragraph{6}
\def\l@subsubsubsection{\@dottedtocline{4}{7em}{4em}}
\def\l@paragraph{\@dottedtocline{5}{10em}{5em}}
\def\l@subparagraph{\@dottedtocline{6}{14em}{6em}}
\makeatother

\setcounter{secnumdepth}{4}
\setcounter{tocdepth}{4}
% --------------


\newcommand{\before}[1]{\textcolor{red}{#1}}
\newcommand{\after}[1]{\textcolor{green}{#1}}

\newcommand{\old}[1]{\textcolor{orange}{#1}}
\newcommand{\rem}[1]{\textcolor{red}{#1}}
\newcommand{\todo}[1]{\textcolor{orange}{\newline \textit{\textbf{TODO:} #1}} \newline \newline }

\makeatletter
\newcommand{\linebreakand}{%
  \end{@IEEEauthorhalign}
  \hfill\mbox{}\par
  \mbox{}\hfill\begin{@IEEEauthorhalign}
}
\makeatother




% ---------------------------


\begin{document}

\title{Streamlining Resilient Kubernetes Autoscaling with Multi-Agent Systems via an Automated Online Design Framework\\
    % {\footnotesize \textsuperscript{Note}}
    % \thanks{Identify applicable funding agency here. If none, delete this.}
}

% \IEEEaftertitletext{\vspace{-1\baselineskip}}

\author{

    \IEEEauthorblockN{Julien Soulé}
    \IEEEauthorblockA{\textit{Thales Land and Air Systems, BU IAS}}
    %Rennes, France \\
    \IEEEauthorblockA{\textit{Univ. Grenoble Alpes,} \\
        \textit{Grenoble INP, LCIS, 26000,}\\
        Valence, France \\
        julien.soule@lcis.grenoble-inp.fr}

    \and

    \IEEEauthorblockN{Jean-Paul Jamont\IEEEauthorrefmark{1}, Michel Occello\IEEEauthorrefmark{2}}
    \IEEEauthorblockA{\textit{Univ. Grenoble Alpes,} \\
        \textit{Grenoble INP, LCIS, 26000,}\\
        Valence, France \\
        \{\IEEEauthorrefmark{1}jean-paul.jamont,\IEEEauthorrefmark{2}michel.occello\}@lcis.grenoble-inp.fr
    }

    % \and

    % \IEEEauthorblockN{Michel Occello}
    % \IEEEauthorblockA{\textit{Univ. Grenoble Alpes,} \\
    % \textit{Grenoble INP, LCIS, 26000,}\\
    % Valence, France \\
    % michel.occello@lcis.grenoble-inp.fr}

    % \and

    \linebreakand

    \hspace{-0.5cm}
    \IEEEauthorblockN{Paul Théron}
    \IEEEauthorblockA{
        \hspace{-0.5cm}
        \textit{AICA IWG} \\
        \hspace{-0.5cm}
        La Guillermie, France \\
        \hspace{-0.5cm}
        %lieu-dit Le Bourg, France \\
        paul.theron@orange.fr}

    \and

    \hspace{0.5cm}
    \IEEEauthorblockN{Louis-Marie Traonouez}
    \IEEEauthorblockA{
        \hspace{0.5cm}
        \textit{Thales Land and Air Systems, BU IAS} \\
        \hspace{0.5cm}
        Rennes, France \\
        \hspace{0.5cm}
        louis-marie.traonouez@thalesgroup.com}}


\maketitle

\begin{abstract}
  In cloud-native critical systems relying on complex Kubernetes clusters with interdependent services, poor workload management can jeopardize cluster availability, creating failures possibly exploitable by attackers. Such failures include resource blocking, bottlenecks, and continuous pod crashes. Conventional Horizontal Pod Autoscaling (HPA) approaches often fall short in such dynamic environments, while reinforcement learning-based ones, though more adaptable, typically focus on a single latency or resource minimization objective without explicitly addressing all known failures.
  A Multi-Agent System (MAS) enables resilient Kubernetes HPA by decomposing the availability maximization objective into failure-related sub-objectives delegated to agents. We streamline the generation of such MASs through an online automated framework in four phases: (1) modeling the cluster as a simulation from collected real cluster traces; (2) training agents in simulation, partially guided by roles and missions incorporating knowledge of failures; (3) optionally analyzing the trained agents' behaviors; and (4) transferring the learned behaviors to the real cluster.
  Experimental results show that the generated MASs are original and outperform eight HPA systems as for availability under two clusters with adversarial scenarios.
\end{abstract}

\begin{IEEEkeywords}
    cyberdefense, MARL, Digital Twins, formal
\end{IEEEkeywords}

\section{Introduction}
\label{sec:introduction}

% Contexte
Cloud-native critical systems are increasingly reliant on Kubernetes to orchestrate and manage interdependent services~\cite{Pahl2019}. Horizontal Pod Autoscaling (HPA) is a widely adopted mechanism to dynamically adjust the number of pods based on resource usage, enabling systems to handle highly dynamic workloads~\cite{Hohpe2012}. However, failures such as pod crashes, resource contention, and bottlenecks can severely jeopardize cluster availability~\cite{Burns2016}. Worse, these failures may be exploited by attackers to degrade performance or induce outages, as seen in adversarial contexts like Distributed Denial-of-Service (DDoS) attacks~\cite{Koller2019}.

In such adversarial scenarios, malicious actors often exploit scaling mechanisms, exposing the limitations of conventional HPA systems~\cite{Kim2020}. Modern approaches have sought to address these gaps using Reinforcement Learning (RL), where an agent optimizes a single global objective such as minimizing latency or resource usage~\cite{Nguyen2019}. While these methods demonstrate adaptability, they often fall short in handling diverse failure scenarios~\cite{Castro2020}. For example, prioritizing responses to cascading pod crashes during an attack may be far more critical than reducing latency under normal conditions. These challenges highlight the need for an autoscaling system capable of dynamically balancing multiple sub-objectives to maximize availability.

Achieving this shift from single-objective optimization to a multi-objective approach is complex, particularly in distributed and dynamic Kubernetes environments~\cite{Shoham2008}. A single-agent RL system struggles to address such complexity due to the difficulty of coordinating responses to diverse and context-dependent failures~\cite{Jennings1998}. In contrast, Multi-Agent Systems (MASs) offer a promising paradigm by decomposing the overarching availability maximization goal into sub-objectives handled by specialized agents~\cite{Shoham2008}. Each agent can collaboratively contribute to complementary scaling actions, enabling more resilient and context-specific responses~\cite{Jennings1998}. We refer to the set of these collaborative agents as an HPA MAS. An HPA MAS actually builds upon the cyberdefense framework of Autonomous Intelligent Cybersecurity Agents (AICAs), where agents with specialized roles and missions collaboratively defend systems against adversarial threats~\cite{Kott2018}.

% Problématique
However, designing HPA MASs tailored to Kubernetes clusters presents significant challenges. These include the need for detailed cluster knowledge, the time-consuming nature of manual design processes, and the difficulty of ensuring optimal agent behavior. Additionally, significant cluster changes often necessitate repeating the design process, increasing operational costs and complexity.

% Contribution
Among methdological works, we inspired from the \textit{Assisted MAS Organization Engineering Approach} (AOMEA)~\cite{soule2024aomea} that shows to align the most with automation and safety challenges. Based on AOMEA, we address these limitations proposing the \textit{Kubernetes Autoscaling with Resilient Multi-Agent system} (KARMA). This framework automates the design and implementation process through four sequential phases:
\begin{enumerate}
    \item \textbf{Modeling}: Creating a digital twin of the cluster from real-world traces to simulate workload dynamics and failure scenarios.
    \item \textbf{Training}: Training agents in simulation using roles and missions that integrate explicit rule-based strategies and guidance during training.
    \item \textbf{Analyzing}: Validating trained agents' behaviors and extracting design insights through empirical analysis.
    \item \textbf{Transferring}: Deploying trained agents to the real cluster, where they apply their learned behaviors via the Kubernetes API.
\end{enumerate}

This framework enables to iteratively updates the simulation model with newly collected traces, enabling adaptation to cluster changes. We validated our approach on two Kubernetes-based environments, "Online Boutique" and "Chained Service", under adversarial scenarios. The MASs were generated with minimal manual intervention, and demonstrate originality and robustness. They consistently outperformed state-of-the-art HPA systems, including AWARE~\cite{AWARE}, Gym-HPA~\cite{GymHPA}, IMAM~\cite{IMAM}, and Libra~\cite{Libra}, in maximizing availability across all evaluated scenarios.

% Organisation
The remainder of this paper is structured as follows:
\autoref{sec:related_work} reviews existing HPA techniques and their limitations in dynamic environments.
\autoref{sec:proposed_approach} details our framework leveraging related concepts for each phase.
\autoref{sec:experiments} describes the experimental setup.
\autoref{sec:results} presents and discusses results.
\autoref{sec:discussion} concludes and provides future directions.

\section{Related Work}
\label{sec:related_work}

\begin{table*}[h!]
  \centering
  \caption{Comparative Study of Autoscaling Systems for Kubernetes HPA}
  \label{tab:autoscaling_criteria}
  \renewcommand{\arraystretch}{1}
  \resizebox{\textwidth}{!}{%
  \begin{tabular}{>{\raggedright\arraybackslash}m{2.5cm}>{\centering\arraybackslash}m{1.5cm}>{\centering\arraybackslash}m{1.5cm}>{\centering\arraybackslash}m{1.5cm}>{\centering\arraybackslash}m{1.5cm}>{\centering\arraybackslash}m{1.5cm}>{\centering\arraybackslash}m{1.5cm}>{\centering\arraybackslash}m{1.5cm}>{\centering\arraybackslash}m{1.5cm}>{\centering\arraybackslash}m{1.5cm}}
  \hline
  \textbf{Criterion} & \textbf{Gym-HPA~\cite{GymHPA}} & \textbf{AWARE~\cite{AWARE}} & \textbf{IMAM~\cite{IMAM}} & \textbf{Libra~\cite{Libra}} & \textbf{QoS-Aware RL~\cite{QoSRL}} & \textbf{AHPA~\cite{AHPA}} & \textbf{KOSMOS~\cite{KOSMOS}} & \textbf{COPA~\cite{COPA}} & \textbf{KARMA} \\
  \hline
  \hline
  Adversarial Scenarios Considered & No & Partial & No & No & No & No & No & Partial & Yes \\
  \hline
  Multi-objective Support & No & Yes & Partial & Indirect & Yes & No & Yes & No & Yes \\
  \hline
  Global Automation Level & High & Middle & Middle & Middle & High & Middle & Middle & Middle & High \\
  \hline
  Learning & Yes & Yes & Yes & Yes & Yes & No & No & No & Yes \\
  \hline
  Multi-Agent System Considered & No & No & Yes & No & No & No & No & No & Yes \\
  \hline
  Simulated Environment & Yes & No & Yes & Yes & Yes & No & No & No & Yes \\
  \hline
  Real Environment & No & Yes & Yes & Yes & Yes & Yes & Yes & Yes & Yes \\
  \hline
  Explainability Considered & No & No & No & No & No & No & No & No & Yes \\
  \hline
  Adaptation Level & High & Middle & High & Middle & Middle & High & High & Middle & High \\
  \hline
  Safety Guarantees & No & No & No & No & No & No & No & No & Yes \\
  \hline
  \end{tabular}%
  }
\end{table*}

Autoscaling in Kubernetes has traditionally relied on metrics-based approaches, such as the default Kubernetes Horizontal Pod Autoscaler (KHPA), which adjusts the number of pods based on CPU and memory utilization~\cite{KubernetesChallenges}. While effective for basic scaling, such methods fail to address dynamic or adversarial workloads, as they rely on reactive, threshold-based rules~\cite{AutoscalingLimitations}. To overcome these limitations, recent research has turned to Machine Learning (ML) and RL.

\subsection*{Reinforcement Learning-Based Systems}
Among RL-based approaches, four systems stand out due to their innovation, applicability, and relevance:

\begin{itemize}
    \item \textbf{Gym-HPA}~\cite{GymHPA} serves as a benchmark RL environment, enabling experimentation with various RL algorithms. It excels in adaptability to simulated workloads with a high degree of automation but lacks multi-objective support, explainability, and real-world applicability.
    \item \textbf{AWARE}~\cite{AWARE} incorporates RL to optimize autoscaling decisions while balancing quality-of-service (QoS) objectives, such as response time and throughput. It partially considers adversarial scenarios but struggles with high automation levels and multi-agent coordination.
    \item \textbf{IMAM}~\cite{IMAM} integrates RL with a multi-agent framework, making it highly adaptive in microservice-based architectures. However, it lacks safety guarantees and explainability, limiting its effectiveness in adversarial contexts.
    \item \textbf{Libra}~\cite{Libra} introduces traffic-aware scaling in edge environments. While it provides valuable insights into traffic optimization, it lacks consideration for multi-objective trade-offs or safety-critical guarantees.
\end{itemize}

These systems highlight significant progress in RL-based autoscaling but share common limitations: a lack of comprehensive adversarial adaptability, limited support for multi-agent systems, and no explicit focus on explainability or safety guarantees.

\subsection*{Hybrid and Rule-Based Approaches}
Other notable systems combine ML or rule-based strategies with traditional autoscaling:

\begin{itemize}
    \item \textbf{QoS-Aware RL}~\cite{QoSRL} focuses on maintaining QoS under dynamic workloads but does not integrate seamlessly with Kubernetes-native features or consider adversarial scenarios.
    \item \textbf{AHPA}~\cite{AHPA} and \textbf{KOSMOS}~\cite{KOSMOS} explore adaptive and combined vertical-horizontal scaling strategies, offering high adaptability but lacking learning capabilities.
    \item \textbf{COPA}~\cite{COPA} emphasizes combined metrics-based autoscaling but remains reactive and limited in adversarial scenarios.
\end{itemize}

\subsection*{Positioning Our Contribution}
KARMA addresses the gaps identified in existing work by introducing KARMA, an online automated design framework, that aims to produce efficient HPA MAS.

specifically designed to handle adversarial scenarios. \autoref{tab:autoscaling_criteria} provides a comparative overview of key Kubernetes HPA systems with our contribution. We position our contribution through following features:
\begin{itemize}
    \item \textbf{Adversarial Scenarios}: the presence of a set of agents aiming to minimize the cluster's availability refered to as attackers while a set of agents aim to maximize availability handling attacks where conventional autoscaling systems are poorly suited;
    \item \textbf{Resilience-based Multi-Objective}: the availability maximization which can be defined as the capability of a cluster to efficiently handle user requests at all times while meeting predefined Quality of Service (QoS) in a multi-objective way~\cite{varghese2019challenges}. KARMA decomposes complex objectives (e.g., availability maximization) into agents' sub-objective enabling flexible failure-specific handling;
    \item \textbf{Engineering Automation}: the systematic use of methodological tools to autonomously design, implement, validate, and deploy MASs with minimal human intervention meeting design requirements~\cite{weyns2020engineering}. KARMA uses digital twin modeling and automated MAS generation with MARL;
    \item \textbf{Explainability}: the ability of the agents to make decisions at individual or organizational levels that can be understood or expected by humans~\cite{Chakraborti2019Explicability}. KARMA integrates inference of roles and related information inference and related information to explain organizational mechanisms and individual agent behaviors; 
    \item \textbf{Constraint Satisfaction}: the probability that sequences of actions made by agents do not lead to undesired states which is zero for hard constraints and non-zero for soft ones~\cite{wooldridge2009introduction}. KARMA enables soft and hard constraints to be incorporated in agents through roles, possibly enabling safety guarantees;
    \item \textbf{Adaptability}: the ability for a MAS to remain available under changing environmental conditions~\cite{wooldridge2009introduction}. Continuous adaptation to workload variations through iterative updates of the digital twin.
\end{itemize}


\section{KARMA: A Framework for MAS-based Kubernetes HPA}
\label{sec:proposed_approach}

This section introduces the KARMA framework to help in designing a HPA MAS.

\subsection{KARMA Overview}

\begin{figure}[h!]
    \centering
    \input{figures/karma_architecture/karma_architecture.tex}
    \caption{Overview of the KARMA framework in use with a Kubernetes cluster}
    \label{fig:karma_architecture}
\end{figure}

As illustrated in \autoref{fig:karma_architecture}, the KARMA framework operates alongside the Kubernetes cluster, which comprises \textbf{worker nodes} hosting \textbf{pods} that are the atomic unit of work in Kubernetes containing \textbf{containers} that run the actual processes. Pods are labeled into \textbf{services} and managed by \textbf{deployments}.
% TODO: est-ce que ce paragraphe est correct ?
%
The KARMA framework is deployed as an aside software process connected to both the Kubernetes API and Prometheus'one. It consists of four primary components which operate iteratively to optimize the system's scaling policies. The interaction between the Kubernetes cluster and the KARMA framework is overviewed below:

\textbf{1)} \textbf{Metrics Collection:} Metrics related to availability (such as CPU, memory usage, or failed pod number) are continuously collected as states by \textit{Prometheus}~\cite{prometheus}, a widely adopted time-series metrics database, and sent to KARMA's \textbf{Modeling Component}.

\textbf{2)} \textbf{Modeling:} These state are used to construct a \textit{digital twin} of the cluster as a near-realistic state transition function. The reward function is defined as a linear combination of reward sub-function each targeting a known Quality of Service (QoS). Then, we couple the defined reward function for availability to the modeled transition function within a MARL-compatible environment using the PettingZoo library~\cite{Terry2021}.

\textbf{2)} \textbf{Agent Training:} The \textbf{Training Component} uses the modeled environment to train MARL agents, which aim to maximize the cumulative rewards. The reward function encourages scaling policies that improve availability while minimizing resource consumption and latency. Agents are optionally assigned \textit{roles} (rule-based constraints to guide policy exploration) and \textit{missions} (incremental objectives to aid policy convergence), following the methodology proposed by AOMEA~\cite{soule2024aomea}.
    
\textbf{3)} \textbf{Policy Analysis and Explainability:} The \textbf{Analyzing Component} evaluates the behavior of trained agents using trajectory clustering and hierarchical visualization techniques. This ensures that the learned policies are interpretable, aligned with system objectives, and resilient under dynamic workloads or adversarial conditions.
    
\textbf{4)} \textbf{Policy Deployment:} The \textbf{Transfer Component} deploys the trained agents' policies to the real Kubernetes cluster. It sends scaling actions, such as adjusting the number of replicas in a deployment, to the Kubernetes API. These actions are executed in the cluster, completing the iterative feedback loop.

KARMA seamlessly integrates simulation-based learning with real-world Kubernetes operations. Metrics from the cluster inform the policy refinement in the \textbf{Modeling Component}, while scaling decisions derived from trained agents are applied back to the cluster. This closed-loop process ensures continuous improvement, adaptability to workload variations, and robustness against adversarial scenarios.

In the next sub-sections, we will detail each phase of the KARMA framework, including the underlying theoretical concepts and their technical implementation.



\subsection{Phase 1: Modeling}
% Modeling:
%  - observation
%  - actions
%  - rewards -> Faire une récompense qui englobe toutes les QoS ayant pour père la QoS (ne pas faire encore référence aux fonctions de récompense pour l'instant) "availability"
%  - transition -> Transition modeling + MLP -> donner détails des paramètres

% Description haut niveau
% The modeling phase in the KARMA framework constructs a digital twin of the Kubernetes cluster by leveraging real-world set of state transitions refered to as trajectories, histories, or traces. We envision the environment considering an attacker trying to leverage actions related to data alteration and sheer amount of data to provoke various failures in the cluster, while three to four defender agents would try to manage pod replica in order to minimize the attacker's impact.

This phase uses collected traces to provide a near-realistic simulation environment where the defender agents' policies and the attacker policy can be safely tested and optimized before transfering in the real system. Inspired by the Gym-HPA framework~\cite{GymHPA}, the modeling component employs a state transition model for which we propose to add a deep learning-based approximation of cluster dynamics.

% Formalisation
Even though AOMEA suggests using \textbf{Decentralized Partially Observable Markov Decision Process} (Dec-POMDP) to formalize the environment, it is over-dimmensioned since the environment is fully observable. Therefore, we formalize the environment as a zero-sum \textbf{Stochastic Game (SG)}~\cite{shapley1953stochastic}, which is an extension of the \textbf{Markov Decision Process} (MDP) to multi-agent interactions. The SG is characterized by the tuple $\mathcal{SG} = (\mathcal{A}, S, A, T, R, \gamma)$, where:

\begin{itemize}
  \item $\mathcal{A} = \{\mathcal{A}_d, \mathcal{A}_a\}$ is the agents set comprising $n = |\mathcal{A}_d|$ defender agents and one single attacker agent in $\mathcal{A}_a$;
  \item $S$ is the shared state space characterized by metrics values for both attacker and defender agents;
  \item $A = A_d^n \times A_a$ is the action space with $A_d$ and $A_a$ are the action spaces for a defender and the attacker agents respectively;
  \item $T: S \times A \rightarrow S = S \times A_d^n \times A_a \to S$ is the state transition function dictating the next state when the joint actions of the defender agents and the attacker's one are applied in the environment;
  \item $R = \{R_d, R_a\}$, where $R_d: S \times A_d^n \to \mathbb{R}$ and $R_a: S \times A_a \to \mathbb{R} = - R_d$ are the reward functions for the defender agents and the attacker one respectively;
  \item $\gamma \in [0, 1)$ is the discount factor for future rewards.
\end{itemize}

The following paragraphs detail each component of this formalisation.\\

\noindent \paragraph{\textbf{State Space}} The state of the Kubernetes cluster at time $t$, denoted as $s \in S$, consists of metrics critical to characterizing system performance for each of the $d = |D|$ micro-service deployments:
$$
s = (n_{id}, d_{dep}, d_{des}, d_{err}, d_{rem}, r_{cpu}, r_{ram}, t_{in}, t_{out})^d
$$
$n_{id} \in \mathbb{N}$: the number of the deployment; \quad
$d_{dep} \in \mathbb{N}$: the number of deployed pods; \quad 
$d_{des} \in \mathbb{N}$: the number of desired pods; \quad
$d_{err} \in \mathbb{N}$: the number of failed pods; \quad
$d_{rem} \in \mathbb{N}$: the number of remaining requests to be processed in the queue; \quad
$r_{cpu} \in \mathbb{R}$: the total aggregated CPU (in m) of the pods; \quad
$r_{ram} \in \mathbb{R}$: the total aggregated memory (in Mi) of the pods; \quad
$t_{in} \in \mathbb{R}$: the average received traffic (in Kbps); \quad
$t_{out} \in \mathbb{R}$: the average transmitted traffic (in Kbps).
% TODO: ajouter les métriques spécifiques pour pouvoir vérifier s'il y a des goulots d'étranglement, 

These metrics are continuously collected using Prometheus~\cite{prometheus}, a widely adopted monitoring and metrics database system, enabling the capture of time-series data for each pod, deployment, and the cluster as a whole. 

\

\noindent \paragraph{\textbf{Action Space}} The action spaces for the defender and the attacker are distinct:\\
\begin{itemize}
    \item \textbf{Defender's Actions:} 
\end{itemize}
$$
a_d \in A_d = (\text{service\_id}, \text{replica\_change}) \text{, with}
$$
$\text{\textbf{service\_id}} \in \mathbb{N}$ identifies the target service (through deployment), and $\text{\textbf{replica\_change}} \in [-\alpha, +\alpha]$ indicates the change in replicas (by default $\alpha = 3$). Actions from this space are one-hot encoded as a Box Gym Space~\cite{openAIGymActionSpaces}: for example, the defender actions $(2,1)$, $(0,-2), (1,0)$ mean the services with id numbers equal to $2$, $0$, and $1$ have their respective replica numbers changed by adding $1$, $-2$, $0$.\\

\begin{itemize}
    \item \textbf{Attacker's Actions:} 
\end{itemize}
$$
a_a \in A_a = (\text{entry\_point\_id}, \text{rate\_change}, \text{data\_change}) \text{, with}
$$
$\text{\textbf{entry\_point\_id}} \in \mathbb{N}$ specifies the service entry point;
$\text{\textbf{rate\_change}} \in \{\text{high\_decrease}, \text{low\_decrease}, \text{no\_change}, \allowbreak \text{low\_increase}, \allowbreak \text{high\_increase}\}$ changes the incoming traffic based on a factor $\kappa$ (by default $\kappa = 1$); and $\text{\textbf{data\_change}} \in \{\text{no\_alteration}, \allowbreak \text{low\_alteration}, \allowbreak \text{high\_alteration}\}$ specifies the degree of data alteration based on factor $\sigma \in [2,\infty[$ (by default $\sigma = 10$). Actions from this space are one-hot encoded as a Box Gym space: for example, the attacker actions $(0,1,2), (2,-1,0), (1,2,1)$ mean that entrypoint services id number 0, 2, and 1 would have their respective traffic-in rates increased by $1 \times \kappa, -1 \times \kappa, 2 \times \kappa$, and respective probabilities to crash due to data alteration are changed by $\frac{2}{\sigma}, \frac{0}{\sigma}, \frac{1}{\sigma}$.

\

\noindent \paragraph{\textbf{Reward Functions}} The availability of a Kubernetes cluster can be defined as the proportion of user requests successfully processed within an acceptable time frame relative to the total number of received requests. This definition relies on specific metrics collected in the Kubernetes environment and can be expressed mathematically.
To measure availability, we use the following metrics:\\

\noindent $
\text{Success Rate} = \frac{\text{Number of Successful Requests (2xx)}}{\text{Total Number of Received Requests}}
$

\noindent $
\text{Pod Failure Rate} = \frac{\text{Number of Failed Pods}}{\text{Total Number of Deployed Pods}}
$

\noindent $
\text{Latency Ratio} = \frac{\text{Measured Latency}}{\text{Maximum Acceptable Latency}}
$

\noindent $
\text{Entry Point Availability} = \frac{\text{Number of Available Entry Points}}{\text{Total Number of Entry Points}}
$

\noindent $
\text{Traffic Capacity Ratio} = \min\left(1, \frac{\text{Outgoing Traffic}}{\text{Expected Traffic}}\right)
$

\

\noindent Then, the $R_d$ defender reward function relies on the overall availability while $R_a$ reward function is opposed:
\begin{itemize}
    \item \textbf{Defenders' Reward:}
    \begin{align*}
        R_d & = w_1 \cdot \text{Success Rate} \\
            & \ + w_2 \cdot (1 - \text{Pod Failure Rate}) \\
            & \ + w_3 \cdot (1 - \text{Latency Ratio}) \\
            & \ + w_4 \cdot \text{Entry Point Availability} \\
            & \ + w_5 \cdot \text{Traffic Capacity Ratio}
    \end{align*}
    where \(w_1, w_2, w_3, w_4, w_5\) are relative weights.
    \item \textbf{Attacker's Reward:}
    $$
    R_a = -R_d(s, a_d, a_a),
    $$
    incentivizing actions that degrade the cluster's availability.
\end{itemize}

\

\noindent \paragraph{\textbf{Transition Modeling}} Relying a representative set of collected transitions $\mathcal{T} = \langle(s, a_d^n, a_a, s')_{t\in \mathbb{N}, t \leq w}\rangle$ over a $w$ time window, we can form a partial state transition function $\hat{T}_t$ simply defined as:

$$
\hat{T}_t(s, a_d^n, a_a) =
\begin{cases}
s' & \text{if } (s, a_d^n, a_a, s') \in \mathcal{T}, \\
\emptyset & \text{otherwise}
\end{cases}
$$

To cover no recorded transition, we introduce a MLP-based approximator $\hat{T}_a$ to learn from collected transitions to predict the next likely state.
% TODO: ajouter les informations sur les hyper-paramètres : nombre de couche, dimmensions, fonction d'activation...
% TODO: justifier du choix d'un MLP pour approcher une fonction de transition d'état dans un cluster K8 -> explication: l'état suivant d'un cluster ne dépend que de l'état précédent et des actions choisies (l'état "porte" déjà en lui l'historique des agents)
%
The MLP approximates $\hat{T}_a$ by minimizing the Mean Squared Error (MSE):

$$
\mathcal{L} = \frac{1}{N} \sum_{i=1}^N \|T(s_i, a_{d,i}, a_{a,i}) - s'_i\|^2,
$$
where $N$ is the number of training samples.

Formally, the complete modeled transition function $\hat{T}$ is defined as:
$$
\hat{T}(s, a_d, a_a) = 
\begin{cases} 
\hat{T}_t(s, a_d, a_a) & \text{if } (s, a_d, a_a) \in \text{Domain}(\hat{T}_t), \\
\hat{T}_a(s, a_d, a_a) & \text{otherwise}.
\end{cases}
$$


\

\noindent \paragraph{\textbf{Digital Twin Environment}} The approximated transition function $\hat{T}$, combined with the reward functions $R_d$ and $R_a$, forms the basis of the digital twin environment implemented using the PettingZoo MARL library~\cite{Terry2021}. This environment models the Kubernetes cluster as an SG, enabling safe exploration of defender and attacker strategies.

By capturing the interactions between defenders and attackers, the modeling phase sets the stage for robust and adversary-aware policy optimization in Phase 2.


\subsection{Phase 2: Training}
% Training:
%  - introduire MOISE+MARL brièvement
%  - goals -> 1 objectif pour chaque QoS -> 1 fonction de récompense
%  - roles -> 1 rôle pour chaque QoS -> 1 ensemble de règles
%  - algorithms -> panel des algos proposés + fonctionnement de l'entrainement en MOISE+MARL
% ======================
% TODO: introduire brièvement MOISE+MARL puis donner les spécifications organisationnelles pour chacun des agents -> proposer des modèles applicables pour Chained Services:
% - 1 agent pour prioriser les pods (killer les pods qui prennent trop de temps)
% - 1 agent pour supprimer les goulots d'étanglement
% - 1 agent pour supprimer les pods qui crash continuellement
% - 1 agent pour detecter et réagir aux attaques DDoS / montées de charge subites
% ...
% TODO: Lister tous les problèmes à prendre en compte
% ======================
% - Revenir sur l'exemple de goulot d'étranglement que l'on pourra noter comme un graphe (à mettre dans la partie KARMA Overview).
% - Montrer comment on peut décomposer le problème de maximiser la disponibilité en plusieurs sous-objectifs (detection et résoudre goulot d'étranglement, detection et résoudre pod crash, detection et résoudre priorisation des pods...) que l'on définira formelement. Ces sous-objectifs peuvent être compris comme des missions associées aux agents auquels on donne aussi des rôles
% - Montrer que l'on peut combiner chacun des agents spécialisé de façon flexible pour prendre en charge les différentes failles de façon plus réactif et ciblé qu'avec un seul agent qui doit tout faire en même temps et pourrait diluer chacune des demandes en une seule action innéfficace -> montrer que cela est vrai sur un exemple au moins dans la partie experimentale.
% - Décrire les algos de RL utilisé pour chacun des agents et comment ils sont combinés entre eux dans l'esprit d'un SMA classique en fonction des priorités établies par les concepteurs et/ou en laissant un algo MARL custom composite déterminer quel serait le meilleur ordre d'execution des agents pour maximiser la disponibilité (on aura donc un genre d'orchestrateur qui pour un état associe un ordre de passage en essayant de reproduire des liens sociaux entre agents -> cet ensemble de liens sociaux peut être appris -> il forme le coeur du SMA nouvellement appris que l'on pourra expliciter plus tard) => en fait l'organisation entre agent est à explorer pour maximiser la récompense (qui est basée sur la disponibilité)

Leveraging the digital twin modeled, this phase applies MARL techniques, emphasizing convergence to stable, role-aligned, and efficient behaviors under organizational constraints.


\

\noindent\textbf{Role and Mission-Based Training:} 

Inspired by MOISE+MARL~\cite{MOISEMARL2025}, KARMA integrates role-based constraints and mission-oriented incentives into the training process:
\begin{itemize}
    \item \textbf{Role Constraints:} Agents are assigned roles dictating their permissible actions, defined by a Role Action Guide ($RAG$):
    \[
    RAG(h, \omega) = \{(a, \text{ch}) \;|\; a \in A, \; \text{ch} \in [0,1]\},
    \]
    where $h$ is the trajectory history, $\omega$ the observation, and $\text{ch}$ the constraint hardness. Hard constraints enforce strict role adherence, while soft constraints allow for exploratory actions.
    \item \textbf{Mission Incentives:} Missions are defined as collections of goals linked to rewards. Agents accrue bonuses for completing intermediate objectives, aligning their trajectories with predefined missions:
    \[
    GRG(h) = \sum_{m \in M} w_m \cdot \mathbf{1}[h \supseteq h_m],
    \]
    where $w_m$ is the weight for mission $m$, and $h_m$ represents a characteristic sub-sequence of $h$.
\end{itemize}
These role and mission structures reduce the policy search space, accelerating convergence and improving explainability.

\

\noindent\textbf{MARL Algorithms:} 
To train agents, KARMA supports state-of-the-art MARL algorithms implemented in the PettingZoo and MARLlib libraries:
\begin{itemize}
    \item \textbf{Policy-Based Methods:} Algorithms like MAPPO~\cite{MAPPO2021} optimize policies directly and are well-suited for environments requiring high stability and convergence rates.
    \item \textbf{Value-Based Methods:} Algorithms like Q-Mix~\cite{QMIX2018} approximate value functions for cooperative tasks, ensuring effective scaling actions across deployments.
    \item \textbf{Actor-Critic Methods:} MADDPG~\cite{MADDPG2017} combines policy optimization with value-based critics, facilitating decentralized execution with centralized training.
\end{itemize}
These algorithms are augmented with the digital twin, enabling agents to explore policy trajectories in a simulated environment, mitigating risks in the real cluster.

\

\noindent\textbf{Training Pipeline:} 
The training phase follows a structured pipeline:
\begin{enumerate}
    \item \textbf{Initialization:} Initialize agent policies $\pi_i$ and reward structures. Embed role constraints ($RAG$) and mission guides ($GRG$) into the environment.
    \item \textbf{Simulation Runs:} Run simulations in the digital twin environment to collect trajectories. Update policies using MARL algorithms.
    \item \textbf{Reward Shaping:} Incorporate mission bonuses and role penalties into the reward function to steer learning toward organizational objectives.
    \item \textbf{Evaluation:} Assess policy performance using metrics such as cumulative reward, convergence rate, and robustness to adversarial scenarios.
    \item \textbf{Refinement:} Retrain agents iteratively with adjusted hyperparameters or additional constraints, if necessary.
\end{enumerate}

\

\noindent\textbf{Metrics and Convergence:} 
Training effectiveness is evaluated through the following metrics:
\begin{itemize}
    \item \textbf{Cumulative Reward:} Reflects the overall policy effectiveness in achieving scaling goals.
    \item \textbf{Convergence Rate:} Measures the speed at which policies stabilize.
    \item \textbf{Organizational Fit:} Quantifies alignment between trained behaviors and predefined roles and missions.
\end{itemize}
Convergence is achieved when cumulative reward variance across episodes falls below a predefined threshold.

\

The training phase bridges the digital twin and real-world implementation, ensuring that policies are not only effective but also robust, explainable, and aligned with organizational goals.


\subsection{Phase 3: Analysis}
% Analyzing:
%  - hierarchical clustering pour détérminer roles pour les agents entrainés
%  - trajectory clustering pour déterminer objectifs intermediaires
%  - On s'interesse à expliciter les rôles, misisons (+ liens de acquointance, communication, autorité...) établies au cours de l'apprentissage entre agents
%  - On regarde si des agents peu contraints au niveau des rôles (ou pas de rôles) font émérger / partagent ou pas un ou des rôles communs nouveaux (par inférence)
%  - On regarde si des agents peu guidés par des misisons (ou pas) font emerger / partagent des missions nouvelles (par inférence)
%  - On peut déduire une bonne partie du reste du modèle MOISE+ une fois ces informations obtenues
%  - Dans les experimentations, on devra montrer que les modèles de SMA obtenus sont bien originaux et trouvés automatiquement et qu'ils sont efficace

The analysis phase in the KARMA framework validates and interprets the behaviors of trained agents to ensure alignment with organizational objectives and robustness in dynamic and adversarial environments. This phase leverages trajectory visualization, clustering techniques, and explainability tools to evaluate the efficiency, effectiveness, and safety of the learned policies.

\

\noindent\textbf{Trajectory Analysis:} 
Agent trajectories, defined as sequences of state-action pairs over episodes, provide a granular view of decision-making processes. Formally, the trajectory of an agent $i$ over an episode $e$ is represented as:
\[
\tau_i^e = \{(s, a_t, r_t)\}_{t=0}^{T},
\]
where $s$ is the state at time $t$, $a_t$ the action taken, $r_t$ the reward obtained, and $T$ the episode length.

Key metrics extracted from trajectories include:
\begin{itemize}
    \item \textbf{Action Frequency:} $f(a_t)$: Frequency distribution of actions to detect patterns or biases in scaling decisions.
    \item \textbf{Reward Distribution:} $\mathbb{E}[r_t]$: Expected reward across episodes to evaluate policy performance.
    \item \textbf{Divergence from Roles:} $\delta_R(\tau_i^e)$: Degree of deviation from predefined roles, calculated as:
    \[
    \delta_R(\tau_i^e) = \frac{1}{T} \sum_{t=0}^T \mathbf{1}[a_t \notin RAG(h_t, \omega_t)].
    \]
\end{itemize}

Trajectory visualization techniques, such as t-SNE or PCA, reduce high-dimensional state-action representations into interpretable 2D or 3D spaces. Clusters in this reduced space reveal behavioral patterns, deviations, or anomalies.

\

\noindent\textbf{Clustering for Behavior Validation:}
Hierarchical clustering techniques are applied to group similar trajectories based on distance metrics, such as Dynamic Time Warping (DTW). For a set of trajectories $\{\tau_i^e\}$, the clustering process involves:
\begin{enumerate}
    \item \textbf{Distance Matrix Calculation:} Compute pairwise distances $d(\tau_i, \tau_j)$ using DTW:
    \[
    d(\tau_i, \tau_j) = \min_{\pi} \sum_{k=1}^{|\pi|} \|s_{t_k}^i - s_{t_k}^j\|_2,
    \]
    where $\pi$ is the optimal alignment path.
    \item \textbf{Clustering Algorithm:} Apply agglomerative clustering to the distance matrix:
    \[
    \mathcal{C} = \text{AgglomerativeClustering}(d(\tau_i, \tau_j), k),
    \]
    where $k$ is the number of desired clusters.
    \item \textbf{Cluster Validation:} Evaluate clustering quality using silhouette scores or Davies-Bouldin indices:
    \[
    S = \frac{1}{n} \sum_{i=1}^n \frac{b(i) - a(i)}{\max(a(i), b(i))},
    \]
    where $a(i)$ is intra-cluster distance and $b(i)$ is inter-cluster distance.
\end{enumerate}

Clusters are annotated with common trajectory characteristics, such as frequent actions or reward trends, enabling targeted analysis of agent behaviors.

\

\noindent\textbf{Explainability through XMARL:}
Explainable Multi-Agent Reinforcement Learning (XMARL) techniques enhance the interpretability of trained policies, focusing on:
\begin{itemize}
    \item \textbf{Feature Attribution:} Techniques like SHAP~\cite{SHAP2020} quantify the contribution of each state feature to action decisions:
    \[
    \phi_{s}(a) = \mathbb{E}_{s' \sim S \setminus s} \left[ Q(s', a) - Q(s, a) \right].
    \]
    \item \textbf{Policy Simplification:} Extract interpretable rules from policies using Decision Trees:
    \[
    \pi(a|s) \approx \text{TreeClassifier}(s, a),
    \]
    where the tree splits on state metrics to mimic agent decision-making.
    \item \textbf{Causal Analysis:} Use causal inference frameworks to evaluate the impact of specific actions on long-term rewards:
    \[
    C(s, a_t) = \mathbb{E}[r_{t+T}|a_t, s] - \mathbb{E}[r_{t+T}|s].
    \]
\end{itemize}

These techniques allow human operators to validate policy decisions, ensuring alignment with system objectives and trust in deployment.

\

\noindent\textbf{Safety Guarantees:} 
The analysis phase also verifies adherence to safety constraints and organizational guidelines:
\begin{itemize}
    \item \textbf{Constraint Validation:} Ensure that all actions $a_t$ taken during training comply with role-based constraints:
    \[
    \mathbf{1}[a_t \notin RAG(h_t, \omega_t)] = 0, \quad \forall t.
    \]
    \item \textbf{Robustness Testing:} Simulate adversarial scenarios, such as DDoS attacks, to evaluate policy stability:
    \[
    R_{\text{adv}} = \sum_{t=0}^T \mathbf{1}[r_t < \epsilon],
    \]
    where $\epsilon$ is a minimum acceptable reward threshold.
    \item \textbf{Fairness Metrics:} Quantify fairness across agents to ensure balanced resource allocation:
    \[
    F = 1 - \frac{\sigma^2(R_{\text{agents}})}{\mu(R_{\text{agents}})},
    \]
    where $\sigma^2$ is the variance and $\mu$ the mean of agent rewards.
\end{itemize}

\

\noindent\textbf{Continuous Feedback Loop:}
Insights from the analysis phase feed back into the training process:
\begin{enumerate}
    \item Update reward structures based on trajectory evaluations.
    \item Refine role and mission definitions to reduce deviations.
    \item Adjust MARL algorithm parameters to improve convergence rates and stability.
\end{enumerate}

\

The analysis phase is critical to ensuring that the trained policies are not only effective but also explainable, safe, and aligned with the organizational objectives. By incorporating advanced validation and explainability tools, KARMA achieves a higher degree of reliability and trustworthiness in its MARL-based HPA framework.


\subsection{Phase 4: Transfer}
% Transfering: Processus de transfert des comportements appris au cluster réel.
%  - récupérer les politiques entrainés dans un sous-module
%  - lancer les politiques entrainés avec l'état courant collecté
%  - appliquer les actions choisies par ces politiques via l'API K8s


\section{Experimental Setup}
\label{sec:experiments}
% Experimental setup:
%  - Présenter "Chained Services" (enlever Online boutique)
%  - CybMASDE: présenter comme un moyen d'implémenter KARMA et présenter la configuration logicielle
%  - Configuration matérielle (pour entrainement et analyse)
%  - Protocole d'experimentation et d'analyse (prend en compte un espèce d'étude d'ablation dans les baselines)
%  - Métriques d'évaluation
%  - Baselines: MA x (Spec. Org.)
%     - modèle sans specifications organisationnelles avec 1 seul agent (état de l'art)
%     - modèle avec spécifications organisationnelles du modèle "fort" avec 1 seul agent (état de l'art?)
%     - modèle sans spécifications organisationnelles avec plusieurs agents (état de l'art?)
%     - modèle avec spécifications organisationnelles du modèle "faible" avec plusieurs agents
%     - modèle avec spécifications organisationnelles du modèle "fort" avec plusieurs agents
\subsection{Experimental Configuration}
% Environnement expérimental et outils utilisés.
\subsection{Test Scenarios}
% Présentation des scénarios utilisés pour l'évaluation.
\subsection{Evaluation Metrics}
% Protocole d'experimentation reproductible
\subsection{Experimental Protocol}



\section{Results and Discussion}
\label{sec:results}
% Results and Discussion:
%  - Résultats et comparaison sous forme de table
%  - Discussion des résultats : des points forts et points faibles (par rapport aux promesses initiales)

% Explication des métriques utilisées (ex : disponibilité, latence).
\subsection{Results and Comparisons}
% Résultats obtenus et comparaison avec les approches existantes.
\subsection{Discussion of Results}
% Analyse des performances et des points clés des résultats.



\section{Discussion}
\label{sec:discussion}
% Conclusion
%  - Résumé
%  - Résumé des Points faibles et Perspectives

\section*{References}

\nocite{alDhuraibi2017elasticDocker}

% \bibliographystyle{abbrv}
\bibliographystyle{IEEEtran}

\bibliography{references}

\end{document}
