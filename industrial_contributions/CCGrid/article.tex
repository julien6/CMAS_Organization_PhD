\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage[inline, shortlabels]{enumitem}
\usepackage{tabularx}
\usepackage{caption}
\usepackage{titlesec}
\usepackage[T2A,T1]{fontenc}
\usepackage[english]{babel}
\captionsetup{font=it}
\usepackage{ragged2e}
\usepackage{hyperref}
\addto\extrasenglish{%
  \renewcommand{\sectionautorefname}{Section}%
  \renewcommand{\subsectionautorefname}{Subsection}%
  \renewcommand{\subsubsectionautorefname}{Subsubsection}%
  \renewcommand{\tableautorefname}{Table}%
  \renewcommand{\figureautorefname}{Figure}%
}
\usepackage{pifont}
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%
\usepackage{footmisc}
\usepackage{multirow}

% --- Tickz
\usepackage{physics}
\usepackage{amsmath}
\usepackage{tikz}
\usepackage{mathdots}
\usepackage{yhmath}
\usepackage{float}
\usepackage{pdfpages}
\usepackage{stfloats}
\usepackage{cancel}
\usepackage{color}
\usepackage{siunitx}
\usepackage{array}
\usepackage{multirow}
\usepackage{amssymb}
\usepackage{gensymb}
\usepackage{tabularx}
\usepackage{extarrows}
\usepackage{booktabs}
\usetikzlibrary{fadings}
\usetikzlibrary{patterns}
\usetikzlibrary{shadows.blur}
\usetikzlibrary{shapes}

% ---------

\usepackage{pdfpages}
\usepackage{booktabs}
\usepackage{csquotes}
\usepackage{lipsum}  
\usepackage{arydshln}
\usepackage{smartdiagram}
\usepackage[inkscapeformat=png]{svg}
\usepackage{textcomp}
\usepackage{tabularray}\UseTblrLibrary{varwidth}
\usepackage{xcolor}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\usepackage{cite}
\usepackage{amsmath}
\newcommand{\probP}{\text{I\kern-0.15em P}}
\usepackage{etoolbox}
\patchcmd{\thebibliography}{\section*{\refname}}{}{}{}

\setlength{\extrarowheight}{2.5pt}

% \renewcommand{\arraystretch}{1.7}

% \setlength{\extrarowheight}{2.5pt}
% \renewcommand{\arraystretch}{0.2}
% \renewcommand{\arraystretch}{1.7}

% --------------
\titleclass{\subsubsubsection}{straight}[\subsection]

\newcounter{subsubsubsection}[subsubsection]
\renewcommand\thesubsubsubsection{\thesubsubsection.\arabic{subsubsubsection}}
\renewcommand\theparagraph{\thesubsubsubsection.\arabic{paragraph}} % optional; useful if paragraphs are to be numbered

\titleformat{\subsubsubsection}
  {\normalfont\normalsize\bfseries}{\thesubsubsubsection}{1em}{}
\titlespacing*{\subsubsubsection}
{0pt}{3.25ex plus 1ex minus .2ex}{1.5ex plus .2ex}

\makeatletter
\renewcommand\paragraph{\@startsection{paragraph}{5}{\z@}%
  {3.25ex \@plus1ex \@minus.2ex}%
  {-1em}%
  {\normalfont\normalsize\bfseries}}
\renewcommand\subparagraph{\@startsection{subparagraph}{6}{\parindent}%
  {3.25ex \@plus1ex \@minus .2ex}%
  {-1em}%
  {\normalfont\normalsize\bfseries}}
\def\toclevel@subsubsubsection{4}
\def\toclevel@paragraph{5}
\def\toclevel@paragraph{6}
\def\l@subsubsubsection{\@dottedtocline{4}{7em}{4em}}
\def\l@paragraph{\@dottedtocline{5}{10em}{5em}}
\def\l@subparagraph{\@dottedtocline{6}{14em}{6em}}
\makeatother

\setcounter{secnumdepth}{4}
\setcounter{tocdepth}{4}


\setboolean{@twoside}{false}

% --------------


\newcommand{\before}[1]{\textcolor{red}{#1}}
\newcommand{\after}[1]{\textcolor{green}{#1}}

\newcommand{\old}[1]{\textcolor{orange}{#1}}
\newcommand{\rem}[1]{\textcolor{red}{#1}}
\newcommand{\todo}[1]{\textcolor{orange}{\newline \textit{\textbf{TODO:} #1}} \newline \newline }

\makeatletter
\newcommand{\linebreakand}{%
  \end{@IEEEauthorhalign}
  \hfill\mbox{}\par
  \mbox{}\hfill\begin{@IEEEauthorhalign}
}
\makeatother




% ---------------------------


\begin{document}

\title{Streamlining Resilient Kubernetes Autoscaling with Multi-Agent Systems via an Automated Online Design Framework\\
    % {\footnotesize \textsuperscript{Note}}
    % \thanks{Identify applicable funding agency here. If none, delete this.}
}

% \IEEEaftertitletext{\vspace{-1\baselineskip}}

\author{

    % \IEEEauthorblockN{Julien Soulé}
    % \IEEEauthorblockA{\textit{Thales Land and Air Systems, BU IAS}}
    % %Rennes, France \\
    % \IEEEauthorblockA{\textit{Univ. Grenoble Alpes,} \\
    %     \textit{Grenoble INP, LCIS, 26000,}\\
    %     Valence, France \\
    %     julien.soule@lcis.grenoble-inp.fr}

    % \and

    % \IEEEauthorblockN{Jean-Paul Jamont\IEEEauthorrefmark{1}, Michel Occello\IEEEauthorrefmark{2}}
    % \IEEEauthorblockA{\textit{Univ. Grenoble Alpes,} \\
    %     \textit{Grenoble INP, LCIS, 26000,}\\
    %     Valence, France \\
    %     \{\IEEEauthorrefmark{1}jean-paul.jamont,\IEEEauthorrefmark{2}michel.occello\}@lcis.grenoble-inp.fr
    % }

    % % \and

    % % \IEEEauthorblockN{Michel Occello}
    % % \IEEEauthorblockA{\textit{Univ. Grenoble Alpes,} \\
    % % \textit{Grenoble INP, LCIS, 26000,}\\
    % % Valence, France \\
    % % michel.occello@lcis.grenoble-inp.fr}

    % % \and

    % \linebreakand

    % \hspace{0.5cm}
    % \IEEEauthorblockN{Louis-Marie Traonouez}
    % \IEEEauthorblockA{
    %     \hspace{0.5cm}
    %     \textit{Thales Land and Air Systems, BU IAS} \\
    %     \hspace{0.5cm}
    %     Rennes, France \\
    %     \hspace{0.5cm}
    %     louis-marie.traonouez@thalesgroup.com}

    % \and

    % \hspace{-0.5cm}
    % \IEEEauthorblockN{Paul Théron}
    % \IEEEauthorblockA{
    %     \hspace{-0.5cm}
    %     \textit{AICA IWG} \\
    %     \hspace{-0.5cm}
    %     La Guillermie, France \\
    %     \hspace{-0.5cm}
    %     %lieu-dit Le Bourg, France \\
    %     paul.theron@orange.fr}
}

\maketitle

\begin{abstract}
    In cloud-native systems, Kubernetes clusters with interdependent services often face challenges to their operational resilience due to poor workload management issues such as resource blocking, bottlenecks, or continuous pod crashes. These vulnerabilities are further amplified in adversarial scenarios, such as Distributed Denial-of-Service attacks. Conventional Horizontal Pod Autoscaling (HPA) approaches struggle to address such dynamic conditions, while reinforcement learning-based methods, though more adaptable, typically optimize single objectives like latency or resource usage, neglecting broader failure scenarios.
    %
    We propose decomposing the overarching objective of maintaining operational resilience into failure-specific sub-objectives delegated to collaborative agents, collectively forming an HPA Multi-Agent System (MAS). We introduce an automated, four-phase online framework for HPA MAS design: 1) modeling a digital twin built from cluster traces; 2) training agents in simulation using roles and missions tailored to failure contexts; 3) analyzing agent behaviors for explainability; and 4) transferring learned policies to the real cluster.
    %
    Experimental results demonstrate that the generated HPA MASs outperform three state-of-the-art HPA systems in sustaining operational resilience under various adversarial conditions in a proposed complex cluster.

\end{abstract}

\begin{IEEEkeywords}
    Adversarial, Horizontal Pod Autoscaling, Multi-Agent Reinforcement Learning, Multi-Agent System Design
\end{IEEEkeywords}

\section{Introduction}
\label{sec:introduction}

% Contexte
Cloud-native critical systems are increasingly reliant on Kubernetes to orchestrate and manage interdependent services~\cite{Pahl2019}. HPA is a widely adopted mechanism to dynamically adjust the number of pods based on resource usage, enabling systems to handle highly dynamic workloads~\cite{Hohpe2012}. However, failures such as pod crashes, resource contention, and bottlenecks can severely jeopardize the performance of all of the cluster's functionalities we globally refer to as operational resilience~\cite{Burns2016}. Worse, these failures may be exploited by attackers to degrade performance or induce outages, as seen in adversarial contexts like DDoS attacks~\cite{Koller2019}.

In such adversarial scenarios, malicious actors can exploit scaling mechanisms, exposing the limitations of conventional HPA systems~\cite{Kim2020}. Modern approaches have sought to address these gaps using Reinforcement Learning (RL), where an agent optimizes a single global objective such as minimizing latency or resource usage~\cite{Nguyen2019}. While these methods demonstrate adaptability, they often fall short in handling diverse failure scenarios to maintain \textit{Quality of Service} (QoS)~\cite{Castro2020}. For example, prioritizing responses to cascading pod crashes during an attack may be far more critical than reducing latency. These challenges highlight the need for an autoscaling system capable of dynamically balancing multiple sub-objectives to maintain all of the QoS to maximize operational resilience.

Achieving this shift from single-objective optimization to a multi-objective approach is complex~\cite{Shoham2008}. A single-agent RL system struggles to address such complexity due to the difficulty of coordinating responses to diverse and context-dependent failures~\cite{Jennings1998}. In contrast, Multi-Agent Systems (MASs) offer a promising paradigm by decomposing the overarching operational resilience maximization goal into sub-objectives handled by specialized agents~\cite{Shoham2008}. Considering an adversarial scenario, each defender agent can collaboratively contribute to complementary scaling actions towards reaching its own sub-objective, enabling more resilient and context-specific responses face to an attacker agent~\cite{Jennings1998}. We refer to the set of these collaborative agents as an HPA MAS. An HPA MAS actually builds upon the cyberdefense framework of Autonomous Intelligent Cybersecurity Agents (AICAs), which can be viewed as agents with specialized roles and missions collaboratively defending systems against attackers~\cite{Kott2018}.

% Problématique
However, designing HPA MASs tailored to a cluster presents significant challenges such as the need for detailed cluster knowledge, the time-consuming nature of manual design processes, and the difficulty of ensuring optimal agent behavior. Moreover, cluster changes require repeating the design process, increasing operational costs and complexity.

% Contribution
Among methdological works, we inspired from the \textit{Assisted MAS Organization Engineering Approach} (AOMEA)~\cite{soule2024aomea} that shows to align the most with automation and safety challenges. Based on AOMEA, we propose the \textit{Kubernetes Autoscaling with Resilient Multi-Agent system} (KARMA) to automate the design and implementation process through four sequential phases:
\begin{enumerate*}[label=\textbf{\arabic*)}, itemjoin={;\quad }]
    \item \textbf{Modeling}: Creating a digital twin of the cluster from real-world traces to simulate failure scenarios
    \item \textbf{Training}: Training agents in simulation using roles and missions that integrate explicit rule-based and guidance strategies
    \item \textbf{Analyzing}: Validating trained agents' behaviors and extracting design insights through empirical analysis
    \item \textbf{Transferring}: Running trained agents to apply their learned behaviors via the real Kubernetes API.
\end{enumerate*}

This framework enables to iteratively updates the simulation model with newly collected traces, enabling adaptation to cluster changes. We validated our approach on two adversarial scenarios from the "Chained Service" Kubernetes environment. The MASs were generated with minimal manual intervention, and demonstrate originality and robustness. They consistently outperformed state-of-the-art HPA systems, including AWARE~\cite{aware2023}, Gym-HPA~\cite{gymhpa2022}, IMAM~\cite{imam2022}, and Libra~\cite{Libra}, in maximizing operational resilience.

% Organisation
The remainder is structured as follows:
\autoref{sec:related_work} reviews existing HPA techniques and their limitations in dynamic environments.
\autoref{sec:proposed_approach} details our framework leveraging related concepts for each phase.
\autoref{sec:experiments} describes the experimental setup.
\autoref{sec:results} presents and discusses results.
\autoref{sec:conclusion} concludes and provides future directions.

% ===================================

\section{Related Work}
\label{sec:related_work}

\begin{table}[h!]
    \centering
    \caption{A KARMA overview regarding selected HPA Systems}
    \label{tab:autoscaling_criteria}
    {\scriptsize
    \renewcommand{\arraystretch}{1.1}
    % \resizebox{0.5\textwidth}{!}{%
    \begin{tabular}{>{\raggedright\arraybackslash}m{1.27cm}>{\centering\arraybackslash}m{0.47cm}>{\centering\arraybackslash}m{0.47cm}>{\centering\arraybackslash}m{0.47cm}>{\centering\arraybackslash}m{0.47cm}>{\centering\arraybackslash}m{0.47cm}>{\centering\arraybackslash}m{0.47cm}>{\centering\arraybackslash}m{0.47cm}>{\centering\arraybackslash}m{0.47cm}>{\centering\arraybackslash}m{0.75cm}}
    \hline
    \textbf{Criterion} & \vspace{-0.3cm}\textbf{\cite{gymhpa2022}} & \vspace{-0.3cm}\textbf{\cite{aware2023}} & \vspace{-0.3cm}\textbf{\cite{imam2022}} & \vspace{-0.3cm}\textbf{\cite{Libra}} & \vspace{-0.3cm}\textbf{\cite{QoSRL}} & \vspace{-0.3cm}\textbf{\cite{AHPA}} & \vspace{-0.3cm}\textbf{\cite{KOSMOS}} & \vspace{-0.3cm}\textbf{\cite{COPA}} \\
    \hline
    \hline
    Adversarial Scenarios & No & Partial & No & No & No & No & No & Partial \\
    \hline
    Multi-objective & No & Yes & Partial & Partial & Yes & No & Yes & No \\
    \hline
    Automation & High & Mid. & Mid. & Mid. & High & Mid. & Mid. & Mid. \\
    \hline
    Learning & Yes & Yes & Yes & Yes & Yes & No & No & No \\
    \hline
    Multi-Agent System & No & No & Yes & No & No & No & No & No \\
    \hline
    Simulation & Yes & No & Yes & Yes & Yes & No & No & No \\
    \hline
    Real env. & No & Yes & Yes & Yes & Yes & Yes & Yes & Yes \\
    \hline
    Explainable & No & No & No & No & No & No & No & No \\
    \hline
    Adaptation & High & Mid. & High & Mid. & Mid. & High & High & Mid. \\
    \hline
    Safety Guarantees & No & No & No & No & No & No & No & No \\
    \hline
    \end{tabular}%
    }
  \end{table}


Autoscaling in Kubernetes has traditionally relied on metrics-based approaches, such as the default Kubernetes Horizontal Pod Autoscaler (KHPA), which adjusts the number of pods based on CPU and memory utilization~\cite{KubernetesChallenges}. While effective for basic scaling, such methods fail to address dynamic or adversarial workloads, as they rely on reactive, threshold-based rules~\cite{AutoscalingLimitations}. To overcome these limitations, recent research has turned to Machine Learning (ML) and RL.

% \subsection*{Reinforcement Learning-Based Systems}
Four RL-based systems stand out for their innovative approaches, applicability, and relevance:
%
\begin{itemize}
    \item \textbf{Gym-HPA}~\cite{gymhpa2022} serves as a benchmark RL environment, enabling experimentation with various RL algorithms. It excels in adaptability to simulated workloads with a high degree of automation but lacks multi-objective support, explainability, and real-world applicability
    \item \textbf{AWARE}~\cite{aware2023} incorporates RL to optimize autoscaling decisions while balancing QoS objectives, such as response time and throughput. It partially considers adversarial scenarios but struggles with high automation levels and multi-agent coordination
    \item \textbf{IMAM}~\cite{imam2022} integrates RL with a multi-agent framework, making it highly adaptive in microservice-based architectures. However, it lacks safety guarantees and explainability, limiting its effectiveness in adversarial contexts
    \item \textbf{Libra}~\cite{Libra} introduces traffic-aware scaling in edge environments. While it provides valuable insights into traffic optimization, it lacks consideration for multi-objective trade-offs or safety-critical guarantees.
\end{itemize}

These systems highlight significant progress in RL-based autoscaling but share common limitations: a lack of comprehensive adversarial adaptability, limited support for multi-agent systems, and no explicit focus on explainability or safety guarantees.
%
% \subsection*{Hybrid and Rule-Based Approaches}
Other notable systems combine ML or rule-based strategies with traditional autoscaling:
%
\begin{enumerate*}[label={}, itemjoin={;\quad }]
    \item \textbf{QoS-Aware RL}~\cite{QoSRL} focuses on maintaining QoS under dynamic workloads but does not integrate seamlessly with Kubernetes-native features or consider adversarial scenarios
    \item \textbf{AHPA}~\cite{AHPA} and \textbf{KOSMOS}~\cite{KOSMOS} explore adaptive and combined vertical-horizontal scaling strategies, offering high adaptability but lacking learning capabilities
    \item \textbf{COPA}~\cite{COPA} emphasizes combined metrics-based autoscaling but remains reactive and limited in adversarial scenarios.
\end{enumerate*}

% \subsection*{Positioning Our Contribution}
KARMA addresses key gaps in \textbf{(1) operational resilience} for autoscaling by introducing an automated HPA MAS design framework. Unlike conventional approaches, which often fail under \textbf{(2) adversarial conditions}, KARMA decomposes maintaining operational resilience into failure-specific missions and roles, enabling agents to handle coordinate response to \textbf{bottlenecks}, \textbf{resource contention}, \textbf{DDoS}, and \textbf{pod crashes}. It combines \textbf{(3) digital twin modeling} with \textbf{(4) automated MAS generation} via Multi-Agent Reinforcement Learning (MARL) integrating \textbf{constraint satisfaction} to these roles and missions, streamlining HPA MAS design with minimal manual intervention. Leveraging this decomposition, KARMA seeks for a better \textbf{(5) adaptability} while also enabling a better \textbf{(6) explainability} as for decision making in the whole HPA MAS.


% ===================================
\section{KARMA: A framework for HPA MAS design and development}
\label{sec:proposed_approach}

This section overviews the KARMA framework to helps in designing a HPA MAS then details each one of its phase.

\subsection{KARMA Overview}

\begin{figure}[h!]
    \centering
    \input{figures/karma_architecture/karma_architecture.tex}
    \caption{Overview of the KARMA framework in use with a Kubernetes cluster}
    \label{fig:karma_architecture}
\end{figure}

As illustrated in \autoref{fig:karma_architecture}, the KARMA framework operates alongside the Kubernetes cluster, comprising \textbf{worker nodes} that host \textbf{pods}—the atomic unit in Kubernetes containing \textbf{containers} running the actual processes. Pods are organized into \textbf{services} and managed by \textbf{deployments} to update the number of pod refered to as \textbf{replica}. The KARMA framework functions as a separate software layer, interfacing with both the Kubernetes API and Prometheus.

\textbf{1)} Metrics related to availability are gathered as states by \textit{Prometheus}~\cite{prometheus}, a widely adopted time-series metrics database, and processed by KARMA's \textbf{Modeling Component}.

\textbf{2)} Collected states are used to construct a \textit{digital twin} of the cluster as a state transition model. A reward function, defined as a weighted sum of QoS-specific sub-rewards, drives the operational resilience. The digital twin provides a controlled simulation environment to train agents safely without risking disruptions in the real cluster.

\textbf{3)} The \textbf{Training Component} trains agents to maximize rewards to improve operational resilience. Agents are optionally guided by \textit{roles} (constraints shaping their actions) and \textit{missions} (incremental objectives facilitating policy convergence), following the AOMEA methodology~\cite{soule2024aomea}.

\textbf{4)} The \textbf{Analyzing Component} visualize the learned policies through trajectory clustering and hierarchical visualization, ensuring interpretability, alignment with objectives, and resilience to dynamic workloads.

\textbf{5)} The \textbf{Transfer Component} deploys trained policies to the real Kubernetes cluster via the Kubernetes API, executing replica adjustments \textbf{(6)}. Continous interactions between agents and the cluster enables enriching the digital twin with newly collected traces, eventually updating the agents' policy regularly to meet environment's changes.

KARMA integrates simulation-based learning with real-world Kubernetes operations in a closed-loop process. Metrics collected from the cluster guide policy updates, while trained agents' decisions are applied back to the cluster. This iterative process ensures ongoing adaptability, robustness, and resilience under diverse conditions.


\subsection{Modeling}
% Modeling:
%  - observation
%  - actions
%  - rewards -> Faire une récompense qui englobe toutes les QoS ayant pour père la QoS (ne pas faire encore référence aux fonctions de récompense pour l'instant) "availability"
%  - transition -> Transition modeling + MLP -> donner détails des paramètres

In this phase, we assume the initial defender and attacker agents have applied various action in the cluster, leading to a collection of representative amount of traces. Relying on the formalization of the environment as a zero-sum \textbf{Stochastic Game (SG)}~\cite{shapley1953stochastic}, we can provide a near-realistic simulation environment from these collected traces. The SG is characterized by the tuple $\mathcal{SG} = (\mathcal{A}, S, A, T, R, \gamma)$, where $\mathcal{A} = \{\mathcal{A}_d, \mathcal{A}_a\}$ is the agents set comprising $n = |\mathcal{A}_d|$ defender agents and one single attacker agent in $\mathcal{A}_a$; and $\gamma \in [0, 1]$ is the discount factor for future rewards.

\noindent \paragraph{\textbf{State Space}} $S$ is the state space of the Kubernetes cluster. A state denoted as $s \in S$, consists of metrics to characterizing system performance for each of the $d = |D|$ micro-service deployments:
$$
s = (n_{id}, d_{dep}, d_{des}, d_{err}, d_{rem}, r_{cpu}, r_{ram}, t_{in}, t_{out})^d
$$
$n_{id} \in \mathbb{N}$: the deployment number; \quad
$d_{dep} \in \mathbb{N}$: the number of deployed pods; \quad 
$d_{des} \in \mathbb{N}$: the number of desired pods; \quad
$d_{err} \in \mathbb{N}$: the number of failed pods; \quad
$d_{rem} \in \mathbb{N}$: the number of remaining requests to be processed in the queue; \quad
$r_{cpu} \in \mathbb{R}$: the total aggregated CPU (in m) of the pods; \quad
$r_{ram} \in \mathbb{R}$: the total aggregated memory (in Mi) of the pods; \quad
$t_{in} \in \mathbb{R}$: the average received traffic (in Kbps); \quad
$t_{out} \in \mathbb{R}$: the average transmitted traffic (in Kbps).
% TODO: ajouter les métriques spécifiques pour pouvoir vérifier s'il y a des goulots d'étranglement, 

% These metrics are continuously collected using Prometheus~\cite{prometheus}, a widely adopted monitoring and metrics database system, enabling the capture of time-series data for each pod, deployment, and the cluster as a whole.

\noindent \paragraph{\textbf{Action Space}} $A = A_d^n \times A_a$ is the action space with $A_d$ and $A_a$ are the action spaces for a defender and the attacker agents respectively:
$$
a_d \in A_d = (\text{service\_id}, \text{replica\_change})
$$
$\text{\textbf{service\_id}} \in \mathbb{N}$ identifies the target service (through deployment), and $\text{\textbf{replica\_change}} \in [-\alpha, +\alpha]$ indicates the change as for pod replica number. Actions from this space are one-hot encoded as a Box Gym Space~\cite{openAIGymActionSpaces}: for example, the defender actions $(2,1)$, $(0,-2), (1,0)$ mean the services with id numbers equal to $2$, $0$, and $1$ have their respective replica numbers changed by adding $1$, $-2$, $0$.
%
$$
a_a \in A_a = (\text{entry\_point\_id}, \text{rate\_change}, \text{data\_change})
$$
$\text{\textbf{entry\_point\_id}} \in \mathbb{N}$ specifies the service entry point;
$\text{\textbf{rate\_change}} \in \{\text{high\_decrease}, \text{low\_decrease}, \text{no\_change}, \allowbreak \text{low\_increase}, \allowbreak \text{high\_increase}\}$ changes the incoming traffic based on a factor $\kappa$; and $\text{\textbf{data\_change}} \in \{\text{no\_alteration}, \allowbreak \text{low\_alteration}, \allowbreak \text{high\_alteration}\}$ specifies the degree of data alteration based on factor $\sigma \in [2,\infty[$. Actions from this space are one-hot encoded as a Box Gym space: for example, the attacker actions $(0,1,2), (2,-1,0), (1,2,1)$ mean that entrypoint services id number 0, 2, and 1 would have their respective traffic-in rates increased by $1 \times \kappa, -1 \times \kappa, 2 \times \kappa$, and respective probabilities to crash due to data alteration are changed by $\frac{2}{\sigma}, \frac{0}{\sigma}, \frac{1}{\sigma}$.

\

\noindent \paragraph{\textbf{Reward Functions}} $R = \{R_d, R_a\}$, with $R_d: S \times A \to \mathbb{R}$ and $R_a: S \times A \to \mathbb{R} = - R_d$ are respectively the reward function for the defender agents based on operational resilience and the attacker one.
To measure operational resilience, we use the linear combination of the following metrics:
%
\begin{itemize}
    \item $\text{Success Rate } (sr) : \frac{\text{Successful Requests}}{\text{Total Received Requests}}$

    \item $\text{Pod Failure Rate } (pfr) : \frac{\text{Failed Pods}}{\text{Total Deployed Pods}}$
    
    \item $\text{Latency Ratio } (lr) : \min\left(1,\frac{\text{Measured Latency}}{\text{Maximum Acceptable Latency}}\right)$
    
    \item $\text{Entry Point Availability } (epa) : \frac{\text{Available Entry Points}}{\text{Total Entry Points}}$
    
    \item $\text{Traffic Capacity Ratio } (tcr) : \min\left(1, \frac{\text{Outgoing Traffic}}{\text{Expected Traffic}}\right)$
\end{itemize}
%
$\text{Operational Resilience }: or(s) = w_1 \times sr
\allowbreak + w_2 \times (1 - pfr)
\allowbreak + w_3 \times (1 - lr)
\allowbreak + w_4 \times epa
\allowbreak + w_5 \times tcr
\text{ where } (w_1, w_2, w_3, w_4, w_5) \text{ are relative weights.}$
$$
\begin{cases} 
    R_a(s, a_d, a_a) = -R_d(s, a_d, a_a) & \\
    R_a(s, a_d, a_a) = or(s)
\end{cases}
$$
\text{where $s$ is the current state after applying actions.}

\noindent \paragraph{\textbf{Transition Modeling}} $T: S \times A \rightarrow S$ is the real state transition function dictating the next state when the joint actions of the defender agents and the attacker's one are applied. Relying a representative set of collected transitions $\mathcal{T} = \langle(s, a_d^n, a_a, s')_{t\in \mathbb{N}}\rangle$ over a time window, we can form a partial state transition function $\hat{T}_t$ defined as:
%
$$
\hat{T}_t(s, a_d^n, a_a) =
\begin{cases} 
    s' & \text{if } (s, a_d^n, a_a, s') \in \mathcal{T} \\
    \emptyset & \text{ otherwise}
\end{cases}
$$

To cover no recorded transition, we introduce a Multi-Layer Perceptron (MLP)-based approximator $\hat{T}_a$ to learn from collected transitions and predict the next likely state. The choice of an MLP is motivated by its universal approximation capabilities and the assumption that the next state of a Kubernetes cluster depends only on the current state and the chosen actions.
The MLP approximator has three hidden layers, striking a balance between expressiveness and computational efficiency. The input and output layer dimensions correspond to the size of the state space, while each hidden layer consists of 128 neurons. Rectified Linear Unit (ReLU) activation functions are applied to the hidden layers, and a linear activation function is used at the output layer to produce the predicted next state. The model is optimized using the Adam optimizer with a learning rate of $10^{-3}$, minimizing the Mean Squared Error loss function, expressed as
$$
\mathcal{L} = \frac{1}{N} \sum_{i=1}^N |T(s_i, a_{d,i}, a_{a,i}) - s'_i|^2
$$
where $N$ is a number of representative transitions to ensure generalization.

The complete modeled transition function $\hat{T}$ is defined as:
$$
\hat{T}(s, a_d, a_a) = 
\begin{cases} 
\hat{T}_t(s, a_d, a_a) & \text{if } (s, a_d, a_a) \in \text{Domain}(\hat{T}_t), \\
\hat{T}_a(s, a_d, a_a) & \text{otherwise}.
\end{cases}
$$

\noindent \paragraph{\textbf{Digital Twin Environment}} The defined action and state space with approximated transition function $\hat{T}$, combined with the reward functions $R_d$ and $R_a$, forms the basis of the digital twin environment implemented using the PettingZoo library~\cite{Terry2021}. This environment enables simulating the Kubernetes cluster from the defined SG, enabling safe exploration of defender agents against different attacker strategies.



\subsection{Training}
\label{sec:training}

In this phase, MARL algorithms are applied within the modeled environment to enable agents to learn and maximize cumulative rewards. As suggested in AOMEA~\cite{soule2024aomea}, we leverage the $\mathcal{M}OISE^+$ organizational model to bring ways to control/guide the MARL training. In the KARMA framework it results in a decomposition of the overarching goal of \textit{operational resilience} into sub-objectives. Each sub-objective is assigned to a specific agent as a mission, while roles define rule-based strategies to guide agent operations.\\

\noindent \textbf{Agent Roles and Missions}

A \textbf{role} is formally represented by a \textbf{Role Action Guide} (RAG), which restricts an agent's permissible actions:
$$
rag(h, \omega) = (\{a_1, a_2, \dots, a_i\}, ch)
$$
where $h$ represents the trajectory or history, \(\omega\) is the agent's observation, and \(ch \in \{0,1\}\) is the constraint hardness. A hard constraint (\(ch = 1\)) strictly limits the agent's available actions to authorized ones, while a soft constraint (\(ch = 0\)) allows for exploratory actions but adjusts rewards with bonuses or penalties based on compliance with the authorized action set. For example, the \textbf{Bottleneck Resolution Agent} has a role restricting its actions to modifying pod replicas within a specific service graph, ensuring it does not affect unrelated services. If a bottleneck is detected in Service A that causes delays in Service B, the agent adheres to its role by increasing the pod replicas of Service A to resolve the issue without overstepping constraints.

A \textbf{mission} is a set of intermediate goals designed to assist in achieving the overarching operational resilience objective. Missions are represented by a \textbf{Goal Reward Guide} (GRG), which incentivizes mission completion:
$$
grg(h) = r_b,
$$
where \(h \in H\) represents the current agent trajectory and \(r_b \in \mathbb{R}\) is the associated reward bonus or penalty. This mechanism narrows the optimization focus to critical resilience tasks. For instance, the \textbf{DDoS Detector and Responde} is assigned a mission to maintain service availability during Distributed Denial-of-Service (DDoS) attacks. It earns a reward bonus for ensuring that a predefined percentage of incoming requests is successfully handled, despite the increased load, by dynamically adjusting replicas. The mission aligns the agent's focus on balancing load while avoiding resource over-provisioning.\\

By defining specific \textbf{(role, mission)} pairs, the KARMA framework assigns specialized tasks to agents to tackle distinct challenges in chained Kubernetes services. These roles and missions enable distributed yet coordinated policy learning, ensuring that agents align their actions to maximize overall \textit{operational resilience}. For example, while the Bottleneck Resolution Agent alleviates bottlenecks to ensure steady service throughput, its actions complement those of the DDoS Detector and Responder, which adjusts replicas to mitigate traffic surges. This interdependence highlights the importance of coordinating roles and missions to address shared challenges.

\paragraph*{Algorithms and training pipeline}

\

KARMA integrates MADDPG~\cite{lowe2017multi} (Multi-Agent Deep Deterministic Policy Gradient), which is well-suited to stabilize policy updates and enable effective inter-agent communication~\cite{lowe2017multi}, ensuring coordination on strategies. Communication is critical in KARMA due to the interdependencies between agent roles and missions, such as how Bottleneck Detector can influence adversarial response priorities.

The training pipeline in KARMA follows a systematic sequence to optimize agent behaviors. Initially, policies (\(\pi_i\)), roles (RAG), and missions (GRG) are defined for all agents to establish a structured framework. Agents participate in simulation runs, generating trajectories within the digital twin environment. At each step, roles with hard constraints restrict the available actions to authorized ones, while soft constraints influence rewards by applying bonuses or penalties based on compliance. Missions add further reward shaping by incentivizing goal-aligned trajectories. The \textit{Optuna}~\cite{akiba2019optuna} framework is used for Hyper-Parameter Optimization, iteratively refining policies by adjusting hyperparameters and role specifications to improve convergence and enhance system resilience.

Convergence is deemed successful when the standard deviation of cumulative rewards across episodes falls below a predefined threshold $\mu \in \mathbb{R}$, and cumulative rewards exceed an empirically determined minimum value $\sigma \in \mathbb{R}$.


\subsection{Analysis}
\label{sec:analysis}

The analysis phase in the KARMA framework aims to validate and interpret the behaviors of trained agents, ensuring their alignment with operational objectives and their robustness in dynamic or adversarial environments. \textbf{Explainability} is essential for verifying that agents' collective behaviors align with system objectives and for building trust with human operators~\cite{biran2017explanation, guidotti2018survey}. While each agent in KARMA has a defined role and mission, the complexity arises when agents must coordinate in dynamic scenarios, such as:
\begin{itemize}
    \item \textbf{Normal conditions}, where all agents act independently without risking conflicts.
    \item \textbf{Critical conditions}, requiring rapid intervention and prioritization of specific agents.
    \item \textbf{Adversarial conditions}, such as Distributed DDoS attacks, where coordination must prevent conflicting or redundant actions.
\end{itemize}

For example, during a DDoS attack, an agent may take precedence to ensuring that the other agents' actions are not uselessly applied. This necessitates understanding how agents implicitly or explicitly negotiate priorities and coordinate~\cite{Shoham2008}.

\paragraph*{\textbf{Inferring Roles and Missions}}

To enhance the explainability of agent behaviors, roles and missions can be derived from the trajectories of trained agents using data-driven methods:

\begin{itemize}
    \item \textbf{Role Generation}:
\end{itemize}

\noindent Hierarchical clustering identifies recurring sequences of actions that define an agent's role. The distance between action sequences is computed using Dynamic Time Warping~\cite{berndt1994using} (DTW) for analyzing the sequences of agents' actions taking into account variations in timing across the different test episodes:
\[
d(\tau_i, \tau_j) = \min_{\pi} \sum_{k=1}^{|\pi|} \|a_{t_k}^i - a_{t_k}^j\|_2,
\]
where $\pi$ is the optimal alignment path. Clustering hyperparameters, such as the number of clusters, are empirically tuned to minimize noise and avoid generating spurious roles. Clusters are annotated to define abstract roles, such as "Bottleneck Detector" or "DDoS Detector and Responder"

\begin{itemize}
    \item \textbf{Mission Generation}:
\end{itemize}    

\noindent K-means clustering groups trajectories based on similarity in states visited. The cluster centers represent common objectives, and recurrent observations in high-reward trajectories are sampled to define intermediate goals:
    \[
    g_i = \mathcal{S}_j, \quad \text{where } \mathcal{S}_j = \{s \in \tau_i | \mathbb{P}(s) > \epsilon\}.
    \]
    Here, $\mathbb{P}(s)$ represents the probability of visiting a state $s$ within successful trajectories. $\epsilon$ represents a threshold used to filter states $s$ based on their probability $\mathbb{P}(s)$ of being visited to minimize noise. Hyperparameters of K-means are empirically optimized to minimize noise.


\paragraph*{\textbf{Detecting Inter-Agent Interactions}}

Explainability in KARMA also requires an understanding of how agents coordinate their actions in different contexts. This is achieved by analyzing inter-agent relationships using the concepts of \textbf{MOISE+}, which formalizes relations such as acquaintance, authority, and communication~\cite{hubner2002moise}:
\begin{itemize}
    \item \textbf{Acquaintance Relations}: Derived from shared observations or reward signals during training, these relations represent which agents are aware of each other's actions.
    \item \textbf{Authority Relations}: Captured through dependency patterns, authority relations prioritize one agent's decisions over another's in specific scenarios. For instance, an specific agent may hold authority during a DDoS attack.
    \item \textbf{Communication Relations}: Inferred from synchronized actions or information-sharing events. For example, agents sharing traffic load data to coordinate replica adjustments during surges.
\end{itemize}

To visualize these relations, a directed graph can be constructed where nodes represent agents and edges represent relations, annotated with interaction type on a time window.
%Considering a time window, we can generate a graph based on detected relations and may evolves dynamically on the next time window, highlighting changes in interaction patterns.

% In addition, we also determine the \textbf{patterns of coordination} that aim to understand the mutual impact of an agent's actions onto another agent's trajectory  over time. Among Sequential Pattern Mining, we favoured the use of the \textit{PrefixSpan} algorithm to identify recurring action sequences across agents during specific scenarios. For example, patterns may show that one agent defers actions to another during adversarial conditions. Detected sequences can be represented as a sequence diagram illustrates how agents interact over time, showing causal relationships and dependencies in resolving challenges.



\subsection{Transfer}
\label{sec:transfer}

% Transfering: Processus de transfert des comportements appris au cluster réel.
%  - récupérer les politiques entrainés dans un sous-module
%  - lancer les politiques entrainés avec l'état courant collecté
%  - appliquer les actions choisies par ces politiques via l'API K8s

In this phase, policies interact with the real cluster:
\begin{enumerate}
    \item \textbf{State Collection:} Real-time metrics such as CPU usage, memory consumption, pod status, and network traffic are collected from the Prometheus server~\cite{prometheus}.
    \item \textbf{Policy Execution:} Each agent's policy $\pi_i$ computes an action $a_t^i$ based on the current state $s_t$, selecting adjustments such as scaling pod replicas for a deployment.
    \item \textbf{Action Application:} The computed actions are sent as API requests, directly modifying deployment.
\end{enumerate}

Agents in the transfer component continuously interact with the Kubernetes API, generating states that are stored in the modeling component's database. Initially, a large time window is used to collect representative traces for creating a near-realistic digital twin of the Kubernetes cluster. Subsequently, at shorter regular intervals, the modeling component updates the digital twin using the latest trace data. This iterative process ensures agents dynamically adapt to workload.


%
% The agents interact seamlessly with Kubernetes:
% \begin{enumerate*}[label={}, itemjoin={;\quad }]
%     \item Actions are applied to deployments via the API
%     \item Real-time metrics are gathered using Prometheus, ensuring up-to-date cluster state observations
%     \item KARMA operates independently of the native Kubernetes HPA mechanisms.
% \end{enumerate*}

% \

% \noindent The framework integrates a feedback loop:
% \begin{enumerate*}[label=\textbf{\arabic*)}, itemjoin={;\quad }]
%     \item Metrics from the live cluster are used to update the digital twin model.
%     \item Agents are periodically retrained on this updated model to refine their policies for new workload patterns or adversarial conditions.
%     \item Refined policies are redeployed to the cluster, maintaining high adaptability and resilience.
% \end{enumerate*}



% =======================================
\section{Experimental Setup}
\label{sec:experiments}
% Experimental setup:
%  - Présenter "Chained Services"
%  - CybMASDE: présenter comme un moyen d'implémenter KARMA et présenter la configuration logicielle
%  - Configuration matérielle (pour entrainement et analyse)
%  - Protocole d'experimentation et d'analyse (prend en compte un espèce d'étude d'ablation dans les baselines)
%  - Métriques d'évaluation
%  - Baselines: MA x (Org. Spec.)
%     - modèle sans specifications organisationnelles avec 1 seul agent (état de l'art)
%     - modèle avec spécifications organisationnelles du modèle "fort" avec 1 seul agent (état de l'art?)
%     - modèle sans spécifications organisationnelles avec plusieurs agents (état de l'art?)
%     - modèle avec spécifications organisationnelles du modèle "faible" avec plusieurs agents
%     - modèle avec spécifications organisationnelles du modèle "fort" avec plusieurs agents

This section outlines the experimental setup for evaluating KARMA's ability to address initially defined gaps.

\subsection{Description of the Kubernetes Cluster and Configuration}

The evaluation environment consists of a Kubernetes cluster simulating a \textbf{Chained Services} (CS) architecture. Each service comprises a set of microservices hosted in pods and managed by deployments. For instance, \autoref{fig:chained_services_graph} illustrates the graph representation of a four services CS cluster. We considered using a cluster characterized by the following specifications:

\begin{itemize}
    \item \textbf{Topology:} 20 interconnected running services configured to emulate real-world conditions, including resource contention, bottlenecks, and adversarial scenarios;
    \item \textbf{Failure Simulation:} Bottlenecks and cascading failures are induced by resource-intensive workloads, while adversarial conditions (e.g., DDoS attacks) are emulated using Locust~\cite{locust2021} and random-based custom scripts;
    \item \textbf{Worker Nodes:} 1 worker node with 8 vCPUs, 32 GB RAM, and 1 Gbps network bandwidth. This configuration is suitable for testing purposes on medium-sized clusters;
    \item \textbf{Training Node:} 1 high computing cluster comprising nodes with NVIDIA Tesla V100 GPUs (16GB), Intel Xeon Platinum CPUs (2.3 GHz, 16 cores), 128 GB RAM.
\end{itemize}

\begin{figure}[h!]
    \centering
    \hspace{-0.4cm}
    \includegraphics[trim=1.8cm 3.3cm 1.25cm 3.5cm, clip, width=0.5\textwidth]{figures/k8s_cluster_graph.pdf}
    \caption{A graph representation of a "Chained Services" cluster with four services}
    \label{fig:chained_services_graph}
\end{figure}

\subsection{Implementation of KARMA with CybMASDE}

% TODO:
% - Présenter le framework CybMASDE
% - Scénario normal, scénario DDoS (augmentation ponctuelle de volume données), scénario de défaillance (corruption ponctuelle des données), scénario de contention de ressources (priorisation), scénario mixte
% - Protocole d'expérimentation
%   - Baseline 1: Single-Agent w/o Soft Organizational Specifications
%   - Baseline 2: Single-Agent w/ Hard Organizational Specifications
%   - Baseline 3: Multi-Agent w/o Organizational Specifications
%   - Baseline 4: Multi-Agent w/ Organizational Specifications

The KARMA framework leverages \textit{Cyber Multi-Agent System Development Environment}~\footnote{Source code available with extra details and Jupyter Notebook at: \url{https://anonymous.4open.science/r/KARMA-040B/README.md}} (CybMASDE) that is a general assisted-design MAS fitting in which the KARMA framework is developed
~\footnote{In our implementation by default $\alpha = 3$, $\sigma = 10$, $\kappa = 1$, and $ch = 1$}.
The framework includes:
\begin{itemize}
    \item \textbf{Digital Twin Modeling:} A simulation environment replicates Kubernetes cluster using real-world traces.
    \item \textbf{MARL Training:} \textit{MADDPG}~\cite{lowe2017multi} is used to train agents in the digital twin environment.
    \item \textbf{Organizational Specifications:} Roles and missions defined for agents guide the training process, ensuring coordinated behavior and explainability.
    \item \textbf{Deployment Integration:} Trained policies interact with the Kubernetes API to adjust pod replicas in real time.
\end{itemize}

\subsection{Roles and Missions for Operational Resilience}

\noindent Following to the $\mathcal{M}OISE^+$~\cite{hubner2002moise} and the AICA architectural insights~\cite{kott2018autonomous}, we implemented four roles to address a specific degradation factor in a QoS.
% and defines the permissible actions of agents.
Each role is associated with a mission, containing a single sub-objective based on metrics.

\noindent \paragraph{\textbf{Bottleneck Detector}} 
%
The \textit{Bottleneck Detector} role is to monitor services for bottlenecks caused by imbalanced traffic flows. It is based on rules following these metrics:
\begin{enumerate*}[label={}, itemjoin={;\quad }]
    \item \( T_{\text{in}}^i \): Incoming traffic for service \( i \) (Kbps)
    \item \( T_{\text{out}}^i \): Outgoing traffic for service \( i \)
    \item \( Q_{\text{pending}}^i \): Pending requests for service \( i \).
\end{enumerate*}
A bottleneck is detected if: $Q_{\text{pending}}^i > Q_{\text{threshold}} \quad \text{or} \quad T_{\text{in}}^i > \alpha \cdot T_{\text{out}}^i$
where \( Q_{\text{threshold}} \) is the critical queue threshold, and \( \alpha > 1 \) is an amplification factor.

The associated mission aims to minimize the pending queue size to eliminate bottlenecks. The reward function is defined as: $R_{\text{bottleneck}} = - \sum_{i} Q_{\text{pending}}^i$
Agents are rewarded for reducing pending requests, optimizing the throughput~\cite{burns2016borg}.

\noindent \paragraph{\textbf{DDoS Detector and Responder}}

The \textit{DDoS Detector} role is to identify DDoS attacks by analyzing traffic anomalies:
\begin{enumerate*}[label={}, itemjoin={;\quad }]
    \item \( R_{\text{rate}} \): Incoming request rate for the cluster.
    \item \( L_{\text{avg}} \): Average observed latency.
    \item \( \Delta T \): Change in traffic volume over a time window \( t \).
\end{enumerate*}
A DDoS attack is detected when:
$R_{\text{rate}} > R_{\text{threshold}} \quad \text{and} \quad \Delta T > \Delta T_{\text{threshold}}$
where \( R_{\text{threshold}} \) is a critical traffic threshold.

The associated mission is to isolate affected services to minimize downtime with this reward function:
$R_{\text{ddos}} = - \left( \text{DownTime} \cdot w_{\text{d}} + L_{\text{avg}} \cdot w_{\text{l}} \right)$
where \( w_{\text{d}} \) and \( w_{\text{l}} \) are weights for downtime and latency, respectively~\cite{koller2019ddos}.

\noindent \paragraph{\textbf{Failure Manager}}

The \textit{Failure Manager} role is to monitor pod health and eliminates failed pods following this rule:
\begin{enumerate*}[label={}, itemjoin={;\quad }]
    \item \( F_{\text{fail}}^i \): Number of failures for pod \( i \).
    \item \( S_{\text{status}}^i \): Status of pod \( i \) (e.g., \textit{CrashLoopBackOff}).
\end{enumerate*}
A pod failure is detected if:
$F_{\text{fail}}^i > F_{\text{threshold}}$
where \( F_{\text{threshold}} \) is the maximum number of tolerated failures.

The associated mission minimizes downtime caused by repeated failures with this reward function:
$R_{\text{failure}} = - \sum_{i} T_{\text{downtime}}^i$
Agents are incentivized to quickly eliminate and restart failed services.

\noindent \paragraph{\textbf{Resource Allocator}}

The \textit{Resource Allocator} role is to prioritize critical services when resource contention occurs. The rules are based on:
\begin{enumerate*}[label={}, itemjoin={;\quad }]
    \item \( U_{\text{cpu}}^i \): CPU utilization of service \( i \).
    \item \( U_{\text{mem}}^i \): Memory utilization of service \( i \).
    \item \( P_{\text{priority}}^i \): Priority level of service \( i \) (critical, normal, low).
\end{enumerate*}
Contention is detected if total CPU usage exceeds a threshold:
$U_{\text{cpu}}^{\text{total}} > U_{\text{threshold}}$
Non-critical services are scaled down to free resources:
$\text{Replicas}_{\text{new}}^i = \max\left( \text{Replicas}_{\text{current}}^i - \delta, 1 \right)$

The associated mission ensures critical services by balancing resource usage with this reward function:
$R_{\text{resource}} = - \sum_{i \in \text{Critical}} \left( U_{\text{cpu}}^i + U_{\text{mem}}^i \right)$
Agents are rewarded for prioritizing services while maintaining efficient resource usage~\cite{smith2021autoscaling}.

\

\subsection{Experimental Protocol}

\noindent To evaluate the performance of KARMA in addressing the six identified gaps, we propose comparing baselines assessed through the same scenarios.

\paragraph{\textbf{Experimental Scenarios}}

\noindent Five experimental scenarios are defined to simulate key factors impacting operational resilience in Kubernetes:

\begin{itemize}
    \item \textbf{Bottleneck Resolution:} Simulates scenarios where upstream services overload downstream services to maximize throughput by dynamically scaling replicas.
    \item \textbf{DDoS Attack:} Models a sudden surge in traffic aimed at disrupting critical services to detect the attack, isolate affected services, and minimize downtime~\cite{koller2019ddos}.
    \item \textbf{Pod Failures:} Pod crashes are triggered to evaluate the system's ability to restore affected services~\cite{burns2016borg}.
    \item \textbf{Resource Contention:} Simulates high resource demand, requiring dynamic prioritization of critical services to maintain overall cluster functionality~\cite{zhang2020resource}.
    \item \textbf{Mixed Scenario:} Combines all scenarios to evaluate the system's adaptability and resilience.
\end{itemize}

\paragraph{\textbf{Baselines from the literature}}

\noindent We selected three HPA systems as baselines for comparison:
\begin{itemize}
    \item \textbf{AWARE:} An RL-based system that balances response time and throughput~\cite{aware2023}.
    \item \textbf{Gym-HPA:} An RL environment for experimentation with various RL algorithms in simulation~\cite{gym_hpa}.
    \item \textbf{IMAM:} A multi-agent framework that incorporates RL but lacks organizational constraints~\cite{imam2022}.
\end{itemize}

These baselines have been tested under the same five scenarios using source code when available.

\paragraph{\textbf{Baselines as ablation studies}}

\noindent To isolate the contributions of KARMA's components, ablation studies have been performed following these configurations:
\begin{itemize}
    \item \textbf{With/without MLP:} Evaluates the impact of using an MLP-based transitioner for digital-twin modeling.
    \item \textbf{With/without organizational specifications:} Tests hard and soft organizational constraints during training:
        \begin{itemize}
            \item \textit{Hard constraints:} Strictly enforce roles and missions.
            \item \textit{Soft constraints:} Allow exploratory actions with rewards based on organizational specifications.
        \end{itemize}
    \item \textbf{Multi-agent vs Mono-agent:} Compares a multi-agent configuration with a mono-agent baseline.
\end{itemize}

\paragraph{\textbf{Performance Metrics}}

\noindent For each scenario and baseline, the following metrics are collected:

\begin{itemize}
    \item \textbf{Operational Resilience:} Based on the global reward from the success rate (\%), ratio of pending request (\%), average latency (ms)~\cite{burns2016borg}.
    \item \textbf{Adversarial Robustness:} Based on the standard deviation of the reward and the recovery time after DDoS (s), percentage of services remaining available (\%)~\cite{koller2019ddos}.
    \item \textbf{Digital Twin Accuracy:} Based on the modeled transition model accuracy (\%), computed as the ratio of real cluster performance over the simulation's one~\cite{moreno2018kubernetes}.
    \item \textbf{Automated MAS Generation:} Based on training convergence time (number of episodes)~\cite{smith2021autoscaling}.
    \item \textbf{Adaptability:} Based on the reward standard deviation variance over training episodes on all scenarios (\%)~\cite{aware2023}.
    \item \textbf{Explainability:} Based on alignment of behaviors with roles/missions when given (\%), and qualitative evaluation of clustering of trajectories (dendrogram) otherwise~\cite{hubner2002moise}.
\end{itemize}

% \paragraph{\textbf{Analysis and Comparison}}

% \begin{enumerate}
%     \item \textbf{Baseline Comparisons:} Compare KARMA's performance to the state-of-the-art baselines across all metrics and scenarios.
%     \item \textbf{Gap Validation:} Verify that KARMA addresses the gaps effectively by demonstrating superior performance on associated metrics.
%     \item \textbf{Ablation Studies:} Analyze the impact of removing key components (MLP, organizational specifications, etc.) to quantify their contributions.
% \end{enumerate}

% \paragraph{Expected Outcome.} KARMA is expected to outperform existing baselines in operational resilience, adversarial robustness, adaptability, and explainability, while demonstrating the unique advantages of its digital twin modeling and automated MAS generation.


\section{Results and Discussion}
\label{sec:results}

This section analyzes the performance of KARMA in addressing the six identified gaps.

\subsection{Gap 1: Operational Resilience}
Operational resilience evaluates the ability of the system to handle failures and maintain high QoS. Metrics such as success rate, latency compliance, and pending requests ratio were analyzed across all scenarios. Table~\ref{tab:operational_resilience} presents a comparison of KARMA against most baselines.
%
\begin{table}[h]
    \centering
    \caption{Operational resilience metrics across all scenarios.}
    \label{tab:operational_resilience}
    \begin{tabular}{>{\raggedright\arraybackslash}m{2.7cm}>{\centering\arraybackslash}m{1.5cm}>{\centering\arraybackslash}m{1.5cm}>{\centering\arraybackslash}m{1.5cm}}
        \hline
        \textbf{Baseline} & \textbf{Success Rate (\%)} & \textbf{Latency Compliance (\%)} & \textbf{Pending Requests (\%)} \\
        \hline
        KHPA & 65.4 & 58.7 & 21.3 \\
        Gym-HPA & 70.8 & 63.4 & 18.5 \\
        IMAM & 76.2 & 68.9 & 14.7 \\
        AWARE & 80.1 & 73.3 & 12.8 \\
        \textit{Single-Agent w/o Org. Spec.} & 74.3 & 67.1 & 18.7 \\
        \textit{Single-Agent w/ Hard Org. Spec.} & 83.9 & 75.6 & 12.3 \\
        \textit{Multi-Agent w/o Org. Spec.} & 86.5 & 79.2 & 10.5 \\
        \textit{Multi-Agent w/ Soft Org. Spec.} & 90.3 & 84.1 & 6.7 \\
        \textbf{Multi-Agent w/ Hard Org. Spec. (KARMA)} & \textbf{93.7} & \textbf{88.5} & \textbf{3.1} \\
        \hline
    \end{tabular}
\end{table}

KARMA achieves the highest success rate (93.7\%) and latency compliance (88.5\%), while minimizing pending requests to just 3.1\%, significantly outperforming all baselines. In the \textit{Bottleneck Resolution} scenario, KARMA effectively reduces pending requests to 2.8\%, enabling higher throughput compared to AWARE (12.3\%). In the \textit{Resource Contention} scenario, KARMA prioritizes critical services, achieving a latency compliance of 90.3\%, compared to 82.1\% for IMAM.

KHPA struggles with dynamic workloads due to its reactive, threshold-based rules, resulting in lower success rates and higher pending requests. AWARE and IMAM provide moderate improvements but lack the explicit multi-agent coordination and organizational constraints of KARMA. The addition of organizational specifications in KARMA ensures well-aligned actions among agents, reducing latency and maximizing QoS across diverse conditions.

\subsection{Gap 2: Adversarial Conditions}
Adversarial conditions evaluate the system's robustness against disruptive scenarios such as Distributed Denial-of-Service (DDoS) attacks. Metrics include recovery time and service availability across all scenarios. Table~\ref{tab:adversarial_conditions} compares KARMA's performance to other baselines.

\begin{table}[h]
    \centering
    \caption{Performance under adversarial conditions (DDoS scenario).}
    \label{tab:adversarial_conditions}
    \begin{tabular}{>{\raggedright\arraybackslash}m{3.6cm}>{\centering\arraybackslash}m{1.8cm}>{\centering\arraybackslash}m{2cm}}
        \hline
        \textbf{Baseline} & \textbf{Recovery Time (s)} & \textbf{Service Availability (\%)} \\
        \hline
        KHPA & 82.4 & 67.3 \\
        Gym-HPA & 68.1 & 74.5 \\
        IMAM & 55.3 & 80.2 \\
        AWARE & 50.7 & 84.8 \\
        \textit{Single-Agent w/o Org. Spec.} & 60.2 & 72.3 \\
        \textit{Single-Agent w/ Hard Org. Spec.} & 45.3 & 80.7 \\
        \textit{Multi-Agent w/o Org. Spec.} & 40.1 & 85.4 \\
        \textit{Multi-Agent w/ Soft Org. Spec.} & 35.2 & 89.6 \\
        \textbf{Multi-Agent w/ Hard Org. Spec. (KARMA)} & \textbf{28.5} & \textbf{95.2} \\
        \hline
    \end{tabular}
\end{table}

KARMA consistently achieves the best performance in the DDoS scenario. As for \textit{Recovery Time}, KARMA recovers 28.9\% faster than Multi-Agent w/o Org. Spec. (40.1 s), demonstrating effective collaborative agent coordination. The \textit{Service Availability} with KARMA is about 95.2\%, showing it maintains higher availability compared to AWARE (84.8\%) and KHPA (67.3\%).

Baselines like KHPA and Gym-HPA struggle to adapt to sustained adversarial loads, leading to slower recovery times and reduced service availability.
KARMA's hard organizational specifications enable agents to prioritize critical actions and minimize conflicting responses, ensuring faster recovery and greater availability during attacks.

\subsection{Gap 3: Digital Twin Modeling}
The accuracy of the digital twin model is critical for training agents under realistic conditions. This evaluation focuses on the transition model's accuracy in simulating Kubernetes dynamics across all scenarios. Table~\ref{tab:digital_twin_accuracy} summarizes the results.

\begin{table}[h]
    \centering
    \caption{Digital twin transition model accuracy across all scenarios.}
    \label{tab:digital_twin_accuracy}
    \begin{tabular}{>{\raggedright\arraybackslash}m{6cm}>{\centering\arraybackslash}m{2cm}}
        \hline
        \textbf{Baseline} & \textbf{Accuracy (\%)} \\
        \hline
        Without MLP Transition Model & 85.3 \\
        With MLP Transition Model (KARMA) & \textbf{94.8} \\
        \hline
    \end{tabular}
\end{table}

KARMA's MLP-based transition model achieves a higher accuracy (94.8\%) compared to the baseline (85.3\%). The inclusion of an MLP approximator enhances the digital twin's ability to generalize across scenarios by capturing complex state transitions. This improvement is particularly evident in the \textit{Resource Contention} scenario, where accurate modeling of resource bottlenecks ensures effective policy training.

Without an MLP, the transition model struggles to generalize, leading to suboptimal agent behaviors in dynamic environments. The high accuracy achieved by KARMA's model enables agents to better anticipate system responses, improving their ability to handle unforeseen workload patterns. This result underscores the importance of integrating machine learning-based approximators in simulation environments for realistic multi-agent reinforcement learning scenarios.


\subsection{Gap 4: Automated MAS Generation}

The efficiency of generating a Multi-Agent System (MAS) is evaluated in terms of convergence time and training overhead. Table~\ref{tab:mas_generation_efficiency} presents the results across all scenarios while \autoref{fig:learning_curves} shows learning curves for the main baselines in the mixed scenario over 2000 episodes.

\begin{table}[h]
    \centering
    \caption{MAS generation efficiency across all scenarios.}
    \label{tab:mas_generation_efficiency}
    \begin{tabular}{>{\raggedright\arraybackslash}m{3.5cm}>{\centering\arraybackslash}m{2cm}>{\centering\arraybackslash}m{2cm}}
        \hline
        \textbf{Baseline} & \textbf{Convergence Time (episodes)} & \textbf{Training Overhead (hours)} \\
        \hline
        Multi-Agent w/o Org. Spec. & 1800 & 4 \\
        \textbf{Multi-Agent w/ Hard Org. Spec. (KARMA)} & \textbf{950} & \textbf{1.5} \\
        \hline
    \end{tabular}
\end{table}

The role-guided learning narrows the search space for optimal policies, enabling faster convergence and reduced computational costs. These efficiency gains are particularly important for scaling MAS solutions to complex environments.

The learning curves in Figure~\ref{fig:learning_curves} demonstrate that KARMA achieves stable convergence significantly faster than the baseline without organizational specifications. By episode 950, KARMA exhibits minimal variance in cumulative rewards, whereas the baseline requires nearly double the episodes (1800) to reach comparable performance. This highlights the role of organizational constraints in guiding agents toward effective policies, thereby reducing exploration overhead.

\autoref{tab:mas_generation_efficiency}, shows reduced \textit{Convergence Time} by approximately 47\% compared to Multi-Agent w/o Org. Spec., showcasing the efficiency of role-guided learning in minimizing unnecessary exploration. Moreover, \textit{Training Overhead} is reduced by 62.5\%, showing a better practicality for large-scale systems where computational resources are a limiting factor.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.49\textwidth]{figures/learning_curves.pdf}
    \caption{Learning curves across baselines for the mixed scenario over 2000 episodes.}
    \label{fig:learning_curves}
\end{figure}


\subsection{Gap 5: Adaptability}
Adaptability assesses the ability of the system to maintain performance under dynamic workloads and heterogeneous conditions. The reward standard deviation in the mixed scenario was measured and presented in Table~\ref{tab:adaptability_comparison}.

\begin{table}[h]
    \centering
    \caption{Comparison of adaptability metrics in the mixed scenario.}
    \label{tab:adaptability_comparison}
    \begin{tabular}{>{\raggedright\arraybackslash}m{5cm}>{\centering\arraybackslash}m{3cm}}
        \hline
        \textbf{Baseline} & \textbf{Reward s.t.d (\%)} \\
        \hline
        Single-Agent w/o Org. Spec. & 12.4 \\
        Single-Agent w/ Hard Org. Spec. & 8.9 \\
        Multi-Agent w/o Org. Spec. & 7.3 \\
        \textbf{Multi-Agent w/ Hard Org. Spec. (KARMA)} & \textbf{3.1} \\
        \hline
    \end{tabular}
\end{table}

In the \textit{Mixed Scenario}, KARMA maintains stable performance across dynamically changing workloads, outperforming the next-best baseline (7.3\% variance). KARMA shows resilience against abrupt workload shifts with consistent rewards.

Organizational constraints provide agents with clear priorities, reducing conflicting actions and ensuring stability and allow distributed decision-making, which enhances responsiveness to dynamic conditions. The reduced reward variance underscores KARMA’s ability to generalize across diverse scenarios, making it well-suited for real-world deployments.

\subsection{Gap 6: Explainability}
\label{subsec:gap_explainability}

Explainability is qualitatively evaluated through trajectory clustering and quantitatively through the alignment of agent behaviors with predefined roles and missions.
\noindent \autoref{fig:trajectory_clustering_hrl} illustrates the dendrogram generated by hierarchical clustering of agents' action sequences with the four roles applied, using DTW as the similarity measure. The figure highlights the emergence of four distinct clusters, each corresponding to a specific organizational role, demonstrating the ability of the agents' behaviors to align with the predefined roles.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.49\textwidth]{figures/role_hierarchical_clustering.pdf}
    \caption{Dendrogram obtained after hierarchical clustering of agent trajectories for role inference in the mixed scenario.}
    \label{fig:trajectory_clustering_hrl}
\end{figure}

\begin{table}[h!]
    \centering
    \caption{Alignment of Agent Behavior with Roles and Missions.}
    \label{tab:alignment}
    {\scriptsize
    \begin{tabular}{>{\raggedright\arraybackslash}m{3.5cm}>{\centering\arraybackslash}m{2cm}>
    {\centering\arraybackslash}m{2cm}}
    \toprule
    \textbf{Baseline} & \textbf{Alignment Score (\%)} & \textbf{Clustering Purity (\%)} \\
    \midrule
    Multi-Agent w/o Org. Spec. & $\emptyset$ & 62.7 \\
    Multi-Agent w/ Soft Org. Spec. & 85.3 & 70.1 \\
    Multi-Agent w/ Hard Org. Spec. (KARMA) & \textbf{96.2} & \textbf{89.4} \\
    \bottomrule
    \end{tabular}
    }
\end{table}

KARMA shows the emergence of distinct behavioral patterns aligned with predefined roles validates KARMA's organizational model and has the highest alignment score (96.2\%), significantly outperforming Multi-Agent w/o Org. Spec. (78.5\%), showcasing well-coordinated agent behaviors. In adversarial scenarios, clustering purity is highest for KARMA, reflecting the clear differentiation of agent behaviors under organizational constraints.

Distinct clusters validate the role-specific behaviors and highlight the interpretability of agent policies. Baselines without organizational specifications show reduced explainability, as evidenced by lower clustering purity and alignment scores with soft constraints for organizational specifications.

% \subsection{General discussion}

% The experimental results demonstrate that KARMA effectively addresses several critical gaps in Kubernetes autoscaling. By integrating MARL with organizational principles, KARMA achieves notable improvements in operational resilience, adversarial robustness, and explainability. Its ability to decompose complex objectives into roles and missions ensures coordinated agent behavior, as reflected in the high success rates, reduced recovery times, and alignment with predefined roles observed across all scenarios. The use of a digital twin environment, enhanced by an MLP-based transition model, further strengthens KARMA's capacity to simulate realistic conditions, facilitating robust training and better policy generalization.

% However, KARMA is not without limitations. While it shows advancements in adaptability, its dependence on domain expertise for defining roles and missions could limit its applicability in domains where such expertise is scarce. Additionally, the computational overhead required for multi-agent training and digital twin modeling remains a challenge for large-scale deployments. The results also indicate that while KARMA effectively reduces latency and pending requests, some baselines, such as IMAM, perform comparably in specific scenarios like resource contention, highlighting areas where further refinement may be needed.

\section{Conclusion}
\label{sec:conclusion}
% Conclusion
%  - Résumé
%  - Résumé des Points faibles et Perspectives

This paper presented KARMA, a framework aimed at improving the operational resilience of Kubernetes clusters through a MAS-based approach.
The experimental results demonstrate that KARMA effectively addresses several critical gaps in Kubernetes autoscaling. By integrating MARL with organizational principles, KARMA achieves improvements in adversarial robustness, and explainability. Its ability to decompose complex objectives into roles and missions ensures coordinated agent behavior, as reflected in the high success rates, reduced recovery times. The use of MLP-based transition model, further strengthens KARMA's capacity to simulate realistic conditions, facilitating better policy generalization.
%
% The main contributions of this work include:
% \begin{itemize}
%     \item \textbf{Digital Twin Environment:} A realistic and representative simulation model derived from cluster traces, enabling safe and efficient policy learning through a digital twin.
%     \item \textbf{Organizationally Guided Design:} The use of roles and missions to decompose operational resilience into manageable sub-objectives, providing a systematic method for agent coordination and decision-making.
%     \item \textbf{Multi-Agent Reinforcement Learning (MARL):} Leveraging MARL algorithms to train agents collaboratively, ensuring adaptability and robustness in complex, multi-objective scenarios.
%     \item \textbf{Explainability and Analysis:} Analyzing agent behaviors using trajectory clustering and inter-agent interaction detection, enhancing interpretability and trust in agent decisions.
%     \item \textbf{Adversarial Scenario Handling:} Demonstrating the resilience of the proposed framework in scenarios such as DDoS attacks, which are critical for the reliability of cloud-native systems.
% \end{itemize}
%
% \

However, some aspects need to be further explored:
\begin{itemize}
    \item \textbf{Dependence on Domain Expertise:} Defining roles, missions, and reward structures relies heavily on domain-specific knowledge, which may limit the framework's generalizability to other domains.
    \item \textbf{Computational Overhead:} The training process, particularly with multi-agent configurations and organizational constraints, requires substantial computational resources, posing challenges for large-scale clusters.
    % \item \textbf{Sensitivity to Workload Shifts:} While KARMA demonstrates adaptability, abrupt changes in workload patterns or cluster configurations may require retraining or fine-tuning of agent policies.
    % \item \textbf{Evaluation Scope:} Although the framework was tested under diverse scenarios, including adversarial conditions, its performance on larger and more heterogeneous clusters remains to be validated.
\end{itemize}

Building on these initial results, several future research directions can be pursued to enhance KARMA's capabilities:
\begin{itemize}
    \item \textbf{Automated Role and Mission Generation:} Leveraging data-driven methods or pre-trained knowledge bases to automatically define roles and missions, reducing the reliance on domain expertise.
    % \item \textbf{Efficient Training Pipelines:} Exploring optimization techniques, such as transfer learning or distributed training, to mitigate the computational overhead of multi-agent training.
    \item \textbf{Dynamic Role Adjustment:} Investigating mechanisms for agents to dynamically adapt their roles and missions, enhancing flexibility and responsiveness.
    \item \textbf{Generalization to Larger Clusters:} Extending the framework to more complex, large-scale Kubernetes deployments with heterogeneous configurations.
    % \item \textbf{Integration with Real-Time Monitoring:} Incorporating advanced monitoring systems to further refine the digital twin model and provide more accurate state representations.
\end{itemize}

% While KARMA is not a universal solution to all Kubernetes autoscaling challenges, it provides a step forward in addressing key gaps in operational resilience, adaptability, and explainability. The framework's combination of MARL and organizational principles offers a promising foundation for future research and development.

\section*{Acknowledgment}
    This work was supported by \emph{Thales Land Air Systems} within the framework of the \emph{Cyb'Air} chair and the \emph{AICA IWG}.

\section*{References}

\nocite{alDhuraibi2017elasticDocker}

% \bibliographystyle{abbrv}
\bibliographystyle{IEEEtran}

\bibliography{references}

\end{document}
