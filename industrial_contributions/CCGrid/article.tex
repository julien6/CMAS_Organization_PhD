\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage[inline, shortlabels]{enumitem}
\usepackage{tabularx}
\usepackage{caption}
\usepackage{titlesec}
\usepackage[T2A,T1]{fontenc}
\usepackage[english]{babel}
\captionsetup{font=it}
\usepackage{ragged2e}
\usepackage{hyperref}
\addto\extrasenglish{%
  \renewcommand{\sectionautorefname}{Section}%
  \renewcommand{\subsectionautorefname}{Subsection}%
  \renewcommand{\subsubsectionautorefname}{Subsubsection}%
  \renewcommand{\tableautorefname}{Table}%
  \renewcommand{\figureautorefname}{Figure}%
}
\usepackage{pifont}
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%
\usepackage{footmisc}
\usepackage{multirow}

% --- Tickz
\usepackage{physics}
\usepackage{amsmath}
\usepackage{tikz}
\usepackage{mathdots}
\usepackage{yhmath}
\usepackage{float}
\usepackage{stfloats}
\usepackage{cancel}
\usepackage{color}
\usepackage{siunitx}
\usepackage{array}
\usepackage{multirow}
\usepackage{amssymb}
\usepackage{gensymb}
\usepackage{tabularx}
\usepackage{extarrows}
\usepackage{booktabs}
\usetikzlibrary{fadings}
\usetikzlibrary{patterns}
\usetikzlibrary{shadows.blur}
\usetikzlibrary{shapes}

% ---------

\usepackage{pdfpages}
\usepackage{booktabs}
\usepackage{csquotes}
\usepackage{lipsum}  
\usepackage{arydshln}
\usepackage{smartdiagram}
\usepackage[inkscapeformat=png]{svg}
\usepackage{textcomp}
\usepackage{tabularray}\UseTblrLibrary{varwidth}
\usepackage{xcolor}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\usepackage{cite}
\usepackage{amsmath}
\newcommand{\probP}{\text{I\kern-0.15em P}}
\usepackage{etoolbox}
\patchcmd{\thebibliography}{\section*{\refname}}{}{}{}

\setlength{\extrarowheight}{2.5pt}

% \renewcommand{\arraystretch}{1.7}

% \setlength{\extrarowheight}{2.5pt}
% \renewcommand{\arraystretch}{0.2}
% \renewcommand{\arraystretch}{1.7}

% --------------
\titleclass{\subsubsubsection}{straight}[\subsection]

\newcounter{subsubsubsection}[subsubsection]
\renewcommand\thesubsubsubsection{\thesubsubsection.\arabic{subsubsubsection}}
\renewcommand\theparagraph{\thesubsubsubsection.\arabic{paragraph}} % optional; useful if paragraphs are to be numbered

\titleformat{\subsubsubsection}
  {\normalfont\normalsize\bfseries}{\thesubsubsubsection}{1em}{}
\titlespacing*{\subsubsubsection}
{0pt}{3.25ex plus 1ex minus .2ex}{1.5ex plus .2ex}

\makeatletter
\renewcommand\paragraph{\@startsection{paragraph}{5}{\z@}%
  {3.25ex \@plus1ex \@minus.2ex}%
  {-1em}%
  {\normalfont\normalsize\bfseries}}
\renewcommand\subparagraph{\@startsection{subparagraph}{6}{\parindent}%
  {3.25ex \@plus1ex \@minus .2ex}%
  {-1em}%
  {\normalfont\normalsize\bfseries}}
\def\toclevel@subsubsubsection{4}
\def\toclevel@paragraph{5}
\def\toclevel@paragraph{6}
\def\l@subsubsubsection{\@dottedtocline{4}{7em}{4em}}
\def\l@paragraph{\@dottedtocline{5}{10em}{5em}}
\def\l@subparagraph{\@dottedtocline{6}{14em}{6em}}
\makeatother

\setcounter{secnumdepth}{4}
\setcounter{tocdepth}{4}
% --------------


\newcommand{\before}[1]{\textcolor{red}{#1}}
\newcommand{\after}[1]{\textcolor{green}{#1}}

\newcommand{\old}[1]{\textcolor{orange}{#1}}
\newcommand{\rem}[1]{\textcolor{red}{#1}}
\newcommand{\todo}[1]{\textcolor{orange}{\newline \textit{\textbf{TODO:} #1}} \newline \newline }

\makeatletter
\newcommand{\linebreakand}{%
  \end{@IEEEauthorhalign}
  \hfill\mbox{}\par
  \mbox{}\hfill\begin{@IEEEauthorhalign}
}
\makeatother




% ---------------------------


\begin{document}

\title{Streamlining Resilient Kubernetes Autoscaling with Multi-Agent Systems via an Automated Online Design Framework\\
    % {\footnotesize \textsuperscript{Note}}
    % \thanks{Identify applicable funding agency here. If none, delete this.}
}

% \IEEEaftertitletext{\vspace{-1\baselineskip}}

\author{

    \IEEEauthorblockN{Julien Soulé}
    \IEEEauthorblockA{\textit{Thales Land and Air Systems, BU IAS}}
    %Rennes, France \\
    \IEEEauthorblockA{\textit{Univ. Grenoble Alpes,} \\
        \textit{Grenoble INP, LCIS, 26000,}\\
        Valence, France \\
        julien.soule@lcis.grenoble-inp.fr}

    \and

    \IEEEauthorblockN{Jean-Paul Jamont\IEEEauthorrefmark{1}, Michel Occello\IEEEauthorrefmark{2}}
    \IEEEauthorblockA{\textit{Univ. Grenoble Alpes,} \\
        \textit{Grenoble INP, LCIS, 26000,}\\
        Valence, France \\
        \{\IEEEauthorrefmark{1}jean-paul.jamont,\IEEEauthorrefmark{2}michel.occello\}@lcis.grenoble-inp.fr
    }

    % \and

    % \IEEEauthorblockN{Michel Occello}
    % \IEEEauthorblockA{\textit{Univ. Grenoble Alpes,} \\
    % \textit{Grenoble INP, LCIS, 26000,}\\
    % Valence, France \\
    % michel.occello@lcis.grenoble-inp.fr}

    % \and

    \linebreakand

    \hspace{-0.5cm}
    \IEEEauthorblockN{Paul Théron}
    \IEEEauthorblockA{
        \hspace{-0.5cm}
        \textit{AICA IWG} \\
        \hspace{-0.5cm}
        La Guillermie, France \\
        \hspace{-0.5cm}
        %lieu-dit Le Bourg, France \\
        paul.theron@orange.fr}

    \and

    \hspace{0.5cm}
    \IEEEauthorblockN{Louis-Marie Traonouez}
    \IEEEauthorblockA{
        \hspace{0.5cm}
        \textit{Thales Land and Air Systems, BU IAS} \\
        \hspace{0.5cm}
        Rennes, France \\
        \hspace{0.5cm}
        louis-marie.traonouez@thalesgroup.com}}


\maketitle

\begin{abstract}
    In cloud-native critical systems, Kubernetes clusters with interdependent services often face challenges to their operational resilience due to poor workload management, resulting in resource blocking, bottlenecks, and continuous pod crashes. These vulnerabilities are further amplified in adversarial scenarios, such as Distributed Denial-of-Service (DDoS) attacks. Conventional Horizontal Pod Autoscaling (HPA) approaches struggle to address such dynamic conditions, while reinforcement learning-based methods, though more adaptable, typically optimize single objectives like latency or resource usage, overlooking broader failure scenarios.
    %
    We proposes a Multi-Agent System (MAS) for Kubernetes HPA, which decomposes operational resilience into failure-specific sub-objectives managed by specialized agents. To automate its development, we propose a four-phase framework: (1) modeling a digital twin from the real cluster traces; (2) training agents in simulation using roles and missions tailored to failure contexts; (3) analyzing agent behaviors for interpretability; and (4) transfering learned policies to the real cluster.
    %
    Experimental results demonstrate that the proposed MAS outperforms four state-of-the-art HPA systems in sustaining operational resilience under adversarial conditions accros two Kubernetes cluster scenarios with chained services.
\end{abstract}

\begin{IEEEkeywords}
    Adversarial, Horizontal Pod Autoscaling, Multi-Agent Reinforcement Learning, Multi-Agent System Design
\end{IEEEkeywords}

\section{Introduction}
\label{sec:introduction}

% Contexte
Cloud-native critical systems are increasingly reliant on Kubernetes to orchestrate and manage interdependent services~\cite{Pahl2019}. Horizontal Pod Autoscaling (HPA) is a widely adopted mechanism to dynamically adjust the number of pods based on resource usage, enabling systems to handle highly dynamic workloads~\cite{Hohpe2012}. However, failures such as pod crashes, resource contention, and bottlenecks can severely jeopardize the performance of all of the cluster's functionalities we globally refer to as operational resilience~\cite{Burns2016}. Worse, these failures may be exploited by attackers to degrade performance or induce outages, as seen in adversarial contexts like Distributed Denial-of-Service (DDoS) attacks~\cite{Koller2019}.

In such adversarial scenarios, malicious actors can exploit scaling mechanisms, exposing the limitations of conventional HPA systems~\cite{Kim2020}. Modern approaches have sought to address these gaps using Reinforcement Learning (RL), where an agent optimizes a single global objective such as minimizing latency or resource usage~\cite{Nguyen2019}. While these methods demonstrate adaptability, they often fall short in handling diverse failure scenarios to maitain \textit{Quality of Service} (QoS)~\cite{Castro2020}. For example, prioritizing responses to cascading pod crashes during an attack may be far more critical than reducing latency. These challenges highlight the need for an autoscaling system capable of dynamically balancing multiple sub-objectives to maitain all of the QoS to maximize operational resilience.

Achieving this shift from single-objective optimization to a multi-objective approach is complex~\cite{Shoham2008}. A single-agent RL system struggles to address such complexity due to the difficulty of coordinating responses to diverse and context-dependent failures~\cite{Jennings1998}. In contrast, Multi-Agent Systems (MASs) offer a promising paradigm by decomposing the overarching operational resilience maximization goal into sub-objectives handled by specialized agents~\cite{Shoham2008}. Considering an adversarial scenario, each defender agent can collaboratively contribute to complementary scaling actions towards reaching its own sub-objective, enabling more resilient and context-specific responses face to an attacker agent~\cite{Jennings1998}. We refer to the set of these collaborative agents as an HPA MAS. An HPA MAS actually builds upon the cyberdefense framework of Autonomous Intelligent Cybersecurity Agents (AICAs), which can be viewed as agents with specialized roles and missions collaboratively defending systems against attackers~\cite{Kott2018}.

% Problématique
However, designing HPA MASs tailored to a cluster presents significant challenges such as the need for detailed cluster knowledge, the time-consuming nature of manual design processes, and the difficulty of ensuring optimal agent behavior. Moreover, cluster changes require repeating the design process, increasing operational costs and complexity.

% Contribution
Among methdological works, we inspired from the \textit{Assisted MAS Organization Engineering Approach} (AOMEA)~\cite{soule2024aomea} that shows to align the most with automation and safety challenges. Based on AOMEA, we propose the \textit{Kubernetes Autoscaling with Resilient Multi-Agent system} (KARMA) to automate the design and implementation process through four sequential phases:
\begin{enumerate*}[label=\textbf{\arabic*)}, itemjoin={;\quad }]
    \item \textbf{Modeling}: Creating a digital twin of the cluster from real-world traces to simulate failure scenarios
    \item \textbf{Training}: Training agents in simulation using roles and missions that integrate explicit rule-based and guidance strategies
    \item \textbf{Analyzing}: Validating trained agents' behaviors and extracting design insights through empirical analysis
    \item \textbf{Transferring}: Running trained agents to apply their learned behaviors via the real Kubernetes API.
\end{enumerate*}

This framework enables to iteratively updates the simulation model with newly collected traces, enabling adaptation to cluster changes. We validated our approach on two adversarial scenarios from the "Chained Service" Kubernetes environment. The MASs were generated with minimal manual intervention, and demonstrate originality and robustness. They consistently outperformed state-of-the-art HPA systems, including AWARE~\cite{AWARE}, Gym-HPA~\cite{GymHPA}, IMAM~\cite{IMAM}, and Libra~\cite{Libra}, in maximizing operational resilience.

% Organisation
The remainder is structured as follows:
\autoref{sec:related_work} reviews existing HPA techniques and their limitations in dynamic environments.
\autoref{sec:proposed_approach} details our framework leveraging related concepts for each phase.
\autoref{sec:experiments} describes the experimental setup.
\autoref{sec:results} presents and discusses results.
\autoref{sec:conclusion} concludes and provides future directions.

% ===================================

\section{Related Work}
\label{sec:related_work}

\begin{table}[h!]
    \centering
    \caption{A KARMA overview regarding selected HPA Systems}
    \label{tab:autoscaling_criteria}
    {\scriptsize
    \renewcommand{\arraystretch}{1.1}
    % \resizebox{0.5\textwidth}{!}{%
    \begin{tabular}{>{\raggedright\arraybackslash}m{1.22cm}>{\centering\arraybackslash}m{0.34cm}>{\centering\arraybackslash}m{0.34cm}>{\centering\arraybackslash}m{0.34cm}>{\centering\arraybackslash}m{0.34cm}>{\centering\arraybackslash}m{0.34cm}>{\centering\arraybackslash}m{0.34cm}>{\centering\arraybackslash}m{0.34cm}>{\centering\arraybackslash}m{0.34cm}>{\centering\arraybackslash}m{0.75cm}}
    \hline
    \textbf{Criterion} & \vspace{-0.3cm}\textbf{\cite{GymHPA}} & \vspace{-0.3cm}\textbf{\cite{AWARE}} & \vspace{-0.3cm}\textbf{\cite{IMAM}} & \vspace{-0.3cm}\textbf{\cite{Libra}} & \vspace{-0.3cm}\textbf{\cite{QoSRL}} & \vspace{-0.3cm}\textbf{\cite{AHPA}} & \vspace{-0.3cm}\textbf{\cite{KOSMOS}} & \vspace{-0.3cm}\textbf{\cite{COPA}} & \textbf{KARMA} \\
    \hline
    \hline
    Adversarial Scenarios & No & Partial & No & No & No & No & No & Partial & Yes \\
    \hline
    Multi-objective & No & Yes & Partial & Partial & Yes & No & Yes & No & Yes \\
    \hline
    Automation & High & Mid. & Mid. & Mid. & High & Mid. & Mid. & Mid. & High \\
    \hline
    Learning & Yes & Yes & Yes & Yes & Yes & No & No & No & Yes \\
    \hline
    Multi-Agent System & No & No & Yes & No & No & No & No & No & Yes \\
    \hline
    Simulation & Yes & No & Yes & Yes & Yes & No & No & No & Yes \\
    \hline
    Real env. & No & Yes & Yes & Yes & Yes & Yes & Yes & Yes & Yes \\
    \hline
    Explainable & No & No & No & No & No & No & No & No & Yes \\
    \hline
    Adaptation & High & Mid. & High & Mid. & Mid. & High & High & Mid. & High \\
    \hline
    Safety Guarantees & No & No & No & No & No & No & No & No & Yes \\
    \hline
    \end{tabular}%
    }
  \end{table}


Autoscaling in Kubernetes has traditionally relied on metrics-based approaches, such as the default Kubernetes Horizontal Pod Autoscaler (KHPA), which adjusts the number of pods based on CPU and memory utilization~\cite{KubernetesChallenges}. While effective for basic scaling, such methods fail to address dynamic or adversarial workloads, as they rely on reactive, threshold-based rules~\cite{AutoscalingLimitations}. To overcome these limitations, recent research has turned to Machine Learning (ML) and RL.

% \subsection*{Reinforcement Learning-Based Systems}
Among RL-based approaches, four systems stand out due to their innovation, applicability, and relevance:
%
\begin{itemize}
    \item \textbf{Gym-HPA}~\cite{GymHPA} serves as a benchmark RL environment, enabling experimentation with various RL algorithms. It excels in adaptability to simulated workloads with a high degree of automation but lacks multi-objective support, explainability, and real-world applicability
    \item \textbf{AWARE}~\cite{AWARE} incorporates RL to optimize autoscaling decisions while balancing quality-of-service (QoS) objectives, such as response time and throughput. It partially considers adversarial scenarios but struggles with high automation levels and multi-agent coordination
    \item \textbf{IMAM}~\cite{IMAM} integrates RL with a multi-agent framework, making it highly adaptive in microservice-based architectures. However, it lacks safety guarantees and explainability, limiting its effectiveness in adversarial contexts
    \item \textbf{Libra}~\cite{Libra} introduces traffic-aware scaling in edge environments. While it provides valuable insights into traffic optimization, it lacks consideration for multi-objective trade-offs or safety-critical guarantees.
\end{itemize}

These systems highlight significant progress in RL-based autoscaling but share common limitations: a lack of comprehensive adversarial adaptability, limited support for multi-agent systems, and no explicit focus on explainability or safety guarantees.
%
% \subsection*{Hybrid and Rule-Based Approaches}
Other notable systems combine ML or rule-based strategies with traditional autoscaling:
%
\begin{enumerate*}[label={}, itemjoin={;\quad }]
    \item \textbf{QoS-Aware RL}~\cite{QoSRL} focuses on maintaining QoS under dynamic workloads but does not integrate seamlessly with Kubernetes-native features or consider adversarial scenarios
    \item \textbf{AHPA}~\cite{AHPA} and \textbf{KOSMOS}~\cite{KOSMOS} explore adaptive and combined vertical-horizontal scaling strategies, offering high adaptability but lacking learning capabilities
    \item \textbf{COPA}~\cite{COPA} emphasizes combined metrics-based autoscaling but remains reactive and limited in adversarial scenarios.
\end{enumerate*}

% \subsection*{Positioning Our Contribution}
KARMA addresses key gaps in \textbf{operational resilience} for autoscaling by introducing an automated HPA MAS design framework. Unlike conventional approaches, which often fail under \textbf{adversarial conditions}, KARMA decomposes \textbf{availability optimization} into failure-specific sub-objectives, enabling agents to handle \textbf{bottlenecks}, \textbf{resource contention}, and \textbf{pod crashes}. It leverages \textbf{digital twin modeling}, \textbf{automated MAS generation}, and reinforcement learning to streamline design while requiring minimal manual intervention. The framework also emphasizes \textbf{explainability}, ensuring interpretable decision-making, and supports \textbf{constraint satisfaction} and \textbf{adaptability} to handle dynamic workloads.


% ===================================
\section{KARMA: A framework for HPA MAS design and development}
\label{sec:proposed_approach}

This section overviews the KARMA framework to helps in designing a HPA MAS then details each one of its phase.

\subsection{KARMA Overview}

\begin{figure}[h!]
    \centering
    \input{figures/karma_architecture/karma_architecture.tex}
    \caption{Overview of the KARMA framework in use with a Kubernetes cluster}
    \label{fig:karma_architecture}
\end{figure}

As illustrated in \autoref{fig:karma_architecture}, the KARMA framework operates alongside the Kubernetes cluster, comprising \textbf{worker nodes} that host \textbf{pods}—the atomic unit in Kubernetes containing \textbf{containers} running the actual processes. Pods are organized into \textbf{services} and managed by \textbf{deployments} to update the number of pod refered to as \textbf{replica}. The KARMA framework functions as a separate software layer, interfacing with both the Kubernetes API and Prometheus.

\textbf{1)} Metrics related to availability are gathered as states by \textit{Prometheus}~\cite{prometheus}, a widely adopted time-series metrics database, and processed by KARMA's \textbf{Modeling Component}.

\textbf{2)} Collected states are used to construct a \textit{digital twin} of the cluster as a state transition model. A reward function, defined as a weighted sum of Quality of Service (QoS)-specific sub-rewards, drives the operational resilience. % The environment is implemented using the PettingZoo library~\cite{Terry2021} to train agents in a MARL-compatible simulation.

\textbf{3)} The \textbf{Training Component} trains agents to maximize rewards to improve operational resilience. Agents are optionally guided by \textit{roles} (constraints shaping their actions) and \textit{missions} (incremental objectives facilitating policy convergence), following the AOMEA methodology~\cite{soule2024aomea}.

\textbf{4)} The \textbf{Analyzing Component} visualize the learned policies through trajectory clustering and hierarchical visualization, ensuring interpretability, alignment with objectives, and resilience to dynamic workloads.

\textbf{5)} The \textbf{Transfer Component} deploys trained policies to the real Kubernetes cluster via the Kubernetes API, executing replica adjustments \textbf{(6)}. This establishes an iterative feedback loop, enabling continuous policy refinement.

KARMA integrates simulation-based learning with real-world Kubernetes operations in a closed-loop process. Metrics collected from the cluster guide policy updates, while trained agents' decisions are applied back to the cluster. This iterative process ensures ongoing adaptability, robustness, and resilience under diverse conditions.


\subsection{Phase 1: Modeling}
% Modeling:
%  - observation
%  - actions
%  - rewards -> Faire une récompense qui englobe toutes les QoS ayant pour père la QoS (ne pas faire encore référence aux fonctions de récompense pour l'instant) "availability"
%  - transition -> Transition modeling + MLP -> donner détails des paramètres

In this phase, we assume the initial defender and attacker agents have applied various action in the cluster, leading to a collection of representative amount of traces. Relying on the formalization of the environment as a zero-sum \textbf{Stochastic Game (SG)}~\cite{shapley1953stochastic}, we can provide a near-realistic simulation environment from these collected traces. The SG is characterized by the tuple $\mathcal{SG} = (\mathcal{A}, S, A, T, R, \gamma)$, where $\mathcal{A} = \{\mathcal{A}_d, \mathcal{A}_a\}$ is the agents set comprising $n = |\mathcal{A}_d|$ defender agents and one single attacker agent in $\mathcal{A}_a$; and $\gamma \in [0, 1)$ is the discount factor for future rewards. The remaining components are detailed below:

\noindent \paragraph{\textbf{State Space}} $S$ is the state space of the Kubernetes cluster. A state denoted as $s \in S$, consists of metrics to characterizing system performance for each of the $d = |D|$ micro-service deployments:
$$
s = (n_{id}, d_{dep}, d_{des}, d_{err}, d_{rem}, r_{cpu}, r_{ram}, t_{in}, t_{out})^d
$$
$n_{id} \in \mathbb{N}$: the number of the deployment; \quad
$d_{dep} \in \mathbb{N}$: the number of deployed pods; \quad 
$d_{des} \in \mathbb{N}$: the number of desired pods; \quad
$d_{err} \in \mathbb{N}$: the number of failed pods; \quad
$d_{rem} \in \mathbb{N}$: the number of remaining requests to be processed in the queue; \quad
$r_{cpu} \in \mathbb{R}$: the total aggregated CPU (in m) of the pods; \quad
$r_{ram} \in \mathbb{R}$: the total aggregated memory (in Mi) of the pods; \quad
$t_{in} \in \mathbb{R}$: the average received traffic (in Kbps); \quad
$t_{out} \in \mathbb{R}$: the average transmitted traffic (in Kbps).
% TODO: ajouter les métriques spécifiques pour pouvoir vérifier s'il y a des goulots d'étranglement, 

% These metrics are continuously collected using Prometheus~\cite{prometheus}, a widely adopted monitoring and metrics database system, enabling the capture of time-series data for each pod, deployment, and the cluster as a whole.

\noindent \paragraph{\textbf{Action Space}} $A = A_d^n \times A_a$ is the action space with $A_d$ and $A_a$ are the action spaces for a defender and the attacker agents respectively:
$$
a_d \in A_d = (\text{service\_id}, \text{replica\_change})
$$
$\text{\textbf{service\_id}} \in \mathbb{N}$ identifies the target service (through deployment), and $\text{\textbf{replica\_change}} \in [-\alpha, +\alpha]$ indicates the change in replicas (by default $\alpha = 3$). Actions from this space are one-hot encoded as a Box Gym Space~\cite{openAIGymActionSpaces}: for example, the defender actions $(2,1)$, $(0,-2), (1,0)$ mean the services with id numbers equal to $2$, $0$, and $1$ have their respective replica numbers changed by adding $1$, $-2$, $0$.
%
$$
a_a \in A_a = (\text{entry\_point\_id}, \text{rate\_change}, \text{data\_change})
$$
$\text{\textbf{entry\_point\_id}} \in \mathbb{N}$ specifies the service entry point;
$\text{\textbf{rate\_change}} \in \{\text{high\_decrease}, \text{low\_decrease}, \text{no\_change}, \allowbreak \text{low\_increase}, \allowbreak \text{high\_increase}\}$ changes the incoming traffic based on a factor $\kappa$ (by default $\kappa = 1$); and $\text{\textbf{data\_change}} \in \{\text{no\_alteration}, \allowbreak \text{low\_alteration}, \allowbreak \text{high\_alteration}\}$ specifies the degree of data alteration based on factor $\sigma \in [2,\infty[$ (by default $\sigma = 10$). Actions from this space are one-hot encoded as a Box Gym space: for example, the attacker actions $(0,1,2), (2,-1,0), (1,2,1)$ mean that entrypoint services id number 0, 2, and 1 would have their respective traffic-in rates increased by $1 \times \kappa, -1 \times \kappa, 2 \times \kappa$, and respective probabilities to crash due to data alteration are changed by $\frac{2}{\sigma}, \frac{0}{\sigma}, \frac{1}{\sigma}$.

\

\noindent \paragraph{\textbf{Reward Functions}} $R = \{R_d, R_a\}$, with $R_d: S \times A_d^n \to \mathbb{R}$ and $R_a: S \times A_a \to \mathbb{R} = - R_d$ are respectively the reward function for the defender agents based on operational resilience and the attacker one as its opposed value.
To measure operational resilience, we use the linear combination of the following metrics:
%
\begin{itemize}
    \item $\text{Success Rate } (sr) : \frac{\text{Successful Requests (2xx)}}{\text{Total Received Requests}}$

    \item $\text{Pod Failure Rate } (pfr) : \frac{\text{Failed Pods}}{\text{Total Deployed Pods}}$
    
    \item $\text{Latency Ratio } (lr) : \frac{\text{Measured Latency}}{\text{Maximum Acceptable Latency}}$
    
    \item $\text{Entry Point Availability } (epa) : \frac{\text{Available Entry Points}}{\text{Total Entry Points}}$
    
    \item $\text{Traffic Capacity Ratio } (tcr) : \min\left(1, \frac{\text{Outgoing Traffic}}{\text{Expected Traffic}}\right)$
\end{itemize}
%
$\text{Operational Resilience }: or(s) = w_1 \times sr
\allowbreak + w_2 \times (1 - pfr)
\allowbreak + w_3 \times (1 - lr)
\allowbreak + w_4 \times epa
\allowbreak + w_5 \times tcr
\text{where } (w_1, w_2, w_3, w_4, w_5) \text{ are relative weights.}$
$$
R_a(s, a_d, a_a) = or(s) = -R_d(s, a_d, a_a)
$$
\text{where $s$ is the current state after applying actions.}

\noindent \paragraph{\textbf{Transition Modeling}} $T: S \times A \rightarrow S = S \times A_d^n \times A_a \to S$ is the real state transition function dictating the next state when the joint actions of the defender agents and the attacker's one are applied. Relying a representative set of collected transitions $\mathcal{T} = \langle(s, a_d^n, a_a, s')_{t\in \mathbb{N}, t \leq w}\rangle$ over a $w$ time window, we can form a partial state transition function $\hat{T}_t$ defined as:
%
$$
\hat{T}_t(s, a_d^n, a_a) =
\begin{cases} 
    s' & \text{if } (s, a_d^n, a_a, s') \in \mathcal{T} \\
    \emptyset & \text{ otherwise}
\end{cases}
$$

To cover no recorded transition, we introduce a Multi-Layer Perceptron (MLP)-based approximator $\hat{T}_a$ to learn from collected transitions and predict the next likely state. The choice of an MLP is motivated by its universal approximation capabilities and the assumption that the next state of a Kubernetes cluster depends only on the current state and the chosen actions.
The MLP approximator has three hidden layers, striking a balance between expressiveness and computational efficiency. The input and output layer dimensions correspond to the size of the state space, while each hidden layer consists of 128 neurons. Rectified Linear Unit (ReLU) activation functions are applied to the hidden layers, and a linear activation function is used at the output layer to produce the predicted next state. The model is optimized using the Adam optimizer with a learning rate of $10^{-3}$, minimizing the Mean Squared Error (MSE) loss function, expressed as
$$
\mathcal{L} = \frac{1}{N} \sum_{i=1}^N |T(s_i, a_{d,i}, a_{a,i}) - s'_i|^2
$$
where $N$ is a number of representative transitions to ensure generalization.

The complete modeled transition function $\hat{T}$ is defined as:
$$
\hat{T}(s, a_d, a_a) = 
\begin{cases} 
\hat{T}_t(s, a_d, a_a) & \text{if } (s, a_d, a_a) \in \text{Domain}(\hat{T}_t), \\
\hat{T}_a(s, a_d, a_a) & \text{otherwise}.
\end{cases}
$$

\noindent \paragraph{\textbf{Digital Twin Environment}} The defined action and state space with approximated transition function $\hat{T}$, combined with the reward functions $R_d$ and $R_a$, forms the basis of the digital twin environment implemented using the PettingZoo library~\cite{Terry2021}. This environment enables simulating the Kubernetes cluster from the defined SG, enabling safe exploration of defender agents against different attacker strategies.



\subsection{Phase 2: Training}
\label{sec:training}

In this phase, MARL algorithms are applied within the modeled environment to enable agents to learn and maximize cumulative rewards. As suggested in AOMEA~\cite{soule2024aomea}, we leverage the $\mathcal{M}OISE^+$ organizational model to bring ways to control/guide the MARL training. In the KARMA framework it results in a decomposition of the overarching goal of \textit{operational resilience} into sub-objectives. Each sub-objective is assigned to a specific agent as a mission, while roles define rule-based strategies to guide agent operations.\\

\noindent \textbf{Agent Roles and Missions}

A \textbf{role} is formally represented by a \textbf{Role Action Guide} (RAG), which restricts an agent's permissible actions:
$$
rag(h, \omega) = (\{a_1, a_2, \dots, a_i\}, ch),
$$
where \(h\) represents the trajectory or history, \(\omega\) is the agent's observation, and \(ch \in \{0,1\}\) is the constraint hardness, set to 1 by default. A hard constraint (\(ch = 1\)) strictly limits the agent's available actions to authorized ones, while a soft constraint (\(ch = 0\)) allows for exploratory actions but adjusts rewards with bonuses or penalties based on compliance with the authorized action set. For example, the \textbf{Bottleneck Resolution Agent} has a role restricting its actions to modifying pod replicas within a specific service graph, ensuring it does not affect unrelated services. If a bottleneck is detected in Service A that causes delays in Service B, the agent adheres to its role by increasing the pod replicas of Service A to resolve the issue without overstepping constraints.

A \textbf{mission} is a set of intermediate goals designed to assist in achieving the overarching operational resilience objective. Missions are represented by a \textbf{Goal Reward Guide} (GRG), which incentivizes mission completion:
$$
grg(h) = r_b,
$$
where \(h \in H\) represents the current agent trajectory and \(r_b \in \mathbb{R}\) is the associated reward bonus or penalty. This mechanism narrows the optimization focus to critical resilience tasks. For instance, the \textbf{Adversarial Response Agent} is assigned a mission to maintain service availability during Distributed Denial-of-Service (DDoS) attacks. It earns a reward bonus for ensuring that a predefined percentage of incoming requests is successfully handled, despite the increased load, by dynamically adjusting replicas. The mission aligns the agent's focus on balancing load while avoiding resource over-provisioning.\\

By defining specific \textbf{(role, mission)} pairs, the KARMA framework assigns specialized tasks to agents to tackle distinct challenges in chained Kubernetes services. These roles and missions enable distributed yet coordinated policy learning, ensuring that agents align their actions to maximize overall \textit{operational resilience}. For example, while the Bottleneck Resolution Agent alleviates bottlenecks to ensure steady service throughput, its actions complement those of the Adversarial Response Agent, which adjusts replicas to mitigate traffic surges. This interdependence highlights the importance of coordinating roles and missions to address shared challenges.

\paragraph*{Algorithms and training pipeline}

\

KARMA integrates MAPPO~\cite{yu2021mappo} (Multi-Agent Proximal Policy Optimization) and MADDPG~\cite{lowe2017multi} (Multi-Agent Deep Deterministic Policy Gradient), which are well-suited for optimizing policies in complex multi-agent environments. These algorithms stabilize policy updates and enable effective inter-agent communication, ensuring coordination on strategies that maximize \textit{operational resilience}. Communication is critical in KARMA due to the interdependencies between agent roles and missions, such as how bottleneck resolution can influence adversarial response priorities. By sharing observations, agents dynamically align their strategies to address shared goals.

The training pipeline in KARMA follows a systematic sequence to optimize agent behaviors. Initially, policies (\(\pi_i\)), roles (RAG), and missions (GRG) are defined for all agents to establish a structured framework. Agents participate in simulation runs, generating trajectories within the digital twin environment. At each step, roles with hard constraints restrict the available actions to authorized ones, while soft constraints influence rewards by applying bonuses or penalties based on compliance. Missions add further reward shaping by incentivizing goal-aligned trajectories. The \textit{Optuna} framework is used for Hyper-Parameter Optimization (HPO), iteratively refining policies by adjusting hyperparameters and role specifications to improve convergence and enhance system resilience.

Convergence is deemed successful when the standard deviation of cumulative rewards across episodes falls below a predefined threshold, and cumulative rewards exceed an empirically determined minimum value.


\subsection{Phase 3: Analysis}
\label{sec:analysis}

The analysis phase in the KARMA framework aims to validate and interpret the behaviors of trained agents, ensuring their alignment with operational objectives and their robustness in dynamic or adversarial environments. \textbf{Explainability} is essential for verifying that agents' collective behaviors align with system objectives and for building trust with human operators~\cite{biran2017explanation, guidotti2018survey}. While each agent in KARMA has a defined role and mission, the complexity arises when agents must coordinate in dynamic scenarios, such as:
\begin{itemize}
    \item \textbf{Normal conditions}, where all agents act independently without risking conflicts.
    \item \textbf{Critical conditions}, requiring rapid intervention and prioritization of specific agents.
    \item \textbf{Adversarial conditions}, such as Distributed Denial-of-Service (DDoS) attacks, where coordination must prevent conflicting or redundant actions.
\end{itemize}

For example, during a DDoS attack, the adversarial response agent must take precedence, ensuring that actions such as adjusting replicas are not misapplied by other agents. This necessitates understanding how agents implicitly or explicitly negotiate priorities and coordinate actions to maximize operational resilience~\cite{shoham2008multiagent}.

\paragraph*{\textbf{Inferring Roles and Missions}}

To enhance the explainability of agent behaviors, roles and missions can be derived from the trajectories of trained agents using data-driven methods:
\begin{itemize}
    \item \textbf{Role Generation}: Hierarchical clustering identifies recurring sequences of actions that define an agent's role. The distance between action sequences is computed using Dynamic Time Warping (DTW)~\cite{berndt1994using}:
    \[
    d(\tau_i, \tau_j) = \min_{\pi} \sum_{k=1}^{|\pi|} \|a_{t_k}^i - a_{t_k}^j\|_2,
    \]
    where $\pi$ is the optimal alignment path. Clustering hyperparameters, such as the number of clusters, are empirically tuned to minimize noise and avoid generating spurious roles. Clusters are annotated to define abstract roles, such as "bottleneck resolution" or "DDoS mitigation."
    \item \textbf{Mission Generation}: K-means clustering groups trajectories based on similarity in states visited. The cluster centers represent common objectives, and recurrent observations in high-reward trajectories are sampled to define intermediate goals:
    \[
    g_i = \mathcal{S}_j, \quad \text{where } \mathcal{S}_j = \{s \in \tau_i | \mathbb{P}(s) > \epsilon\}.
    \]
    Here, $\mathbb{P}(s)$ represents the probability of visiting a state $s$ within successful trajectories. Hyperparameters of K-means are empirically optimized to minimize noise in generated missions.
\end{itemize}

\paragraph*{\textbf{Detecting Inter-Agent Interactions}}

Explainability in KARMA also requires an understanding of how agents coordinate their actions in different contexts. This is achieved by analyzing inter-agent relationships using the concepts of \textbf{MOISE+}, which formalizes relations such as acquaintance, authority, and communication~\cite{hubner2002moise}:
\begin{itemize}
    \item \textbf{Acquaintance Relations}: Derived from shared observations or reward signals during training, these relations represent which agents are aware of each other's actions.
    \item \textbf{Authority Relations}: Captured through dependency patterns, authority relations prioritize one agent's decisions over another's in specific scenarios. For instance, an specific agent may hold authority during a DDoS attack.
    \item \textbf{Communication Relations}: Inferred from synchronized actions or information-sharing events. For example, agents sharing traffic load data to coordinate replica adjustments during surges.
\end{itemize}

To visualize these relations, a directed graph is constructed where nodes represent agents and edges represent relations, annotated with interaction type. Considering a time window, we can generate a graph based on detected relations and may evolves dynamically on the next time window, highlighting changes in interaction patterns.

In addition, we also determine the \textbf{patterns of coordination} that aim to understand the mutual impact of an agent's actions onto another agent's trajectory  over time. Among Sequential Pattern Mining, we favoured the use of the \textit{PrefixSpan} algorithm to identify recurring action sequences across agents during specific scenarios. For example, patterns may show that one agent defers actions to another during adversarial conditions. Detected sequences can be represented as a sequence diagram illustrates how agents interact over time, showing causal relationships and dependencies in resolving challenges.



\subsection{Phase 4: Transfer}
\label{sec:transfer}

% Transfering: Processus de transfert des comportements appris au cluster réel.
%  - récupérer les politiques entrainés dans un sous-module
%  - lancer les politiques entrainés avec l'état courant collecté
%  - appliquer les actions choisies par ces politiques via l'API K8s

The final phase in the KARMA framework deploys trained and validated agent policies into the live Kubernetes cluster. This is achieved through a streamlined execution loop that dynamically adapts to real-time cluster conditions.

The transfer mechanism operates as a continuous loop:
\begin{enumerate}
    \item \textbf{State Collection:} Real-time metrics such as CPU usage, memory consumption, pod status, and network traffic are collected from the Kubernetes Metrics Server and Prometheus~\cite{prometheus}.
    \item \textbf{Policy Execution:} Each agent's policy $\pi_i$ computes an action $a_t^i$ based on the current state $s_t$, selecting adjustments such as scaling pod replicas for specific deployments.
    \item \textbf{Action Application:} The computed actions are translated into Kubernetes API requests, directly modifying deployment configurations.
\end{enumerate}
This loop ensures agents continuously respond to changes in workload and system conditions, maintaining operational resilience.

The agents interact seamlessly with Kubernetes:
\begin{itemize}
    \item Actions are applied to deployments via the Kubernetes API.
    \item Real-time metrics are gathered using Prometheus, ensuring up-to-date cluster state observations.
    \item KARMA operates independently of native Kubernetes controllers, allowing smooth coexistence with default Horizontal Pod Autoscaler (HPA) mechanisms.
\end{itemize}

To ensure robustness over time, the framework integrates a feedback loop:
\begin{itemize}
    \item Metrics from the live cluster are used to update the digital twin model.
    \item Agents are periodically retrained on this updated model to refine their policies for new workload patterns or adversarial conditions.
    \item Refined policies are redeployed to the cluster, maintaining high adaptability and resilience.
\end{itemize}



% =======================================
\section{Experimental Setup}
\label{sec:experiments}
% Experimental setup:
%  - Présenter "Chained Services"
%  - CybMASDE: présenter comme un moyen d'implémenter KARMA et présenter la configuration logicielle
%  - Configuration matérielle (pour entrainement et analyse)
%  - Protocole d'experimentation et d'analyse (prend en compte un espèce d'étude d'ablation dans les baselines)
%  - Métriques d'évaluation
%  - Baselines: MA x (Org. Spec.)
%     - modèle sans specifications organisationnelles avec 1 seul agent (état de l'art)
%     - modèle avec spécifications organisationnelles du modèle "fort" avec 1 seul agent (état de l'art?)
%     - modèle sans spécifications organisationnelles avec plusieurs agents (état de l'art?)
%     - modèle avec spécifications organisationnelles du modèle "faible" avec plusieurs agents
%     - modèle avec spécifications organisationnelles du modèle "fort" avec plusieurs agents

This section outlines the experimental setup for evaluating KARMA's ability to address gaps in \textbf{operational resilience} in Kubernetes clusters under dynamic and adversarial conditions. The setup validates how KARMA handles \textbf{bottlenecks}, \textbf{resource contention}, and \textbf{pod crashes} while ensuring \textbf{availability optimization}. The evaluation also emphasizes \textbf{explainability} and highlights the efficiency of \textbf{digital twin modeling} and \textbf{automated MAS generation}.

\subsection{Description of the Kubernetes Cluster and Configuration}

The evaluation environment consists of a Kubernetes cluster simulating a \textbf{chained services} architecture. Each service comprises a set of microservices hosted in pods and managed by deployments. Pods are scaled horizontally using KARMA's Multi-Agent System (MAS) policies, compared against baseline methods.

\textbf{Cluster Configuration:}
\begin{itemize}
    \item \textbf{Worker Nodes:} 20 worker nodes with 8 vCPUs, 32 GB RAM, and 1 Gbps network bandwidth.
    \item \textbf{Metrics Collection:} \textit{Prometheus} monitors pod and service metrics, including CPU usage, memory consumption, latency, and traffic throughput.
    \item \textbf{Failure Simulation:} Bottlenecks and cascading failures are induced by resource-intensive workloads, while adversarial conditions (e.g., DDoS attacks) are emulated using Locust~\cite{locust2021}.
\end{itemize}

\subsection{Implementation of KARMA with CybMASDE}

The KARMA framework leverages \textit{CybMASDE}~\footnote{Source code available with extra detail and Jupyter Notebook at: \url{https://anonymous.4open.science/r/KARMA-8FCB/README.md}} to automate Multi-Agent System (MAS) design and deployment. The framework includes:
\begin{itemize}
    \item \textbf{Digital Twin Modeling:} A simulation environment replicates Kubernetes dynamics using real-world workload traces.
    \item \textbf{MARL Training:} \textit{MAPPO}~\cite{yu2021mappo} and \textit{MADDPG}~\cite{lowe2017multi} are used to train agents in the digital twin environment.
    \item \textbf{Organizational Specifications:} Roles and missions defined for agents guide the training process, ensuring coordinated behavior and explainability.
    \item \textbf{Deployment Integration:} Trained policies interact with the Kubernetes API to adjust pod replicas in real time.
\end{itemize}

\subsection{Hardware Configuration}

The experimental setup is deployed on a high-performance computing cluster:
\begin{itemize}
    \item \textbf{Training Nodes:} NVIDIA Tesla V100 GPUs (16GB), Intel Xeon Platinum CPUs (2.3 GHz, 16 cores), 128 GB RAM.
    \item \textbf{Cluster Nodes:} Kubernetes worker nodes with 8-core CPUs, 32 GB RAM, and 1 Gbps network bandwidth.
    \item \textbf{Analysis Nodes:} Intel Xeon Gold CPUs (2.5 GHz, 24 cores), 64 GB RAM.
\end{itemize}

\subsection{MARL Algorithms Used}

KARMA employs two state-of-the-art Multi-Agent Reinforcement Learning (MARL) algorithms to train agents in simulation:
\begin{itemize}
    \item \textbf{MAPPO (Multi-Agent Proximal Policy Optimization)}~\cite{yu2021mappo}: A centralized policy gradient algorithm ensuring stable learning in cooperative settings.
    \item \textbf{MADDPG (Multi-Agent Deep Deterministic Policy Gradient)}~\cite{lowe2017multi}: A decentralized actor-critic method facilitating inter-agent communication and coordination.
\end{itemize}

These algorithms are chosen for their robustness in optimizing multi-agent behaviors while ensuring adaptability and constraint satisfaction.

\subsection{Experimental Protocol}

The evaluation protocol follows a structured process to ensure reproducibility:
\begin{enumerate}
    \item \textbf{Modeling and Training:} A digital twin environment models Kubernetes cluster dynamics, and agents are trained to maximize operational resilience.
    \item \textbf{Deployment:} Trained policies are deployed in the real cluster, and their performance is observed.
    \item \textbf{Scenarios:} The following scenarios are executed:
    \begin{enumerate}
        \item \textbf{Baseline Workload:} Gradually increasing workloads simulate real-world traffic patterns.
        \item \textbf{Adversarial Load:} DDoS attacks, simulated using Locust~\cite{locust2021}, test the system's robustness.
        \item \textbf{Failure Scenarios:} Cascading failures and resource contention are induced to evaluate recovery efficiency.
    \end{enumerate}
    \item \textbf{Data Collection and Analysis:} Metrics such as latency, throughput, and resource utilization are analyzed to validate operational resilience.
\end{enumerate}

\subsection{Evaluation Metrics}

To assess KARMA's effectiveness, the following metrics are used:
\begin{itemize}
    \item \textbf{Operational Resilience:} The percentage of successfully handled requests under varying loads.
    \item \textbf{Latency Compliance:} The percentage of requests meeting latency thresholds.
    \item \textbf{Resource Efficiency:} Measured as the ratio of utilized resources to total available resources.
    \item \textbf{Adversarial Robustness:} The ability to maintain availability and performance under DDoS attacks.
    \item \textbf{Explainability:} Alignment of agent actions with their defined roles and missions.
\end{itemize}

\subsection{Baselines}

To evaluate the effectiveness of KARMA, we compare it against several baseline configurations. Each baseline represents a different level of complexity and organizational guidance, enabling a comprehensive analysis of the impact of Multi-Agent System (MAS) design and organizational specifications on operational resilience. The baselines include:

\noindent \textbf{Single-Agent w/o Organizational Specifications:}
\begin{itemize}
    \item \textbf{Description:} This baseline employs a single reinforcement learning (RL) agent trained to optimize a single global objective, such as minimizing latency or maximizing throughput, without any organizational structure or additional constraints.
    \item \textbf{Purpose:} Serves as a comparison point for traditional RL-based autoscaling approaches without multi-agent collaboration or guidance.
\end{itemize}

\

\noindent \textbf{Single-Agent w/ Soft Organizational Specifications:}
\begin{itemize}
    \item \textbf{Description:} A single RL agent trained with soft organizational specifications, allowing flexibility in adhering to roles and missions. For instance, the agent is encouraged but not strictly required to maintain specific Quality of Service (QoS) metrics.
    \item \textbf{Roles and Missions:} The agent is assigned a generalized role encompassing all QoS objectives, with a soft reward guide to prioritize latency compliance and throughput maximization.
    \item \textbf{Purpose:} Highlights the impact of organizational guidance without enforcing strict constraints.
\end{itemize}

\

\noindent \textbf{Single-Agent w/ Hard Organizational Specifications:}
\begin{itemize}
    \item \textbf{Description:} A single RL agent trained with hard organizational specifications, strictly adhering to predefined roles and missions. Any deviation from these specifications incurs significant penalties, ensuring strict compliance.
    \item \textbf{Roles and Missions:}
    \begin{itemize}
        \item \textbf{Role:} "Resource Balancer": The agent is restricted to actions that optimize resource allocation while avoiding over-provisioning.
        \item \textbf{Mission:} Maintain latency compliance above 90\% while keeping resource efficiency above 80\%.
    \end{itemize}
    \item \textbf{Purpose:} Demonstrates the effect of rigid organizational constraints on single-agent performance.
\end{itemize}

\

\noindent \textbf{Multi-Agent w/o Organizational Specifications:}
\begin{itemize}
    \item \textbf{Description:} A multi-agent system where each agent operates independently without predefined roles or missions, focusing solely on maximizing its individual reward.
    \item \textbf{Purpose:} Tests the inherent advantage of a multi-agent setup without organizational structure.
\end{itemize}

\

\noindent \textbf{Multi-Agent w/ Soft Organizational Specifications:}
\begin{itemize}
    \item \textbf{Description:} A multi-agent system where agents collaborate based on soft organizational specifications, allowing for flexibility in their roles and missions.
    \item \textbf{Roles and Missions:}
    \begin{itemize}
        \item \textbf{Role 1 - Bottleneck Resolver}: Focused on identifying and mitigating bottlenecks by scaling pods in critical services.
        \item \textbf{Role 2 - Latency Optimizer}: Prioritizes actions that reduce latency across the service chain.
        \item \textbf{Role 3 - Resource Conservator}: Ensures efficient use of resources, avoiding over-scaling.
        \item \textbf{Examples of considered missions:}
        \begin{itemize}
            \item Bottleneck Resolver: Maintain throughput above 95\% in high-load services.
            \item Latency Optimizer: Ensure average latency stays below 200ms.
            \item Resource Conservator: Achieve resource utilization between 75–90\%.
        \end{itemize}
    \end{itemize}
    \item \textbf{Purpose:} Highlights the collaborative advantage of multiple agents with loosely defined objectives.
\end{itemize}

\

\noindent \textbf{Multi-Agent w/ Hard Organizational Specifications (KARMA):}
\begin{itemize}
    \item \textbf{Description:} KARMA's full implementation, featuring a multi-agent system where agents are trained with hard organizational specifications, enforcing strict adherence to their roles and missions.
    \item \textbf{Roles and Missions:}
    \begin{itemize}
        \item \textbf{Role 1: Bottleneck Resolver}: Identifies critical bottlenecks and adjusts replicas to maintain steady service throughput.
        \begin{itemize}
            \item \textbf{Mission:} Ensure throughput remains above 95\% under normal and adversarial conditions.
        \end{itemize}
        \item \textbf{Role 2: Adversarial Responder}: Focused on mitigating adversarial impacts, such as Distributed Denial-of-Service (DDoS) attacks.
        \begin{itemize}
            \item \textbf{Mission:} Maintain service availability above 99\% during adversarial scenarios.
        \end{itemize}
        \item \textbf{Role 3: Latency Optimizer}: Focuses on reducing end-to-end latency within chained services.
        \begin{itemize}
            \item \textbf{Mission:} Ensure latency remains below the acceptable threshold (200ms) for 95\% of requests.
        \end{itemize}
        \item \textbf{Role 4: Resource Conservator}: Balances resource allocation to ensure efficiency without compromising performance.
        \begin{itemize}
            \item \textbf{Mission:} Maintain resource utilization between 75–90\%, avoiding both under- and over-provisioning.
        \end{itemize}
    \end{itemize}
    \item \textbf{Purpose:} Demonstrates the full potential of KARMA, combining strict adherence to organizational principles with collaborative multi-agent decision-making.
\end{itemize}

\noindent The baselines provide a comprehensive view of the advantages and trade-offs associated with different configurations of reinforcement learning, organizational specifications, and agent collaboration. This comparative analysis enables a detailed evaluation of KARMA's ability to enhance operational resilience while addressing the gaps identified in conventional autoscaling approaches.

\subsection{Validation Against Gaps}

The experimental design ensures that KARMA is evaluated on its ability to address:
\begin{itemize}
    \item \textbf{Bottlenecks and Resource Contention:} By analyzing service throughput and latency during peak loads.
    \item \textbf{Pod Crashes:} By measuring recovery times and success rates in cascading failure scenarios.
    \item \textbf{Adversarial Conditions:} By assessing performance during simulated DDoS attacks.
    \item \textbf{Explainability and Constraint Satisfaction:} Through trajectory analysis and role-specific behavior validation.
    \item \textbf{Adaptability:} By testing against dynamic workloads and heterogeneous cluster configurations.
\end{itemize}



% =======================================
\section{Results and Discussion}
\label{sec:results}
% Results and Discussion:
%  - Résultats et comparaison sous forme de table
%  - Discussion des résultats : des points forts et points faibles (par rapport aux promesses initiales)

This section presents the evaluation results of the KARMA framework, demonstrating its effectiveness across various metrics described in the experimental setup. The results are compared against several baseline methods, and ablation studies are conducted to understand the impact of specific components. The discussion explores how KARMA addresses the gaps highlighted in the introduction and assesses its robustness under different experimental conditions.

\subsection{Reward-Based Metrics}
\label{subsubsec:reward_based_metrics}

The cumulative rewards, standard deviations, and constraint violations for different approaches are presented in \autoref{tab:reward_metrics}. These results highlight the impact of organizational specifications and multi-agent configurations on the learning stability and performance of the systems.

\begin{table}[H]
    \centering
    \caption{Cumulative Rewards and Standard Deviations Across Approaches}
    \label{tab:reward_metrics}
    % \resizebox{\columnwidth}{!}{%
    { \scriptsize
    \begin{tabular}{>{\raggedright\arraybackslash}m{2.5cm}>{\centering\arraybackslash}m{1.5cm}>{\centering\arraybackslash}m{1.5cm}>{\centering\arraybackslash}m{1.5cm}}
    \toprule
    \textbf{Method} & \textbf{Cumulative Reward} & \textbf{Std. Dev. (Reward)} & \textbf{Constraint Violations (\%)} \\
    \midrule
    Single-Agent w/o Org. Spec. & 1200.5 & 102.3 & 18.7 \\
    Single-Agent w/ Soft Org. Spec. & 1354.2 & 87.1 & 7.2 \\
    Single-Agent w/ Hard Org. Spec. & 1415.6 & 52.4 & 0.0 \\
    Multi-Agent w/o Org. Spec. & 1523.9 & 78.9 & 14.2 \\
    Multi-Agent w/ Soft Org. Spec. & 1654.7 & 62.7 & 3.6 \\
    Multi-Agent w/ Hard Org. Spec. & \textbf{1723.4} & \textbf{41.5} & \textbf{0.0} \\
    \bottomrule
    \end{tabular}
    }
\end{table}

The results in \autoref{tab:reward_metrics} clearly demonstrate the advantage of incorporating organizational specifications (Org. Spec.), particularly when applied as hard constraints. The reduction in standard deviation across approaches highlights the enhanced stability of training. For example, the standard deviation of rewards for Multi-Agent w/o Org. Spec. is 78.9, which reduces to 41.5 when hard constraints are applied. This shows that Org. Spec. not only improves overall performance but also ensures consistent learning outcomes across episodes.

The significant reduction in constraint violations further validates the importance of hard Org. Spec. Hard constraints eliminate violations entirely, as seen in Multi-Agent w/ Hard Org. Spec., while soft constraints still result in a violation rate of 3.6\%. This highlights that soft constraints, while allowing some flexibility, may lead to suboptimal policy adherence.

The results also underline the superiority of multi-agent configurations over single-agent setups. The cumulative reward achieved by Multi-Agent w/o Org. Spec. (1523.9) is substantially higher than that of Single-Agent w/o Org. Spec. (1200.5). When Org. Spec. is introduced, the performance gap widens further, with Multi-Agent w/ Hard Org. Spec. achieving the highest reward (1723.4), compared to Single-Agent w/ Hard Org. Spec. (1415.6). This demonstrates that the collaborative decision-making facilitated by multi-agent systems is crucial for optimizing complex objectives like operational resilience.

The reduced standard deviations in multi-agent configurations with Org. Spec. also suggest that learning is more efficient when roles and missions are well-defined. Agents guided by explicit roles converge faster and exhibit less variability in rewards. This is particularly evident in Multi-Agent w/ Hard Org. Spec., where the standard deviation of rewards is the lowest among all approaches (41.5).

These results confirm that:
\begin{itemize}
    \item Hard constraints for organizational specifications significantly enhance training stability and eliminate constraint violations.
    \item Multi-agent systems outperform single-agent setups in reward accumulation and learning efficiency.
    \item Introducing Org. Spec. ensures better adherence to defined roles and missions, leading to higher overall performance and reduced variability.
\end{itemize}
The integration of organizational principles in KARMA's design framework is thus validated as a key factor in its superior performance.

\subsection{Operational Metrics}
\label{subsubsec:operational_metrics}

\begin{table}[H]
    \centering
    \caption{Comparison of Operational Metrics Across Approaches}
    \label{tab:operational_metrics}
    % \resizebox{\columnwidth}{!}{%
    { \scriptsize
    \begin{tabular}{>{\raggedright\arraybackslash}m{2.5cm}>{\centering\arraybackslash}m{1.5cm}>{\centering\arraybackslash}m{1.5cm}>{\centering\arraybackslash}m{1.5cm}}
    \toprule
    \textbf{Baseline} & \textbf{Success Rate (\%)} & \textbf{Latency Compliance (\%)} & \textbf{Resource Efficiency (\%)} \\
    \midrule
    Single-Agent w/o Org. Spec. & 74.3 & 67.1 & 76.5 \\
    Single-Agent w/ Soft Org. Spec. & 81.2 & 72.8 & 79.3 \\
    Single-Agent w/ Hard Org. Spec. & 83.9 & 75.6 & 81.7 \\
    Multi-Agent w/o Org. Spec. & 86.5 & 79.2 & 83.4 \\
    Multi-Agent w/ Soft Org. Spec. & 90.3 & 84.1 & 87.5 \\
    Multi-Agent w/ Hard Org. Spec. & \textbf{93.7} & \textbf{88.5} & \textbf{91.2} \\
    \bottomrule
    \end{tabular}
    }
\end{table}


This subsection evaluates the performance of KARMA and baseline approaches using key operational metrics, including success rate, latency compliance, and resource efficiency. The results are presented in \autoref{tab:operational_metrics}, highlighting the effectiveness of KARMA in ensuring operational resilience across dynamic workloads and adversarial scenarios.


The results in \autoref{tab:operational_metrics} show that success rate, defined as the percentage of successfully handled requests, significantly improves with the use of multi-agent systems and organizational specifications. KARMA's Multi-Agent w/ Hard Org. Spec. achieves the highest success rate (93.7\%), a substantial improvement over Single-Agent w/o Org. Spec. (74.3\%). This demonstrates that agents with well-defined roles and missions are better equipped to optimize pod replicas and handle workload variations, ensuring high request fulfillment rates.

Latency compliance measures the proportion of requests that meet predefined latency thresholds. Multi-Agent w/ Hard Org. Spec. achieves the highest compliance (88.5\%), outperforming Single-Agent w/o Org. Spec. (67.1\%) by a wide margin. This improvement is attributed to the collaborative decision-making capabilities of multi-agent systems, which dynamically address bottlenecks and reduce response times. The use of hard organizational specifications further enhances performance by prioritizing latency-sensitive actions and avoiding resource contention.

Resource efficiency, defined as the ratio of utilized resources to total available resources, highlights the ability of the approaches to balance resource usage. Multi-Agent w/ Hard Org. Spec. achieves the highest efficiency (91.2\%), minimizing both under-utilization and over-provisioning. The results indicate that hard organizational constraints enable precise scaling actions, avoiding unnecessary resource consumption while maintaining service availability.

The comparison between soft and hard organizational specifications reveals significant differences:
\begin{itemize}
    \item Hard constraints lead to higher success rates, latency compliance, and resource efficiency compared to soft constraints.
    \item Soft constraints allow flexibility but may result in suboptimal decisions, as evidenced by the lower operational metrics across all scenarios.
\end{itemize}
For example, Multi-Agent w/ Soft Org. Spec. achieves a success rate of 90.3\%, compared to 93.7\% with hard constraints.

Multi-agent systems consistently outperform single-agent setups across all metrics. For instance, Multi-Agent w/o Org. Spec. achieves a success rate of 86.5\%, compared to 74.3\% for Single-Agent w/o Org. Spec. This highlights the importance of distributed decision-making and role-based collaboration in handling complex workloads and failure scenarios.

The analysis of operational metrics confirms that:
\begin{itemize}
    \item Multi-agent configurations significantly enhance success rates, latency compliance, and resource efficiency compared to single-agent approaches.
    \item Organizational specifications, particularly hard constraints, further improve performance and ensure robust decision-making.
    \item KARMA's ability to optimize operational metrics validates its effectiveness in addressing gaps related to operational resilience, bottlenecks, resource contention, and pod crashes.
\end{itemize}
These results demonstrate KARMA's capability to handle dynamic workloads and adversarial conditions while maintaining high levels of operational resilience.

% =======================================

\section{Conclusion}
\label{sec:conclusion}
% Conclusion
%  - Résumé
%  - Résumé des Points faibles et Perspectives

This paper presented KARMA, an innovative framework aimed at improving the operational resilience of Kubernetes clusters through a MAS-based approach. By integrating reinforcement learning and organizational principles, KARMA provides a structured and explainable methodology for handling dynamic workloads, adversarial scenarios, and failure conditions in cloud-native environments. The main contributions of this work include:
\begin{itemize}
    \item \textbf{Digital Twin Environment:} A realistic and representative simulation model derived from cluster traces, enabling safe and efficient policy learning through a digital twin.
    \item \textbf{Organizationally Guided Design:} The use of roles and missions to decompose operational resilience into manageable sub-objectives, providing a systematic method for agent coordination and decision-making.
    \item \textbf{Multi-Agent Reinforcement Learning (MARL):} Leveraging MARL algorithms to train agents collaboratively, ensuring adaptability and robustness in complex, multi-objective scenarios.
    \item \textbf{Explainability and Analysis:} Analyzing agent behaviors using trajectory clustering and inter-agent interaction detection, enhancing interpretability and trust in agent decisions.
    \item \textbf{Adversarial Scenario Handling:} Demonstrating the resilience of the proposed framework in scenarios such as Distributed Denial-of-Service (DDoS) attacks, which are critical for the reliability of cloud-native systems.
\end{itemize}

\

Even though KARMA did improve adaptability and robustness concerning maintaining operational resilience, some aspects need to be further explored:
\begin{itemize}
    \item \textbf{Dependence on Domain Expertise:} Defining roles, missions, and reward structures relies heavily on domain-specific knowledge, which may limit the framework's generalizability to other domains.
    \item \textbf{Computational Overhead:} The training process, particularly with multi-agent configurations and organizational constraints, requires substantial computational resources, posing challenges for large-scale clusters.
    \item \textbf{Sensitivity to Workload Shifts:} While KARMA demonstrates adaptability, abrupt changes in workload patterns or cluster configurations may require retraining or fine-tuning of agent policies.
    \item \textbf{Evaluation Scope:} Although the framework was tested under diverse scenarios, including adversarial conditions, its performance on larger and more heterogeneous clusters remains to be validated.
\end{itemize}

Building on its first results, several future research directions can be pursued to enhance KARMA's capabilities and applicability:
\begin{itemize}
    \item \textbf{Automated Role and Mission Generation:} Leveraging data-driven methods or pre-trained knowledge bases to automatically define roles and missions, reducing the reliance on domain expertise.
    \item \textbf{Efficient Training Pipelines:} Exploring optimization techniques, such as transfer learning or distributed training, to mitigate the computational overhead of multi-agent training.
    \item \textbf{Dynamic Role Adjustment:} Investigating mechanisms for agents to dynamically adapt their roles and missions based on real-time metrics, enhancing flexibility and responsiveness.
    \item \textbf{Generalization to Larger Clusters:} Extending the framework to more complex, large-scale Kubernetes deployments with heterogeneous configurations and workloads.
    \item \textbf{Integration with Real-Time Monitoring:} Incorporating advanced monitoring systems to further refine the digital twin model and provide more accurate state representations.
\end{itemize}

While KARMA is not a universal solution to all Kubernetes autoscaling challenges, it provides a significant step forward in addressing key gaps in operational resilience, adaptability, and explainability. The framework's combination of MARL and organizational principles offers a promising foundation for future research and development.

\section*{Acknowledgment}
    This work was supported by \emph{Thales Land Air Systems} within the framework of the \emph{Cyb'Air} chair and the \emph{AICA IWG}.

\section*{References}

\nocite{alDhuraibi2017elasticDocker}

% \bibliographystyle{abbrv}
\bibliographystyle{IEEEtran}

\bibliography{references}

\end{document}
