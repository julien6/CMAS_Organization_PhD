% Patron de document pour un article a JFSMA pour LaTeX 2e
% Copyright (c) 2007 Bruno BEAUFILS
% Les quolibets et autres insultes sont à envoyer à bruno.beaufils@univ-lille.fr
% Modifications par Gauthier Picard (gauthier.picard@onera.fr) - 31-mai-07
% Modifications par Pierre Chevaillier (chevaillier@enib.fr) - 20-mar-2007
% Modifications par Yann Krupa (krupa@emse.fr) et Gauthier Picard (gauthier.picard@onera.fr) - 31-mar-2011
% Modifications par Emmanuel Adam (emmanuel.adam@univ-valenciennes.fr) - 31-mar-2011
% Modifications par Gauthier Picard (gauthier.picard@onera.fr) - 16-nov-2014
% Modifications par Gauthier Picard (gauthier.picard@onera.fr) - 29-nov-2017
% Modifications mineures par Maxime Morge (maxime.morge@univ-lille.fr) - 3-nov-2023
% L'option doit être :
% - contribution, pour une contribution scientifique originale
% - dissemination, pour une contribution scientifique déjà publiée mais
% inédite en français et traduite ici
% - sota, pour un état de l'art
% - demonstration, pour un article support à démonstration de logiciels
% - final, pour la version finale
% Pass enumitem options before the class loads it.
\PassOptionsToPackage{inline,shortlabels}{enumitem}
\documentclass[contribution]{jfsma}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2026} with \usepackage[nohyperref]{icml2026} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% For preprint, use
% \usepackage[preprint]{icml2026}

% If accepted, instead use the following line for the camera-ready submission:
% \usepackage[accepted]{icml2026}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}


% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

% ---- Packages ----
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{amsmath, amssymb, amsfonts}
\usepackage{mathtools}
\usepackage{bm}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{hyperref}
\usepackage{cleveref}
% enumitem is loaded by the class; options are passed above.
% \usepackage{xcolor}
% \usepackage{amsthm}

% ---- Optional: TikZ for figures ----
\usepackage{tikz}
\usetikzlibrary{positioning, arrows.meta, calc}
\newcommand{\probP}{\text{I\kern-0.15em P}}

% --- Tickz
\usepackage{physics}
\usepackage{tikz}
\usetikzlibrary{arrows.meta,positioning,fit,calc}
\usepackage{amsmath}
\usepackage{mathdots}
% \usepackage{yhmath}
\usepackage{cancel}
\usepackage{color}
\usepackage{siunitx}
\usepackage{array}
\usepackage{stmaryrd}
\usepackage{multirow}
% \usepackage{amssymb}
\usepackage{gensymb}
\usepackage{tabularx}
\usepackage{extarrows}
\usepackage{booktabs}
\usetikzlibrary{fadings}
\usetikzlibrary{patterns}
\usetikzlibrary{shadows.blur}
\usetikzlibrary{shapes}
\captionsetup{font=footnotesize,skip=3pt}
\setlength{\textfloatsep}{8pt plus 1pt minus 2pt}
\setlength{\floatsep}{6pt plus 1pt minus 2pt}
\setlength{\intextsep}{6pt plus 1pt minus 2pt}
\setlength{\abovedisplayskip}{5pt plus 1pt minus 2pt}
\setlength{\belowdisplayskip}{5pt plus 1pt minus 2pt}
\setlength{\abovedisplayshortskip}{4pt plus 1pt minus 2pt}
\setlength{\belowdisplayshortskip}{4pt plus 1pt minus 2pt}

% ---- Math shortcuts ----
\newcommand{\E}{\mathbb{E}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\I}{\mathbb{I}}

\usepackage{amssymb}
\usepackage{pifont}

\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%

\usepackage[french]{babel}
\addto\extrasfrench{  
    \def\figureautorefname{Figure}
    \def\tableautorefname{Tableau}
    \def\algorithmautorefname{Algorithme}
    \def\sectionautorefname{Section}
    \def\subsectionautorefname{Sous-section}
    \def\proofoutlineautorefname{Plan de preuve}
}

\newcommand{\myCustomSize}[1]{%
  \fontsize{9}{10}\selectfont
  #1
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Théorème}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemme}
\newtheorem{corollary}[theorem]{Corollaire}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Définition}
\newtheorem{assumption}[theorem]{Hypothèse}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remarque}

% ==========================================

\titre{Un Cadre Neuro-Symbolique pour les Modèles Du Monde Multi-Agents}
% \\ pour l'apprentissage par renforcement multi-agents basé modèle}

  % julien.soule@hotmail.fr
\auteur{Julien Soulé\up{a}}{julien.soule@uni.lu}
%%%Si besoin d'ajouter des auteurs à la ligne :

\institution{\up{a}%
  University of Luxembourg, Luxembourg, Luxembourg}

\begin{document}

\maketitle

\begin{resume}
  L'apprentissage par renforcement basé modèle permet la planification via des \textbf{modèles du monde} (World Models -- WMs), mais étendre ces approches aux environnements multi-agents est difficile en raison de l'observabilité partielle et de l'accumulation d'erreurs dans la dynamique des observations conjointes. Les \textbf{WMs multi-agents} (Multi-Agent World Models -- MAWMs) purement neuronaux produisent souvent des prédictions sémantiquement incohérentes, violant la cohérence spatiale, la persistance des objets et les contraintes d'interaction. Nous introduisons \textbf{Neuro-Symbolic Multi-Agent World Models} (NS-MAWM), qui incorporent le raisonnement symbolique dans les transitions d'observation sous forme de contraintes de cohérence différentiables au sein des WMs. Ces contraintes agissent comme des biais inductifs structurés qui réduisent la dérive sémantique à long horizon. Nous évaluons NS-MAWM sur quatre environnements multi-agents représentatifs en utilisant trois stratégies d'intégration neuro-symbolique. Les résultats expérimentaux montrent que NS-MAWM améliore la cohérence des prédictions à long horizon, les performances de planification et la généralisation par rapport aux bases purement neuronales.
\end{resume}

\motscles{Apprentissage par Renforcement Multi-Agent Basé Modèle, Modèles du Monde Multi-Agents, Neuro-Symbolique}

\bigskip

\begin{abstract}
  Model-Based Reinforcement Learning enables planning through learned \textbf{World Models} (WMs), but extending these approaches to multi-agent settings is challenging due to partial observability and error accumulation in joint observation dynamics. Purely neural \textbf{Multi-Agent WMs} often produce semantically inconsistent predictions, violating spatial coherence, object persistence, and interaction constraints. We introduce \textbf{Neuro-Symbolic Multi-Agent WMs} (NS-MAWM), which incorporate symbolic reasoning into observation transitions as differentiable consistency constraints within WMs. These constraints act as structured inductive biases that reduce long-horizon semantic drift. We evaluate NS-MAWM on four representative multi-agent environments using three neuro-symbolic integration strategies. Experimental results demonstrate that NS-MAWM improves long-horizon prediction consistency, planning performance, and generalization compared to purely neural baselines.
\end{abstract}
\keywords{Model-Based Multi-Agent Reinforcement Learning, Multi-Agent World Models, Neuro-Symbolic}


% ============================
% 1. Introduction
% ============================
\section{Introduction}
\label{sec:intro}

L'apprentissage de modèles prédictifs de la dynamique de l'environnement, appelés \textbf{modèles du monde} (World Models -- WMs)~\cite{ha2018worldmodels}, est un paradigme central de \textbf{l'apprentissage par renforcement basé modèle} (Model-Based Reinforcement Learning -- MBRL)~\cite{hafner2019learning,hafner2020dreamer,hafner2021dreamerv2}. En encodant l'historique d'interaction de l'agent dans un espace latent, ces modèles permettent la planification par imagination, l'apprentissage de politiques et l'attribution de crédit à long horizon~\cite{schrittwieser2020}. Les progrès récents ont montré de solides résultats en contexte mono-agent et pleinement observable ; toutefois, leur extension aux environnements multi-agents en observabilité partielle reste une frontière ouverte~\cite{Wong2023,Venugopal2024MABL,agravante-etal-2023-learning}.

Dans ces environnements, les observations consistent généralement en plusieurs entités interactives régies par des règles spatiales, logiques ou physiques (par exemple deux agents ne peuvent pas occuper la même position, les objets suivent des transitions déterministes, etc.). Pourtant, la plupart des WMs traitent les observations comme des vecteurs plats et non structurés~\cite{ha2018worldmodels,hafner2019learning}, et apprennent des dynamiques latentes monolithiques de bout en bout à partir de pixels bruts. Cela conduit souvent à des représentations latentes qui ne parviennent pas à démêler la sémantique (identité d'agent, types d'objets) ni à refléter la structure compositionnelle de l'environnement~\cite{kipf2020contrastivelearningstructuredworld}. En conséquence, les modèles peuvent produire des prédictions de forte vraisemblance qui sont logiquement incohérentes ou physiquement implausibles.

Par exemple, un WM entraîné peut prédire que deux agents occupent la même cellule d'une grille, violant les règles de collision. De telles incohérences symboliques ne sont pas pénalisées par les pertes de vraisemblance ou de reconstruction, et passent souvent inaperçues lors de l'évaluation. Pire, ces erreurs se composent lors de longues simulations, dégradant les performances de planification et brisant la logique de l'environnement~\cite{talvitie2014modelbias}.

Plusieurs travaux ont pionnièrement intégré des connaissances a priori comme biais inductifs dans les WMs~\cite{kipf2020contrastivelearningstructuredworld}. Cependant, un cadre général pour intégrer le raisonnement symbolique dans l'apprentissage des WMs reste à établir, en particulier concernant trois lacunes générales :
%
\begin{itemize}
  \item[\textbf{G1}] \textbf{Manque de structures de WMs orientées multi-agents.}
    Les WMs existants négligent des a priori structurels pour les environnements multi-agents : représentation explicite de l'observabilité partielle, non-stationnarité induite par les interactions, et modularité des entités~\cite{zhang2021worldmodelgraph, Willemsen2021MAMBPO, Venugopal2024MABL}. Ils supposent des dynamiques stationnaires et traitent les agents de manière uniforme, échouant à généraliser sur des configurations variées.

  \item[\textbf{G2}] \textbf{Manque d'intégration neuro-symbolique dans les WMs.}
    Bien que des a priori symboliques (contraintes d'action, lois physiques, règles d'interaction) améliorent l'efficacité en échantillons et la robustesse, peu de modèles les intègrent dans l'apprentissage ou l'architecture des WMs~\cite{Xu2018semanticloss, Fan2017differentiablelearninglogicalrules, balloch2023neurosymbolicworldmodelsadapting, garcez2023neurosymbolic}. La plupart s'appuient uniquement sur des dynamiques latentes boîte noire, manquant des opportunités d'exploiter la connaissance symbolique comme biais inductif.

  \item[\textbf{G3}] \textbf{Manque de cohérence symbolique comme principe d'optimisation.}
    Les métriques standard des WMs se concentrent sur la précision prédictive ou la récompense en aval, en ignorant la cohérence symbolique. Pourtant, le raisonnement dans des environnements dynamiques bénéficie de la vérification du respect des contraintes par les prédictions~\cite{talvitie2014modelbias, balloch2023neurosymbolicworldmodelsadapting}.
\end{itemize}

En nous appuyant sur des cadres d'intégration neuro-symbolique~\cite{manhaeve2018deepproblog,garcez2023neurosymbolic,Xu2018semanticloss,Fan2017differentiablelearninglogicalrules}, nous proposons \textbf{Neuro-Symbolic Multi-Agents World Model (NS-MAWM)} pour augmenter les WMs avec du raisonnement symbolique via :
%
\begin{itemize}
  \item Un \emph{espace latent structuré} qui décompose les observations en entités typées (agents, objets) avec des attributs interprétables (par exemple position ou type), permettant une application efficace des règles symboliques.

  \item Un \emph{formalisme de règles symboliques} qui exprime la dynamique de l'environnement comme des contraintes différentiables sur les observations et les actions, spécifiant clairement les invariants et les équivariances.

  \item Trois stratégies d'intégration neuro-symbolique :
        \begin{enumerate*}[label={\roman*) },itemjoin={; \quad}]
          \item \textbf{Régularisation par perte symbolique} : les règles sont encodées comme des contraintes souples différentiables, de manière analogue à la perte sémantique ou aux cadres de régularisation logique.
          \item \textbf{Projection symbolique} : les violations sont corrigées en projetant les prédictions sur des configurations valides, assurant l'adhérence aux règles définies.
          \item \textbf{Dynamique symbolique résiduelle} : les transitions sont factorisées en un processus symbolique et des résidus appris, permettant un apprentissage plus précis.
        \end{enumerate*}

  \item Un indicateur \textbf{taux de violation des règles} (RVR) qui quantifie les violations de contraintes pour évaluer la cohérence logique.
\end{itemize}

Nous évaluons NS-MAWM sur quatre environnements discrets et continus : Gridcraft, Overcooked~\cite{Micah2020overcooked}, Predator--Prey~\cite{lowe2017multi} et \textbf{SMAC}~\cite{Ellis2023smacv2}. La comparaison entre base neuronale et variantes neuro-symboliques, avec des métriques prédictives et symboliques, montre des gains systématiques de précision, de cohérence et de généralisation. La dynamique symbolique résiduelle offre le meilleur compromis global.

Le reste de l'article est organisé comme suit : la \autoref{sec:related_work} présente les travaux connexes, la \autoref{sec:background} rappelle brièvement les concepts de base, la \autoref{sec:method} détaille NS-MAWM, la \autoref{sec:eval} présente les expériences, et la \autoref{sec:conclusion} conclut.



% ============================
% 3. Related Work
% ============================

\section{Travaux connexes}
\label{sec:related_work}
% TODO: On cherche les travaux qui permettent d'avoir un WM qui permette une meilleure prise en compte de :
%  - l'observabilité partielle,
%  - de la non-stationnarité (autres agents qui interagissent en même temps),
%  - l'aspect multi-agent dans l'architecture du modèle,
%  - l'interpretation des observations en particulier au regard d'un environnement multi-agent complexe
%  - la possibilité d'injecter des règles symboliques
%  - la possibilité d'évaluer la cohérence au regard des règles symboliques
%  - la robustesse à des variations de l'environnement
%  - la généralisation à de nouvelles configurations d'agents/objets
%  - l'efficacité en termes de temps de calcul et de données pour converger vers une performance comparable par rapport à un WM classique (la frugalité du temps de calcul)
%  - la frugalité en termes de données (moins de données nécessaires pour atteindre une certaine performance)
%  - la scalabilité avec le nombre d'agents/objets/règles
%  - le drift sémantique sur le long terme (est-ce que le modèle reste cohérent sur le long terme ?)

Nous passons en revue le paysage des travaux connexes selon quatre axes majeurs de pertinence pour NS-MAWM :

\paragraph{Architectures de WMs.} Les approches initiales telles que Dreamer~\cite{hafner2020dreamer, hafner2021dreamerv2} et PlaNet~\cite{hafner2019learning} proposent des modèles de dynamique latente boîte noire puissants, utilisant souvent un espace latent monolithique sans biais architectural vers les entités ou les agents. Des efforts plus récents adoptent une représentation structurée. Par exemple, WM as Graph~\cite{zhang2021worldmodelgraph} et \textbf{Bi-Level Latent-Variable World Model for Sample-Efficient Multi-Agent Reinforcement Learning} (MABL)~\cite{Venugopal2024MABL} proposent une modularité latente alignée avec les entités et la topologie de l'environnement. \textbf{Multi-Agent Model-Based Policy Optimization} (MAMBPO)~\cite{Willemsen2021MAMBPO} intègre une structure multi-agents explicite, permettant des simulations conditionnées par les vues individuelles des agents.

\paragraph{Apprentissage en environnements multi-agents.} Certains modèles traitent simplement les autres agents comme faisant partie de l'environnement, et sont donc implicitement stationnaires. Mingling Foresight~\cite{Xu2022mingling} et Multi-Timescale Multi-Agent Reinforcement Learning~\cite{nekoei2023dealing} améliorent la gestion des interactions simultanées via un raisonnement hiérarchique ou une adaptation d'échelle temporelle. La modélisation des adversaires est abordée explicitement dans~\cite{Yuxiaopeng2022modelbased}, qui propose des WMs conditionnés sur des politiques adverses latentes.

\paragraph{Connaissances symboliques dans les WMs.} Les a priori symboliques sont peu représentés dans la littérature sur les WMs. Plusieurs travaux injectent des connaissances de domaine comme contraintes, telles que la perte sémantique~\cite{Xu2018semanticloss} et des règles logiques différentiables~\cite{Fan2017differentiablelearninglogicalrules}. DeepProbLog~\cite{manhaeve2018deepproblog} fusionne la logique probabiliste avec des modèles profonds. Des WMs neuro-symboliques récents~\cite{balloch2023neurosymbolicworldmodelsadapting, agravante-etal-2023-learning} proposent une structure symbolique explicite sur les dynamiques latentes pour soutenir l'adaptation et l'interprétabilité.

\paragraph{Dimensions d'évaluation.} La plupart des WMs sont évalués via la perte de reconstruction ou la performance de contrôle, mais peu évaluent la cohérence avec des règles symboliques ou la généralisation sous des changements structurels. Des enquêtes récentes~\cite{Wong2023} recommandent des métriques plus larges, incluant la cohérence sémantique, les taux de violation de règles et la frugalité en données/temps.

\begin{table*}[t]
  \centering
  \resizebox{\textwidth}{!}{%
    \footnotesize
    \begin{tabular}{p{4.69cm}cccccccc}
      \toprule
      \textbf{Travail}                                               & WM struct. & Sens. MA & Sens. NS & Priors symb. & Cohér. symb. & Robust & Gén.   & Frugal \\
      \midrule
      Dreamer~\cite{hafner2020dreamer}                               & \xmark     & \xmark   & \xmark   & \xmark       & \xmark       & \xmark & $\sim$ & $\sim$ \\
      WM as Graph~\cite{zhang2021worldmodelgraph}                    & \cmark     & $\sim$   & \xmark   & \xmark       & \xmark       & \xmark & $\sim$ & \xmark \\
      MABL~\cite{Venugopal2024MABL}                                  & \cmark     & \cmark   & $\sim$   & \xmark       & \xmark       & $\sim$ & \cmark & $\sim$ \\
      MAMBPO~\cite{Willemsen2021MAMBPO}                              & \cmark     & \cmark   & \xmark   & \xmark       & \xmark       & \xmark & $\sim$ & \cmark \\
      WorldCloner~\cite{balloch2023neurosymbolicworldmodelsadapting} & \cmark     & \cmark   & \cmark   & \cmark       & \cmark       & $\sim$ & \cmark & \xmark \\
      Mingling~\cite{Xu2022mingling}                                 & $\sim$     & \cmark   & \cmark   & \xmark       & \xmark       & \cmark & \cmark & \cmark \\
      OppMod~\cite{Yuxiaopeng2022modelbased}                         & \xmark     & \cmark   & \cmark   & \xmark       & \xmark       & $\sim$ & $\sim$ & \xmark \\
      DeepProbLog~\cite{manhaeve2018deepproblog}                     & \xmark     & \xmark   & \xmark   & \cmark       & \xmark       & \xmark & \xmark & \xmark \\
      NS-WM~\cite{agravante-etal-2023-learning}                      & \cmark     & \cmark   & $\sim$   & \cmark       & \cmark       & $\sim$ & $\sim$ & \xmark \\
      \bottomrule
    \end{tabular}
  }
  \caption{Comparaison de travaux représentatifs. WM struct. : WM structuré ; Sens. MA : prise en compte multi-agents ; Sens. NS : non-stationnarité ; Priors symb. : priors symboliques ; Cohér. symb. : évaluation de la cohérence symbolique ; Robust : robustesse ; Gén. : généralisation ; Frugal : efficacité en données/temps.}
  \label{tab:related_work}
\end{table*}

Le \autoref{tab:related_work} résume ce paysage, en soulignant comment les travaux existants adressent (ou négligent) les dimensions critiques que nous visons. Malgré des progrès significatifs, la plupart des WMs existants ne parviennent pas à traiter simultanément les trois dimensions critiques que nous ciblons. En particulier, ils manquent souvent : (\textbf{G1}) de biais architecturaux spécifiques multi-agents, (\textbf{G2}) d'intégration de priors ou de mécanismes de raisonnement symboliques, et (\textbf{G3}) de vérification de la cohérence vis-à-vis des connaissances symboliques. Ces lacunes motivent notre cadre proposé pour soutenir un apprentissage structuré, interprétable et efficace dans des environnements multi-agents partiellement observables à dynamique évolutive.


\section{Contexte et bases}
\label{sec:background}

\subsection{Cadre de l'apprentissage par renforcement multi-agents}
\label{sec:decpomdp}

Pour étudier l'\textbf{apprentissage par renforcement multi-agents} (Multi-Agent Reinforcement Learning -- MARL) coopératif sous observabilité partielle, nous adoptons le formalisme \textbf{Processus de décision de Markov partiellement observable décentralisé} (Decentralized Partially Observable Markov Decision Process -- Dec-POMDP)~\cite{Oliehoek2016}, qui fournit un cadre de principe pour la prise de décision décentralisée où les agents agissent sur des observations locales tout en optimisant conjointement un objectif partagé.
Les Dec-POMDPs se spécialisent pour des contextes pleinement coopératifs avec récompense commune et politique optimisée conjointement.

Formellement, un Dec-POMDP est un 7-uplet
$d = \langle S, \{A_i\}_{i=1}^n, T, R, \{\Omega_i\}_{i=1}^n, O, \gamma \rangle$,
où $S$ est l'espace d'états ;
$A_i$ et $\Omega_i$ sont les espaces d'actions et d'observations de l'agent $i$ ;
$T(s,a,s') = \mathbb{P}(s' \mid s,a)$ est la fonction de transition pour les actions conjointes
$a = (a_1,\dots,a_n)$ ;
$R : S \times A \times S \rightarrow \mathbb{R}$ est la récompense partagée ;
$O(s',a,o) = \mathbb{P}(o \mid s',a)$ est la fonction d'observation ;
et $\gamma \in [0,1)$ est le facteur d'actualisation.

Chaque agent $i \in \mathcal{A} = \{1,\dots,n\}$ suit une politique (éventuellement stochastique)
$\pi_i : \Omega_i \rightarrow \Delta(A_i)$,
et la \textbf{politique conjointe} est
$\pi^j = (\pi_1,\dots,\pi_n)$.
Soit $H_i$ les historiques d'agent
$h_i = \langle (\omega_k^i, a_k^i) \rangle_{k=0}^{z}$,
et $H_{\text{joint}} = H_1 \times \dots \times H_n$ l'ensemble des historiques conjoints.
L'objectif est de trouver $\pi^j$ maximisant le retour attendu avec horizon fini $T$ ou horizon infini et $\gamma < 1$ :
\[
  V(\pi^j) =
  \mathbb{E}_{\pi^j}
  \left[ \sum_{t=0}^{T} \gamma^t R(s_t, a^j_t, s_{t+1}) \right],
\]

\subsection{Apprentissage par renforcement multi-agents basé modèle}
\label{sec:mb-marl}

\textbf{L'apprentissage par renforcement multi-agents basé modèle} (Model-Based Multi-Agent Reinforcement Learning -- MB-MARL) étend le MBRL à la prise de décision multi-agents.
L'idée centrale est d'apprendre un modèle approximatif de l'environnement
et de l'exploiter pour la planification ou l'optimisation de politiques,
améliorant ainsi l'efficacité en échantillons et la coordination
\cite{Moerland2023modelbased,Wang2022modelbasedmultiagentreinforcementlearning}.

\paragraph{Modèles d'environnement.}
Dans des contextes coopératifs avec $n$ agents, la dynamique est gouvernée par
$P(s' \mid s, a_1,\dots,a_n)$ avec une récompense partagée $R(s,a_1,\dots,a_n)$.
MB-MARL apprend des approximations
$\hat{P}_\phi$ et $\hat{R}_\phi$ à partir de données d'interaction.
Comparé au MBRL mono-agent, apprendre de tels modèles est plus difficile
à cause des espaces d'actions conjoints, de l'observabilité partielle, et de la non-stationnarité induite par l'apprentissage
\cite{Wang2022modelbasedmultiagentreinforcementlearning}.

\paragraph{Utilisation du modèle.}
Suivant les taxonomies standard \cite{Moerland2023modelbased,Wang2022modelbasedmultiagentreinforcementlearning},
les modèles appris sont utilisés pour la planification (MPC), l'apprentissage de type Dyna, ou l'optimisation directe de politiques dans le modèle appris, avec un compromis classique entre efficacité en échantillons et biais de modèle~\cite{Moerland2023modelbased}.

\paragraph{Structure du modèle.}
Les méthodes MB-MARL diffèrent par la structure du modèle : des approches centralisées (proches CTDE) aux approches décentralisées/factorisées, avec un compromis entre précision globale et scalabilité \cite{Wang2022modelbasedmultiagentreinforcementlearning}.

Sous des hypothèses appropriées, la réutilisation des données via simulations réduit les interactions avec l'environnement et améliore la complexité en échantillons~\cite{Wang2022modelbasedmultiagentreinforcementlearning,Moerland2023modelbased}.




\subsection{Bases des WMs}
\label{sec:wm_basics}

Les WMs sont une classe de méthodes de MBRL
qui apprennent la dynamique de l'environnement~\cite{ha2018worldmodels,hafner2020dreamer}.
Ils sont particulièrement adaptés aux contextes partiellement observables tels que les Dec-POMDPs,
permettant la planification, une meilleure efficacité en échantillons, et un raisonnement par imagination.
Nous présentons ici une formulation déterministe simplifiée tout en notant que
les WMs modernes reposent généralement sur des variables latentes stochastiques entraînées par inférence variationnelle.

Formellement, à chaque pas de temps $t$, soit $\omega_t \in \Omega$ l'observation,
$a_t \in A$ l'action exécutée, et $\tilde{h}_{t-1} \in \mathcal{H}$ l'état caché récurrent résumant l'historique d'interaction jusqu'au temps $t-1$. Un encodeur $Enc : \Omega \rightarrow Z$ projette les observations de grande dimension dans une représentation latente compacte $z_t = Enc(\omega_t)$.
%
L'évolution temporelle de l'état latent est modélisée par :
\[
  \tilde{h}_t = f_{\theta}(concat(\tilde{h}_{t-1}, z_t, a_t)), \qquad \hat{z}_{t+1} = g_{\theta}(\tilde{h}_t),
\]
où $f_{\theta}(\cdot)$ est typiquement implémenté comme un réseau de neurones récurrent,
comme un \textbf{Long-Short-Term Memory} (LSTM)~\cite{hochreiter1997long},
et $g_{\theta}(\cdot)$ est un mappage paramétrique tel qu'un \textbf{Multi-Layer Perceptron} (MLP).
%
L'état latent prédit est décodé par
$Dec : Z \rightarrow \Omega$ pour produire l'observation prédite
$\hat{\omega}_{t+1} = Dec(\hat{z}_{t+1})$.


\subsection{Une extension aux WMs multi-agents}
\label{sec:mawm_basics}

Une extension courante des WMs aux contextes multi-agents consiste à apprendre un modèle centralisé de dynamiques latentes sur les observations conjointes et les actions conjointes, similaire aux paradigmes CTDE.
\cite{Wang2022modelbasedmultiagentreinforcementlearning}.

Étant donné un jeu de trajectoires conjointes $\mathcal{D}_{H^j} = \{ h^j \}$, avec $h^j = \langle (\omega_t^j, a_t^j) \rangle_{t=0}^{T}$, l'objectif est d'apprendre un modèle prédictif de la dynamique des observations conjointes, comme cela est courant en MB-MARL\cite{Wang2022modelbasedmultiagentreinforcementlearning,Xu2022mingling}.
%
Nous définissons un \textbf{modèle de prédiction d'observations conjointes} (JOPM) comme un modèle neuronal pour capturer la dynamique de l'environnement :
%
\[
  \mathcal{T}_{\theta}^j :
  \mathcal{H} \times \Omega^j \times A^j
  \rightarrow \hat{\Omega}^j
\]
%
où $\mathcal{H}$ désigne l'espace des états cachés récurrents comme représentation compacte de l'historique conjoint pour capturer la dynamique \cite{ha2018worldmodels,hafner2020dreamer}. Il est paramétré par $\theta$ et peut être implémenté comme :
%
\begin{equation*}
  \begin{aligned}
    \mathcal{T}_{\theta}^j(\tilde{h}_{t-1}, \omega^j_t, a^j_t)
    =
    \mathrm{unflatten}\Big(
     & Dec^j\Big(
    g_{\theta}\Big(
    f_{\theta}\Big(                      \\
     & \hspace{-12em} \mathrm{concat}(
    \tilde{h}_{t-1},
    Enc^j(\mathrm{flatten}(\omega^j_t)), \\
     &
    \hspace{-3em}
    \mathrm{flatten}(a^j_t)
    )
    \hspace{-0.2em}\Big)\hspace{-0.2em}
    \Big)\hspace{-0.2em}
    \Big)\hspace{-0.2em}
    \Big)
    \hspace{-0.1em} = \hspace{-0.1em} \hat{\omega}^j_{t+1}
  \end{aligned}
\end{equation*}
%
Au temps $t$, étant donné $\tilde{h}_{t-1} \in \mathcal{H}$, l'observation conjointe $\omega_t^j$ et l'action conjointe $a_t^j$, le modèle produit un état caché mis à jour $\tilde{h}_t$ et une observation conjointe prédite $\hat{\omega}_{t+1}^j$.
%
Pour réduire la dimensionnalité, un encodeur partagé $Enc^j : \overline{\Omega^j} \rightarrow Z$ projette les observations conjointes aplaties dans un espace latent, $z_t = Enc^j(flatten(\omega_t^j))$\cite{ha2018worldmodels,Xu2022mingling}.
Les dynamiques latentes sont modélisées par
\[
  \tilde{h}_t = f_{\theta}(concat(\tilde{h}_{t-1}, z_t, a_t^j)),
  \quad
  \hat{z}_{t+1} = g_{\theta}(\tilde{h}_t),
\]
et décodées via $Dec^j : Z \rightarrow \overline{\Omega^j}$, donnant $\hat{\omega}_{t+1}^j$ à partir de $Dec^j(\hat{z}_{t+1})$.
%
La paire encodeur--décodeur est typiquement implémentée avec des architectures MLP ou à attention
pour agréger l'information multi-agents et capturer les dépendances inter-agents
\cite{Xu2022mingling,Wang2022modelbasedmultiagentreinforcementlearning}.
Cette formulation centralisée permet une modélisation précise des dynamiques conjointes sous entraînement centralisé,
mais souffre de limitations de scalabilité et d'exécution décentralisée
\cite{Wang2022modelbasedmultiagentreinforcementlearning,Moerland2023modelbased}.


\section{WMs multi-agents neuro-symboliques}
\label{sec:method}

Nous proposons \textbf{NS-MAWM}, un cadre qui augmente les WMs avec un raisonnement neuro-symbolique structuré. Un aperçu schématique de NS-MAWM est donné en \autoref{fig:ns-mawm_overview}.

\subsection{Espace latent d'observation structuré et formalisme neuro-symbolique unifié}
\label{sec:ns}

Pour intégrer le raisonnement symbolique dans les \textbf{WMs multi-agents} (MAWMs), nous proposons une décomposition formelle des observations individuelles et définissons un opérateur de transition symbolique capable de prédire partiellement les observations futures.

\paragraph{Représentation des observations.} Nous supposons que l'observation d'un agent $i$ au temps $t$ peut être exprimée comme un vecteur :
\begin{align*}
  \omega_{i,t} = (f_{1,t}, f_{2,t}, \dots, f_{n_f,t}),
\end{align*}
où chaque $f_{k,t}$ (avec $0 \leq k \leq n_f$) est une caractéristique numérique ou catégorielle véhiculant un attribut perceptif spécifique (par exemple position relative à un autre agent, type d'objet en vue, etc.), et $n_f$ est le nombre de caractéristiques par observation.

\paragraph{Déterminabilité des caractéristiques.} Nous postulons que certaines caractéristiques sont \emph{déterminables} uniquement à partir de l'observation conjointe $\omega^j_t$, de l'action conjointe $a^j_t$, et de l'historique conjoint $h^j_{t-1}$. Ces caractéristiques sont sensibles aux invariants et régularités symboliques, telles que la cohérence spatiale ou le mouvement relatif des agents. Par exemple, dans une grille, si l'agent $j$ se déplace vers le sud, et qu'il n'y a pas d'obstacles, alors l'agent $i$ observant l'agent $j$ à sa gauche devrait ensuite l'observer en bas à gauche. Cela nous conduit à définir une observation comme suit :
%
\begin{equation*}
  \begin{aligned}
    \omega_{i,t} \hspace{-0.2em} = \hspace{-0.2em} (f^d_{1,t}\dots, f^d_{m,t}, f^u_{m+1,t} \dots, f^u_{n_f,t}) \hspace{-0.2em} \\
     & \hspace{-4em} = \hspace{-0.2em} concat(\omega^d_t, \omega^u_t),
  \end{aligned}
\end{equation*}
%
où $f^d_{k,t} \in \mathcal{F}^d$ sont des caractéristiques déterminables via le raisonnement symbolique tandis que $f^u_{k,t} \in \mathcal{F}^u$ sont indéterminables. En conséquence, une observation se compose de deux sous-parties :
\begin{enumerate*}[label={\roman*) },itemjoin={; \quad}]
  \item $\omega^d_t$ : le sous-vecteur des caractéristiques déterminables.
  \item $\omega^u_t$ : le sous-vecteur des caractéristiques indéterminables.
\end{enumerate*}

\paragraph{Règles symboliques atomiques.} Nous définissons une règle de transition symbolique atomique $s \in \mathcal{S}$ comme :
\begin{align*}
  s(h^j_{t-1}, \omega^j_t, a^j_t, f^d_{l,t}) = f^d_{l,t+1},
\end{align*}
qui exprime comment une caractéristique déterminable donnée $f^d_{l,t} \in \mathcal{F}^d$ (avec $0\leq l \leq n_f$) évolue sous connaissance symbolique (par exemple en imposant des contraintes de mouvement spatial basées sur l'information passée et présente pour prédire la valeur dans des caractéristiques connexes).

\paragraph{Transition symbolique partielle.} En agrégeant toutes les règles symboliques, nous définissons un opérateur de transition symbolique partiel :
\begin{align*}
  \mathcal{T}^j_s(h^j_{t-1}, \omega^j_t, a^j_t) = \omega^{d,j}_{t+1},
\end{align*}
où $\omega^{d,j}_{t+1}$ est l'observation conjointe prédite au temps $t+1$, ne contenant que les caractéristiques déterminables pour tous les agents.

\paragraph{Vue matricielle de la transition symbolique partielle.} Soit $\mathcal{M}_{\omega^j_t}$ la matrice d'observation dont les colonnes sont des vecteurs de caractéristiques spécifiques aux agents. Les éléments avec notation conjointe (tels que $\omega^j_t$) sont aussi éventuellement implicitement matriciels par commodité. Alors la transition symbolique s'écrit :
%
\begin{equation*}
  \begin{aligned}
    \mathcal{T}^j_s\!\left(
    \mathcal{M}_{h^j_{t-1}},
    \mathcal{M}_{\omega^j_t},
    \mathcal{M}_{a^j_t}
    \right)
     & = \mathcal{M}_{\omega^{d,j}_{t+1}}, \\
    \mathcal{M}_{\omega^{d,j}_{t+1}}
    + \mathcal{M}_{\omega^{u,j}_{t+1}}
     & = \mathcal{M}_{\omega^{j}_{t+1}} .
  \end{aligned}
\end{equation*}
%
Nous définissons le masque de déterminabilité $\mathcal{M}_d = \mathrm{sgn}\!\left(\left| \mathcal{M}_{\omega^{d,j}_{t+1}} \right| \right)$ pour indiquer les positions de caractéristiques dérivées via des règles symboliques. Ce masque permet une modélisation hybride symbolique/neuronale sélective.
Ce formalisme unifié sous-tend toutes les stratégies d'intégration neuro-symboliques ultérieures en permettant une prédiction partielle et l'application des caractéristiques déterminables symboliquement, tout en laissant le reste au modèle de dynamique neuronale.


\subsection{Stratégies d'intégration neuro-symbolique}
\label{sec:strategies}

Nous présentons trois stratégies pour intégrer le raisonnement symbolique avec les WMs appris. Toutes les stratégies opèrent sur l'espace latent d'observation structuré défini en \autoref{sec:ns}, où la prochaine observation conjointe $\omega^j_{t+1}$ est partiellement prédite par des règles symboliques et complétée par un prédicteur neuronal.

Soit $\hat{\omega}^j_{t+1} = \mathcal{T}_{\theta}(h^j_{t-1}, \omega^j_t, a^j_t)$ la prédiction neuronale de l'observation conjointe complète et $\omega^{d,j}_{t+1} = \mathcal{T}^j_s(h^j_{t-1}, \omega^j_t, a^j_t)$ la prédiction symbolique partielle.

\subsubsection{Régularisation de cohérence symbolique}
\label{sec:symbolic_loss_regularization}

Cette stratégie ajoute une supervision symbolique comme contrainte souple. L'idée est d'encourager le modèle neuronal (JOPM) à produire des sorties ($\hat{\omega}^{j}_{t+1} = \hat{\omega}^{reg,j}_{t+1}$) qui s'alignent sur les prédictions symboliques là où cette connaissance est disponible. Ceci s'inspire des stratégies de perte auxiliaire dans les modèles hybrides~\cite{Xu2018semanticloss, garcez2023neurosymbolic}. Soit $\hat{\omega}^{d,j}_{t+1} = \mathcal{M}_d \odot \hat{\omega}^j_{t+1}$ et $\omega^{d,j}_{t+1}$ la contrepartie symbolique.
%
La perte de cohérence symbolique est définie comme :
\begin{align*}
  \mathcal{L}_{\text{symb}} = \| \hat{\omega}^{d,j}_{t+1} - \omega^{d,j}_{t+1} \|^2,
\end{align*}
qui est ajoutée à la perte prédictive principale :
\begin{align*}
  \mathcal{L}_{\text{total}} = \mathcal{L}_{\text{pred}} + \lambda \mathcal{L}_{\text{symb}},
\end{align*}
où $\lambda$ est un coefficient de régularisation. Cette approche incite le modèle à apprendre des dynamiques symboliquement cohérentes de manière différentiable.

\subsubsection{Stratégie de projection symbolique}
\label{sec:symbolic_projection}

Cette stratégie utilise la connaissance symbolique pour remplacer une partie de la prédiction neuronale. Au lieu d'encourager uniquement la cohérence, nous l'imposons en projetant les caractéristiques déterminables depuis le modèle symbolique :
\begin{align*}
  \hat{\omega}^{j}_{t+1} = \mathcal{M}_d \odot \omega^{d,j}_{t+1} + (1 - \mathcal{M}_d) \odot \hat{\omega}^j_{t+1} \\
   & \hspace{-2.5em} = \hat{\omega}^{proj,j}_{t+1}
\end{align*}

La sortie projetée $\hat{\omega}^{proj,j}_{t+1}$ est garantie de respecter toutes les contraintes symboliques. Elle peut être utilisée comme :
\begin{enumerate*}[label={\roman*) },itemjoin={; \quad}]
  \item Entrée pour la planification en aval.
  \item Cible pour réentraîner le WM (amorçage de la conformité symbolique).
  \item Couche de correction explicite pour améliorer la fidélité des simulations.
\end{enumerate*}

Cette approche impose directement les contraintes symboliques sur les caractéristiques déterminables.

\begin{figure}[htb]
  \centering
  \include{figures/method_figure}
  \caption{Aperçu de NS-MAWM : $\mathcal{T}^j_s$ prédit les caractéristiques déterminables et $\mathcal{T}^j_\theta$ la transition complète. Les trois variantes illustrées sont la régularisation, la projection et la dynamique résiduelle.}
  \label{fig:ns-mawm_overview}
\end{figure}

\subsubsection{Stratégie de dynamique symbolique résiduelle}
\label{sec:residual_symbolic_dynamics}

Ici, nous séparons explicitement la prédiction symbolique et neuronale. Le prédicteur neuronal est entraîné \emph{uniquement} sur les caractéristiques qui ne sont pas déterminables à partir des connaissances symboliques. Ce partitionnement architectural s'inspire des cadres d'apprentissage résiduel en raisonnement neuro-symbolique~\cite{garcez2023neurosymbolic, balloch2023neurosymbolicworldmodelsadapting}. Le modèle symbolique calcule :
\begin{align*}
  \omega^{d,j}_{t+1} = \mathcal{T}^j_s(h^j_{t-1}, \omega^j_t, a^j_t),
\end{align*}
tandis que le modèle neuronal $\mathcal{T}_{\theta}^{u,j}$ est entraîné à prédire :
\begin{align*}
  \hat{\omega}^{u,j}_{t+1} = \mathcal{T}_{\theta}^{u,j}(h^j_{t-1}, \omega^{u,j}_t, a^j_t).
\end{align*}

La prédiction finale est assemblée par :
\begin{align*}
  \hat{\omega}^{j}_{t+1} = \hat{\omega}^{res,j}_{t+1} = [\omega^{d,j}_{t+1}, \hat{\omega}^{u,j}_{t+1}].
\end{align*}

Cette stratégie réduit la complexité du modèle : les règles gèrent les motifs déterministes, tandis que le réseau apprend les composantes ambiguës.

\subsection{Correction symbolique post-prédiction des caractéristiques neuronales}
\label{sec:post_prediction_correction}

Nous introduisons un mécanisme de post-correction symbolique qui impose la cohérence symbolique
sur la prédiction neuronale $\hat{\omega}^j_{t+1}$ au moment de l'inférence, sans réentraînement, via :
%
$\tilde{\omega}^{j}_{t+1} = \mathcal{C}^{s}(\hat{\omega}^{j}_{t+1})$,
%
où $\mathcal{C}^{s}$ applique des contraintes symboliques spécifiques au domaine
(comme l'exclusivité mutuelle, des transitions d'états valides, ou la discrétisation de caractéristiques catégorielles).
Cette correction légère améliore la cohérence sémantique et l'interprétabilité
des observations prédites.

\subsection{Taux de violation des règles}
\label{sec:rvr}

Nous quantifions la cohérence symbolique via le \textbf{taux de violation des règles} (RVR),
qui mesure à quelle fréquence les prédictions du modèle violent les règles de transition symboliques (\autoref{sec:ns}).

Étant donnée la prédiction symbolique
$\omega^{d,j}_{t+1} = \mathcal{T}^j_s(h^j_{t-1}, \omega^j_t, a^j_t)$
et la prédiction JOPM $\hat{\omega}^j_{t+1}$,
nous définissons l'indicateur de violation :
\begin{equation*}
  v_{i,k,t} =
  \begin{cases}
    1 & \text{si } \left| \hat{\omega}^{d}_{i,k,t+1} - \omega^{d}_{i,k,t+1} \right| > \epsilon, \\
    0 & \text{sinon},
  \end{cases}
\end{equation*}
où $(i,k)$ indexe les agents et les caractéristiques déterminables, et $\epsilon$ est un seuil de tolérance.
%
Le \textbf{RVR} sur $T$ transitions est :
\begin{equation*}
  \text{RVR} =
  \frac{1}{n T |\mathcal{F}^d|}
  \sum_{t=1}^T \sum_{i=1}^n \sum_{k \in \mathcal{F}^d} v_{i,k,t}.
\end{equation*}

Un RVR plus faible indique une cohérence symbolique plus forte.
RVR est principalement informatif pour les WMs purement neuronaux
et pour la régularisation de cohérence symbolique (\autoref{sec:symbolic_loss_regularization});
pour la projection symbolique et la dynamique symbolique résiduelle,
les caractéristiques déterminables sont imposées par construction et le RVR vaut trivialement zéro,
auquel cas il est surtout utilisé pour l'ablation et l'analyse diagnostique.


% ============================
% ============================
\section{Évaluation}
\label{sec:eval}

Nous évaluons NS-MAWM selon quatre axes correspondant aux lacunes de la \autoref{sec:intro} :
(i) \textbf{fidélité prédictive à long horizon} (biais de modèle et erreurs cumulées),
(ii) \textbf{cohérence symbolique} (respect des règles),
(iii) \textbf{contrôle en aval} (planification / performance de politique),
et (iv) \textbf{généralisation et robustesse} (nouvelles configurations et couverture partielle des règles).

\subsection{Implémentation}
\label{sec:impl_overview}

\footnotetext[1]{\label{fn:github}
  Une implémentation de NS-MAWM et le détail complet des expériences
  (y compris les hyperparamètres, architectures et définitions de règles)
  sont disponibles à l'adresse \url{https://github.com/julien6/NS-MAWM}.}

\paragraph{Backbone du WM.}
Notre cadre \textit{NS-MAWM}~\hyperref[fn:github]{\footnotemark[1]} s'appuie sur un backbone MAWM partagé
(\autoref{sec:mawm_basics}) et est implémenté en \texttt{PyTorch}.
Nous utilisons un JOPM centralisé comprenant
(i) un encodeur partagé $Enc^j$ sur les observations conjointes,
(ii) un noyau de dynamique latente basé LSTM~\cite{hochreiter1997long},
et (iii) un décodeur $Dec^j$ prédisant l'observation conjointe suivante.
Les observations conjointes sont concaténées le long de la dimension agent et encodées via des MLP avec activations ReLU.
Les caractéristiques discrètes structurées sont encodées en one-hot et traitées comme sorties continues,
tandis que les caractéristiques continues sont prédites directement.
La perte de prédiction est
$\mathcal{L}_{\text{pred}} = \sum_{k \in \mathcal{F}_{\text{cont}}}\|\hat{f}_{k}-f_{k}\|^2$.
Tous les composants neuronaux sont optimisés avec Adam
avec un taux d'apprentissage fixe.

\paragraph{Composants neuro-symboliques.}
L'opérateur de transition symbolique $\mathcal{T}^j_s$ (\autoref{sec:ns}) est implémenté comme un moteur de règles Python léger.
Chaque règle produit de manière déterministe un sous-ensemble de caractéristiques déterminables $\omega^{d,j}_{t+1}$
et un masque correspondant $\mathcal{M}_d$.
Ce design modulaire prend en charge l'activation sélective des règles pour des études d'ablation et de robustesse.
Nous évaluons trois stratégies d'intégration (\autoref{sec:strategies}) :
(i) régularisation de cohérence symbolique via une perte auxiliaire inspirée de la perte sémantique~\cite{Xu2018semanticloss,Fan2017differentiablelearninglogicalrules},
(ii) projection symbolique à l'inférence,
et (iii) dynamique symbolique résiduelle, restreignant la prédiction neuronale aux caractéristiques non déterminables.
Nous évaluons en outre un opérateur de correction symbolique en inférence $\mathcal{C}^s$
(\autoref{sec:post_prediction_correction}),
qui impose des contraintes de validité (telles que l'exclusivité mutuelle ou la faisabilité spatiale)
à l'aide d'opérateurs simples de projection et de discrétisation~\cite{garcez2023neurosymbolic}.

\paragraph{Données d'entraînement et interfaces.}
Les WMs sont entraînés hors ligne à partir de buffers de rejouement collectés avec des politiques aléatoires ou faiblement exploratoires.
Nous nous appuyons sur des interfaces MARL standard :
\texttt{Overcooked-AI}~\cite{Micah2020overcooked}
et le benchmark \texttt{SMAC}~\cite{Ellis2023smacv2}.
Les buffers de rejouement stockent les observations conjointes, les actions, et les observations suivantes.
Pour évaluer la frugalité en données, les modèles sont entraînés sur plusieurs tailles de jeux de données
(telles que $N \in \{N_{\text{small}}, N_{\text{med}}, N_{\text{full}}\}$),
tout en conservant des architectures et des calendriers d'entraînement fixes entre méthodes.

\paragraph{Sélection d'hyperparamètres.}
Les hyperparamètres neuronaux (taux d'apprentissage, dimension latente, taille LSTM, profondeur)
sont sélectionnés automatiquement
en minimisant la perte de prédiction de validation.
L'optimisation d'hyperparamètres est appliquée exclusivement aux composants neuronaux
et indépendamment des règles symboliques, avec des espaces de recherche et budgets identiques entre méthodes.
Les hyperparamètres sélectionnés sont fixes pour toutes les expériences ultérieures.



\subsection{Environnements et ensembles de règles}
\label{sec:envs}

Nous considérons quatre environnements multi-agents représentatifs :

\paragraph{(E1) Gridcraft}~\hyperref[fn:github-gridcraft]{\footnotemark[2]}, une grille de type Minecraft (discrète, partiellement observable).
Un environnement en grille avec $n$ agents, murs/obstacles et objets typés.
Les observations sont des caractéristiques structurées (positions relatives, occupation, présence d'objets),
rendant explicites les invariants et équivariances.
Les règles~\hyperref[fn:github]{\footnotemark[1]} incluent l'évitement des collisions, les contraintes de frontière, la persistance des objets, et les équivariances de mouvement.

\footnotetext[2]{\label{fn:github-gridcraft}
  Une version est disponible à l'adresse : \url{https://github.com/julien6/Gridcraft}.}

\paragraph{(E2) Overcooked} ~\cite{Micah2020overcooked} (discret, coordination coopérative).
Nous utilisons des layouts Overcooked caractérisés par des récompenses clairsemées et de fortes exigences de coordination.
Les règles~\hyperref[fn:github]{\footnotemark[1]} capturent les contraintes de transition d'état (interactions pickup/drop), les états d'objets mutuellement exclusifs,
et les contraintes de mouvement spatial.

\paragraph{(E3) Predator--Prey}~\cite{lowe2017multi} (continu ou mixte, interaction multi-agents).
Nous utilisons un benchmark Predator--Prey standard avec positions/vitesses continues (ou actions discrètes mixtes).
Les règles~\hyperref[fn:github]{\footnotemark[1]} encodent des contraintes cinématiques (vitesse bornée), des conditions de capture basées sur la proximité, et la persistance.

\paragraph{(E4) SMAC}~\cite{Ellis2023smacv2} (StarCraft Multi-Agent Challenge, partiellement observable).
Nous évaluons un sous-ensemble de scénarios SMAC avec des tailles d'équipe variables.
Les règles~\hyperref[fn:github]{\footnotemark[1]} se concentrent sur la faisabilité du mouvement et les invariances conditionnées par l'action
(par exemple, si un agent ne bouge pas, sa position reste cohérente ; contraintes de type cooldown).

% \paragraph{Rule sets.}
% For each environment, we define a rule set $\mathcal{R}$~\hyperref[fn:github]{\footnotemark[1]} implementing $\mathcal{T}^j_s$ and the feature mask $\mathcal{M}_d$.
% TODO : donner un aperçu du nombre/type de règles par environnement

\subsection{Ressources de calcul}
\label{sec:resources}

Toutes les expériences ont été menées sur un cluster académique avec des GPU NVIDIA A100/V100 et AMD MI210.

% \input{tables/results_overview}

\begin{table*}[!htb]
  \centering
  \footnotesize
  \resizebox{\textwidth}{!}{%
    \begin{tabular}{lcccccccc}
      \toprule
      \multirow{2}{*}{\textbf{Méthode}}           &
      \multicolumn{2}{c}{\textbf{Gridcraft}}      &
      \multicolumn{2}{c}{\textbf{Overcooked}}     &
      \multicolumn{2}{c}{\textbf{Predator--Prey}} &
      \multicolumn{2}{c}{\textbf{SMAC}}                                                                                                       \\
      \cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7} \cmidrule(lr){8-9}
                                                  & MSE$_{H}$ $\downarrow$ & RVR $\downarrow$
                                                  & MSE$_{H}$ $\downarrow$ & RVR $\downarrow$
                                                  & MSE$_{H}$ $\downarrow$ & RVR $\downarrow$
                                                  & MSE$_{H}$ $\downarrow$ & RVR $\downarrow$                                                 \\
      \midrule
      MAWM (Neural)                               & 0.094                  & 0.187            & 0.141 & 0.228 & 0.083 & 0.161 & 0.168 & 0.201 \\
      NS-MAWM (Reg.)                              & 0.072                  & 0.064            & 0.106 & 0.089 & 0.069 & 0.055 & 0.134 & 0.076 \\
      NS-MAWM (Projection)                        & 0.067                  & 0.000            & 0.095 & 0.000 & 0.064 & 0.000 & 0.125 & 0.000 \\
      NS-MAWM (Résiduel)                          & 0.061                  & 0.000            & 0.087 & 0.000 & 0.059 & 0.000 & 0.116 & 0.000 \\
      \bottomrule
    \end{tabular}
  }
  \caption{Évaluation quantitative. MSE$_H$ est l'erreur quadratique moyenne sur des rollouts ouverts d'horizon $H$ (plus petit est meilleur). RVR est le taux de violation des règles (\autoref{sec:rvr}). Les variantes projection/résiduel imposent $\mathcal{F}^d$ par construction (RVR nul).}
  \label{tab:results_overview}
\end{table*}

\subsection{Métriques d'évaluation, bases et protocole}
\label{sec:metrics_baselines_protocol}

\paragraph{Bases de comparaison.}
Nous comparons un modèle du monde multi-agents purement neuronal
(\textbf{MAWM (Neural)}) à trois variantes NS-MAWM :
(i) \textbf{régularisation de cohérence symbolique},
(ii) \textbf{projection symbolique},
et (iii) \textbf{dynamique symbolique résiduelle}.
Toutes les méthodes partagent le même backbone neuronal et le même budget d'entraînement.
Sauf mention contraire, la post-correction symbolique $\mathcal{C}^s$ est désactivée.

\paragraph{Fidélité prédictive.}
La précision des prédictions est évaluée à un pas et sur des simulations en boucle ouverte

d'horizon $H=25$, où les prédictions du modèle sont réinjectées comme entrées.
Nous rapportons l'erreur multi-pas agrégée
$
  \text{MSE}_H =
  \frac{1}{H}
  \sum_{t=1}^{H}
  \mathbb{E}\!\left[
    \|\hat{\omega}^j_t - \omega^j_t\|_2^2
    \right]
$,
moyennée sur les trajectoires.

\paragraph{Cohérence symbolique.}
La cohérence symbolique est mesurée par le
\textbf{RVR} (\autoref{sec:rvr})
calculé sur des simulations en boucle ouverte, qui quantifie les violations des caractéristiques déterminables
$\mathcal{F}^d$.
Pour les stratégies de projection et résiduelle, les caractéristiques déterminables sont imposées
par construction, donnant un RVR $=0$ ; le RVR diagnostique donc principalement
le modèle neuronal de base et la régularisation symbolique.

\paragraph{Généralisation et robustesse.}
La généralisation est évaluée sur des configurations hors distribution,
incluant des layouts inédits, des nombres d'agents variables,
et des scénarios SMAC alternatifs.
La robustesse à une connaissance symbolique incomplète est évaluée
en supprimant aléatoirement une fraction $\rho \in \{0.25, 0.5\}$
des règles symboliques.

\paragraph{Protocole.}
Pour chaque environnement, toutes les méthodes sont entraînées sur le même jeu de données hors ligne
et des partitions entraînement/validation identiques, puis évaluées sur des trajectoires tenues à part.
Les résultats sont rapportés comme moyenne $\pm$ erreur standard sur 5 graines aléatoires.

\subsection{Résultats}
\label{sec:results}

Le \autoref{tab:results_overview} rapporte les résultats quantitatifs
dans tous les environnements et dimensions d'évaluation.

\paragraph{Précision des prédictions à long horizon et accumulation d'erreurs (G1).}
Dans tous les environnements, NS-MAWM améliore la fidélité prédictive à long horizon par rapport à MAWM. Sur Gridcraft, la stratégie résiduelle passe de MSE$_H=0.094$ à $0.061$ ($\approx 35\%$). Les gains sont encore plus marqués sur Overcooked et SMAC, où l'erreur se propage plus vite en boucle ouverte (réduction relative de $38\%$ et $31\%$). Les contraintes symboliques agissent ainsi comme des biais inductifs efficaces contre la dérive sémantique.

\paragraph{Cohérence symbolique et respect des contraintes (G2, G3).}
Le RVR montre l'écart entre précision numérique et validité sémantique : malgré des MSE$_H$ modérées, MAWM viole $16\%$ à $23\%$ des caractéristiques déterminables. La régularisation réduit ce taux d'environ un facteur trois (par exemple de $0.228$ à $0.089$ dans Overcooked), tandis que projection et résiduel obtiennent un RVR nul par construction.

\paragraph{Comparaison des stratégies d'intégration.}
Toutes les variantes NS-MAWM surpassent la base. La régularisation est la plus simple à intégrer mais laisse des violations résiduelles ; la projection garantit la conformité des caractéristiques déterminables. La dynamique symbolique résiduelle offre le meilleur compromis, avec la plus faible MSE$_H$ et un RVR nul sur tous les environnements.

\section{Conclusion et travaux futurs}
\label{sec:conclusion}

Nous avons proposé \textbf{NS-MAWM}, un cadre neuro-symbolique pour les MAWM qui intègre le raisonnement symbolique. En imposant la cohérence symbolique, NS-MAWM réduit la dérive sémantique à long horizon et améliore la planification par imagination, offrant des gains cohérents en fidélité prédictive et en généralisation par rapport aux WMs purement neuronaux.

Malgré ses avantages, NS-MAWM entraîne un surcoût computationnel,
pose des défis de scalabilité à mesure que le nombre d'agents, de caractéristiques ou de règles symboliques augmente,
et repose sur des règles symboliques conçues à la main.
Les travaux futurs exploreront :
(1) l'utilisation de grands modèles de langage pour assister l'extraction ou le raffinement automatique des règles ;
(2) le développement de formulations matricielles des transitions symboliques pour réduire le surcoût et améliorer la scalabilité ;
et (3) l'adoption de cadres numériques hautes performances tels que \texttt{JAX}
pour permettre une vectorisation efficace, la compilation, et l'accélération matérielle.

% ============================
% Acknowledgements (remove for submission if needed)
% ============================

% \section*{Remerciements}

% Ce travail est financé par \emph{Thales Land Air Systems} et s'inscrit dans les travaux de la chaire \emph{Cyb'Air} ainsi que de l'\emph{AICA IWG}.

% ============================
% References
% ============================
\renewcommand{\bibname}{}

\bibliographystyle{ieeetr}
\begin{myCustomSize}
  \bibliography{references}
\end{myCustomSize}

\end{document}

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "jfsmaLatex"
%%% ispell-local-dictionary: "francais"
%%% TeX-command-extra-options: "-shell-escape"
%%% End: 
