\clearpage
\thispagestyle{empty}
\null
\newpage

\cleardoublepage
\phantomsection
% \pdfbookmark[1]{Experimental validation of the method}{Experimental validation of the method}
\markboth{\textsc{Experimental validation of the method}}{\textsc{Experimental validation of the method}}
\part{Experimental validation of the method}
\label{part:experimentation}

\clearpage
\thispagestyle{empty}
\null
\newpage

% \acn{TODO}:
% - In the chapter on CybMASDE, start by giving a general overview of the platform (screenshots, activity diagram), its architecture (\acn{UML}: sequence, class, component) and the technical choices made (hyperparameter values).
% - For the chapter on the experimentation protocol, include the table linking each criterion to a set of metrics. The idea is to establish a common evaluation grid for the six different environments in order to verify or see how the set of criteria (C1 to C5) are valid for each environment tested according to the different baselines. The rest of the protocol must include elements such as hardware and software configuration, the different baselines (with or without organizational specifications, changing the severity of constraints, training frequency, etc. \acn{TODO}: but to be defined completely).
% - For the next chapter, we can keep the description of each environment (including observations, actions, states, rewards, etc.), briefly recalling the specific application of the protocol for that environment.
% - For the next chapter, we can present the results and how, according to the evaluation grid (based on the metrics) of the experimental protocol, the criteria are covered. At the end, we can provide a general summary of the results using the common evaluation grid (since we have numerical values, we can calculate averages, study standard deviations, etc.). This allows us to see, in general terms, how our applied method consistently and broadly covers all of the criteria.


\chapter*{Introduction}
\addcontentsline{toc}{chapter}{\textbf{Introduction}}

\noindent
This section aims to demonstrate the applicability and relevance of the \acn{MAMAD} method in all or part of its activities in different \acplu{SMA} design contexts . To this end, we have developed a dedicated platform, \acn{CybMASDE}, which implements the entire proposed pipeline (modeling, learning, analysis, transfer) in a modular and reproducible manner.

\medskip

\noindent
First, we describe in detail the experimental environment, the software and hardware tools used, and the test environments selected. We also present the organizational specifications associated with each environment, as well as the evaluation metrics used to validate the method's performance. \autoref{fig:organisation_manuscript_part_4} illustrates the organization of this section.

\medskip

\noindent
Second, we analyze the results obtained in order to meet the research objectives identified in the previous section. This includes an evaluation of the method's effectiveness, its capacity for automation, the adequacy of the learned policies with respect to organizational constraints, and their explainability.

\medskip

\noindent
This experimental study will enable us to better understand the strengths and limitations of the \acn{MAMAD} method and to identify opportunities for improvement for even greater automation of organizational design in \acn{MARL}.

\begin{figure}[h!]
  \centering
  \resizebox{0.7\linewidth}{!}{%
    \input{figures/organisation_manuscript_part_4}
  }
  \caption{Structure of Part IV: Experimental framework and analysis of results}
  \label{fig:organisation_manuscript_part_4}
\end{figure}

\clearpage
\thispagestyle{empty}
\null
\newpage

\chapter{CybMASDE: A framework supporting MAMAD}
\label{sec:cybmasde}

% 1 - Introduction
% 2 - Features
% 3 - Internal architecture
% 4 - Implementation
% 5 - Illustrative scenario

\acn{CybMASDE}\footnotemark[1] is a modular and extensible platform that we propose to support the \acn{MAMAD} method. It integrates all the implementations of the contributions proposed in the method in order to meet the various expectations of the designer.
It allows designers to model the target environment, determine effective policies, and then analyze them to infer understandable organizational specifications by alternating between training and analysis. At the same time, \acn{CybMASDE} maintains consistency between the simulated environment and the real environment by updating both the simulated model and the deployed policies.
%
\acn{CybMASDE} can be used primarily in \acn{CLI} mode with the \textquote{cybmasde} command (see the manual in \autoref{appendix:cybmasde-manual}), but also offers a graphical interface for performing the main configuration tasks.


\section{Features offered by \texttt{CybMASDE}}

This section describes the typical journey of a \acn{CybMASDE} user, from modeling the environment to obtaining a satisfactory joint policy accompanied by performance measures, stability, and explanatory organizational specifications. Each step indicates both the user's actions and the platform's internal processes.

In general, the \textbf{Designer} can activate any feature as long as the dependencies between features are respected.
% (see \autoref{fig:cybmasde_usecase}).
For example, it is possible to start training if the environment has been previously modeled manually.

% \begin{figure}[h!]
% \centering
% \includegraphics[width=\linewidth]{figures/use_case_cybmasde.pdf}
% \caption{CybMASDE use case diagram}
% \label{fig:cybmasde_usecase}
% \end{figure}

Although the \textbf{Designer} represents the \textquote{general} role, they actually play different roles when interacting with \acn{CybMASDE}~:
\begin{itemize}
  \item \textbf{Environment modeler}, responsible for defining or completing the description of the simulated environment, either automatically using a \textbf{World Model}, or manually using the \acn{MCAS} model;
  \item \textbf{Agent trainer}, who conducts the learning phases of multi-agent policies, taking into account organizational specifications;
  \item \textbf{Organizational analyst}, who uses the \acn{TEMM} or \acn{Auto-TEMM} method to infer organizational roles and objectives from the trajectories produced;
  \item \textbf{Specification refiner}, which combines the two previous roles to perform iterative training and analysis cycles until a satisfactory policy is obtained~;
  \item \textbf{Deployment operator}, which supervises the integration of the final joint policy into the real environment, either in \textit{DIRECT} mode (policy executed locally by agents) or in \textit{REMOTE} mode (policy executed by the \textit{Transferring} process).
\end{itemize}

In practice, \acn{CybMASDE} is designed as a tool that can mainly be used in \acn{CLI} mode: each step of the \acn{MAMAD} cycle (Modeling, Training, Analyzing, Transferring) is exposed through an explicit command.
%
After creating a new project, \acn{CybMASDE} automatically generates a folder tree structured around the four main activities of the \acn{MAMAD} method (modeling, training, analysis, transfer), as well as a central configuration file \texttt{project\_configuration.json} . A detailed example of this file for the Overcooked-AI environment is given in \autoref{tab:cybmasde_config}.

\input{tables/oa_configuration_file}

The general principle of \acn{CybMASDE} is that the user must invest a significant initial effort to configure all the parameters of their project, centralized in the \texttt {project_configuration.json}.
The designer then enters the necessary information for each activity, mainly in the form of Python code, JSON, or by specifying the paths to existing files. For example, for modeling, the user can provide the path to an existing simulation, or choose to create a new one by defining the space of observations, actions, reward, stop, and possibly rendering functions based on history, as well as the hyperparameters for training the World Model.
Once this preparatory work has been completed, the entire pipeline becomes automatable and no longer necessarily requires manual intervention except for refinement cycles.

\medskip

\noindent The typical path for the command lines entered is generally as follows:
\begin{enumerate}
  \item \textbf{\texttt{init}}: creates the project tree and a blank configuration file.
        The user then completes this file by filling in the observation and action spaces, reward and stop functions, organizational specifications, and training hyperparameters. A required step for configuration is to implement an \textbf{Environmental API} by implementing the \textquote{environment\_api} interface, which will allow \acn{CybMASDE} to communicate with the target environment (see \autoref{appendix:cybmasde-environment-api}).
  \item \textbf{\texttt{validate}}: checks the completeness and consistency of the project.
        In case of errors (missing file, unimplemented function, invalid API), execution is stopped with an explicit message.
  \item \textbf{\texttt{model}}: models a simulated environment from the real environment.
        Two variants exist:
        \texttt{model --auto}, which triggers the generation of a World Model from traces, and
        \texttt{model --manual}, which loads an instance \acn{MCAS} filled in by the user.
  \item \textbf{\texttt{train --algo <alg>}}: trains multi-agent policies using the algorithms available in MARLlib (MAPPO, MADDPG, QMix, etc.).
        Training automatically applies the organizational constraints defined in the project.
  \item \textbf{\texttt{analyze --auto-temm}}: applies the Auto-TEMM method to extract organizational specifications (roles, objectives) and organizational fit metrics.
        The results are saved in the \texttt{analyzing/} folder.
  \item \textbf{\texttt{refine --max <N>}}: launches an iterative loop combining training and analysis until a satisfactory joint policy is obtained (reward and stability thresholds reached, or manual stop by the user).
  \item \textbf{\texttt{deploy}}: deploys the validated joint policy in the real environment, either in \texttt{--direct} mode (agents execute the policy autonomously) or in \texttt{--remote} mode (the \textit{Transferring} process executes the policy and sends the actions).
  \item \textbf{Utility commands}:
        \texttt{run --full-auto} (complete execution of the pipeline without interruption),
        \texttt{run --manual} (step-by-step execution),
        \texttt{status} (monitoring of a project in progress),
        \texttt{export --format <fmt>} (export results and metrics),
        \texttt{clean --all} (reset the working environment).
\end{enumerate}

\medskip
\noindent

\paragraph{Typical example of use in automatic mode.}
A common scenario for using \acn{CybMASDE} is to execute the entire \textbf{MTA+T} pipeline (\textit{Modeling}, \textit{Training}, \textit{Analyzing}, \textit{Transferring}) in fully automated mode, for example during reproducible experiments on a computing cluster.
In this case, a single command line is sufficient to orchestrate all the steps, without any human interaction, as illustrated below:

\begin{lstlisting}[language=bash, caption={Complete execution of CybMASDE in full-auto mode}, label={lst:cybmasde_full_auto}]
cybmasde run \
--full-auto \ # complete pipeline (MTA+T) without interaction
--project /home/john/Documents/new_test \ # path to the project
--config /home/john/Documents/new_test/project_configuration.json \ # config file
--max-refine 10 \ # maximum number of refinement iterations
--reward-threshold 3.5 \ # performance threshold
--std-threshold 0.05 \ # stability threshold (standard deviation)
--accept-inferred \ # accept automatically inferred org. specs
--skip-model \ # avoid restarting modeling
--skip-analyze # skip analysis if reward is sufficient
\end{lstlisting}

This example corresponds to a classic configuration: the user sets a \textit{reward threshold} to validate joint policies, a \textit{maximum number of refinement iterations} to improve stability, and activates the \texttt{--accept-inferred} option to automatically integrate the organizational specifications inferred by \acn{Auto-TEMM}.
In practice, this command illustrates the \textbf{batch/HPC} mode, used for large-scale experimental campaigns, as it guarantees continuous execution of the pipeline, from the processing of initial traces to the deployment of a finalized joint policy.

Note that \acn{CybMASDE} also provides a less configurable graphical interface, but one that allows the project to be configured in an accessible way and also allows the various activities to be executed. This interface is described in more detail in \autoref{appendix:cybmasde-gui}.


\section{CybMASDE life cycle}

Below, we detail the sets of exchanges between the different entities that constitute the stages, which also include internal processes and interactions with the user (see \autoref{fig:cybmasde_sequence}).

\paragraph{1. Initial configuration between the user, the real environment, and CybMASDE}

The user begins by creating a new project with the command \texttt{cybmasde init <project\_name>}, which generates the necessary folder tree and file templates. In this new project, the user must complete the configuration file \texttt{project_configuration.json} by filling in the action and observation spaces, the \texttt{reward()} and \texttt{stop()} functions, the organizational specifications (roles, objectives, missions), as well as the hyperparameters for training the World Model and multi-agent policies. They must also implement an environmental API to allow \acn{CybMASDE} to communicate with the target environment. Once the configuration is complete, the user can validate the project with the \texttt{cybmasde validate} command, which checks the completeness and consistency of the files. If errors are detected, execution is interrupted with an explicit message.

\begin{figure}[H]
\includegraphics[trim={0cm 0cm 0cm 0cm},clip,height=\textheight]{figures/diagramme_sequence_CybMASDE.pdf}
% }
\caption{Sequence diagram for using CybMASDE}
\label{fig:cybmasde_sequence}
\end {figure}

\paragraph{2. \textit{Transferring} process}

If the project configuration is correct, the entire pipeline is launched in automatic mode with the command \texttt{cybmasde run --full-auto}. This creates and executes the \textit{Transferring} process, which is a continuous process that runs independently of the others. For the first execution, this process initializes a random policy and saves it locally in the project folder, along with an empty batch of joint histories.

Then, the execution of \textit{Transferring} enters an asynchronously executed loop that allows the deployed policies and the simulation model to be updated. In this loop, \textit{Transferring} loads the latest trained policy and deploys it in the real environment using the environmental API in two possible modes:
\begin{itemize}
  \item in \textit{DIRECT} mode, the policy is sent directly to the agents who execute it locally, storing their own histories which are retrieved and concatenated in the current batch at regular intervals via the environmental API;
  \item in \textit{REMOTE} mode, the policy is executed on the \acn{CybMASDE} side, actions are sent via the environmental API, observations are received, and new histories are thus formed on the \acn{CybMASDE} side and stored in the current batch.
\end{itemize}

Still in this loop, if the current batch of stored joint histories exceeds a threshold in terms of the number of joint histories (defined in the configuration), then it is saved in the history database (which is a simple folder containing the joint histories in JSON format) and emptied. This backup automatically triggers the \textit{MTA} process.

\paragraph {3. MTA process: Modeling–Training–Analyzing}

The \textit{MTA} process is therefore triggered each time a batch of new joint histories is added in order to take into account changes in the environment and generate appropriate policies. This process is independent of \textit{Transferring} and sequentially executes the three main activities: \textit{Modeling}, \textit{Training}, and \textit{Analyzing}. Each step uses the data and configurations provided by the user to accomplish its specific task.

First, in the modeling activity, if a simulated environment \textit{PettingZoo} has already been provided in the configuration file settings, it is loaded. Otherwise, an environment is generated by training a \textbf{World Model} from the latest recorded data. In the latter case, the last saved \textbf{World Model} is loaded (if it does not yet exist, then a new \textbf{World Model} is created). Next, all joint histories saved in the database are loaded to train the \textbf{World Model} using the hyperparameters provided in the configuration file. Once training (or updating) is complete, the \textbf {World Model} is then saved locally. A simulated \textit{PettingZoo} environment is then created from the trained \textbf{World Model} and the information manually provided by the user in the configuration: the initial joint observations, the action/observation space, the reward, rendering (optional), and stopping functions.

\noindent \textbf{Refinement loop} \\

Next, the \textit{MTA} process enters a refinement loop that only stops when the learned joint policy reaches a certain level of performance and stability, or when the maximum number of iterations is reached, or when the user decides to stop the process. In this loop, the joint policy is improved with each iteration by alternating between training and analysis activities.

In the training activity, the simulated environment set up previously is loaded to train the multi-agent policies using the MOISE+MARL implementation (see \autoref{sec:cybmasde_tech_stack}) to take into account the organizational specifications. Following the hyperparameters defined in the configuration, training is performed with a chosen algorithm (MAPPO, MADDPG, QMix, etc.) and according to a given maximum number of episodes. Once training is complete, the learned joint policy is saved locally in the project folder. Once training is complete, the analysis activity is launched.

In the analysis activity, the learned joint policy is loaded and evaluated by running several episodes in the simulated environment (according to the value of the sliding window on the last episodes). The resulting trajectories are collected and analyzed using the \acn{Auto-TEMM} or \acn{TEMM} method depending on the parameters given in the configuration file. Implicit organizational specifications (roles, objectives) are obtained in the form of constraint guides (mainly RAG and GRG) as well as organizational adequacy metrics (\acn{SOF}, \acn{FOF}, \acn{OF}). The stability of the policy is also evaluated by calculating the variance of the rewards obtained over these episodes as well as the average reward over these same episodes. The results of the analysis are saved locally in the project folder. At this point, the user is invited to consult the results of the analysis to decide whether to continue, stop, or manually refine the organizational specifications (for example, keep the most relevant observation-action rules). To do this, they can use a series of figures (visualizations \acn {PCA} visualizations and dendrograms of trajectories including centroids calculated by \acn{Auto-TEMM}/\acn{TEMM}).

\

The refinement loop continues until the average reward reaches the set threshold, the standard deviation remains above the stability threshold, the maximum number of iterations is reached, or the user chooses to continue. When the refinement cycles are complete, as with all trained policies, the last joint policy learned is also saved to be loaded by the \textit{Transferring} process, which continues its parallel execution. The \textit{MTA} process then stops until the next time a new batch of joint histories is added to the joint history database.


\medskip
In summary, \acn{CybMASDE} articulates two parallel processes: \textit{Transferring}, responsible for continuous deployment and data collection, and the \textit{MTA} process, triggered periodically to model, train, analyze, and refine policies.
User interactions occur mainly during initial configuration, refinement loop control, and final deployment.

\section{Life cycle implemented on Overcooked-AI}

\autoref{fig:cybmasde_cycle} shows the usage cycle of \acn{CybMASDE}.
To make this process more concrete, we provide a detailed step-by-step tutorial using the example of the \textbf{Overcooked-AI}~\cite{overcookedai} environment, which simulates a cooperative kitchen where two agents must prepare and serve soups. \autoref {tab:cybmasde_config} describes the configuration of this project.

\subsection{Initial configuration between the user, the real environment, and CybMASDE}

The user begins by installing Overcooked-AI and running a local instance accessible via a REST API, in order to \textquote{simulate a real environment} (and allow \acn{CybMASDE} to interact directly with the environment via the environmental API, which happens to be a REST API in another process -- see \autoref{appendix:cybmasde-environment-api}) . This setup step consists of preparing a daemon that manages the agents, the state of the kitchen, and the rules of the game. On the \acn{CybMASDE} side, no processes are activated yet: the platform simply waits for a project to be created and its configuration to be initialized before it can start the complete cycle.

\noindent
\textbf{Creating a project} \quad
Once the environment is ready, the user creates a new project with the command \texttt{cybmasde init -n overcooked_coop --template worldmodel}. This command automatically generates the working tree, with the folders \texttt{modeling}, \texttt{training}, \texttt{analyzing}, and \texttt{transferring} . A central file \texttt{project_configuration.json} is also produced, serving as an entry point for the project description. At the same time, \acn{CybMASDE} creates template files such as \texttt{label_manager.py} and reward or stop function skeletons to guide the user in completing the essential elements.

\noindent
\textbf{Providing the initial elements} \quad
In this phase, the user fills in the \texttt{project_configuration.json} file. They describe the spaces for actions and observations needed to represent the kitchen environment: agent movements, picking up and putting down ingredients, cooking and serving actions. They then define a \texttt{reward()} function that awards a point when soup is served, for example, and possibly a penalty in the event of a collision between the two agents. A termination criterion is set, such as a limit of 30 game steps. The user can leave the \texttt{handcrafted_environment.py} file empty, in which case no \acn{MCAS} model is used. Once these elements have been entered, the user executes the \texttt{cybmasde validate} command, which allows \acn{CybMASDE} to check the consistency and completeness of the configuration. If there is an error, execution is interrupted with an explicit message; otherwise, the project is ready to be executed.

\subsection{Transferring process}
Once a satisfactory joint policy has been obtained, it is deployed in the real environment with the command \texttt{cybmasde deploy --remote --api http://localhost:5000/api}. In this mode, the policy is executed by \acn{CybMASDE}, which sends actions to the game agents and receives their observations. The \texttt{--direct} mode, which directly integrates the policy into the agents, is also possible, but has not been considered here. Throughout the deployment and \textit{MTA} process, the \textit{Transferring} process continues to collect new traces that can be used to feed subsequent refinement cycles.

\begin{figure}[H]
  \centering
  \includegraphics[trim={5cm 1cm 5cm 1cm},clip,height=\textheight]{figures/CybMASDE_user_flowchart.pdf}
  % }
  \caption{CybMASDE usage cycle}
  \label{fig:cybmasde_cycle}
\end{figure}

\subsection{MTA Process: Modeling–Training–Analyzing}

\noindent
\textbf{Modeling} \quad
The command \texttt{cybmasde model --auto}, \acn{CybMASDE} is launched and proceeds with an initial collection of trajectories by executing a random policy in Overcooked-AI. The two agents then move around aimlessly, sometimes picking up an onion or occupying the same squares in the kitchen. This raw data is used to train a \acn{JOPM} type \textbf{World Model}. This model consists of a variational encoder (VAE) that compresses the agents' joint observations into latent vectors and an RNN+MLP (\acn{RLDM}) network that predicts the next joint observation based on the action performed. The trained model is then saved in the \texttt{generated\_environment/} folder and forms the basis of a simulated environment equivalent to that of Overcooked-AI, which can be used without requiring the actual API.

\

\noindent
\textbf{Training} \quad
Once the simulated model of the environment has been generated, the agents are trained automatically with the command \texttt{cybmasde train --algo MAPPO} . \acn{CybMASDE} then mobilizes \texttt{MARLlib} and \texttt{Ray RLlib} to perform multi-agent learning. The organizational roles defined by MOISE+MARL guide the process: one agent specializes in picking and preparing onions (cook), the other in serving soup (server), and versatile roles are also considered (see \autoref{lst:org_spec_oa}). As training progresses, checkpoints are saved along with reward curves that allow performance to be tracked over episodes.

\begin{lstlisting}[language=Python,basicstyle=\scriptsize, label={lst:org_spec_oa}, caption={Excerpt from the organizational configuration file for Overcooked-AI}]
organizational_model(
structural_specifications(
roles={
“role_server”: role_logic(label_manager=oa_label_mngr).register_script_rule(primary_fun),
“role_polyvalent”: role_logic(label_manager=oa_label_mngr).register_script_rule(secondary_fun)},
role_inheritance_relations={}, root_groups={}),
functional_specifications=functional_specifications(
goals={}, social_scheme={}, mission_preferences=[]),
deontic_specifications=deontic_specifications(permissions=[], obligations=[
deontic_specification(
“role_server”, [“agent_0”], [], time_constraint_type.ANY),
deontic_specification(
“role_polyvalent”, [“agent_1”], [], time_constraint_type.ANY)
]))
\end{lstlisting}

\

\textbf{Analysis} \quad
In the configuration step, the user set a performance threshold, for example an average reward of 3.5 per episode. After training, the \acn{CybMASDE} analysis activity calculates the average rewards obtained by the agents and compares it to this threshold. In the case of Overcooked-AI, it is not uncommon for agents to achieve a lower average after the first few iterations.

The command \texttt{cybmasde analyze --auto-temm} applies the \acn{Auto-TEMM} method to the collected trajectories (see \autoref{lst:cybmasde_auto_temm_output}). This analysis consists of grouping trajectories into clusters according to their similarity, inferring the implicit roles played by the agents, and calculating metrics of stability and organizational adequacy. In the Overcooked-AI environment, for example, we can observe that agent 0 systematically adopts a server behavior while agent 1 assumes a versatile one. The results of this analysis are saved in the \texttt{analyzing/ } folder and can be viewed by the user (see \autocite{lst:cybmasde_auto_temm_spec_output}).


\begin{lstlisting}[language={},basicstyle=\scriptsize, label={lst:cybmasde_auto_temm_output}, caption={Excerpt from the log output after training and application of TEMM}]
Running TEMM analysis...
1. Loading trajectories...
2. Clustering trajectories...
3. Generating visualizations...
4. Computing centroids.. .
5. Selecting near-centroid trajectories...
6. Extracting roles and goals...
7. Summarizing roles and goals...
8. Computing organizational fit scores...
Structural Fit (SOF): 1.000
Functional Fit (FOF): 1.000
Overall Organizational Fit (OF): 1.000
Finished TEMM analysis
\end{lstlisting}


\begin{lstlisting}[language=json,basicstyle=\tiny, label={lst:cybmasde_auto_temm_spec_output}, caption={Extract of organizational specifications after inference to be analyzed for manual refinement}]
{
“role_2”: {
“rules”: [
{
“observation”: [
0.6,
0.4,
0.0,
0.0,
...],
“action”: 3,
“weight”: 0.15
},
{
‘observation’: [...],
“action”: [
0.0,
0.0,
-0.2,
-0.2,
...],
“weight”: 0.15
},
{
“observation”: [...],
“action”: 5,
“weight”: 0.15
}
],
“support”: 8
},
“role_1”: {
‘rules’: [
{
“observation”: [
0.6,
0.4,
0.0,
0.0,
...],
“action”: 0,
“weight”: 0.19
},
],
“support”: 8
},
“1”: {
“rules”: [
{
“observation”: [
0.6,
0.4,
0.0,
0.0,
...],
‘action’: 2,
“weight”: 0.15
},
],
“support”: 8
}
}
\end{lstlisting}

\

\noindent
\textbf{Refinement loop}
If performance remains insufficient, the user can decide to restart a series of cycles with the command \texttt{cybmasde refine --max 5 --interactive}. In this mode, the system automatically alternates between \textbf{training} and \textbf{analysis}, re-injecting the manually corrected organizational specifications at each iteration or keeping those inferred by \acn{TEMM}/\ acn{Auto-TEMM}. In Overcooked-AI, this may mean strengthening the role of cook for one of the agents, or specifying several versatile roles to diversify strategies. The cycle repeats until the set threshold is reached or the maximum number of iterations is exceeded.

\

To conclude on this example based on the Overcooked-AI environment, \acn{CybMASDE} shows how, with just a few commands, it is possible to move from an unknown environment such as Overcooked-AI to an \acn{SMA} by reducing manual interventions.


\section{Technology base (development)}\label {sec:cybmasde_tech_stack}

The development of \acn{CybMASDE} is based on a set of technologies chosen to meet the constraints of performance, modularity, and accessibility.
Below, we describe the role of each of these technologies, the reasons for their selection, and their practical use in the pipeline.

\paragraph{Python (backend).}
The core of \acn{CybMASDE} is written in \textbf{Python}, which offers a rich ecosystem of machine learning and scientific orchestration libraries. Python was chosen because:
\begin{itemize}
  \item it allows for easy integration of existing multi-agent learning frameworks (MARLlib, Ray RLlib);
  \item it is suitable for rapid prototyping of complex models (e.g., World Models or custom reward functions);
  \item it is widely used in the research and industrial community, which promotes reproducibility.
\end{itemize}
All modules of the MTA+T pipeline (Modeling, Training, Analyzing, Transferring) are implemented in Python.

\paragraph{PyTorch.}
The choice of \textbf{PyTorch} meets two needs:
\begin{enumerate}
  \item training simulation models (autoencoders, LSTM, VAE) to build \textit{World Models};
  \item serve as a common backend for multi-agent reinforcement algorithms.
\end{enumerate}
Its flexible API allows the implementation of specific network architectures, and its integration with CUDA ensures accelerated training on GPUs.

\paragraph{MARLlib and Ray RLlib.}
For multi-agent training, \acn{CybMASDE} uses \textbf{MARLlib} backed by \textbf{Ray RLlib}.
\begin{itemize}
  \item \textbf{MARLlib} provides a representative collection of reference algorithms (MAPPO, MADDPG, QMix, COMA, etc.) already configured for multi-agent.
  \item \textbf{Ray RLlib} ensures scalability: it allows distributed training to be executed on multiple GPUs or HPC nodes, which is essential for our cyberdefense experiments.
\end{itemize}
This combination was chosen to avoid reimplementing each algorithm while ensuring good performance on clusters.

\paragraph{Optuna.}
Hyperparameter optimization is delegated to \textbf{Optuna}, a flexible library that:
\begin{itemize}
  \item automatically explores the hyperparameter space (learning rate, discount factor, network size, clipping coefficients, etc.);
  \item integrates early stopping strategies (pruning) to save computation time;
  \item allows exploration ranges to be set in the project configuration file.
\end{itemize}
The use of Optuna ensures that each experiment makes efficient use of the available computing resources.

\paragraph{Angular (frontend).}
Although the main use of \acn{CybMASDE} remains the command line, an \textbf{Angular graphical interface} is also available.
Its purpose is to:
\begin{itemize}
  \item provide a unified view of the project configuration (rather than directly modifying the file tree);
  \item facilitate the monitoring of activities through dedicated tabs (modeling, training, analysis, transfer);
  \item allow the injection and visualization of execution traces or metrics without coding.
\end{itemize}
Angular was chosen because it allows for the rapid development of a modular, modern, and easily extensible web interface.

\paragraph{REST API and CLI.}
The pivot between these components is an internal \textbf{REST API}:
\begin{itemize}
  \item all CLI commands (\texttt{cybmasde init}, \texttt{cybmasde train}, etc.) call this API;
  \item the Angular interface also communicates via this API, which ensures consistency between the graphical mode and the console mode.
\end{itemize}
Thus, the CLI and frontend simply expose two different modes of access to the same backend.

\medskip
\noindent
In summary, the technological base combines \textbf{Python/PyTorch} for computation, \textbf{MARLlib + RLlib} for multi-agent learning, \textbf{Optuna} for optimization, and \textbf{Angular + REST + CLI} for accessibility.

\paragraph{Software architecture and classic interactions}

The C4 component deployment diagram (see \autoref{fig:cybmasde_uml}) illustrates the \acn{CybMASDE} modules and their sequence during typical use.

\begin{enumerate}
  \item The \textbf{user} interacts either via the \textbf{CLI} or via the \textbf{Angular} interface.
  \item These two access points call the same \textbf{REST API} from the backend.
  \item The REST API triggers the \textbf{Transferring process}, which:
        \begin{itemize}
          \item communicates with the \textbf{environmental API} of the target environment (real or simulated);
          \item continuously generates and stores histories in the local project database.
        \end{itemize}
  \item As soon as a batch of histories is complete, the \textbf{MTA} process is activated: it successively instantiates the \textbf{Modeling}, \textbf {Training}, and \textbf{Analyzing} modules.
  \item The results (trained policies, metrics, organizational specifications) are stored and made available to the user (via \acn{CLI} or interface).
\end{enumerate}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{figures/CybMASDE_internal_component_diagram.pdf}
\caption{C4 component diagram illustrating the software architecture of CybMASDE}
\label{fig:cybmasde_uml}
\end {figure}

\section{Integration of the different contributions}

\subsection{Implementation of the pre-specialized Dec-POMDP model for Cyberdefense}

In addition to automatic modeling based on \textit{World Models}, we propose a manual modeling approach guided by a pre-specialized model for Cyberdefense environments.
The development of the \acn{Dec-POMDP} formalism applied to this domain has led to a model that we call \acn{MCAS} (\textit{Multi-Cyberdefense Agent Simulator}). This is a specific instance of \acn{Dec-POMDP} adapted to the explicit description of Cyberdefense scenarios.

\paragraph {Why an MCAS model?}
In cyberdefense environments, it is sometimes difficult to collect enough historical data to feed a \textit{World Model}. In addition, some defensive behaviors are better specified by experts. MCAS therefore allows:
\begin{itemize}
  \item to give the user a “turnkey” entry point for modeling a specific environment;
  \item structure the description in accordance with the Dec-POMDP formalism (observations, actions, transitions, rewards, termination);
  \item directly integrate this model into the CybMASDE cycle.
\end{itemize}

\paragraph{How is it provided?}
When a project is created, CybMASDE generates a template file in modeling/simulated_environment/handcrafted_environment.py.

This file is a Python interface with gaps that the user must fill in by defining:
\begin{itemize}
  \item the observation space (e.g., status of a server or network node),
  \item the action space (e.g., block traffic, scan a port, restart a service),
  \item the transition dynamics (how the environment evolves after an action),
  \item the reward function (bonus/penalty depending on successful defense or attack),
  \item the stopping function (episode end conditions).
\end{itemize}

\paragraph{Execution and integration.}
Once completed, MCAS is automatically recognized by CybMASDE:
\begin{itemize}
  \item it can be run in simulation mode to generate multi-agent training trajectories;
  \item it is compatible with “turn-based” mode: the user can view the evolution of the environment in real time, represent the topology in graph form, and consult metrics (cumulative rewards, success rates, etc.);
  \item it can also be serialized in JSON format in order to share or reuse the description of the environment.
\end{itemize}

Thus, MCAS offers a manual and structured alternative to automatic modeling, ensuring that simulation environments remain appropriate and understandable in cyberdefense scenarios while integrating naturally into the \acn{MTA}+T chain of \acn{CybMASDE}.


\subsection{Implementation of the MOISE+MARL framework}

One of the key contributions of \acn{CybMASDE}\footnotemark[1] is to integrate the MOISE+ organizational framework with multi-agent learning methods. To do this, we have developed a Python implementation called \acn {MMA}\footnotemark[2].

\footnotetext[2]{ \label{fn:github} The \textquote{MOISE+MARL API} implementation (\acn{MMA}), the hyperparameters and specifications used are available at \url {https://github.com/julien6/MOISE-MARL}. A demonstration video is also available at \url{https://www.youtube.com/watch?v=b3wqFpfXZi0}.}


\paragraph{Why MMA?}
The MOISE+ framework allows organizations (roles, missions, permissions) to be formalized, but its direct use can be tedious. \acn{MMA} aims to:
\begin{itemize}
  \item encapsulate MOISE+ relationships in the form of object-oriented Python classes,
  \item minimize manual intervention by offering a clear, ready-to-use API,
  \item easily interface with simulated environments (\acn{Dec-POMDP}, PettingZoo).
\end{itemize}

\paragraph{Internal structure.}
The \acn{MMA} API defines:
\begin{itemize}
  \item a root class \texttt{Moise}, containing roles, objectives, and permissions;
  \item specialized classes for each type of constraint guide (e.g., $rag$, $rrg$, $grg$);
  \item a dictionary for mapping action/observation labels to organizational behaviors.
\end{itemize}
These guides can be instantiated either by Python code or by JSON rules.

\paragraph{Integration with PettingZoo.}
To interact with a multi-agent environment, \acn{MMA} encapsulates it in a PettingZoo \textit{wrapper}:
\begin{itemize}
\item action masks are automatically applied to prevent agents from violating organizational constraints,
\item rewards are adjusted according to organizational objectives (shaping),
\item consistency between prescribed organization and emerging behaviors is ensured during training.
\end {itemize}

\paragraph{Link to training.}
\acn{MMA} integrates naturally with \textbf{MARLlib} and \textbf{Ray RLlib}:
\begin{itemize}
  \item the user chooses an algorithm (MAPPO, QMix, MADDPG, etc.);
  \item the \acn{MMA} wrapper applies MOISE+ constraints on top of the chosen algorithm;
  \item the training results therefore take into account not only performance, but also compliance with organizational specifications.
\end{itemize}

\paragraph{Post-training analysis.}
After training, CybMASDE applies the \acn{TEMM} (or \acn{Auto-TEMM}) method to:
\begin{itemize}
  \item automatically identify implicit roles and missions,
  \item represent behaviors using hierarchical clustering and K-means,
  \item generate visual outputs (role dendrograms, transition graphs),
  \item export the inferred specifications in JSON format for reuse.
\end{itemize}

\

In summary, \acn{MCAS} offers specialized manual modeling for cyber defense, while \acn{MMA} provides an operational organizational framework integrated into the learning cycle. Their combination in \acn{CybMASDE} makes it possible to combine organizational rigor with multi-agent training flexibility.


\section{Conclusion}

This chapter presented the implementation of \acn{MAMAD} through the \acn{CybMASDE} platform. The modular approach, structured around modeling, training, analysis, and transfer, has been adapted to various scenarios, from toy environments to real-world cases in cyber defense and microservices.
The integration of MOISE+MARL into the multi-agent learning pipeline has made it possible to obtain policies that are consistent with organizational objectives. \acn{CybMASDE} is available as open-source software\footnotemark [1].

Despite these contributions, limitations remain: dependence on trace quality, high computational requirements, difficulty in generalizing to constrained distributed systems, and explainability that could still be improved. This chapter thus lays the foundations for advanced automation of organizational design in \acn{MARL}, paving the way for future optimizations and applications on real \acplu{SMA}.

\footnotetext[1] { \label{fn:github} The \textquote{CybMASDE} implementation not only brings together our contributions, but also integrates a significant number of packages, resulting in a relatively large software package ($\sim$250,000 lines of code) once the dependencies are installed: \url{https://github.com/julien6/CybMASDE}.}

\clearpage
\thispagestyle{empty}
\null
\newpage


\chapter{Experimental and evaluation framework}
\label{chap:experimental_framework}

The purpose of this chapter is to define a generic experimental framework applicable to all case studies presented in \autoref{chap:case_studies}. The objective is to provide a \textit{framework} that each scenario can instantiate by specifying the chosen elements of the \acn{MAMAD} method. This framework is based on the \acn{MAMAD} method, the taxonomy of activities and sub-activities presented in \autoref{tab:mamad_taxonomy}, and the evaluation criteria defined above.

\section{Description of the sets of environments and algorithms considered}

To ensure the generality and robustness of the evaluation, we consider a varied set of reference environments from the \acn{MARL} literature:
\begin{itemize}
\item \textbf{Overcooked-AI}~\cite{overcookedai}: a complex cooperative toy-type environment requiring coordination and sequential planning.
\item \textbf{Predator-Prey}~\cite{lowe2017multi}: a chase-and-escape toy-type environment used to test coordination and competition.
\item \textbf{Warehouse Management}: a new toy environment that we proposed to simulate multi-agent logistics management with flow and resource constraints.
\item \textbf{Company Infrastructure}~\cite{cyberbattlesim}: a simulation of attacks and defenses on a network, inspired by MITRE ATT&CK.
\item \textbf{Drone Swarm}~\cite{cage_challenge_3_announcement}: a simulation of a swarm of drones subjected to software attacks and requiring collective defense.
\item \textbf{Kubernetes Microservices}: a real environment consisting of a cluster of four interconnected microservices that we proposed for the orchestration of microservices with auto-scaling and resilience against intentional failures.
\end {itemize}

The \textbf{Kubernetes Microservices} environment differs from the others in that it is real: it is a system of four interconnected services, whereas the other environments are simulated. For practical reasons, the simulated environment serves as a “real” reference, allowing \acn{MAMAD} to be applied as if on an operational system, with the \textit {World Model} acting here as a simulation of the simulation. This choice validates the principle of \acn{MAMAD}: if the \textit{World Model} reconstructed from traces is sufficiently accurate, the learned policies can be transferred effectively, thus facilitating comparison between simulation and reality. In this scenario, the \textbf{Kubernetes Microservices} environment can be used directly with \acn{MAMAD} without requiring additional modeling, and the transfer (\textbf{TRF}) is performed to this real system, demonstrating the validity of the approach in an operational context.

\medskip

The selected \acn{MARL} algorithms cover the main families recognized in the literature:
\begin{itemize}
  \item \acn{MAPPO}~\cite{Yu2022}\index{MAPPO}~: algorithm based on \acn{PPO}~\cite{Schulman2017} adapted to multi-agent systems, using centralized policies to stabilize learning while allowing decentralized execution.
  \item \acn{MADDPG}~\cite{lowe2017multi}\index{MADDPG}~: deterministic gradient learning method, combining individual policies with centralized criticism to handle multi-agent non-stationarity.
  \item \acn{QMIX}~\cite{rashid2018qmix}\index{QMIX}~: value factorization algorithm, combining individual Q-values of agents via a nonlinear mixing network to optimize a global reward.
  \item \acn{COMA}~\cite{foerster2018counterfactual}\index{COMA}~: actor-critic-based approach, using counterfactual estimation to accurately attribute each agent's contribution to the collective reward.
  \item \acn{IQL}~\cite{Jiang2022}\index{IQL}~: each agent learns its own Q function independently, without explicit coordination, which can lead to non-stationarity, but remains simple to implement.
  \item \acn{VDN}~\cite{sunehag2018value}\index {VDN}~: decomposes the overall value into a sum of the individual values of the agents, facilitating cooperative learning while maintaining a simple structure.
\end{itemize}
The environments are implemented via \textit{PettingZoo}~\cite{terry2020pettingzoo} and the algorithms via \textit{MARLlib}~\cite {hu2022marllib}.

\section{Reproducibility conditions}

\subsection{Experimental hardware conditions}
\label{par:compute_conditions}
The experiments are performed on an academic HPC cluster. Unless otherwise stated, the following constants apply to all scenarios:
\begin{itemize}
  \item Accelerators: NVIDIA A100 / V100, AMD MI210.
  \item \textbf{DL frameworks}: PyTorch~\cite{Paszke2019} and TensorFlow~\cite{Abadi2016} (implementations \acn{MARLlib}/\acn{MAPPO}, etc.)\index{PyTorch}.
  \item \textbf{Hyperparameter optimization}~: \textbf{Optuna}~\cite{akiba2019optuna} (\acn{TPE}) for \acn{LR}, exploration/exploitation, network sizes; standardized search space by algorithm family.
  \item \textbf{Parallelism}~: $\sim$ 5 independent executions per condition (algorithm $\times$ environment $\times$ constraint).
  \item \textbf{\acn{OS} and libs}~: Linux 64-bit, \acn{CUDA}/cuDNN or ROCm depending on \acn{GPU}; fixed environments (conda/pip).
\end{itemize}
The case studies only mention specific deviations (e.g., number of runs, particular GPU).

\subsection{Hyperparameter management (default and overrides)}
Each \{algorithm, environment\} pair is initialized with a standard profile from MARLlib and preliminary experiments.
A controlled \acn{HPO} pass can be performed with \textbf{Optuna} (bounded budget, same search space priorities between scenarios)~\cite{akiba2019optuna}. The best \textit{trial} is then \textbf{replayed 5 times} to aggregate the results.
Scenarios can:
\begin{enumerate}[label=\alph*)]
  \item accept default values,
  \item restrict \acn{HPO},
  \item explicitly override certain hyperparameters.
\end{enumerate}

\section{Experimental baselines}

An experimental baseline is a set of data used to reproduce the experiment. In our case, a baseline includes the following elements~:
\begin{itemize}
  \item \textbf{Activities}: A selection of \acn{MAMAD} activities.
  \item \textbf {Algorithm}: A \acn{MARL} algorithm or another algorithm from the literature that may enable agents to achieve their objectives using a different approach.
  \item \textbf{Organizational specifications}: A set of MOISE+MARL-type organizational specifications if the chosen algorithm belongs to the \acn{MARL} domain.
  \item \textbf{Environment}: An environment with scenario variants (e.g., Overcooked-AI~\cite{overcookedai}, Predator-Prey~\cite{lowe2017multi}, Drone Swarm~\cite{cage_challenge_3_announcement}).
  \item \textbf{Specific metrics}: A set of metrics specific to the environment or study (environment-specific).
  \item \textbf{Execution conditions}: A set of execution conditions (number of runs, seeds, hardware, possible disturbances).
\end{itemize}

In the context of experiments, certain characteristics of baselines (such as the environment or specific metrics) remain fixed. To instantiate a baseline, we therefore focus on variable elements such as the algorithm used, execution conditions, or organizational specifications. The main objective of defining baselines is to have points of comparison to measure the impact of the different components of the \acn{MAMAD} method method. By comparing the results obtained with these baselines, we can isolate and analyze the contribution of each activity (modeling, training, analysis, transfer) as well as the effect of organizational specifications on the behavior and performance of the \acn{SMA}.



\section{Evaluation grid}\label{sec:evaluation_grid}

\subsection{Criteria and associated metrics}\label{sec:criteria_metrics}

The evaluation is based on a grid of criteria (see \autoref{tab:grille}) inspired by the recommendations of the \acn{RL}/\acn{MARL} community~\cite{papoudakis2021agent}:

\begin{table}[h!]
  \centering
  \caption{Correspondence between overall criteria and metrics}
  \renewcommand{\arraystretch}{1.2}
  \begin{tabular}{ll}
    \hline
    \textbf{Criterion}   & \textbf{Associated metrics}                                    \\
    \hline
    C1 -- Autonomy       & Proportion of intervention (design/operation)                  \\
    C2 -- Performance    & Cumulative reward; Convergence rate                            \\
    C3 -- Adaptation     & Standard deviation of rewards; Robustness score                \\
    C4 -- Control        & Constraint violation rate; Consistency score                   \\
    C5 -- Explainability & Organizational suitability; Quality of inferred specifications \\
    \hline
  \end{tabular}
  \label{tab:grid}
\end{table}

These different metrics aim to determine the extent to which the five overall criteria (C1--C5) are covered on a given baseline.
Each metric is described separately for clarity.

\paragraph{Proportion of intervention (design/operation).}\index{Proportion of intervention}
\textit{Unit: percentage (\%). Source: platform usage logs, user questionnaires, automation scripts, number of refinement cycles, manual estimates.}
It is calculated as the following ratio:
\[
  \hspace{2.5cm}\frac{\text{estimated time for automated design}}{\text{estimated time for manual design}}
\]
This proportion, which is less precise, aims to quantify at least approximately the impact of automation on the design process: what percentage of manual interventions is necessary to achieve performance similar to manually designed and implemented \acplu{SMA}. It can be estimated via software instrumentation by indicating that a large number of refinement cycles increases the proportion of intervention (i.e., the ratio of the number of cycles used to the number of empirical cycles of manual design). Considering the already modeled environment, this proportion is also obtained manually by estimating the time spent (in number of hours) during completely manual design, which in our method corresponds to the estimated time spent (in number of hours) in the refinement cycles alternating between training and analysis.

\paragraph{Cumulative reward.}\index{Cumulative reward}
\textit{Unit: numerical value without unit (often normalized). Source: \acn{MARLlib}/RLlib training logs, episode result files.}
This is the sum of the rewards obtained by all agents over an episode or a sliding window.
It is extracted automatically via analysis scripts.

\paragraph{Convergence rate.}\index{Convergence rate}
\textit{Unit: number of episodes (integer). Source: learning curves, training logs. }
This corresponds to the number of episodes required for the average reward to exceed a predefined threshold and remain stable.
The calculation is automated by detecting a plateau on the learning curve.

\paragraph{Standard deviation of rewards.}\index{Standard deviation of rewards}
\textit{Unit: same as reward (often without unit). Source: training logs, results of multiple runs.}
This metric corresponds to the statistical standard deviation of average rewards between several independent runs.
It is calculated automatically when the results are aggregated.

\paragraph{Robustness score.}\index{Robustness score}
\textit{Unit: ratio or percentage (\%). Source: tests under disturbances (failures, attacks), episode logs.}
It is defined as:
\[
  \hspace{3.5cm}\frac{\text{performance under disturbance}}{\text{nominal performance}}
\]
This score is obtained by comparing average performance in disturbed scenarios and reference scenarios.
Disturbances are generated either by different seeds in simulated environments or by explicit changes (topology, failures, attacks, etc.).

\paragraph{Constraint violation rate.}\index{Constraint violation rate}
\textit{Unit: percentage (\%). Source: execution logs, environment wrappers, post-hoc analysis.}
The formula is:
\[
  \hspace{3.5cm}\frac{\text{number of violations detected}}{\text{total number of steps per episode}}.
\]
This measure evaluates the ability of agents to comply with the rules imposed by their roles.
It varies with the strictness of the constraints: a zero rate is expected when the strictness is maximum, and a high rate when the constraints are zero.
Intermediate values must be analyzed in relation to the cumulative reward in order to identify any overly restrictive rules that would reduce overall performance.

\paragraph{Consistency score.}\index{Consistency score}
\textit{Unit: ratio (0–1) or percentage (\%). Source: trajectory clustering, \acn{TEMM}/\acn{Auto-TEMM} analysis.}
This score measures the similarity between observed behaviors and expected roles.
The idea is to compare the original organizational specifications with those inferred automatically.
The smaller the distance between the two sets of trajectories, the higher the consistency score, indicating that MOISE+MARL has taken the organizational specifications into account and that \acn{TEMM}/\ acn{Auto-TEMM}.

\paragraph{Organizational adequacy.}\index{Organizational adequacy}
\textit{Unit: ratio (0–1). Source: \acn{TEMM} analysis, role-mission correspondence matrices.}
As explained previously in \autoref{sec:TEMM_algorithm}, it is calculated as a weighted average of the structural (\acn{SOF}) and functional (\acn{FOF}) scores with $\alpha = 0.5$ by default:
\[
  \hspace{3.5cm}\text{OF} = \alpha \cdot \text{SOF} + (1-\alpha) \cdot \text{FOF}.
\]
This measure quantifies the extent to which the learned policies comply with the expected organizational structure.

\paragraph{Quality of inferred specifications.}\index{Quality of inferred specifications}
\textit{Unit: percentage (\%) or similarity score. Source: automatic comparison between inferred specifications (\acn{JSON}, \acn{TEMM}) and reference specifications} .
This metric measures the quality of inferred organizational specifications by comparing their similarity to reference specifications, using indicators such as the Jaccard index~\cite{Jaccard1908} (proportion of shared elements) or the Euclidean distance. Unlike the consistency score, it focuses solely on the fidelity of the extracted specifications, regardless of the learned policies. The calculation is based on a systematic comparison between the inferred specifications and the initial specifications, varying the scenarios to evaluate the generalization capacity. The objective is to find a compromise between fidelity to the initial specifications and generalization capacity, avoiding overfitting and limiting the complexity of the inferred rules or objectives, while maximizing similarity with the reference trajectories.

\section{Experimentation and evaluation protocol}\label{sec:experimental_protocol}

The proposed experimental protocol follows a progressive reasoning similar to the practices of the community~\cite{papoudakis2021agent}.
Steps 1 to 4 constitute the experimental protocol, while steps 5 and 6 concern the evaluation of the results:

\paragraph{1. Initial configuration}
Required configuration including the implementation of the \acn{API} \acn{REST} for communication with the environment and the various components required to model the environment and other data necessary to create a \acn{CybMASDE} project for a given baseline.

\paragraph{2. Implementation of the advanced baseline}
Definition of at least one \textbf {advanced baseline} considered as the \textbf{default baseline}:
\begin{table}[h!]
  \centering
  \caption{Generic characterization of the \textquote{advanced baseline}}
  \label{tab:baseline_generic}
  \renewcommand{\arraystretch}{1.4}
  \footnotesize
  \begin{tabularx}{\textwidth}{
      >{\raggedright\arraybackslash\hsize=0.3\hsize}X
      >{\raggedright\arraybackslash\hsize=0.7\hsize}X
    }
    \hline
    \textbf{Element}              & \textbf{Instantiated value}                                                                                                \\
    \hline
    Environment                   & Name of the environment and possible scenario variants (e.g., Overcooked-AI, classic layout).                              \\
    Activities \acn{MAMAD}        & \acn{MOD-AUT},\acn{TRN-CON},\acn{ANL-AUT},\acn{TRF-AUT}                                                                    \\
    Algorithm                     & Selected algorithm with its implementation (e.g., \acn{MAPPO} via \acn{MARLlib}).                                          \\
    Organizational specifications & MOISE+MARL set containing both roles and missions.                                                                         \\
    Specific metrics              & Additional indicators specific to the environment (e.g., dishes served, collision rate, infected drones, average latency). \\
    Execution conditions          & Number of runs, seeds, type of hardware (\acn{CPU}/\acn{GPU}), disruptions/stress tests.                                   \\
    \hline
  \end{tabularx}
\end{table}

\paragraph{3. Implementation of alternative baselines}
Possible definition of other baselines based on other parameters such as \acn{MARL} algorithms, organizational constraints, or modeling according to the specific issues of each scenario. Baseline

\paragraph{4. Definition of ablation studies}
Conducting at least one \textbf{ablation study}: each ablation consists of taking the \textbf{advanced baseline} and changing one or more parameters, in particular replacing one or more \acn{MAMAD} activities with less advanced variants (for example, \textquote{\acn{ANL-MAN}} using \acn{TEMM} rather than \acn{Auto-TEMM}, or \textquote{\acn{TRN-UNC}} without organizational constraints), but also removing organizational constraints, changing algorithms, etc. Each ablation must be justified according to the specific questions of each scenario.

\paragraph{5. Comparison and validation}
Systematic comparison of results on the evaluation grid (\autoref{tab:grille}) to measure the impact of each component on the target criteria. The analysis of the results must include visualizations (learning curves, heatmaps, dendrograms, etc.) and a critical discussion of the compromises observed (e.g., performance vs. compliance with constraints).

\paragraph{6. Repetition and aggregation}
Repetition of experiments (5 independent runs, recorded seeds), statistical aggregation (mean, standard deviation), and statistical tests (t-test or non-parametric depending on normality).


\section {Summary}

This chapter laid the methodological foundations for the experimental evaluation of the \acn{MAMAD} method. By defining a generic, reproducible, and structured framework, it allows for the comparison of performance, robustness, and explainability of the method in various environments, ranging from toy cases to real systems. The major advantage of this framework is that it ensures the traceability of choices, the transparency of protocols, and the validity of results, while facilitating critical analysis of the contributions and limitations of each component of the method. This methodological framework is essential for ensuring the scientific credibility of experiments and identifying levers for future improvement. The following chapters draw on this framework to instantiate, analyze, and discuss the application of \acn{MAMAD} in concrete contexts, highlighting its ability to meet the criteria of autonomy, performance, adaptability, control, and explainability.


\clearpage
\thispagestyle{empty}
\null
\newpage


\chapter{Case Studies}
\label{chap:case_studies}

This chapter presents case studies conducted to evaluate the \acn{MAMAD} method in various contexts. Each section details the instantiation of the experimental framework defined in \autoref {chap:experimental_framework} for a specific environment, specifying the methodological choices and experimental configurations.

\section{Experiments on non-Cyberdefense-oriented environments}

In this section, we describe the experiments that were conducted on a set of non-Cyberdefense-oriented reference environments. The main objective is to validate the \acn{MAMAD} method in various contexts, with an emphasis on coordination, competition, and resource management in partially observed environments. The goal is to provide proof of concept that the \acn{MAMAD} method is applicable to simple cases. From there, it is possible to effectively address its application to scenarios strictly related to cyberdefense.

This section proposes the same instance of the \autoref{sec:protocole_experimental} experimentation protocol (steps 1 to 4) for three non-cyber defense-oriented environments (\textit{Overcooked-AI}, \textit{Predator-Prey}, \textit{Warehouse Management}).
The objective is to describe the experiments to be conducted to apply the evaluation protocol of \autoref{sec:experimental_protocol} (steps 5 to 6) in the following chapter in order to validate the \acn{MAMAD} method in various cooperative/competitive contexts, with partial observation and coordination requirements.


\subsection{Description of environments}

\subsubsection*{Overcooked-AI}

The \textbf{Overcooked-AI}~\cite{Carroll2019} environment simulates a cooperative cooking scenario where agents must collaborate to prepare and serve meals in a structured kitchen. This environment is illustrated in \autoref{fig:overcooked}\index{Overcooked-AI}.

% \begin{enumerate*}[label={\roman*)}, itemjoin={; \quad}]
\begin{itemize}
  \item \textbf{State space:} A discrete grid-based kitchen with workstations (cutting board, stove, serving counter), ingredients, and agents
  \item \textbf{Observation space:} Agents observe kitchen elements within a defined radius
  \item \textbf{Action space:}
        \begin{enumerate*}[label={\roman*)}, itemjoin={; \quad}]
          \item Movement: \textquote{Up, Down, Left, Right}
          \item Interact: \textquote{Choose an ingredient, cut, cook, serve}.
        \end{enumerate*}
  \item \textbf{Reward structure:}
        \begin{enumerate*}[label={\roman*)}, itemjoin={; \quad}]
          \item Successful meal preparation: $+20$
          \item Incorrect use of ingredients: $-5$
          \item Passive behavior: $-1$ per step without significant action.
        \end{enumerate*}
  \item \textbf{Objective:} Maximize the number of meal orders completed within a set time limit.
\end{itemize}
% \end{enumerate*}
%
\textbf{Organizational specifications:}
% \begin{enumerate*}[label={\roman*)}, itemjoin={; \quad}]
\begin{itemize}
  \item \textbf{Roles:} \textquote{Chef, assistant, server}
  \item \textbf{Tasks:} The chef prepares the dishes, the assistant provides the ingredients, and the server serves the meals
  \item \textbf{Constraints:} Tasks must be synchronized to avoid bottlenecks.
\end{itemize}
% \end{enumerate*}

\begin{figure}[h!]
  \centering
  \includegraphics[trim=0cm -0.5cm 0cm -0.5cm, clip, width=0.9\linewidth]{figures/overcooked.png}
  \caption[Screenshot of the Overcooked-AI environment]{Screenshot of the Overcooked-AI environment: two agents (chefs) must collaborate to efficiently prepare and serve onion soups. The process involves taking three onions (one at a time) from the dispenser, placing them in a pot, waiting for the soup to cook, fetching a clean bowl, ladling the soup, and delivering it to the serving counter. The layout of the kitchen includes obstacles and narrow passages, requiring the agents to coordinate their movements to avoid collisions and optimize task completion.}
  \label{fig:overcooked}
\end{figure}

\subsubsection*{Predator-Prey}

The \textbf{Predator-Prey} environment is a well-known \acn{MARL} benchmark~\cite{lowe2017multi}, designed to evaluate coordination between cooperative pursuers (predators) attempting to capture an elusive agent (prey) . This environment is illustrated in \autoref{fig:predator_prey}\index{Predator-Prey}.
%
% \begin{enumerate*}[label={\roman*)}, itemjoin={; \quad}]
\begin{itemize}
  \item \textbf{State space:} A continuous 2D space where agents (predators and prey) have positions $(x, y)$ and velocities
  \item \textbf{Observation space:} Agents detect nearby entities within a limited radius $r$
  \item \textbf{Action space:}
        \begin{enumerate*}[label={\roman*)}, itemjoin={; \quad}]
          \item Movement: \textquote{Up, Down, Left, Right, Stay in place}.
        \end{enumerate*}
  \item \textbf{Reward structure:}
        \begin{enumerate*}[label={\roman*)}, itemjoin={; \quad}]
          \item Predators earn $+50$ for each prey captured
          \item Prey earns $+1$ per time step survived;.
        \end{enumerate*}
  \item \textbf{Objective:} Predators must cooperate to trap the prey, while the prey tries to escape for as long as possible.
        % \end{enumerate*}
\end{itemize}
%
\textbf{Organizational specifications:}
% \begin{enumerate*}[label={\roman*)}, itemjoin={; \quad}]
\begin{itemize}
  \item \textbf{Roles:} \textquote{Predator, Prey}
  \item \textbf{Missions:} Predators coordinate their efforts to surround the prey; the prey seeks the best escape routes
  \item \textbf{Constraints:} Predators must find a balance between aggressive pursuit and blocking strategies.
        % \end{enumerate*}
\end{itemize}
%
\begin{figure}[h!]
  \centering
  \includegraphics[trim=0cm 4.5cm 0cm 1cm, clip,width=0.9\linewidth]{figures/predator_prey.png}
  \caption[Screenshot of the Predator-Prey environment]{Screenshot of the Predator-Prey environment: \textbf{green agents} (cooperative) and \textbf{red agents} (adversaries). The green agents aim to collect food scattered throughout the environment while avoiding detection by the red agents. The environment includes \textbf{forest areas} that provide shelter; when a green agent enters a forest, it becomes partially or completely invisible to red agents. A red agent acts as a \textbf{leader} with enhanced observation abilities and can communicate with other red agents to coordinate their pursuit.}
  \label{fig:predator_prey}
\end{figure}

\subsubsection*{Warehouse Management}

The \textbf{Warehouse Management}~\cite{warehouse_management} environment models a grid-based logistics warehouse where multiple robots must collaborate to efficiently transport goods. This environment is inspired by industrial warehouse automation scenarios and provides an ideal test bed for evaluating task distribution, role specialization, and real-time coordination. This environment is illustrated in \autoref{fig:warehouse}\index{Warehouse Management}\index{Warehouse Management}.
%
% \begin{enumerate*}[label={\roman*)}, itemjoin={; \quad}]
\begin{itemize}
  \item \textbf{State space:} An $N \times M$ grid where each cell contains a robot, a product, a manufacturing machine, or a storage location. The system tracks agent positions, stock levels, and machine states
  \item \textbf{Observation space:} Each agent has a local view $V \times V$, allowing it to perceive nearby products, teammates, and machines
  \item \textbf{Action space:}
        \begin{enumerate*}[label={\roman*)}, itemjoin={; \quad}]
          \item Movement: \textquote{Up, Down, Left, Right}
          \item Interaction: \textquote{Pick up a product, drop off a product}.
        \end{enumerate*}
  \item \textbf{Reward structure:}
        \begin{enumerate*}[label={\roman*)}, itemjoin={; \quad}]
          \item Successful product delivery: $+10$
          \item Inefficient movement: $-1$ per unnecessary step
          \item Improper product handling: $-5$ for incorrect deliveries.
        \end{enumerate*}
  \item \textbf{Objective:} Transport raw materials to processing machines and deliver finished products to delivery locations.
        % \end{enumerate*}
\end{itemize}
%
\textbf{Organizational specifications:}
% \begin{enumerate*}[label={\roman*)}, itemjoin={; \quad}]
\begin{itemize}
  \item \textbf{Roles:} \textquote{Transporter, inventory manager}
  \item \textbf{Tasks:} Carriers transport products, while inventory managers oversee inventory levels.
  \item \textbf{Constraints:} Carriers must prioritize essential deliveries.
\end{itemize}
% \end{enumerate*}

\begin{figure}[h!]
\centering
\includegraphics[trim=0cm 3cm 0cm 3cm, clip, width=0.9\linewidth]{figures/wm.png}
\caption[Screenshot of the Warehouse Management environment]{Screenshot of the Warehouse Management environment: agents can move up, down, left, and right. Multiple agents operate within a warehouse grid, performing tasks to process and deliver products. Agents can move in four directions (up, down, left, right) and interact with pick/drop zones when adjacent. The workflow includes: (i) collecting primary products from the pick/drop zones of the input conveyor (blue zones); (ii) transporting them to the pick/drop zones of the manufacturing machines (brown zones), where the primary products are transformed into a single secondary product according to a predefined manufacturing scheme; (iii) retrieval of the secondary products obtained and their delivery to the pick/place areas of the output conveyor (pink areas). For the operation to be successful, agents must coordinate their movements and actions to optimize throughput and efficiency within the warehouse.}
\label{fig:warehouse}
\end {figure}


\subsection{Description of the common instance of the experimentation protocol}

\paragraph{1. Initial configuration}

We use the \emph{PettingZoo} implementations of \phantom{X} \textbf{Overcooked-AI}, \textbf{Predator-Prey}, and \textbf{Warehouse Management} as \emph{real environments} in the sense of \textquote{CybMASDE}. Specifically, for a given environment, an instance of the game is launched in the background and exposed via a \acn{REST} adapter compliant with the \acn{API} of \textquote{CybMASDE} (attached observations, action masks, \emph{step}/\emph{reset}) . This gateway enables (i) the collection of traces for modeling (\textquote{\acn{MOD-AUT}); (ii) \acn{MARL} training with organizational constraints; (iii) organizational analysis (\textbf{ANL}); (iv) transfer (\textbf{TRF}) to the standardized simulated executor.
For \textquote{\acn{MOD-AUT}}, a \emph{Joint-Observation Prediction Model} (\acn{JOPM}, \acn{VAE}+\acn{LSTM}) is trained on the collected histories (random policies and preliminary policies) in order to learn the dynamics $\langle o_{1:t},a_ {1:t} \rangle \mapsto o_{t+1}$ and a derived stop function. The reward function reconstructed from the traces is validated by cross-checking with the native reward of the environment. The observation/action label mapping ($l_o, l_a$) is defined to synchronize \textquote{PettingZoo} and \acn{MMA}, activate action masking and bonus/penalty injection by role. Training is based on \textquote{MARLlib}/\ textquote{RLlib} (default \textquote{MAPPO} profile, \textquote{Optuna} enabled), with fixed \textit{seeds} and 5 independent runs in accordance with \autoref{chap:experimental_framework}. The hardware resources are those presented \autoref{par:compute_conditions}; only the exceptions (episode length, log frequency) are specified in the results.

\paragraph{2 to 4. Definition of baselines}

For the three environments, we define a \textbf{default advanced baseline} and \textbf{alternative baselines} (ablations) that vary the \acn{MAMAD} activities, the \acn{MARL} algorithm, the mode of constraint integration, and their hardness, in order to isolate the contribution of each component (organizational guidance, modeling, analysis). We do not take into account specific metrics such as \emph{dishes served}/episode, collisions, inactive steps, time to performance plateau for Overcooked-AI, for example. \autoref{tab:non_cyberdefense_baselines} summarizes the experimental baselines planned for all three environments.


\begin{table*}[h!]
  \centering
  \caption{Synthetic baselines for non-cyberdefense-oriented environments.}
  \label{tab:non_cyberdefense_baselines}
  \renewcommand{\arraystretch}{1}
  \tiny
  \begin{tabularx}{\textwidth}{
      >{\raggedright\arraybackslash\hsize=0.3\hsize}X
      >{\raggedright\arraybackslash\hsize=0.15\hsize}X
      >{\raggedright\arraybackslash\hsize=0.15\hsize}X
      >{\raggedright\arraybackslash\hsize=0.3\hsize}X
    }
    \toprule
    \textbf{MAMAD activity profile} & \textbf{MARL algorithms} & \textbf{Organizational constraints (rigidity)} & \textbf{Comments}                                                                             \\
    \midrule
    % --- Profile A (Default) ---
    \multirow{3}{*}{\parbox{3.8cm}{\textbf{Profile A -- Default}                                                                                                                                                \\\acn{MOD-AUT};\;\acn{TRN-CON};\ ;\acn{ANL-AUT};\;\acn{TRF-AUT}}}
                                    & \acn{MAPPO}              & Yes (1.0)                                      & Action masking + role-based shaping (\acn{MMA}); \acn{JOPM} enabled for \acn{MOD-AUT}.        \\
                                    & \;\acn{MADDPG}           & Soft (0.5)                                     & Mitigated constraints: bonuses/penalties and partial masks; same pipeline as default.         \\
                                    & \acn{QMIX}               & None (0.0)                                     & \textit{Ablation}: \acn{TRN-UNC}, native environment reward, remains unchanged.               \\
    \hdashline
    % --- Profile B (Manual Analysis) ---
    \multirow{3}{*}{\parbox{3.8cm}{\textbf{Profile B -- Manual Analysis}                                                                                                                                        \\\acn{MOD-AUT};\;\acn{TRN-CON};\;\acn{ANL-MAN};\;\acn{TRF-AUT}}}
                                    & \acn{MAPPO}              & Yes (1.0)                                      & Manual configuration of \acn{TEMM}; rules/masks edited by hand.                               \\
                                    & \acn{COMA}               & Soft (0.5)                                     & Flexible guidance: reduced penalties; post-hoc verification by manually adjusted \acn {TEMM}. \\
                                    &                          & None (0.0)                                     & \acn{TRN-UNC}; \acn{TEMM} analysis only for explainability/diagnosis, without reinjection.    \\
    \hdashline
    % --- Profile C (Mainly manual cycle) ---
    \multirow{3}{*}{\parbox{3.8cm}{\textbf{Profile C -- Mainly manual cycle}                                                                                                                                    \\\acn{MOD-MAN}; \acn{TRN-CON}; \acn{ANL-MAN}; \acn{TRF-MAN}}}
                                    & \acn{IQL}                & Yes (1.0)                                      & \textquote{Handcrafted} environment; fixed hyperparameters; manual transfer and deployment.   \\
                                    & \acn{VDN}                & Soft (0.5)                                     & Soft constraints defined manually (roles/missions + softened scales).                         \\
                                    & \acn{MADDPG}             & None (0.0)                                     & \acn{TRN-UNC} entirely manual; zero organizational constraint hardness.                       \\
    \bottomrule
  \end{tabularx}
\end{table*}



\section{Experiments on the Company Infrastructure environment}
\textbf{Company Infrastructure}~\cite{cyberbattlesim}: a simulation of attacks and defenses on a network.

The \textbf{Company Infrastructure} environment is a simulation inspired by the CyberbatlleSim simulator~\cite{cyberbattlesim} and MITRE ATT\&CK~\cite{MITREATTACKWebsite}. This environment was developed with the \acn{Dec-POMDP} pre-specialized for cyber defense and with \acn {MCAS} integrated into \acn{CybMASDE}. It simulates a corporate network divided into subnetworks (EXT, DMZ, ACC, MAR, SRV) where \emph{cyber attack agents} and \emph{cyber defense agents} interact via pre/post-condition actions, according to a \emph{Dec-POMDP} formalism. The synthetic topology is illustrated in \autoref{fig:scenario_network_topology}, and the attack/defense paths are structured via an Attack–Defense tree (\autoref{fig:ADTree})\index {Company Infrastructure}.

% \begin{enumerate*}[label={\roman*)}, itemjoin={; \quad}]
\begin{itemize}
  \item \textbf{State space:} A set of discrete properties describing the state of network nodes (files, active services, versions, firewall rules, sessions, logs, agent knowledge). The topology includes:
        \begin{enumerate*}[label={\alph*)}, itemjoin={; \ }]
          \item \textbf{EXT}: two attacker workstations \textquote{At1, At2}
          \item \textbf{DMZ}: \textquote{\acn{WS}, \acn{ES}, \acn{VPN}, FTP}
          \item \textbf{ACC}: \textquote{E1, E2, CTO}
          \item \textbf{MAR}: \textquote{\acn{PS}, E3, \allowbreak \acn{TAB}}
          \item \textbf{SRV}: \textquote{\acn{API}, \acn{DB}, DC}.
        \end{enumerate*}
        Transitions modify all properties (addition/deletion/update) when the preconditions for action are met
  \item \textbf{Observation space:} Partial observations specific to each agent (relation $Obs$) such as file/log contents, command results, port scans, session states, detection alerts. Observations are returned after action is applied and depend on local visibility (sensors, privileges)
  \item \textbf{Action space:} Actions with \emph{preconditions} (conjunctions/disjunctions of properties) and \emph{postconditions} (writes to state), divided into broad families~:
        \begin{enumerate*}[label={\alph*)}, itemjoin={; \ }]
          \item \textbf{Attackers}: \textquote{Network/account reconnaissance}, \textquote{Exploiting service vulnerabilities}, \textquote{Privilege escalation}, \textquote{Lateral movement }, \textquote{Persistence (e.g., backdoor)}, \textquote{Data exfiltration (\acn{DB})}, \textquote{Spyware installation (\acn{PS})}
          \item \textbf{Defenders}: \textquote{Log detection (\acn{WS})}, \textquote{Privileged account management (\acn{PAM})}, \textquote{Command/argument monitoring (\acn{DB})}, \textquote{Traffic blocking/FW rules}, \textquote{Malicious session deletion}, \textquote{Service restoration}.
        \end{enumerate*}
  \item \textbf{Reward structure:} Function $R = Eval \circ Metrics$ evaluating status & action using metrics (attack progress toward objectives, detections, deleted sessions, service integrity, etc.):
        \begin{enumerate*}[label={\alph*)}, itemjoin={; \ }]
          \item \textbf{Attackers}: progression bonus along the AD-tree ($+r_{\small step}$), ultimate objectives: \textquote{DB exfiltration} and \textquote{PS spyware} ($+R_{\small goal}$), penalties if detected/neutralized ($-\lambda_{\small det}$)
          \item \textbf{Defenders} : detection/prevention bonus ($+r_{\small det}$), session deletion ($+r_{\small purge}$), availability/integrity maintenance ($+r_{\small avail}$), penalties if attacker objectives achieved ($-\Lambda_{\small goal}$).
        \end{enumerate*}
  \item \textbf{Objective:} For \textbf{Attackers}, achieve \emph{DB Exfiltration} and \emph{Spyware PS} while minimizing detection; for \textbf{Defenders}, prevent/detect/neutralize these attack paths while maintaining critical services.
        % \end{enumerate*}
\end{itemize}

\medskip
\textbf{Organizational Specifications:} \emph{(Baseline)} no organizational constraints are imposed (\textquote{\acn{TRN-UNC}}), in order to provide a \emph{raw} reference for guided cyberdefense-oriented scenarios. \emph{(Optional variant, for ablative analyses)}:
% \begin{enumerate*}[label={\roman*)}, itemjoin= {; \quad}]
\begin{itemize}
  \item \textbf{Roles (ex.)}: \textquote{Attacker\_LateralMove}, \textquote{Attacker\_ExfilDB}, \textquote{Defender\_WS\_Monitor}, \textquote{Defender\_DB\_PAM}
  \item \textbf{Missions}: chain MITRE techniques by sub-objective (recon \textrightarrow{} exploitation \textrightarrow{} elevation \textrightarrow{} lateral movement \textrightarrow{} action on the objective) red side; detection \textrightarrow{} containment \textrightarrow{} eradication \textrightarrow{} recovery blue side
  \item \textbf{Constraints}: role-based permissions (action masking), minimum mission sequencing, conditional containment triggers (logs/\acn{IOC}).
\end{itemize}
% \end{enumerate*}

\begin{figure}[h!]
\centering
\includegraphics[width=\linewidth]{figures/topology.pdf}
\caption{Synthetic network topology: EXT, DMZ, ACC, MAR, SRV. Subnets are interconnected via implicit routers/firewalls.}
\label{fig:scenario_network_topology}
\end {figure}

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.86\linewidth]{figures/ADTree.pdf}
  \caption{Overview of the Attack–Defense (AD) tree structuring attack paths (MITRE tactics/techniques) and associated countermeasures. }
  \label{fig:ADTree}
\end{figure}


\subsection{Description of the experimental protocol instance}

\paragraph{1. Initial configuration}

We use the pre-specialized \emph{Dec-POMDP} model \textbf{Cyberdefense} and the \textbf{MCAS} simulator integrated into \textquote{CybMASDE}, exposed via a \acn{REST} adapter compliant with the \acn{API} of I/O \textquote{CybMASDE} (attached observations, action masks, \emph{step}/\emph{reset}). The \emph{Company Infrastructure} environment is therefore treated, on the pipeline side, exactly like non-Cyberdefense-oriented toy environments: (i) trace collection for automatic modeling (\textquote{\acn{MOD-AUT}} ); (ii) \acn{MARL} training with or without organizational constraints; (iii) organizational analysis (\textbf{ANL}); (iv) automatic transfer (\textquote{\acn{TRF-AUT}}) to the simulated executor.
\textbf{Justification:} no \textquote{\acn{TRF-MAN}} is required here, as the environment is not an instrumentation of a real system, but a natively compatible simulator (no \textquote{field gateway} to maintain).
For \textquote{\acn{MOD-AUT}}, a \acn{JOPM} (\acn{VAE}+\acn{LSTM}) is trained on histories combining random trajectories and preliminary \emph{red/blue} policies (progression in the \acn{AD}-tree, detections, quarantines). The \acn{JOPM} approximates the dynamics $\langle o_{1:t},a_{1:t} \rangle \ mapsto o_{t+1}$ and provides a derived stop function (objectives achieved, neutralization of malicious sessions, \emph{step-limit}). The reconstructed reward function is validated by cross-checking with native metrics (progress along the \acn{AD}-tree, service availability) . Training is based on \textquote{MARLlib}/\textquote{RLlib} (default \textquote{MAPPO} profile, but variants tested below), fixed seeds, and 5 independent runs as in \autoref{chap:experimental_framework}. The calculation conditions follow those presented in \autoref{par:compute_conditions}; only the exceptions (episode length, log frequency) are specified in the results.

\paragraph{2 to 4. Definition of baselines}

We define a \textbf{default advanced baseline} and \textbf{ablations} that vary (a) \acn{MAMAD} activities, (b) the \acn{MARL} algorithm, (c) the \emph{strictness} of organizational constraints: \emph{strong} (1.0), \emph{soft} (0.5), \emph{none} (0.0). All are based on \textquote{\acn{TRF-AUT}} (no \textquote{\acn{TRF-MAN}}, see justification above).

\begin{table*}[h!]
  \centering
  \caption {Synthetic baselines for Company Infrastructure.}
  \label{tab:baselines_company}
  \renewcommand{\arraystretch}{1}
  \tiny
  \begin{tabularx}{\textwidth}{
      >{\raggedright\arraybackslash\hsize=0.3\hsize}X
      >{\raggedright\arraybackslash\hsize=0.15\hsize}X
      >{\raggedright\arraybackslash\hsize=0.15\hsize}X
      > {\raggedright\arraybackslash\hsize=0.3\hsize}X
    }
    \toprule
    \textbf{MAMAD activity profile} & \textbf{MARL algorithms}            & \textbf{Organizational constraints (rigidity)} & \textbf{Comments}                                                                                                                   \\
    \midrule
    % --- Profile A (Default) ---
    \multirow{3}{*}{\parbox{3.8cm}{\textbf{Profile A -- Default}                                                                                                                                                                                                 \\
        \textquote{\acn{MOD-AUT}};\;\textquote{\acn{TRN-CON}};\;\textquote{\acn{ANL-AUT}};\;\textquote{\acn{TRF-AUT}}}}
                                    & \acn{MAPPO}, \acn{QMIX}, \acn{COMA} & Yes (1.0)                                      & Role-based masking (red/blue), \acn{AD}-tree aligned shaping (attack progress/detection/quarantine); \acn{JOPM} enabled.            \\
                                    & \acn{MAPPO}, \acn{QMIX}, \acn{COMA} & Soft (0.5)                                     & Same pipeline, mitigated penalties/bonuses; partial masks (expanded exploration).                                                   \\
                                    & \acn{MAPPO}, \acn{QMIX}             & None (0.0)                                     & \textit{Ablation} \acn{TRN-UNC}: native reward (state/metrics), no organizational guidance.                                         \\
    \hdashline
    % -- - Profile B (Manual Analysis) ---
    \multirow{3}{*}{\parbox{3.8cm}{\textbf{Profile B -- Manual Analysis}                                                                                                                                                                                         \\
        \textquote{\acn{MOD-AUT}};\;\textquote{\acn{TRN-CON}};\;\textquote{\acn{ANL-MAN}} (\acn{TEMM} parameterized);\; \textquote{\acn{TRF-AUT}}}}
                                    & \acn{MAPPO}, \acn{COMA}             & Yes (1.0)                                      & \acn{TEMM} set manually; rules/masks edited (sequencing recon$\rightarrow$exploitation$\rightarrow$\acn{LM}$\rightarrow$objective). \\
                                    & \acn{MAPPO}, \acn{COMA}             & Soft (0.5)                                     & Flexible guidance, post-hoc control by \acn{TEMM}.                                                                                  \\
                                    & \acn{MAPPO}                         & None (0.0)                                     & \acn{TRN-UNC}; \acn{TEMM} only for explainability/diagnosis (no rule reinjection).                                                  \\
    \hdashline
    % --- Profile C (Semi-manual cycle) ---
    \multirow{3}{*}{\parbox{3.8cm}{\textbf{Profile C -- Semi-manual cycle}                                                                                                                                                                                       \\
        \textquote{\acn{MOD-MAN}} (extended actions/props);\;\textquote{\acn{TRN-CON}} (manual hp); \;\textquote{\acn{ANL-MAN}};\;\textquote{\acn{TRF-AUT}}}}
                                    & \acn{IQL}, \acn{VDN}, \acn{QMIX}    & Yes (1.0)                                      & \textquote{Handcrafted} expansion of the action catalog (\acn{FW}, \acn{PAM}, persistence); fixed hyperparameters (not \acn{HPO}).  \\
                                    & \acn{IQL}, \acn{VDN}, \acn{QMIX}    & Soft (0.5)                                     & Manually defined soft constraints (log thresholds/\acn{IOC}, service priorities).                                                   \\
                                    & \acn{IQL}, \acn{VDN}                & None (0.0)                                     & Zero constraint hardness; useful for gauging the contribution of constraints and shaping.                                           \\
    \bottomrule
  \end{tabularx}
\end{table*}


\section{Experiments on the Kubernetes Microservices environment}


The \textbf{Kubernetes Microservices} environment is a \emph{real cluster} (1 \textit{worker} node: 8 vCPUs, 32 GB \acn{RAM}, 1 Gbps) used as the input environment for the MAMAD method. A schematic illustration of this cluster and its services is provided in \autoref{fig:k8s_microservices_real}. The cluster hosts an e-commerce web application composed of 4 chained microservices (API, Auth, Products, Orders) orchestrated via Kubernetes. Each service is deployed in a pod with a variable number of replicas (1 to 5). The cluster is {API}, Auth, Products, Orders) orchestrated via Kubernetes. Each service is deployed in a \emph{pod} with a variable number of replicas (1 to 5). The cluster is monitored in real time via Prometheus/Grafana, collecting metrics such as \acn{CPU}/memory usage, request rate, latency, queues, and pod status. Stress test scenarios are applied to simulate real-world conditions: bottlenecks (increased traffic), DDoS attacks (malicious traffic), pod failures (simulated via \textquote{kubectl delete pod}), and resource contention (\acn {CPU}/memory). The goal is to evaluate the agents' ability to maintain the cluster's operational resilience by dynamically adapting resources and responding to incidents. A summary illustration of this type of scenario is provided in \autoref{fig:k8s_cluster_graph_intro}\index{Kubernetes Microservices}.

% \begin{enumerate*}[label={\roman*)}, itemjoin={;\quad}]
\begin{itemize}
\item \textbf{State space:} current state of the actual cluster and the 4 chained services \((i \in \{1..4\})\):
\(
s = \{\text{replicas}^i,\,
U_{\text{cpu}}^i, U_{\text{mem}}^i,\,
T_{\text{in}}^i, T_{\text{out}}^i,\ ,
Q_{\text{pending}}^i,\,
S_{\text{status}}^{i,\text{pods}},\,
P_{\text{priority}}^i\}_{i=1..4}
\)
and global aggregates (average latency \(L_{\text{avg}}\), request rate \(R_{\text{rate}}\), availability)
\item \textbf{Observation space:} partial \emph{actual} observation (\acn{Dec-POMDP}) obtained via the Kubernetes/metrics collection \acn{API}. It is \emph{role-specific}:
\begin{enumerate*}[label={}, itemjoin={;\,}]
  \item bottlenecks: \(Q_{\text{pending}}^i, T_{\text{in}}^i/T_{\text{out}}^i\)
  \item \phantom{XXX} DDoS: \(R_{\text{rate}}, \Delta T, L_{\text{avg}}\)
  \item failures: \(S_{\text{status}}^{i,\text{pods}}, F_{\text{fail}}^i\)
  \ item resources: \(U_{\text{cpu}}^i, U_{\text{mem}}^i, P_{\text{priority}}^i\)
\end{enumerate*}
\item \textbf{Scope of action:} \emph{actual actions} applied to the cluster via the Kubernetes API: \begin{enumerate*}[label={\roman*)}, itemjoin={;\quad}] \item \emph{Targeted scaling}: \textquote{scale\_up}(Kubernetes API): \begin{enumerate*}[label={\roman*)}, itemjoin={;\quad}] \item \emph{Targeted scaling}: \textquote{scale\_up}(Kubernetes {API}~Kubernetes:
\begin{enumerate*}[label={\roman*)}, itemjoin={;\quad}]
  \item \emph{Targeted scaling}: \textquote{scale\_up}(i), \textquote{scale\_down} (i)
  \item \emph{DDoS management}: \textquote{rate_limit_ingress}(i), \textquote{isolate_service}(i)
  \item \emph{Failure recovery}: \textquote{restart_failed_pod}(i), \textquote{reschedule_pod}(i)
  \item \emph{Resource arbitration}: \textquote{throttle_low_prio}(i), \textquote{rebalance_quota}(i)
\end{enumerate*}
\item \textbf{Reward structure:} reward \emph{calculated based on actual telemetry} (QoS/resilience):
\[
  R_{\text{global}}=
  \text{SuccessRate} -w_2\cdot\text{DownTime} -w_3\cdot L_{\text{avg}} -w_4\cdot \text{Resilience} -w_5\cdot \text{Pending} -w_6\cdot \text{Pending_Time} -w_7\cdot \text{Pending_Time} -w_8\cdot \text{Pending_Time} -w_9\cdot \text{Pending_Time} -w_10\cdot \text{Pending_Time} -w
  {SuccessRate}
  -w_2\cdot\overline{Q_{\text{pending}}}
  -w_3\cdot L_{\text{avg}}
  -w_4\cdot \text{DownTime}
  -w_5\cdot \text{OverProvision},
\]
supplemented by sub-rewards per role:
\begin{gather*}
  R_{\text{bottleneck}}=-\sum_i Q_{\text{pending}}^i \\
  R_{\text{ddos}}=-(\text{Downtime}\cdot w_d+L_{\text{avg}}\cdot w_l) \\
  R_{\text{failure}}=-\sum_i T_{\text{downtime}}^i \\
  R_{\text{resource}}=-\sum_{i\in\text{Critical}}(U_{\text{cpu}}^i+U_{\text{mem}}^i)
\end{gather*}

\item \textbf{Objective:} maximize the \emph{operational resilience} of the actual cluster (high success rate, low latency/queues, maximum availability) under 5 scenarios: bottlenecks, DDoS, pod failures, resource contention, \emph{mixed}
% \end{enumerate*}
\end {itemize}

\noindent\textbf{Organizational specifications:}
% \begin{enumerate*}[label={\roman*)}, itemjoin={;\quad}]
\begin {itemize}
\item \textbf{Roles:} \textquote{Bottleneck Manager}, \textquote{DDoS Manager}, \textquote{Outage Manager}, \textquote{Resource Manager} Resources}
\item \textbf{Tasks:}
\(\langle\)minimize \(Q_{\text{pending}}^i\)\(\rangle\);
\(\langle\)detect/isolate DDoS and reduce downtime/latency\(\rangle\);
\(\langle\)\(\downarrow T_{\text{downtime}}\) through rapid recovery\(\rangle\);
\(\langle\)prioritize critical services under \(U_{\text{cpu/mem}}\) constraint\(\rangle\)
\item \textbf{Constraints:}
(a) \emph{deontic} by role (e.g., only \textquote{DDoS Manager} can \textquote{isolate\_ \allowbreak service});
(b) \emph{segregation of responsibilities} (avoid contradictory \textquote{scale\_up});
(c) \emph{QoS safeguards} (\(Q_{\text{pending}}^i<Q_{\text{threshold}}, U_{\text{cpu}}^{\text{total}}<U_{\text{threshold}}\));
(d) two integration modes during training: \textit{hard constraints} (action masking) vs. \textit{soft constraints} (reward shaping)
% \end{enumerate*}
\end{itemize}

\medskip
\noindent\textit{Note (specificity \textquote{real}):}
% \begin{enumerate*}[label={--}, itemjoin={\quad}]
\begin{itemize}
  \item \textsc{MOD/TRN} on digital twin derived from real cluster traces;
  \item \textsc{TRF}: deployment of policies learned on the cluster via the Kubernetes API; {API};
  \item operational security: bounded actions (quotas/limits), rollbacks, and rate limiting to preserve QoS.
        % \end{enumerate*}
\end{itemize}

\begin{figure}[h!]
  \centering
  \includegraphics[trim=1.8cm 3.3cm 1.25cm 3.5cm, clip, width=\linewidth]{figures/k8s_cluster_graph.pdf}
  \caption{Real cluster \textquote{Chained services} (4 services) and levers of action exposed to CybMASDE/MAMAD via the Kubernetes API.}
  \label{fig:k8s_microservices_real}
\end{figure}

\begin{figure}[h!]
  \centering
  \hspace{-0.4cm}
  \includegraphics[trim=0cm 0cm 0cm 0cm, clip, width=\textwidth]{figures/scenario_introduction.pdf}
  \caption [An abstract view of the Kubernetes cluster scenario]{An abstract view of the Kubernetes cluster scenario where each service runs in pods managed by \textit{Deployments} and can be dynamically replicated. Defender agents with specific roles (e.g., \textit{DDoS detector}, \textit{resource manager}, \textit{bottleneck manager}, \textit{failure manager}) continuously monitor key metrics (latency, pending requests, incoming/outgoing traffic, pod states, \acn{CPU}/memory usage) and apply via the \ acn{API} to adjust replicas, isolate affected services, or restart failed components. The illustration highlights four types of disruptions targeting the chain (\textit{bottleneck}, \textit{contention} (resource conflict), \textit{crash} (pod failure), and \textit{DDoS}/massive injections), as well as pod compromises. Agents must detect anomalies, isolate/segment compromised components, and reallocate resources to preserve overall operational resilience (availability, throughput, latency) and minimize the impact on end users.
  \label{fig:k8s_cluster_graph_intro}
\end{figure}


\subsection{Description of the experimental protocol instance}

\paragraph{1. Initial configuration}

We operate \emph{on the actual cluster} (no command line simulation playing the role of the \textquote{actual environment}). The cluster (\textbf{\acn{VM} 8~vCPU, 32 GB \acn{RAM}, 1 Gbps}) hosts the e-commerce application (4 microservices in a chain). Telemetry is collected by \textit{Prometheus} and visualized via \textit{Grafana}. \textquote{CybMASDE}/\textquote{KARMA} interfaces with the Kubernetes \acn{API} to \textit {observe} the status and \textit{act} (scaling, isolation, restart). A \textbf{digital twin} is built from the traces to train the policies offline, which are then \textbf{transferred} and \textit{closed in a loop} on the cluster (iterative learning by refreshing traces).

\begin{figure}[h!]
  \centering
  \resizebox{\textwidth}{!}{%
    \input{figures/karma_architecture/karma_architecture}
  }
  \caption[Overall diagram of \acn{KARMA}] {Overview of \acn{KARMA} with a Kubernetes cluster: Prometheus collection, modeling (digital twin), role/mission-guided training (MOISE+MARL), analysis, transfer to the cluster, and relearning loop.}
  \label {fig:karma_architecture}
\end{figure}

As illustrated in \autoref{fig:karma_architecture}:
\begin{enumerate}[label=\textbf{\arabic*)}, leftmargin=3.5mm, itemsep=2pt, topsep=2pt]
  \item \textbf{Collection}: \textit{Prometheus}~\cite{prometheus} aggregates time series (latency, files, \acn{CPU}/\acn{MEM}, pod status), used as \emph {states} by the modeling component.
  \item \textbf{Modeling}: construction of a \emph{digital twin} (transition model) and a multi-objective reward function (QoS/resilience).
  \item \textbf{Training}: \acn{MARL} learning with \textit{roles} (action constraints) and \textit{missions} (sub-objectives) according to MOISE+MARL~\cite{soule2024aomea}.
  \item \textbf{Analysis}: policy inspection/explainability (trajectory clustering, hierarchical visualizations).
  \item \textbf{Transfer}: policy deployment to the cluster (scaling/isolation/restart) and continuous policy \textbf{update loop} with new traces.
\end{enumerate}

\paragraph{2 to 4. Definition of baselines}

We evaluate \textbf{three families} of baselines: (A) \emph{default} profile (complete \acn{MAMAD} pipeline); (B) \emph{manual analysis} profile (\acn{TEMM}/edited rules); (C) \emph{mainly manual cycle} profile (this profile does not use an algorithm, \acn{MARL}, but the default auto-scaler \acn{HPA}). Each profile is available in three levels of organizational specification \emph{strictness}: \textbf{1.0} (strict), \textbf{0.5} (loose), \textbf{0.0} (none). We add (D) a line of Kubernetes/ML autoscaling references (\acn{HPA} and known \acn{ML} approaches).


\begin{table*}[h!]
  \centering
  \caption{Synthetic baselines Kubernetes Microservices.}
  \label{tab:baselines_k8s}
  \renewcommand{\arraystretch}{1.2}
  \tiny
  \begin{tabularx}{\textwidth}{
      >{\raggedright\arraybackslash\hsize=0.3\hsize}X
      >{\raggedright\arraybackslash\hsize=0.15\hsize}X
      >{\raggedright\arraybackslash\hsize=0.15\hsize}X
      >{\raggedright\arraybackslash\hsize=0.3\hsize}X
    }
    \toprule
    \textbf{MAMAD activity profile} & \textbf{Algorithms / Approaches}                                                                                                                                                                                                              & \textbf{Organizational constraints (hardness)} & \textbf{Comments}                                                              \\
    \midrule
    \multirow{3}{*}{\parbox{4.1cm}{\textbf{Profile A -- Default}                                                                                                                                                                                                                                                                                                                                                      \\\acn{MOD-AUT} ; \acn{TRN-CON} ; \acn{ANL-AUT} ; \acn{TRF-AUT}}}
                                    & \acn{MAPPO}, \acn{MADDPG}, \acn{QMIX}                                                                                                                                                                                                         & Yes (1.0)                                      & Action masking + role shaping; digital twin enabled; continuous transfer.      \\
                                    & \acn{MAPPO}, \acn{MADDPG}, \acn{QMIX}                                                                                                                                                                                                         & Soft (0.5)                                     & Mitigated penalties/bonuses; partial masks; same pipeline as default.          \\
                                    & \acn{MAPPO}, \acn{MADDPG}, \acn{QMIX}                                                                                                                                                                                                         & None (0.0)                                     & \textit{Ablation}: \acn{TRN -UNC}, remains unchanged.                          \\
    \midrule
    \multirow{3}{*}{\parbox{4.1cm}{\textbf{Profile B -- Manual analysis}                                                                                                                                                                                                                                                                                                                                              \\\acn{MOD-AUT} ; \acn{TRN-CON} ; \acn{ANL-MAN} ; \acn{TRF-AUT}}}
                                    & \acn{MAPPO}, \acn{COMA}                                                                                                                                                                                                                       & Yes (1.0)                                      & Manual configuration of \acn{TEMM} ; edited rules/masks.                       \\
                                    & \acn{MAPPO}, \acn{COMA}                                                                                                                                                                                                                       & Soft (0.5)                                     & Flexible guidance; post-hoc checks by manually adjusted \acn{TEMM}.            \\
                                    & \acn{MAPPO}, \acn{COMA}                                                                                                                                                                                                                       & None (0.0)                                     & \acn{TRN-UNC}; \acn{TEMM} for explainability/diagnosis, without reinjection.   \\
    \midrule
    \multirow{3}{*}{\parbox{4.1cm}{\textbf{Profile C -- Mainly manual cycle}                                                                                                                                                                                                                                                                                                                                          \\\acn{MOD-MAN}; \acn{TRN-CON} ; \acn{ANL-MAN}; \acn{TRF-MAN}}}
                                    & \acn{IQL}, \acn{VDN}, \acn{MADDPG}                                                                                                                                                                                                            & Yes (1.0)                                      & \textquote{Handcrafted} environment; fixed hypers; manual transfer/deployment. \\
                                    & \acn{IQL}, \acn{VDN}, \acn{MADDPG}                                                                                                                                                                                                            & Soft (0.5)                                     & Manually defined soft constraints (roles/tasks and softened scales).           \\
                                    & \acn{IQL}, \acn{VDN}                                                                                                                                                                                                                          & None (0.0)                                     & \acn {TRN-UNC} manual; zero constraint hardness.                               \\
    \midrule
    \parbox{4.1cm}{\textbf{Profile D -- K8s/ML autoscaling references}}
                                    & \parbox{3.4cm}{\acn{HPA} classic; \acn{AWARE}~\cite{aware2023}; Gym-\acn{HPA}~\cite{gymhpa2022}; Rlad-core~\cite{Rossi2019}; \acn{AHPA}~\cite{Zhou2024}; \acn{KOSMOS}~\cite{KOSMOS}; \acn{COPA}~\cite{COPA}; QoS-aware \acn{RL}~\cite{QoSRL}}
                                    & N/A
                                    & Non-\acn{MAS}/\acn{MARL} or generic \acn{RL} baselines for autoscaling: useful for comparing robustness under dynamic/adversarial load; K8s integration and consideration of variable attacks depending on the system.                                                                                                                                                          \\
    \bottomrule
  \end{tabularx}
\end{table*}


\paragraph {Evaluation indicators and scenarios}

\textbf{Scenarios}: (1) bottlenecks (\(Q_{\text{pending}}\uparrow)); (2) DDoS (\(R_{\text{rate}}\uparrow,\ \Delta T\uparrow)); (3) failures (\textit{CrashLoopBackOff}/\textquote{delete pod}); (4) contention (\(U_{\text{cpu/mem}}^{\text{tot}}>\) threshold) ; (5) mixed. \textbf{Indicators}: \emph{Operational resilience} (overall reward, success rate, \(L_{\text{avg}}\), \(\overline{Q_{\text{pending}}}\), availability); \emph{Robustness to attacks} (standard deviation of reward, DDoS recovery time, % services available) ; \emph{Digital twin accuracy} (simulation/actual deviation); \emph{Convergence} (episodes until plateau); \emph{Adaptability} (reward variance between loads); \emph{Explainability} (role/mission alignment, trajectory analysis).




\section{Experiments on the Drone Swarm environment}

The \textbf{Drone Swarm} environment is an ad hoc network of drone swarms that defense agents must protect against malicious intrusions in various cyberattack scenarios~\cite{Standen2021}. This environment is illustrated in \autoref{fig:cyborg}\index{Drone Swarm}.

% \begin{enumerate*}[label={\roman*)}, itemjoin={; \quad}]
\begin{itemize}
  \item \textbf{State space:} Dynamic network graph where nodes represent devices and edges indicate active connections
  \item \textbf{Observation space: } Agents receive security alerts and network status updates.
  \item \textbf{Action space:}
        \begin{enumerate*}[label={\roman*)}, itemjoin={; \quad}]
          \item \textquote{Monitoring}: Analysis of node activity
          \item \textquote{Block IP}: Restrict access from a suspicious source
          \item \textquote{Reimage drone}: Reinstall the operating system of a suspicious drone to eliminate it.
        \end{enumerate*}
  \item \textbf{Reward structure:}
        \begin{enumerate*} [label={\roman*)}, itemjoin={; \quad}]
          \item Overall health status: percentage of compromised drones $\times$ $-100$
          \item Attack prevention: $+30$
          \item False positive blocking: $-10$
          \item Reimaging a drone: $-50$.
        \end{enumerate*}
  \item \textbf{Objective:} Detect and mitigate cyber threats while avoiding false positives.
\end{itemize}
%
\textbf{Organizational specifications:}
% \begin{enumerate*}[label={\roman*)}, itemjoin={; \quad}]
\begin{itemize}
  \item \textbf{Roles:} \textquote{Threat analyst, firewall manager, security operator}
  \item \textbf{Tasks:} Detect threats, block unauthorized access, maintain network integrity
  \item \textbf{Constraints:} Minimize false positives while ensuring security coverage.
\end{itemize}

\begin{figure}[h!]
  \centering
  \includegraphics[trim=0cm 1cm 0cm 1cm, clip, width=0.6\linewidth]{figures/cyborg.png}
  \caption[Screenshot of the \acn{CybORG} environment]{Screenshot of the \acn {CybORG} environment: a swarm of 18 autonomous drones, initially controlled by blue (defensive) agents, forms an ad hoc network to facilitate communication between ground units. Each drone is susceptible to be infected by a hardware Trojan horse that can activate randomly, replacing the blue agent with a red (offensive) agent. Red agents aim to compromise the network by intercepting or blocking communications. The drones move according to a swarm algorithm, dynamically changing the network topology. Blue agents must detect and neutralize compromised drones while maintaining the integrity of communications.}
  \label{fig:cyborg}
\end{figure}

\subsection{Description of the experimental protocol instance}

\paragraph{1. Initial configuration}

We use \textbf{\acn{CybORG}}~\cite{Standen2021} as the reference simulator for the drone swarm (dynamic \textit{ad hoc} network, mobile nodes, possible compromises). In accordance with non-Cyberdefense-oriented simulated environments, an instance of \acn {CybORG} instance is launched in the background and exposed via a \textbf{REST adapter} compliant with the \acn{API} of \textquote{CybMASDE} I/O (\textquote{reset/step}, joint observations, action masks). This gateway allows: (i) the collection of traces for \textquote{\acn{MOD-AUT}} (histories $\langle o_{1:t}, a_{1:t} \rangle$); (ii) training \acn{MARL} \textbf{TRN} with or without \textit{organizational specifications}; (iii) \textbf{ANL} analysis (trajectory clustering, role/mission alignment verification); (iv) \textbf{TRF} transfer to the standardized simulated executor.

For \acn{MOD-AUT}, a \acn{Joint-Observation Prediction Model} (\acn{JOPM}, \acn{VAE}+\acn{LSTM}) can be trained to learn an approximate dynamic $\langle o_{1:t}, a_{1:t} \rangle \mapsto o_ {t+1}$ and a derived stop function (optional, as the simulator is already the \textquote{source of truth}). The reconstructed reward function is crossed with the native reward (attack prevention, service continuity, false positive minimization). An observation/action \emph{labels} mapping ($l_o, l_a$) synchronizes \acn{CybORG} and \acn{MMA} to enable action masking and role-based shaping. Training is based on \textquote{MARLlib}/\textquote{RLlib} (profiles \textquote{MAPPO}, \textquote{MADDPG}, \textquote {QMIX}, \textquote{COMA}, \textquote{IQL}, \textquote{VDN}) with fixed \textit{seeds}, in accordance with \autoref{chap:experimental_framework} and the resources presented in \autoref{par:compute_conditions}. The episodes and log frequencies are harmonized with the other simulated environments.

\paragraph{2 to 4. Definition of baselines}

We define a \textbf{default advanced baseline} and \textbf{ablations} that vary: (i) \acn{MAMAD} activities, (ii) the \acn{MARL} algorithm, (iii) integration and \textit {strictness} of organizational specifications (strict $=1.0$, lenient $=0.5$, none $=0.0$), and (iv) \textit{classical cyber references} (rule-based detection / supervised learning) to position the gains of \acn{MAMAD} in a cyber framework. The \autoref {tab:baselines_drone_swarm} summarizes these configurations.


\begin{table*}[h!]
  \centering
  \caption{Synthetic baselines for Drone Swarm.}
  \label{tab:baselines_drone_swarm}
  \renewcommand{\arraystretch}{1.2}
  \tiny
  \begin{tabularx}{\textwidth}{
      >{\raggedright\arraybackslash\hsize=0.3\hsize}X
      >{\raggedright\arraybackslash\hsize=0.15\hsize}X
      >{\raggedright\arraybackslash\hsize=0.15\hsize}X
      >{\raggedright\arraybackslash\hsize=0.3\hsize}X
    }
    \toprule
    \textbf{MAMAD activity profile} & \textbf{Algorithms \acn{MARL} / Methods}     & \textbf{Organizational constraints (hardness)} & \textbf{Comments}                                                                                                                                                       \\
    \midrule
    % --- Profile A (Default) ---
    \multirow{3}{*}{\parbox{3.8cm}{\textbf{Profile A -- Default}                                                                                                                                                                                                                                              \\\acn{MOD-AUT};\;\acn{TRN-CON};\;\acn{ANL-AUT};\;\acn{TRF-AUT}}}
                                    & \acn{MAPPO},\;\acn {MADDPG},\;\acn{QMIX}     & Yes (1.0)                                      & Action masking by roles (\textquote{Analyst}, \textquote{Firewall}, \textquote{Operator}); mission shaping: detection $\rightarrow$ containment $\rightarrow$ recovery. \\
                                    & \acn{MAPPO},\;\acn{MADDPG},\;\acn{QMIX}      & Mild (0.5)                                     & Mitigated constraints (bonus/penalty, partial masks); same pipeline as default.                                                                                         \\
                                    & \acn{MAPPO},\;\acn{MADDPG},\;\acn{QMIX}      & None (0.0)                                     & \textit{Ablation} \acn{TRN-UNC}: no organizational guidance, native reward (prevention, continuity, \acn{FP}/\acn{FN}).                                                 \\
    \hdashline
    % --- Profile B (Manual Analysis) ---
    \multirow{3}{*}{\parbox{3.8cm}{\textbf{Profile B -- Manual Analysis}                                                                                                                                                                                                                                      \\\acn{MOD-AUT};\;\acn{TRN-CON};\;\acn{ANL-MAN};\;\acn{TRF-AUT}}}
                                    & \acn{MAPPO},\;\acn{COMA}                     & Yes (1.0)                                      & Manual configuration (alert thresholds, escalation rules); manual editing of masks/rules.                                                                               \\
                                    & \acn{MAPPO},\;\acn{COMA}                     & Soft (0.5)                                     & Flexible guidance; post-hoc validation by \acn{TEMM}.                                                                                                                   \\
                                    & \acn{MAPPO},\;\acn{COMA}                     & None (0.0)                                     & \acn{TRN-UNC}; \textbf{ANL} for explainability/diagnosis only, without reinjection.                                                                                     \\
    \hdashline
    % --- Profile C (Mainly manual cycle) ---
    \multirow{3}{*}{\parbox{3.8cm}{\textbf{Profile C -- Mainly manual cycle}                                                                                                                                                                                                                                  \\\acn{MOD-MAN};\;\acn{TRN-CON};\ ;\acn{ANL-MAN};\;\acn{TRF-MAN}}}
                                    & \acn{IQL},\;\acn{VDN},\;\acn{MADDPG}         & Yes (1.0)                                      & Handcrafted environment (subset of observations/actions); fixed hyperparameters.                                                                                        \\
                                    & \acn{IQL},\;\acn{VDN},\;\acn{MADDPG}         & Soft (0.5)                                     & Soft constraints defined manually (roles/missions + softened scales).                                                                                                   \\
                                    & \acn{IQL},\;\acn{VDN}                        & None (0.0)                                     & Zero constraint hardness; serves as a fully manual \textquote{pure \acn{RL}} reference.                                                                                 \\
    \hdashline
    % --- Profile D (Classic cyber references) ---
    \multirow{2}{*}{\parbox{3.8cm}{\textbf{Profile D -- Classic cyber references}                                                                                                                                                                                                                             \\(Without \acn {MARL}, for positioning)}}
                                    & \acn{IDS} with rules (rule-based type),                                                                                                                                                                                                                                 \\\acn{ML} sup.~(\acn{SVM}/\acn{KNN}) & & n/a & Detection + scripted reaction (IP/port blocking, node isolation) ; reactive, not very adaptable to changing topologies. \\
                                    & Network heuristics (anomaly \textit{score}),                                                                                                                                                                                                                            \\threshold-based control & & n/a & Non-learning baselines (static thresholds, sliding windows); low robustness to adaptive attacks. \\
    \bottomrule
  \end{tabularx}
\end{table*}



\section{Summary}
This chapter laid the methodological and practical foundations for the experimental evaluation of the \acn{MAMAD} method. By defining a generic, reproducible, and structured protocol, it enabled the approach to be instantiated in a variety of environments, ranging from toy cases to real systems. The main advantage of this framework is that it ensures the traceability of choices, the transparency of protocols, and the validity of results, while facilitating critical analysis of the contributions and limitations of each component of the method.
%
This chapter thus provides an essential foundation for the scientific validation of the \acn{MAMAD} method, ensuring that the results presented in the rest of the manuscript are comparable, interpretable, and generalizable.

\clearpage
\thispagestyle{empty}
\null
\newpage



\chapter{Experimental results and analysis}

This chapter presents and discusses the experimental results obtained from the evaluation protocol detailed in \autoref{sec:evaluation_grid}.
The objective is twofold: on the one hand, to validate the feasibility and effectiveness of the proposed method in various environments, and on the other hand, to analyze the coverage of the evaluation criteria (autonomy, performance, adaptation, control, explainability, robustness) defined in \autoref{sec:evaluation-criteria}.

The experiments are organized along two main lines:
% \begin{enumerate*}[label={\roman*)}, itemjoin={; \quad}]
\begin{itemize}
  \item generic, non-cyberdefense-oriented environments (such as \textit{Overcooked-AI}, \textit{Predator-Prey}), allowing the robustness of the method to be tested in abstract and controlled contexts;
  \item specialized cyberdefense environments (\textit{Company Infrastructure}, \textit{Microservices Kubernetes}, \textit{Drone Swarm}), designed to evaluate the method under realistic conditions of threats and organizational constraints.
        % \end{enumerate*}
\end{itemize}

Each section details the results obtained, systematically comparing them with the baselines described in \autoref{chap:case_studies}, according to the metrics introduced in \autoref{sec:criteria_metrics}.
Finally, a cross-cutting discussion concludes the chapter by examining the method's coverage of the criteria, as well as the biases and limitations that may influence the interpretation of the results.

\section{Results and discussion of non-cyberdefense-oriented environments}\label{sec:results_and_discussion_cyberdefense}

\subsection*{Performance, convergence, and human interventions}

The three toy environments (Overcooked-AI, Predator-Prey, Warehouse Management) allow us to evaluate the \acn{MAMAD} method in various cooperative/competitive contexts.
\autoref{fig:noncyber_learning_curves} shows the learning curves (normalized rewards).
Overall, profiles \textbf{with organizational constraints} converge faster than \textquote{\acn{TRN-UNC}} ablations.
For example, in Overcooked-AI, \acn {MAPPO} with strong constraints converges on average after $1.8\times 10^4$ episodes compared to $2.6\times 10^4$ without constraints.
In Predator-Prey, \acn{QMIX} converges in $2.2\times 10^4$ episodes (strong) compared to $3.4\times 10^4$ (without).
Finally, in Warehouse Management, \acn{MAPPO} achieves convergence in $2.9\times 10^4$ episodes (strong) versus $4.1\times 10^4$ (without).

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.75\linewidth]{figures/results_noncyber_learning.pdf}
  \caption{Learning curves (normalized rewards, mean $\pm$ standard deviation over 5 runs).}
  \label{fig:noncyber_learning_curves}
\end{figure}

In general, each environment requires a little over a day to establish specifications that completely constrain the agents, allowing them to imitate a completely manual design of each \acplu {SMA}. On the other hand, it only takes one to two refinement cycles to obtain \acplu{SMA} achieving performance similar to the manually defined \acplu{SMA}, i.e., between three and four hours for each of them. This leads to an estimated \textbf {proportion of manual interventions} estimated between 15 and 25\%.


\subsection*{Comparison of cumulative rewards}

\autoref{tab:noncyber_rewards} summarizes the nominal results (cumulative rewards, convergence).
In all cases, the introduction of roles/missions via MOISE+MARL results in gains of $+15$ to $+25\%$ on the final cumulative reward and reduces the standard deviation between runs, demonstrating greater stability.

\begin{table}[h!]
  \centering
  \caption{Cumulative rewards and convergence (mean $\pm$ standard deviation, 5 runs).}
  \label{tab:noncyber_rewards}
  \renewcommand{\arraystretch}{1.2}
  \small
  \begin {tabular}{lccc}
  \hline
  \textbf{Environment} & \textbf{Hard constraints} & \textbf{Soft constraints} & \textbf{No constraints} \\
  \hline
  Overcooked-AI & $+1340 \pm 90$ & $\mathbf{+1380 \pm 85} $ & $+1110 \pm 130$ (26k) \\
  Predator-Prey & $+890 \pm 70$ & $\mathbf{+910 \pm 65}$ & $+730 \pm 100$ (34k) \\
  Warehouse Mgmt & $+1740 \pm 110$ & $\mathbf{+1780 \pm 100}$ & $+1410 \pm 140$ (41k) \\
  \hline
  \end{tabular}
\end{table}

\subsection*{Robustness and Adaptation}

By introducing perturbations (inactive agents in Predator-Prey, random delays and different seeds in Overcooked-AI, and different seeds in Warehouse Management), the \textbf{robustness scores} (perturbed/nominal performance) generally reach scores of $0.68$ –$0.73$ under strong constraints.
Soft constraints offer higher robustness scores ($0.72$–$0.84$) by allowing greater flexibility in the face of variations. Highly constrained agents are sometimes too rigid, which can hinder performance in scenarios that are very different from training.
However, it should be noted that organizational specifications designed to cover most possible observations/histories do not necessarily reduce the robustness score. This is particularly the case in Overcooked-AI, where agents with roles that completely constrain their policies show virtually no difference in robustness scores compared to agents trained to equivalent performance.

\subsection*{Control and compliance with rules}

The \textbf{constraint violation rate} is zero in hard constraint mode ($0.0\%$), moderate in soft mode ($3$–$5\%$), and exceeds $20\%$ without strong constraints.
For example, in Overcooked-AI, role collisions (two agents simultaneously taking on the same task) occur in $24.7\%$ $ of episodes without constraints, compared to only $2.8\%$ in soft mode.

  \subsection*{Organizational explainability}

  The \acn{Auto-TEMM} analyses show an average \textbf{organizational adequacy} ($\acn{OF}$) of $0.82$ (strong), $0.79$ (soft), and $0.65$ (without constraints).
  The \textbf{quality of inferred specifications} is higher in Warehouse Management ($93\%$ Jaccard similarity), reflecting the more deterministic structure of the tasks, than in Predator-Prey ($84\%$).
  In Overcooked-AI, \acn{Auto-TEMM} correctly infers the roles \textquote{Chef} and \textquote{Server} but sometimes confuses the \textquote{Assistant} and the \textquote {Chef}, explaining a slightly lower score ($87\%$).


  \subsubsection*{Elements of explainability: example of Overcooked-AI}

  We applied \acn{MMA} as well as the \acn{TEMM} method to generate about fifteen trajectories of agents trained with \textbf {MAPPO} in \textbf{Overcooked-AI}, according to the following organizational specifications for the two cook agents:
  %
  \begin{itemize}
    \item “Versatile” role: “if the agent has a bowl and sees a full pot in an adjacent square, they must interact with the pot to retrieve the soup” and “if the agent has soup and sees the serving counter in an adjacent square, they must interact with the counter to deliver the soup”
    \item “Hold soup bowl” objective: “holds a bowl of soup”
  \end{itemize}

  After applying TEMM, we obtained an organizational adequacy score of 0.87, indicating fairly consistent behaviors among trained agents, even outside of constrained behaviors. TEMM allows us to infer new rules and observations in vector form (with a Euclidean distance). After analysis, these rules can be transcribed into natural language and confirm that the agents have supplemented the initial rules with others that seem to lead them to achieve the set objective. For example:
  %
  \begin{itemize}
    \item RAG rules: “if the agent does not have a bowl and sees an empty bowl in an adjacent box, it must interact with the bowl to pick it up” and “if the agent does not have an onion and sees an onion in an adjacent box, it must interact with the onion to pick it up”
    \item GRG observations: “Cooking in progress”: “sees a pot cooking.”
  \end{itemize}

  To obtain a better representation of trajectories and centroids, our TEMM implementation also generates figures such as dendrograms or two-dimensional visualizations via PCA. In both visualizations, we note the similarity in behavior between the two agents, which is consistent with the fact that they share the same role and objective. Furthermore, although training could have led to divergent behaviors, we still observe a preservation of the expected behavior, probably due to the spatial symmetry of the environment used. This translates into two clusters (one for each agent) which, although distinct, are grouped together in the same macro-cluster (\autoref{fig:overcooked_dendrogram}) representing the " Versatile" enriched with post-training rules. This phenomenon can also be observed in the two-dimensional visualization (\autoref{fig:overcooked_pca}), where the trajectories of the two agents are in fact symmetrical to each other, which is consistent with the symmetrical nature of the spatial environment of Overcooked-AI.

  \begin{figure}[h!]
    \centering
    \includegraphics[width=0.8\linewidth]{figures/overcooked_figures/full_dendrogram.pdf}
    \caption{Dendrogram of transition trajectories in Overcooked-AI}
    \label {fig:overcooked_dendrogram}
  \end{figure}

  \begin{figure}[h!]
    \centering
    \includegraphics[width=0.8\linewidth]{figures/overcooked_figures/transition_pca.pdf}
    \caption{PCA of transition trajectories in Overcooked-AI}
    \ label{fig:overcooked_pca}
  \end{figure}

  \

  These results confirm that the \acn{MAMAD} method provides \textbf{added value in toy environments}, generally accelerating convergence, enhancing robustness, and improving explainability.
  \textbf {soft constraints} appear to be the best compromise between performance and organizational compliance, while hard constraints maximize robustness and role discipline at the cost of a slight decrease in cumulative reward.
  Simple environments also show that the absence of constraints leads to suboptimal behaviors (collisions, disorganization) that are less robust and more difficult to interpret.

  \section{Results and discussion of the Company Infrastructure environment}\label{sec:results_and_discussion_infra}

  \subsection*{Performance, convergence, and manual interventions}

  \autoref{fig:infra_learning_curves} illustrates the learning curves for the different baselines.
  The \textbf{advanced baseline} (Profile~A, strong constraints, \acn{MAPPO}) converges on average after $3.2 \times 10^4$ episodes, compared to $4.5 \times 10^4$ for ablation without constraints (\acn{TRN-UNC}).
  The \textquote{QMIX} and \textquote {COMA} show slower convergence ($\sim 5.0 \times 10^4$ episodes), but achieve comparable rewards.

  The average cumulative reward (average over 5 independent runs) reaches $+2450 \pm 120$ for \acn{MAPPO}, compared to $+1930 \pm 150$ for \acn{TRN-UNC}, indicating a \textbf{gain of $+27\%$} thanks to organizational constraints.

  \begin{figure} [h!]
    \centering
    \includegraphics[width=0.75\linewidth]{figures/results_infra_learning.pdf}
    \caption{Learning curves (average reward per episode) for the Company Infrastructure environment, average $\pm$ standard deviation over 5 runs.}
    \label{fig:infra_learning_curves}
  \end{figure}

  In the Company Infrastructure environment, the average number of manual interventions required is 1 to 3 refinement cycles (i.e., 3 to 6 hours in total) to obtain an SMA whose performance is equivalent to that of a solution designed and implemented entirely manually, which generally requires more than a day's work. This represents a significant reduction in human intervention, estimated at between 15 and 25%.

  \subsection*{Robustness and adaptation}

  Under disturbed scenarios (simultaneous attacks, intensive lateral movement, injected false positives), the \textbf{robustness score} (disturbed/nominal performance ratio) reaches $0.81$ for \acn{MAPPO} with strong constraints, compared to $0.63$ for \acn{TRN-UNC}.
  The standard deviation of rewards is reduced ($\sigma = 140$ versus $220$), showing better inter-run stability. This may seem contradictory if we take into account the fact that constraints can also limit exploration and adaptation and thus reduce the ability to adapt to new scenarios. Nevertheless, in this specific case, it should be noted that there is a potential bias in that the organizational constraints were designed to cover a wide range of possible situations, which allowed the agents to develop robust strategies while remaining within the framework of the defined roles and missions.
  On the other hand, soft constraints ($0.5$) offer an interesting compromise with a robustness score of $0.76$ and a slightly higher cumulative reward ($+2520$) thanks to greater freedom of exploration.

  \subsection*{Constraint compliance and organizational control}

  The \textbf{constraint violation rate} is zero (0.0%) in hard constraint mode, 4.3% in soft mode, and 21.7% without strong constraints (zero constraint hardness).
  These results confirm the effectiveness of action masking.
  However, there is an inverse correlation with cumulative reward: too many constraints can slow down initial learning, although the final plateau remains higher.

  \subsection*{Organizational explainability}

  The \acn{Auto-TEMM} analysis of trajectories shows an \textbf{organizational fit} ($\acn{OF}$) of $0.84$ for \acn{MAPPO} with strong constraints, compared to $0.67$ for \acn{TRN-UNC} .
  The quality of the inferred specifications (Jaccard similarity between expected roles/missions and extracts) reaches $92\%$ for constrained profiles, compared to $71\%$ without constraints.
  The dendrograms produced (not included here for brevity) reveal clear clusters aligned with the roles \textquote{Attacker\ _ExfilDB} and \textquote{Defender\_DB\_PAM}, while the absence of constraints results in more diffuse clusters.

  \subsection*{Comparative summary}

  \autoref{tab:infra_results} summarizes the main results according to the evaluation grid.

  \begin{table}[h!]
    \centering
    \caption{Summary of results (average over 5 runs, $\pm$ standard deviation) for Company Infrastructure.}
    \label{tab:infra_results}
    \renewcommand{\arraystretch}{1.2}
    \small
    \begin{tabular}{lccc}
      \hline
      \textbf{Metric}               & \textbf{Profile A (strong)} & \textbf{Profile A (soft)} & \textbf{\acn{TRN-UNC}} \\
      \hline
      Cumulative reward             & $2450 \pm 120$              & $2520 \pm 130$            & $1930 \pm 150$         \\
      Convergence rate (ep.)        & $32,000$                    & $29,500$                  & $45,000$               \\
      Robustness score              & $0.81$                      & $0.76$                    & $0.63$                 \\
      Standard deviation of rewards & $140$                       & $160$                     & $220$                  \\
      Constraint violations         & $0.0\%$                     & $4.3\%$                   & $21.7\%$               \\
      Organizational fit (\acn{OF}) & $0.84$                      & $0.79$                    & $0.67$                 \\
      Inferred specifications       & $92\%$                      & $88\%$                    & $71\%$                 \\
      \hline
    \end{tabular}
  \end{table}

  \

  The results confirm that integrating \textbf{organizational constraints} (MOISE+ \allowbreak MARL) significantly improves the robustness, stability, and explainability of learned policies.
  Nevertheless, hard constraints can slow convergence and slightly reduce the final cumulative reward compared to soft constraints, which offer an interesting compromise between performance and role compliance.
  The absence of constraints leads to less robust and more difficult to interpret policies, which would limit their relevance in a real cyberdefense setting.


  \section{Results and discussion of the Kubernetes Microservices environment}\label{sec:results_and_discussion_ms}

  \subsection* {Summary of QoS performance, convergence, and manual interventions}

  \autoref{fig:k8s_learning_curves} shows the learning curves (normalized overall QoS reward, moving average over $20$ episodes) for the main profiles.
  Profile A (strong constraints, MAPPO) converges in 2.6×10⁴ episodes (change-point plateau detection), compared to 3.9×10⁴ for the TRN-UNC ablation.
  The \textquote{MADDPG} and \textquote{QMIX} variants converge at $3.1\times 10^4$ and $3.5\times 10^4$ episodes, respectively.
  Over $5$ independent runs, the final reward reaches $+0.91 \pm 0.03$ (normalized) for \acn{MAPPO}, $+0.88 \pm 0.04$ (strong), and $+0.79 \pm 0.05$ without constraints.

  \begin{figure}[h!]
    \centering
    \includegraphics[width=0.75\linewidth]{figures/results_k8s_learning.pdf}
    \ caption[Learning curves (normalized QoS reward) for Kubernetes Microservices]{Learning curves (normalized QoS reward) for Kubernetes Microservices, average $\pm$ standard deviation over 5 runs.}
    \label{fig:k8s_learning_curves}
  \end{figure}

  Finally, for the Kubernetes Microservices environment, the average number of manual interventions required is approximately 4 to 5 refinement cycles (i.e., 6 to 7 hours in total) for the resulting SMA to achieve performance comparable to that of a solution designed and implemented entirely manually, which generally takes more than a day. This corresponds to a reduced intervention ratio, estimated at around 25%.

  \subsection*{QoS indicators under nominal conditions}

  The \autoref{tab:k8s_nominal} tab lists the main QoS indicators under nominal load (p95 application latency, average queues, availability over 2 hours).
  \textbf{Soft} constraints offer the best latency/availability compromise, while \textbf{hard} constraints guarantee stricter control with a slight latency penalty.

  \begin{table}[h!]
    \centering
    \caption{Nominal regime (mean $\pm$ standard deviation over 5 runs, 2-hour windows).}
    \label{tab:k8s_nominal}
    \renewcommand{\arraystretch}{1.2}
    \small
    \begin{tabular}{lcccc}
      \hline
      \textbf{Profile / Algo}       & \textbf{Latency p95 (ms)} & \textbf{$\overline{Q_{\text{pending}}}$} & \textbf{SuccessRate (\%)} & \textbf{Availability (\%)} \\
      \hline
      A (strong) \acn{MAPPO}        & $180 \pm 12$              & $6.1 \pm 0.8$                            & $99.1 \pm 0.3$            & $99.96 \pm 0.02$           \\
      A (soft) \acn{MAPPO}          & $\mathbf{168 \pm 10}$     & $\mathbf{5.3 \pm 0.7}$                   & $\mathbf{99.3 \pm 0.2}$   & $\mathbf{99.97 \pm 0.01}$  \\
      À (\acn{TRN-UNC}) \acn{MAPPO} & $216 \pm 17$              & $8.4 \pm 1.1$                            & $98.5 \pm 0.4$            & $99.92 \pm 0.03$           \\
      \hdashline
      B (\acn{ANL-MAN}) \acn{COMA}  & $191 \pm 14$              & $6.8 \pm 0.9$                            & $99.0 \pm 0.3$            & $99.95 \pm 0.02$           \\
      \hdashline
      C (manual) \acn{HPA}          & $310 \pm 24$              & $14.2 \pm 1.9$                           & $97.6 \pm 0.8$            & $99.20 \pm 0.10$           \\
      \hline
    \end{tabular}
  \end{table}

  \subsection* {Robustness to disruptions}

  We consider four scenarios: \textbf{bottleneck} (service saturation), \textbf{DDoS} (malicious traffic), \textbf{failures} (pod crashes/restarts), and \textbf{contention} (\acn{CPU}/\acn{MEM} constraints), plus a \textbf{mixed} scenario.
  The \textbf{robustness score} is calculated as the ratio of disrupted performance to nominal performance (QoS reward).
  \autoref{tab:k8s_robustness} shows that \textbf {strong} constraints maximize resilience to DDoS attacks and failures, while \textbf{soft} constraints maintain a slight advantage in bottleneck latency.

  \begin{table}[h!]
    \centering
    \caption{Robustness per scenario (average over 5 runs).}
    \label{tab:k8s_robustness}
    \renewcommand{\arraystretch}{1.2}
    \small
    \begin{tabular}{lccccc}
      \hline
      \textbf{Profile}              & \textbf{Bottleneck} & \textbf{DDoS}   & \textbf{Failures} & \textbf{Contention} & \textbf{Mixed}  \\
      \hline
      A (strong) \acn{MAPPO}        & $0.84$              & $\mathbf{0.86}$ & $\mathbf{0.88}$   & $0.83$              & $\mathbf{0.85}$ \\
      A (soft) \acn{MAPPO}          & $\mathbf{0.86}$     & $0.82$          & $0.84$            & $\mathbf {0.85}$    & $0.83$          \\
      A (\acn{TRN-UNC}) \acn{MAPPO} & $0.73$              & $0.69$          & $0.71$            & $0.72$              & $0.68$          \\
      \acn{HPA}                     & $0.64$              & $0.58$          & $0.61$            & $0.62$              & $0.57$          \\
      \hline
    \end{tabular}
  \end{table}

  However, it should be noted that the comparison with the baseline \acn{HPA} should be interpreted with caution, as this algorithm does not directly optimize the same objective function (combined latency and availability) as the \acn{MARL} approaches, which may bias the performance comparison.

  \subsection*{Recovery time and action discipline}

  Under DDoS, the \textbf{recovery time} (return to $L_{\text{avg}}<200$ ms) is $3.7 \pm 0.6$ min for \acn{MAPPO} compared to $5.2 \pm 0.8$ min (soft) and $7.9 \pm 1.1$ min (\acn{TRN-UNC}).
  The \textbf{rate of guardrail violations} (contradictory actions between roles, e.g., simultaneous \textquote{scale\_up} actions) is zero in hard constraint mode ($0.0\%$), $3.1\%$ in soft mode, and $18.4\%$ without hard constraints (zero constraint hardness).
  The \textbf{inter-run standard deviation} on reward is reduced with constraints ($\sigma=0.028$ hard, $0.031$ soft) vs $0.049$ (\acn{TRN-UNC}), highlighting increased stability. The baseline using the default auto-scaler \acn{HPA} consistently gives the worst score, suggesting that a rule-based algorithm is not as effective as \acn{MARL} approaches for accounting for changes in the Kubernetes cluster.

  \subsection* {Digital twin accuracy (simulation/actual deviation)}

  The digital twin trains policies offline before transfer.
  The \textbf{mean absolute error} (\acn{MAE}) on the predicted p95 latency is $+12.7$~ms (bottleneck), $+18.4$~ms (DDoS), $+15.1$ ms (outages), and $+21.3$ ms (mixed), representing a \textbf{relative error} of $6$–$9\% $.
  The divergence on $\overline{Q_{\text{pending}}}$ remains $<1.7$ requests on average.
  After fine-tuning on recent traces (one iteration), the MAE on p95 drops by $\sim 28\%$ (DDoS).

  \subsection*{Organizational explainability}

  \acn{Auto-TEMM} applied to trajectories (post-training) produces an \textbf{organizational adequacy score} $\acn{OF}=0.86$ (strong constraints) , $0.83$ (soft) and $0.71$ (\acn{TRN-UNC}).
  The \textbf{quality of inferred specifications} (Jaccard similarity on roles/missions and triggers) reaches $93\%$ (strong), $90\%$ (soft), $76\%$ (\acn{TRN-UNC}).
  The dendrograms reveal distinct clusters corresponding to the roles \textquote{DDoS Manager} and \textquote{Bottleneck Manager}, with stable trajectories in hard mode.

  \subsection*{Summary comparison}

  \begin{table}[h!]
    \centering
    \caption{Multi-metric summary (mean $\pm$ standard deviation over 5 runs).}
    \label{tab:k8s_summary}
    \renewcommand{\arraystretch}{1.2}
    \small
    \begin{tabular}{lcccc}
      \hline
      \textbf{Metric}                    & \textbf{A (strong)} & \textbf{A (soft)}        & \textbf{A (\acn{TRN-UNC})} & \textbf{\acn{HPA}} \\
      \hline
      QoS reward (norm.)                 & $0.88 \pm 0.04$     & $\mathbf{0.91 \pm 0.03}$ & $0.79 \pm 0.05$            & $0.66 \pm 0.06$    \\
      Convergence (episodes)             & $26,000$            & $\mathbf{24,000}$        & $39,000$                   & n/a                \\
      Nominal p95 latency                & $180 \pm 12$ ms     & $\mathbf{168 \pm 10}$ ms & $216 \pm 17$ ms            & $310 \pm 24$ ms    \\
      Robustness (mixed)                 & $\mathbf{0.85} $    & $0.83$                   & $0.68$                     & $0.57$             \\
      Constraint violations              & $\mathbf{0.0\%}$    & $3.1\%$                  & $18.4\%$                   & n/a                \\
      Organizational adequacy (\acn{OF}) & $\mathbf{0.86}$     & $0.83$                   & $0.71$                     & n/a                \\
      \hline
    \end{tabular}
  \end{table}

  \

  The results show that integrating \textbf{organizational specifications} simultaneously improves (i) \emph{robustness} under disturbances (particularly DDoS and failures), (ii) \emph{action discipline} (zero conflict in critical decisions), and (iii) \emph{explainability} (consistent roles/missions).
  Soft constraints maximize QoS performance (p95 latency, queues), while hard constraints maximize resilience and reduce inter-run variance.
  The ablation \textquote {\acn{TRN-UNC}} underperforms and exhibits increased variability, confirming the contribution of organizational guidance in an operational context.
  Finally, the accuracy of the digital twin (\acn{MAE} $6$--$9\%$) is sufficient for effective offline training and improves rapidly after one iteration of retraining on fresh traces.


  \section{Results and discussion of the Drone Swarm environment}\label{sec:results_and_discussion_drone_swarm}

  \subsection*{Summary of performance, convergence, and manual interventions}

  \autoref{fig:drone_learning_curves} illustrates the learning curves (normalized reward) on the drone swarm (18 nodes).
  Profile A (strong constraints, \acn{MAPPO}) converges in $3.1\times 10^4$ episodes, compared to $4.7\times 10^4$ for the ablation \textquote{\acn{TRN-UNC}}.
  The variants \acn {MADDPG} and \acn{QMIX} variants reach $3.6\times 10^4$ and $4.2\times 10^4$ episodes, respectively.
  In steady state, the average normalized rewards are $+0.87 \pm 0.04$ (\acn{MAPPO} strong), $+0.89 \pm 0.03$ (soft), and $+0.72 \pm 0.07$ without constraints.

  \begin{figure}[h!]
    \centering
    \includegraphics[width=0.75\linewidth]{figures/results_drone_learning.pdf}
    \caption[Learning curves (normalized reward) for Drone Swarm]{Learning curves (normalized reward) for Drone Swarm, mean $\pm$ standard deviation over 5 runs.}
    \label{fig:drone_learning_curves}
  \end{figure}

  For the Drone Swarm environment, the average number of manual interventions required is approximately 2 to 3 refinement cycles (i.e., 4 to 5 hours in total) for the resulting SMA to achieve performance comparable to that of a solution designed and implemented entirely manually, which typically takes more than a day. This corresponds to a reduced intervention ratio, estimated at around 20%.


  \subsection*{Indicators in nominal operation}

  The \autoref{tab:drone_nominal} shows the average results in the absence of massive compromises (5 runs, 10,000 steps).
  Soft constraints minimize false positives while maintaining a high detection rate and near-maximum network availability.
  The absence of guidance leads to an increase in false positives ($\sim 11\%$) and a decrease in detection ($< 90\%$).

  \begin{table}[h!]
    \centering
    \caption{Nominal results for Drone Swarm (mean $\pm$ standard deviation, 5 runs).}
    \label{tab:drone_nominal}
    \renewcommand{\arraystretch}{1.2}
    \scriptsize
    \begin{tabular}{lcccc}
      \hline
      \textbf{Profile / Algo}       & \textbf{Detection rate (\%)} & \textbf{False positives (\%)} & \textbf{Network availability (\%)} & \textbf{Normal reward}   \\
      \hline
      A (strong) \acn{MAPPO}        & $96.8 \pm 0.7$               & $3.1 \pm 0.5$                 & $99.2 \pm 0.3$                     & $0.87 \pm 0.04$          \\
      A (soft) \acn{MAPPO}          & $\mathbf{97.3 \pm 0.6}$      & $\mathbf{2.7 \pm 0.4}$        & $\mathbf{99.4 \pm 0.2}$            & $\mathbf{0.89 \pm 0.03}$ \\
      À (\acn{TRN-UNC}) \acn{MAPPO} & $88.5 \pm 1.2$               & $11.2 \pm 1.6$                & $97.9 \pm 0.6$                     & $0.72 \pm 0.07$          \\
      \hdashline
      B (\acn{ANL-MAN}) \acn{COMA}  & $95.2 \pm 0.9$               & $4.5 \pm 0.8$                 & $99.0 \pm 0.3$                     & $0.85 \pm 0.04$          \\
      \hdashline
      C (manual) \acn{VDN}          & $91.7 \pm 1.4$               & $7.9 \pm 1.1$                 & $98.4 \pm 0.5$                     & $0.77 \pm 0.06$          \\
      \acn{IDS} rules (ref.)        & $83.4 \pm 2.1$               & $15.6 \pm 2.7$                & $96.1 \pm 1.0$                     & $0.61 \pm 0.08$          \\
      \acn{ML} sup. (ref.)          & $87.9 \pm 1.8$               & $12.3 \pm 1.9$                & $97.0 \pm 0.8$                     & $0.68 \pm 0.07$          \\
      \hline
    \end{tabular}
  \end{table}

  However, these results must be viewed in context, as false positive and detection rates can vary greatly depending on the type and intensity of the simulated attack scenarios, which limits the direct generalization of the values obtained.

  \subsection*{Robustness to compromise}

  We evaluate three scenarios: (i) \textbf{single compromise} (1 active red drone), (ii) \textbf{cascade} (4 drones infected in 60s), (iii) \textbf{coordinated attack} (6 drones in a cluster).
  The robustness score (disturbed/nominal performance) is presented in \autoref{tab:drone_robustness}.
  Strong constraints ensure the best resilience during coordinated attacks, while soft constraints better preserve QoS in the event of isolated compromise.

  \begin{table}[h!]
    \centering
    \caption{Robustness according to compromise scenario (mean $\pm$ standard deviation, 5 runs).}
    \label{tab:drone_robustness}
    \renewcommand{\arraystretch}{1.4}
    \small
    \begin{tabular}{lccc}
      \hline
      \textbf{Profile}              & \textbf{Unique} & \textbf{Cascade} & \textbf{Coordinated} \\
      \hline
      A (strong) \acn{MAPPO}        & $0.91$          & $\mathbf{0.87}$  & $\mathbf{0.83}$      \\
      A (soft) \acn{MAPPO}          & $\mathbf{0.93}$ & $0.84$           & $0.79$               \\
      À (\acn{TRN-UNC}) \acn{MAPPO} & $0.79$          & $0.68$           & $0.61$               \\
      \acn{IDS} rules (ref.)        & $0.72$          & $0.55$           & $0.47$               \\
      \hline
    \end{tabular}
  \end{table}

  \subsection*{Reaction time and stability}

  The \textbf{average reaction time} (detection interval $\rightarrow$ neutralization) is $4.1 \pm 0.7$ s for \acn{MAPPO}, $4.8 \pm 0.6$ s (soft) and $7.3 \pm 1.2$ s without constraints.
  The \textbf{rate of organizational violations} (role rules not respected) is zero under strong constraints ($0.0\%$), $2.9\%$ in soft mode, and $>15\%$ without hard constraints.
  The \textbf{standard deviation of rewards} between runs is reduced ($\sigma=0.032$ strong, $0.037$ soft, $0.065$ without constraints).

  \subsection*{Organizational explainability}

  \acn{Auto-TEMM} infers distinct behavior clusters: \textquote{Analyst}, \textquote{Firewall}, \textquote{Operator}.
  The \textbf{consistency score} reaches $0.84$ (strong) and $0.82$ (soft).
  The \textbf{quality of the inferred specifications} is high (Jaccard similarity $92\%$ strong, $89\%$ soft, $74\%$ unconstrained).
  The dendrograms confirm that roles are consistently respected when organizational constraints are active.

  \subsection*{Summary comparison}

  \begin{table}[h!]
  \centering
  \caption{Multi-metric summary for Drone Swarm (mean $\pm$ standard deviation, 5 runs).}
  \label{tab:drone_summary}
  \renewcommand{\arraystretch}{1.4}
  \small
  \begin{tabular}{lcccc}
    \hline
    \textbf{Metric}           & \textbf{A (strong)} & \textbf{A (soft)}        & \textbf{A (\acn{TRN-UNC})} & \textbf{\acn{IDS}} \\
    \hline
    Normalized reward         & $0.87 \pm 0.04$     & $\mathbf{0.89 \pm 0.03}$ & $0.72 \pm 0.07$            & $0.61 \pm 0.08$    \\
    Convergence (episodes)    & $31,000$            & $\mathbf{29,000}$        & $47,000$                   & n/a                \\
    Detection (\%)            & $96.8$              & $\mathbf{97.3}$          & $88.5$                     & $83.4$             \\
    False positives (%) & $3.1$ & $\mathbf{2.7}$ & $11.2$ & $15.6$ \\
    Coord. robustness         & $\mathbf{0.83}$     & $0.79$                   & $0.61$                     & $0.47$             \\
    Organizational violations & $\mathbf{0.0\%}$    & $2.9\%$                  & $16.2\%$                   & n/a                \\
    Consistency (\acn{TEMM})  & $\mathbf{0.84}$     & $0.82$                   & n/a                        & n/a                \\
    \hline
  \end{tabular}
  \end {table}

  \

  The results indicate that the \acn{MAMAD} approach significantly improves \textbf{detection}, \textbf{robustness}, and \textbf{explainability} compared to conventional references (\acn{IDS} rules, \acn {ML} supervised).
  Soft constraints maximize detection and limit false positives, while hard constraints strengthen resilience during coordinated attacks and reduce response time.
  Ablation without constraints shows unstable behavior, high false positives, and low organizational consistency.
  Thus, the explicit integration of roles and missions proves essential to maintaining a resilient and interpretable swarm under dynamic threats.


  \section{Comparative discussion of results}

  \subsection{Coverage of criteria by the method}

  The \autoref{tab:criteria_summary} summarizes the coverage of the five evaluation criteria (C1--C5) by the \acn{MAMAD} method across all the environments studied.
  To obtain these aggregate values, each global criterion (C1--C5) is calculated
  as the \textbf{average of the metrics associated with it}, in accordance with the grid
  presented in \autoref{sec:criteria_metrics}. More specifically:
  \begin{itemize}
    \item \textbf{C1 Autonomy}: proportion of human intervention (design/operation);
    \item \textbf{C2 Performance}: average of the cumulative reward and convergence rate;
    \item \textbf{C3 Adaptation}: average of the standard deviation of rewards and robustness score;
    \item \textbf{C4 Control}: average of the constraint violation rate and organizational consistency score;
    \item \textbf{C5 Explainability}: average of organizational adequacy (\acn{OF}) and quality of inferred specifications.
  \end{itemize}
  All values are normalized to the interval [0,1] for ease of comparison,
  and averages are calculated over five independent runs.
  Non-cyberdefense-oriented environments serve as a controlled reference, while cyberdefense-oriented environments allow for validation of applicability under realistic conditions.

  \begin{table}[h!]
    \centering
    \caption{Multi-environment summary: coverage of criteria C1--C5 by MAMAD (average of associated metrics, normalized to [0,1], calculated over 5 independent runs).}
    \label{tab:criteria_summary}
    \renewcommand{\arraystretch}{1.4}
    \scriptsize
    \begin{tabular}{lccccc}
      \hline
      \textbf{Environment}   & \textbf{C1 Autonomy} & \textbf{C2 Perf.} & \textbf{C3 Adaptation} & \textbf{C4 Control} & \textbf{C5 Explainability} \\
      \hline
      Overcooked-AI          & $\sim0.20$           & $0.82$            & $0.80$                 & $0.75$              & $0.72$                     \\
      Predator-Prey          & $\sim0.20$           & $0.79$            & $0.77$                 & $0.73$              & $0.69$                     \\
      Warehouse Management   & $\sim0.20$           & $0.85$            & $0.82$                 & $0.77$              & $0.76$                     \\
      Company Infrastructure & $\sim0.25$           & $0.88$            & $0.83$                 & $0.85$              & $0.81$                     \\
      Microservices K8s      & $0.25$               & $0.91$            & $0.86$                 & $0.88$              & $0.83$                     \\
      Drone Swarm            & $\sim0.20$           & $0.89$            & $0.84$                 & $0.86$              & $0.82$                     \\
      \hdashline
      \textbf{Average}       & $0.21$               & $0.86$            & $0.82$                 & $0.81$              & $0.77$                     \\
      \hline
    \end{tabular}
  \end{table}

  \subsection{Critical analysis}

  The results highlight several key points:
  \begin{itemize}
  \item \textbf{Performance (C2)} and \textbf{adaptation (C3)} are systematically improved by the use of organizational constraints (soft or strong), particularly in complex environments (Kubernetes, Drone Swarm).
  .
  \item \textbf{Control (C4)} benefits most from strong constraints, which ensure strict compliance with roles and missions, sometimes at the cost of a slight decrease in performance.
  \item \textbf{Explainability (C5)} is generally satisfactory ($\sim 0.8$). More chaotic environments (Predator-Prey, Overcooked) lead to less stable organizational inferences.
  \item \textbf{Autonomy (C1)} achieves scores showing that in operational environments (Company Infrastructure, Microservices, Drone Swarm), the complete \textbf{MOD}–\textbf{TRN}–\textbf{ANL}–\textbf{TRF} loop reduces human intervention by around 20%.
  \end {itemize}

  \subsection{Potential biases and limitations}

  Several factors may influence the interpretation of the results:
  \begin{enumerate}[label={\alph*)}]
    \item \textbf{Choice of MARL algorithms}: the predominance of \acn{MAPPO} and \acn{QMIX} in the default profiles favors stable results, but limits generalization to other algorithms (e.g., multi-agent \acparen{DQN}).
    \item \textbf{Experimental conditions}: the use of an \acn{HPC} cluster reduces resource-related variance, but does not always reflect real-world constrained deployments (edge, IoT).
    \item \textbf{Constraint design}: the definition of roles and missions directly influences control and explainability (excessive strictness can bias comparisons).
    \item \textbf{Explainability measures}: Jaccard similarity and consistency scores are based on limited trajectories; more refined metrics (causal traceability, \acn{SHAP}) could improve validity.
  \end{enumerate}

  \medskip
  In summary, the \acn{MAMAD} method particularly covers performance, adaptation, and autonomy. The biases identified open up prospects for refining the evaluation (more algorithms, physical deployments, advanced metrics).

  \section{Conclusion}

  This chapter applied the \acn{MAMAD} method through an experimental evaluation on various environments, ranging from toy cases to real systems. The results highlight several strengths: accelerated convergence, improved robustness and stability of multi-agent policies, and increased explainability through the integration of organizational specifications. The modular and automated approach, supported by the \acn{CybMASDE} platform, reduces human intervention while ensuring the traceability and reproducibility of experiments. Finally, the balanced coverage of the criteria of autonomy, performance, adaptation, control, and explainability confirms the added value of the method for the design of robust and interpretable \acplu{SMA} in complex contexts.

  \clearpage
  \thispagestyle{empty}
  \null
  \newpage


  \chapter*{Conclusion}
  \addcontentsline{toc}{chapter}{\textbf{Conclusion}}

  This section has experimentally validated the \acn{MAMAD} method through simulated scenarios covering various \acplu{SMA} design contexts: enterprise infrastructure, drone swarms, and microservices orchestration. At each stage of the proposed pipeline, implementation via the \acn{CybMASDE} platform demonstrated the feasibility of the approach, while highlighting the specific contributions of $\mathcal{M}OISE^+$ coupling with multi-agent learning.

The results obtained show significant gains in terms of autonomy, resilience, and organizational compliance of agents. The comparative analysis between the organization's \textquote{guided} and \textquote{unguided} versions made it possible to evaluate the impact of each component of the method, both on the observed performance and on the ability to extract consistent emerging specifications. The metrics introduced (such as \acn{SOF} or \acn{FOF}) provided a novel interpretation of collective behaviors, linking learned trajectories to the structural objectives of the system.

Despite these encouraging results, several limitations were identified: dependence on simulated environments, partial coverage of application contexts, and the need for significant computational resources. These elements will be discussed in more detail in the last part of this manuscript, which offers a reflective review of the entire process undertaken.

\vspace{1em}

\noindent
In the following, we will summarize the contributions, discuss the limitations of the method, and open up perspectives on its future extension, both in research and in application.
