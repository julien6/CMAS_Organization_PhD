\documentclass[11pt,a4paper,sans]{moderncv}        

\moderncvstyle{banking}
\moderncvcolor{red}                                
\usepackage{dirtytalk}
\usepackage[utf8]{inputenc}
\usepackage[scale=0.9]{geometry}
\usepackage{graphicx}
\usepackage{ragged2e}
\usepackage{setspace}
\usepackage{parskip}
\setstretch{1.05}

\AfterPreamble{
  \hypersetup{
    colorlinks=false,        % désactive les liens colorés (sinon pas de cadre)
    pdfborder={0 0 1},       % active un cadre autour des liens
    pdfborderstyle={/S/U/W 1}, % facultatif : style souligné
    linkbordercolor={0 0 1}, % cadre bleu pour liens internes
    urlbordercolor={0 0 1},  % cadre bleu pour les URL
    citebordercolor={0 0 1}  % cadre bleu pour citations (si utilisées)
  }
}

\name{Julien}{Soulé}
\title{PhD Student -- Thales/UGA}
\address{35 Rue Mathieu de la Drôme, Valence}{26000}{France}

\phone[mobile]{06 77 63 12 13}
\email{julien.soule@hotmail.fr}
\homepage{julien6.github.io/home/}

%----------------------------------------------------------------------------------
%            content
%----------------------------------------------------------------------------------
\begin{document}

\recipient{\vspace{-0.9cm}Research Committee}{\textit{\textit{Télécom Paris}} -- \textit{Institut Mines-Télécom}\\Palaiseau, France}
\date{\today}
\opening{Dear Members of the Research Committee,}
\closing{{I look forward to the opportunity to further discuss my background and research plans in an interview.}\\[0.6cm]Sincerely,\vspace{-0.6cm}}

\makelettertitle

\justifying

\noindent
My research focuses on the intersection of \textbf{multi-agent systems (MAS)}, \textbf{reinforcement learning (RL)}, and \textbf{organizational modeling}, with the overarching objective of developing frameworks that make multi-agent learning \textbf{interpretable, controllable, and adaptable}.
Throughout my PhD, I have pursued an integrative perspective in which the design of MAS is treated as an optimization problem over a joint policy space, constrained by sub-symbolic structures that encode both environmental and organizational requirements.
This formal yet practical approach naturally connects with several of the ODI group's research themes—particularly \textbf{theory of reinforcement learning}, \textbf{learning in games}, and \textbf{representation learning in RL}.

\section*{Background and main contributions}

During my PhD at \emph{Université Grenoble Alpes} (CIFRE—Thales LAS “La Ruche” Rennes), I developed the \textbf{MAMAD} (MOISE+MARL Assisted Multi-Agent system Development) methodology, which assists the design of MAS by combining \textbf{organizational modeling (MOISE$+$)} with \textbf{multi-agent reinforcement learning (MARL)}.
The core idea is to formalize MAS design as a \emph{Dec-POMDP} problem where the exploration of the policy space is guided by organizational constraints.
This framework allows agents to learn under both hard constraints (roles, defining authorized actions) and soft constraints (objectives, defining intermediate goals and reward bonuses).
The approach was implemented in the \textbf{CybMASDE platform}, which supports the full lifecycle of MAS design—modeling, training, analysis, and transfer.

From a methodological standpoint, my work has contributed three main advances:

\begin{enumerate}
      \item \textbf{Organization-guided MARL.}
            I proposed a mechanism to encode symbolic and sub-symbolic organizational rules that guide agent policies during learning, improving convergence and addressing non-stationarity and credit-assignment issues in MARL. This integration establishes a bridge between symbolic design methods and continuous learning frameworks.

      \item \textbf{World-model-based multi-agent learning.}
            I explored the use of \textbf{world models} to capture the observational dynamics of agents and provide simulated environments for model-based MARL. These models act as digital twins that approximate real environments, enabling faster and more controlled learning loops. This research aligns with the ODI group's interests in representation learning and data-driven control.

      \item \textbf{Behavior analysis and explicability.}
            I developed an unsupervised analysis method that extracts implicit roles and implicit objectives from the trajectories of trained agents. This allows the designer to interpret emergent behaviors in organizational terms and refine specifications iteratively, contributing to \textbf{explainable MARL}.
\end{enumerate}

Together, these contributions form a coherent methodological continuum—from \textbf{formal modeling} to \textbf{learning} and \textbf{analysis}—offering a structured process for developing, training, and understanding multi-agent systems in complex environments.

For further reading, see:
\begin{itemize}
      \item \href{https://julien6.github.io/publications/JAAMAS2025_MAMAD.pdf}{Soule, J. \emph{Assisting Multi-Agent System Design with MOISE$+$ and MARL: The MAMAD Method}, JAAMAS (major revision, 2025)}
      \item \href{https://julien6.github.io/publications/AAMAS2025_MARL_Org.pdf}{Soule, J., et al., \emph{Organizationally-Guided Multi-Agent Reinforcement Learning}, AAMAS 2025}
      \item \href{https://julien6.github.io/publications/Workshop2024_WorldModel.pdf}{Soule, J., \emph{World-Model-Based MARL for Complex Simulation Environments}, MARL Workshop 2024}
\end{itemize}

\section*{Research vision and future directions}

Building on these foundations, my postdoctoral research aims to advance the \textbf{theoretical and representational aspects of MARL}, particularly in the following directions:

\begin{enumerate}
      \item \textbf{Theoretical analysis of organization-guided MARL.}
            I plan to formalize convergence properties and stability conditions of organization-guided MARL under Dec-POMDP settings, contributing to the broader \emph{theory of online and reinforcement learning}. This includes studying how structured constraints influence policy-space exploration and accelerate learning stability.

      \item \textbf{Representation learning for multi-agent world models.}
            I aim to improve multi-agent world models through \emph{neuro-symbolic representations}, enabling agents to build internal models that are both expressive and interpretable. This research connects to the ODI group's ongoing work on \emph{representation learning} and could open pathways toward \emph{symbolic reasoning-aware RL}.

      \item \textbf{Bridging simulation and physical environments.}
            I intend to extend my simulation-based research toward real-world robotic or industrial multi-agent systems, such as cooperative robotic setups for process optimization, providing empirical validation of the proposed frameworks.
\end{enumerate}

I also maintain an active interest in \emph{multi-agent learning in games} and \emph{transfer learning}, as natural extensions of my prior work on world-model synchronization and digital-twin environments.

\section*{Collaboration and mentoring}

My research has always been conducted in a collaborative, interdisciplinary environment.
The industrial dimension of my PhD at Thales LAS exposed me to applied research in cyber-defense and network security (e.g., ad-hoc drone networks, enterprise infrastructures, microservice architectures).
These collaborations strengthened my ability to translate theoretical advances into practical, high-impact applications.
I also collaborated with Thales Germany researchers on behavior-analysis methods for MARL agents, leading to an ongoing joint publication.

In parallel, I have supervised several Master's and engineering students at \emph{Grenoble INP—Esisar} on exploratory topics such as blockchain and anomaly detection with machine learning, which strengthened my mentoring experience and my motivation to contribute to team-based academic projects.

\section*{Long-term perspective}

My long-term objective is to pursue an academic career combining \textbf{theoretical reinforcement learning} and \textbf{methodological development for multi-agent systems}, while maintaining strong connections with industrial and interdisciplinary applications.
I am particularly motivated by the prospect of working in an international environment like ETH Zurich, where rigorous theoretical research meets methodological innovation and real-world impact.

\vspace{0.5em}
\noindent
I believe that my background in multi-agent reinforcement learning, formal modeling, and explainability provides a unique perspective that aligns well with the ODI group's objectives.
I am eager to contribute to its ongoing research on the theory and foundations of RL, while developing new directions that connect symbolic structure, learning dynamics, and interpretability in complex systems.


\makeletterclosing

\end{document}
